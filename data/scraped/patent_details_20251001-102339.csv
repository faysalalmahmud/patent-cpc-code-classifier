publication_number,title,abstract,cpc_codes
CN110543639B,An English sentence simplification algorithm based on pre-trained Transformer language model,"The invention discloses an English sentence simplification algorithm based on a pre-training transducer language model, which comprises the following steps: step 1, counting word frequency by using the disclosed wikipedia corpus; step 2, utilizing a disclosed pre-trained word embedding model to obtain vectorized expression of words; step 3, preprocessing sentences to be simplified to obtain content words; step 4, obtaining a candidate substituted word set of the content word in the sentence by using a published pre-training transducer language model Bert; step 5, sequencing the candidate alternative word sets of each content word by utilizing a plurality of characteristics; step 6, comparing the word frequency of the highest-ranking candidate word with that of the original content word to determine a final alternative word; and 7, processing other content words in the sentence according to the steps 4 to 6 in turn to obtain a final simplified sentence.","['G06F18/22', 'Y02D10/00']"
US11532378B2,Protein database search using learned representations,"A method for efficient search of protein sequence databases for proteins that have sequence, structural, and/or functional homology with respect to information derived from a search query. The method involves transforming the protein sequences into vector representations and searching in a vector space. Given a database of protein sequences and a learned embedding model, the embedding model is applied to each amino acid sequence to transform it into a sequence of vector representations. A query sequence is also transformed into a sequence of vector representations, preferably using the same learned embedding model. Once the query has been embedded in this manner, proteins are retrieved from the database based on distance between the query embedding and the protein embeddings contained within the database. Rapid and accurate search of the vector space is carried out using exact search using metric data structures, or approximate search using locality sensitive hashing.","['G06N3/084', 'G06F16/2255', 'G06F16/24534', 'G06N3/044', 'G06N3/0445', 'G06N3/08', 'G16B30/10', 'G16B40/20', 'G06N3/045', 'G06N3/047']"
US11961511B2,System and method for disambiguation and error resolution in call transcripts,"A system and method for detecting and resolving mis-transcriptions in a transcript generated by an automatic speech recognition system when transcribing spoken words. The system and method receive a machine language generated transcript of a speech signal by at least one of a first machine learning system and a second machine learning system, and analyze the machine language generated transcript to find a region of low confidence indicative of a mis-transcription and predict an improvement to the region of low confidence indicative of the mis-transcription. The system and method select a replacement word for the mis-transcription based on the predicted improvement to the region of low confidence and replace the mis-transcription by the replacement word to generate a corrected transcript.","['G10L15/32', 'G10L15/16', 'G06F40/279', 'G10L15/183', 'G10L15/26']"
US20210365643A1,Natural language outputs for path prescriber model simulation for nodes in a time-series network,"A method of generating natural language outputs may include accessing a model of a system, where the system may be represented by a hierarchy of nodes in a data structure, and nodes in the hierarchy of nodes may include time series of data. The method may also include identifying a time series represented by a node in the data structure that will generate a future anomaly; accessing a template corresponding to a type of the time series; populating semantic tags in the template using data from the time series; sending a phrase from the template to a natural language model; receiving a plurality of similar phrases from the natural language model; selecting one of the plurality of similar phrases and replacing the phrase in the template; and causing language from the template to be displayed on a display device.","['G06F7/24', 'G06F40/56', 'G06F16/3326', 'G06F16/3329', 'G06F16/3344', 'G06F40/186', 'G06F40/247', 'G06F40/30', 'G06F40/35', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N5/01', 'G06Q30/0201']"
US12327085B2,Sentence similarity scoring using neural network distillation,The disclosure herein describes a system and method for attentive sentence similarity scoring. A distilled sentence embedding (DSE) language model is trained by decoupling a transformer language model using knowledge distillation. The trained DSE language model calculates sentence embeddings for a plurality of candidate sentences for sentence similarity comparisons. An embedding component associated with the trained DSE language model generates a plurality of candidate sentence representations representing each candidate sentence in the plurality of candidate sentences which are stored for use in analyzing input sentences associated with queries or searches. A representation is created for the selected sentence. This selected sentence representation is used with the plurality of candidate sentence representations to create a similarity score for each candidate sentence-selected sentence pair. A retrieval component identifies a set of similar sentences from the plurality of candidate sentences responsive to the input query based on the set of similarity scores.,"['G06F40/30', 'G06F40/284', 'G06F16/34', 'G06N3/045', 'G06N3/08', 'G06N3/048']"
CN110532397B,"Question-answering method and device based on artificial intelligence, computer equipment and storage medium","An artificial intelligence-based question-answering method, device, computer equipment and storage medium perform language model training based on first training data, NER model training based on second training data and the trained language model, and relationship matching model training; identifying an entity in a sentence to be processed based on the trained NER model, and obtaining a corresponding relation of the sentence to be processed based on the trained relation matching model; and determining and outputting an answer corresponding to the sentence to be processed according to the corresponding relation of the sentence to be processed and the entity in the sentence to be processed. The method is based on the language model transfer learning and the atlas transfer learning technology, improves the common training method of the language model, can achieve higher accuracy through a smaller amount of manual marking data, and is more suitable for constructing a knowledge atlas question-answering system.","['G06F16/3329', 'G06F16/367']"
US12353409B2,Methods and systems for improved document processing and information retrieval,"Disclosed are methods, systems, devices, apparatus, media, and other implementations that include a method for document processing (particularly for training of a machine learning question answering platform, and for ingestion of documents). The method includes obtaining a question dataset (e.g., either from public or private repositories of questions) comprising one or more source questions for document processing by a machine learning question-and-answer system that provides answer data in response to question data submitted by a user, modifying a source question from the question dataset to generate one or more augmented questions with equivalent semantic meanings as that of the source question, and processing a document with the one or more augmented questions.","['G06F16/24522', 'G06F16/36', 'G06F16/248', 'G06F16/3329', 'G06F16/338', 'G06F16/9024', 'G06F16/90332', 'G06F16/9038', 'G06F16/93', 'G06F40/30', 'G06F40/35', 'G06N20/00', 'G06N5/022', 'G06N5/04', 'G06V30/414', 'G06N3/0464', 'G06N3/09', 'G06N5/046']"
US11854540B2,Utilizing machine learning models to generate automated empathetic conversations,"A device may receive text data, audio data, and video data associated with a user, and may process the received data, with a first model, to determine a stress level of the user. The device may process the received data, with second models, to determine depression levels of the user, and may combine the depression levels to identify an overall depression level. The device may process the received data, with a third model, to determine a continuous affect prediction, and may process the received data, with a fourth model, to determine an emotion of the user. The device may process the received data, with a fifth model, to determine a response to the user, and may utilize a sixth model to determine a context for the response. The device may utilize seventh models to generate contextual conversation data, and may perform actions based on the contextual conversational data.","['G10L15/1815', 'A61B5/0077', 'A61B5/1107', 'A61B5/1114', 'A61B5/1128', 'A61B5/163', 'A61B5/165', 'A61B5/4803', 'A61B5/7267', 'G06F18/24133', 'G06F40/35', 'G06N20/10', 'G06N3/04', 'G06N3/045', 'G06N3/0455', 'G06N3/08', 'G06T7/70', 'G06V20/41', 'G06V40/16', 'G06V40/168', 'G10L15/16', 'G10L15/183', 'G10L15/22', 'G10L25/57', 'G10L25/63', 'G10L25/90', 'G06F40/279', 'G06N20/20', 'G06N3/044', 'G06N5/01', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30201', 'G10L2015/223']"
US11551668B1,Generating representations of speech signals using self-supervised learning,"In one embodiment, a method includes generating audio segments from a speech signal, generating latent representations that respectively correspond to the audio segments, the latent representations comprising a first subset and a second subset, generating quantized representations that respectively correspond to the latent representations, masking the second subset of the latent representations, using a machine-learning model to process the first subset of the latent representations and the masked second subset of the latent representations to generate contextualized representations that respectively correspond to the latent representations, pre-training the machine-learning model based on comparisons between (1) a subset of the contextualized representations that respectively correspond to the masked second subset of the latent representations and (2) a subset of the quantized representations that respectively correspond to the masked second subset of the latent representations, and training the pre-trained machine-learning model to perform a speech analysis task.","['G10L15/063', 'G06F18/2155', 'G06F18/2413', 'G06K9/6259', 'G10L15/065', 'G10L15/16', 'G10L15/26']"
US11475210B2,Language model for abstractive summarization,"Methods, systems, and computer programs are presented for abstractive summarization of text by viewing sequence transduction as a language modeling problem. One method comprises an operation for training a machine-learning program to create a machine-learning model that estimates a word to be added to a running summary for the text being summarized. The method further comprises operations for detecting the text to be summarized, initializing the running summary, and performing a plurality of iterations. Each iteration comprises providing, to the machine-learning model, the source text and the running summary, and adding, using the machine-learning model, a new word to the running summary. Further, the method comprises an operation for storing, on a memory, the running summary as the summary of the text.","['G06F40/35', 'G06F40/166', 'G06F16/345', 'G06F40/284', 'G06N3/04', 'G06N3/045', 'G06N3/08', 'H04M2203/2061', 'H04M3/5183']"
US11593560B2,System and method for relation extraction with adaptive thresholding and localized context pooling,"System and method for relation extraction using adaptive thresholding and localized context pooling (ATLOP). The system includes a computing device, the computing device has a processer and a storage device storing computer executable code. The computer executable code is configured to provide a document; embed entities in the document into embedding vectors; and predict relations between a pair of entities in the document using their embedding vectors. The relation prediction is performed based on an improved language model. Each relation has an adaptive threshold, and the relation between the pair of entities is determined to exist when a logit of the relation between the pair of entities is greater than a logit function of the corresponding adaptive threshold.","['G06F40/295', 'G06F40/284', 'G06F16/316', 'G06F16/3344', 'G06F16/3347', 'G06F16/35', 'G06F16/93', 'G06N20/00', 'G06N3/045', 'G06N3/047', 'G06N3/048', 'G06N3/08', 'G06N5/022', 'G06Q10/10', 'G16H70/60']"
US12061876B2,Efficient transformer language models with disentangled attention and multi-step decoding,Systems and methods are provided for facilitating the building and use of natural language understanding models. The systems and methods identify a plurality of tokens and use them to generate one or more pre-trained natural language models using a transformer. The transformer disentangles the content embedding and positional embedding in the computation of its attention matrix. Systems and methods are also provided to facilitate self-training of the pre-trained natural language model by utilizing multi-step decoding to better reconstruct masked tokens and improve pre-training convergence.,"['G06N3/088', 'G06F40/40', 'G06F40/237', 'G06F40/30', 'G06N20/00', 'G06N3/045', 'G06N3/048']"
US12299385B2,Machine content generation,"Computerized systems and methods are disclosed to generate a document from one or more first and second text prompts, generating one or more context-sensitive text suggestions using a transformer with an encoder on the text prompts and a decoder that produces a text expansion to provide the context-sensitive text suggestions based on the one or more first and second text prompts by applying generative artificial intelligence with token biased weights for zero-shot, one-shot or some-shot generation of the artificial intelligence context-sensitive text suggestions from the one or more first and second text prompts.","['G06F21/1015', 'G06F21/32', 'G06F40/137', 'G06F40/169', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/088', 'G06N5/022', 'G06F21/105', 'G06F40/30']"
US20230386646A1,Combined vision and language learning models for automated medical reports generation,"A method of generating a medical report is presented herein. In some embodiments, the method includes receiving a medical image and at least one natural language medical question, extracting at least one image feature from the image; extracting at least one text feature from the question; and fusing the at least one image feature with the at least one text feature to form a combined feature. Some embodiments further include encoding, by an encoder, the combined feature to form a transformed combined feature; computing a set of prior context features based on a similarity between the transformed combined feature and each of a set of transformed text features derived from a set of training natural language answers; and generating, by a decoder, a first natural language answer conditioned on the transformed combined feature and the set of prior context features.","['G06V10/82', 'G16H30/40', 'G06F40/40', 'G06V10/774', 'G06V10/803', 'G06V10/806', 'G16H10/20', 'G16H15/00', 'G16H50/20', 'G06V2201/03']"
US11397892B2,Method of and system for training machine learning algorithm to generate text summary,"A method and a system for generating an abstractive summary of a document using an abstractive machine learning algorithm (MLA) and a method and a system for training the abstractive MLA. A document including a plurality of text sequences is received. An extractive summary of the document is generated, the extractive summary including a set of summary text sequences which is a subset of the plurality of text sequences. The abstractive MLA generates, based on the set of summary text sequences and at least a portion of the plurality of text sequences, an abstractive summary of the document including a set of abstractive text sequences, at least one abstractive text sequence not being included in the plurality of text sequences. In some aspects, the extractive summary is generated by an extractive MLA having been trained to generate extractive summaries.","['G06N3/08', 'G06F40/166', 'G06F40/20', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'G06F16/345', 'G06F30/27', 'G06F40/103', 'G06F40/216', 'G06N3/048', 'G06N5/025']"
US11620515B2,Multi-task knowledge distillation for language model,"Systems and methods are provided that employ knowledge distillation under a multi-task learning setting. In some embodiments, the systems and methods are implemented with a larger teacher model and a smaller student model, each of which comprise one or more shared layers and a plurality of task layers for performing multiple tasks. During training of the teacher model, its shared layers are initialized, and then the teacher model is multi-task refined. The teacher model predicts teacher logits. During training of the student model, its shared layers are initialized. Knowledge distillation is employed to transfer knowledge from the teacher model to the student model by the student model updating its shared layers and task layers, for example, according to the teacher logits of the teacher model. Other features are also provided.","['G06F40/30', 'G06F40/216', 'G06F40/284', 'G06F40/289', 'G06F40/40', 'G06N3/044', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'G06N3/047', 'G06N3/048']"
US20240160853A1,Systems and methods for a vision-language pretraining framework,"Embodiments described herein provide a multimodal vision-language model. The multimodal vision-language model contains a Generalist Multimodal Transformer capable of complete multiple tasks using the same set of parameters learning from pre-training. The Generalist Multimodal Transformer allows alignment between frozen, unimodal encoders, such as image encoders and large language models. The Generalist Multimodal Transformer eliminates the need for fine-tuning the image encoders and large language models.","['G06F40/40', 'G06V20/70', 'G06F40/10', 'G06F40/126', 'G06F40/284', 'G06F40/30', 'G06F40/35', 'G06F40/56', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06T9/00', 'G06V10/74', 'G06V10/764', 'G06V10/774', 'G06V10/82']"
US20210279577A1,Testing of Computing Processes Using Artificial Intelligence,"Technology is described for generating tests to execute against a process. The method can include identifying a data store of input operation requests for the process, and the input operation requests are recorded for requests received to operate functionality of the process. Another operation can be training a deep neural network model using the input operation requests to enable the deep neural network model to generate output series based in part on the input operation requests. Test series can be generated using the deep neural network model. The test series are executable to activate functionality of the process in order to test portions of the process. A further operation may be executing the test series on the process in order to test functionality of the process.","['G06N3/08', 'G06N3/006', 'G06N3/04', 'G06N3/044', 'G06N3/045']"
US20230252224A1,Systems and methods for machine content generation,"Computerized systems and methods are disclosed to generate a document with a transformer by prompt-engineering the transformer with a title and a summary to generate a description of the document; displaying a set of claims and allowing user editing of the set of claims; receiving one or more figures; receiving a part list with a plurality of element names for each figure; generating an expanded description of each element name through prompt engineering based on prior text in the document; selecting one or more boilerplate texts for major sections of the document; and organizing the document with the title, a background, the summary, a brief description of the drawings, and a detailed description.","['G06F40/151', 'G06F3/0482', 'G06F3/04845', 'G06F40/166', 'G06F40/40', 'G06F40/56', 'G06Q10/10', 'G06Q50/184']"
US20230306205A1,System and method for personalized conversational agents travelling through space and time,"A method and system is provided for creating and implementing personalized conversational agents representing historical figures travelling through space and time. These agents can process and analyse natural language utterances expressed by a user and generate knowledgeable and contextualized responses when prompted. The proposed agents can reply to existing conversations, initiate meaningful conversation topics to engage other users. The personalized conversational agents have the ability to navigate freely in space and time, while conditioning their conversational responses on their current space and time coordinates. Natural language processing (NLP) models are used to derive a conversation topic, its related space and time information based on the existing information available about the historical figure of interest. A dialogue model is then trained on popular datasets along with the retrieved knowledge and persona information about the historical characters to allow the agents to conduct meaningful and engaging conversations with multiple users.","['G06F16/3329', 'G06F40/35', 'G06F16/3334', 'G06F16/3344', 'G06F40/30']"
US20230244938A1,Using Chains of Thought to Prompt Machine-Learned Models Pre-Trained on Diversified Objectives,"An example method for pretraining a machine-learned model is provided. The example method includes obtaining a plurality of different combinations of configuration parameters of a pretraining objective framework. The example method includes generating, using the pretraining objective framework, a plurality of corrupted training examples from one or more training examples, wherein the plurality of corrupted training examples are respectively generated according to the plurality of different combinations. The example method includes inputting the plurality of corrupted training examples into the machine-learned model, wherein the machine-learned model is configured to generate uncorrupted subportions corresponding to corrupted subportions of the corrupted training examples. The example method includes obtaining, from the machine-learned model, a plurality of outputs respectively generated by the machine-learned model based on the plurality of corrupted training examples. The example method includes updating one or more parameters of the machine-learned model based on an evaluation of the plurality of outputs.","['G06N3/08', 'G06N3/044', 'G06N3/045', 'G06N5/022', 'G06N20/00']"
US11990124B2,Language model prediction of API call invocations and verbal responses,"A method includes obtaining an utterance from a user including a user query directed toward a digital assistant. The method includes generating, using a language model, a first prediction string based on the utterance and determining whether the first prediction string includes an application programming interface (API) call to invoke a program via an API. When the first prediction string includes the API call to invoke the program, the method includes calling, using the API call, the program via the API to retrieve a program result; receiving, via the API, the program result; updating a conversational context with the program result that includes the utterance; and generating, using the language model, a second prediction string based on the updated conversational context. When the first prediction string does not include the API call, the method includes providing an utterance response to the utterance based on the first prediction string.","['G10L15/197', 'G06F3/167', 'G10L13/02', 'G10L15/063', 'G10L15/22', 'G10L15/26']"
US11948665B2,Systems and methods for language modeling of protein engineering,"The present disclosure provides systems and methods for controllable protein generation. According to some embodiments, the systems and methods leverage neural network models and techniques that have been developed for other fields, in particular, natural language processing (NLP). In some embodiments, the systems and methods use or employ models implemented with transformer architectures developed for language modeling and apply the same to generative modeling for protein engineering.","['G16B35/10', 'G16B40/30', 'G06F30/20', 'G16B15/20', 'G16B20/50', 'G16B25/10', 'G16B30/00', 'G16B40/20', 'G16B5/20', 'G16B50/10', 'G06F2111/08']"
US20220237682A1,Scalable architecture for recommendation,"Systems and methods for item recommendation are described. Embodiments identify a sequence of items selected by a user, embed each item of the sequence of items to produce item embeddings having a reduced number of dimensions, predict a next item based on the item embeddings using a recommendation network, wherein the recommendation network includes a sequential encoder trained based at least in part on a sampled softmax classifier, and wherein predicting the next item represents a prediction that the user will interact with the next item, and provide a recommendation to the user, wherein the recommendation includes the next item.","['G06Q30/0631', 'G06F18/24', 'G06K9/6267', 'G06N3/04', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06V10/82']"
US12277870B2,Interface to natural language generator for generation of knowledge assessment items,"An item generation interface may generate knowledge assessment items directed a subject area based on a set of model items collectively directed to the subject area. The item generation interface may group the set of model assessment items into a plurality of similar item groups using numeric features corresponding to the model assessment items. Similar item groups may include model assessment items covering conceptually similar concepts within the subject area. A conditioning input may be generated for each of the item groups based on the numeric features corresponding to the model assessment items in the item group. Responsive to providing the conditioning inputs to a transformer-based natural language generation model, the item generation interface may receive raw assessment items from the transformer-based natural language generation model. Knowledge assessment items may be identified from the raw assessment items.","['G06F40/279', 'G06F40/30', 'G09B7/06', 'G06F40/56', 'G09B5/02']"
CN113178193A,Chinese self-defined awakening and Internet of things interaction method based on intelligent voice chip,"A Chinese self-defined awakening and Internet of things interaction method based on an intelligent voice chip comprises the following steps: step 1: the voice awakening specifically includes: step 1-1: extracting voice features; acquiring a voice signal from a microphone, and performing feature extraction on the voice signal to obtain a two-dimensional feature sequence; step 1-2: detecting a wake-up word; inputting the voice characteristics obtained in the step 1-1 into a CNN-CTC acoustic model nerve; step 2: recognizing offline voice; the method comprises the following steps: step 2-1: converting the input voice into pinyin by using a CNN-CTC acoustic model; step 2-2: inputting pinyin obtained by the CNN-CTC acoustic model into a Transformer language model, and outputting characters converted from the pinyin; and step 3: intent identification and slot filling; inputting the characters obtained in the step 2-2 into a BERT model, and outputting the intentions represented by the characters; performing weather inquiry and music playing according to the identified intentions; and 4, step 4: generating a dialog text; and (3) inputting the characters obtained in the step (2-2) into a GPT model, outputting a text of a conversation, and providing a chatting function.","['G10L15/22', 'G10L15/02', 'G10L15/063', 'G10L15/08', 'G10L15/16', 'G10L15/1822', 'G10L15/26', 'G10L2015/0631', 'G10L2015/081', 'G10L2015/223']"
CN117370736A,"Fine granularity emotion recognition method, electronic equipment and storage medium","The invention discloses a fine granularity emotion recognition method, electronic equipment and a storage medium, wherein the method comprises the steps of acquiring the data of an evaluation text, preprocessing the data, confirming the evaluation terms, adding emotion tendency labels for the evaluation terms, and preparing the data added by the labels into a data set; performing data enhancement on the data set by adopting an aspect word replacement algorithm to expand the data set; taking the BLOOM pre-training language model as a basic network, and training by adopting the expanded data set to obtain a trained fine granularity emotion analysis model; and carrying out fine granularity emotion analysis on the film evaluation text by using a trained fine granularity emotion analysis model. The number of the data sets is increased, so that the model obtains rich characteristics in learning, and the accuracy of the downstream emotion analysis task is improved. The BLOOM model structure is used, a pre-training model is introduced, and the model contains rich semantic features due to large-scale expected pre-training, so that the effect of emotion analysis tasks is improved.","['G06F18/10', 'G06F18/213', 'G06F18/214', 'G06N3/0455', 'G06N3/0499', 'G06N3/08']"
CN118886016A,Source code vulnerability detection method and system based on Transformer language model,"The invention relates to the technical field of Internet security, in particular to a source code vulnerability detection method and system based on a Transformer language model, which are used for acquiring code sample data, and labeling category labels for vulnerability types in the code sample data so as to construct a vulnerability detection sample data set; constructing a multi-classification CodeBERT model, training the multi-classification CodeBERT model by utilizing a vulnerability detection sample data set, and taking the trained multi-classification CodeBERT model as a vulnerability detection target model, wherein the multi-classification CodeBERT model is constructed based on a RoBERTa word segmentation machine and a Transformer language model; and inputting the source code to be detected into a vulnerability detection target model, and identifying and outputting the vulnerability category in the source code to be detected by utilizing the vulnerability detection target model. The invention can fully consider the deep semantic, structure and grammar characteristics of the source code, and has better application prospect in the field of software security by utilizing the better target source code vulnerability mining detection of the trimmed model.","['G06F21/577', 'G06F18/214', 'G06F18/24', 'G06F40/289', 'G06N3/0455', 'G06N3/08']"
US20240378654A1,Machine-learned large language model for sentiment analysis for curating replacements for an online system,"An online system determines whether to recommend a replacement item to a user based on a predicted sentiment score. The online system receives one or more comments from user feedback on the replacement items. The online system generates a prompt for each user comment for input to a machine-learned model. The online system generates a sentiment score for the ordered item and a replacement item based on the inferred sentiments by the model serving system. Using the sentiment score, the online system determines whether to recommend the replacement item.","['G06Q30/0631', 'G06Q30/0282', 'G06Q30/0629']"
US12099802B1,Integration of machine learning models with dialectical logic frameworks,"This disclosure generally relates to techniques for executing dialectical analyses using large language models and/or types of deep learning models. A dialectic logic engine can store and execute various programmatic processes or functions associated with applying dialectic analyses to input strings. The programmatic processes or functions executed by the dialectic logic engine can initiate communication exchanges with one or more generative language models to derive parameters for performing dialectic analyses and/or to derive outputs based on the parameters. In some embodiments, the dialectic logic engine also can execute functions for enforcing constraint conditions and/or eliminating bias from responses generated by the one or more generative language models to improve the accuracy, precision, and quality of the parameters and/or outputs derived from the parameters. Other embodiments are disclosed herein as well.","['G06F40/20', 'G06F40/30']"
US20240070403A1,"Grounded dialogue generation with cross-encoding re-ranker, grounding span prediction, and passage dropout","An information-seeking dialogue system can be trained using a pipeline process having stages, or components, of passage retrieval (selecting passages relevant to a query from a corpus or knowledge base), re-ranking, and generating a response to the query based on one or more of the re-ranked passages. Each stage, or component, of the pipeline can be individually optimized based on ground truth data.","['G06F40/51', 'G06F16/3329', 'G06F40/35']"
US20240354320A1,System and method for diverse thought object selection using llm and embedding models,"A thought object selection server receives a plurality of thought objects and the thought objects include text present in qualitative responses from plurality of user devices in a communication environments. The plurality of thought objects consists of M thought objects that are most recently seen thought objects and N thought objects that are least seen by the plurality of user devices. A prompt is provided to the LLM with the M thought objects, the N thought objects, and a request to identify diverse thought objects. LLM compares the M thought objects and the N thought objects to identify dissimilar one or more dissimilar thought objects from the N thought objects based on semantic distances. One thought object is selected from the dissimilar thought objects as a diverse thought object. In some cases, non-transformer based embedding tools may be used to identify diverse thought in the received thought objects.","['G06F16/3344', 'G06F16/3334', 'G06F16/335', 'G06F16/338', 'G06F40/103', 'G06F40/177', 'G06F40/284', 'G06F40/30']"
KR20230103782A,Transformer-based text summarization method and device using pre-trained language model,The present invention relates to a transformer-based text summarization method and device utilizing a pre-learning language model. The method includes: an embedding matrix extraction step of extracting an embedding matrix from a language model built by pre-learning context information of text; an abstract summary model building step of defining an abstract summary model to which the embedding matrix is applied and building the abstract summary model by performing summary learning to generate a text summary based on a learning dataset; and a text summary generating step of generating a summary for input text using the abstract summary model.,"['G06F16/345', 'G06F40/211', 'G06F40/284', 'G06N20/00']"
CN110263158A,"A kind of processing method of data, device and equipment","This specification embodiment discloses a kind of processing method of data, device and equipment, the described method includes: obtaining the first text data for being directed to specified services, based on the preset language model of first text data training, obtain opriginal language model, the opriginal language model is for determining the incidence relation being input between the word for including in the text data of the opriginal language model, obtain the second text data of the predetermined sensitive kind for the specified services, it include the sensitive information of the predetermined sensitive kind marked in second text data, the opriginal language model is adjusted using second sample data, obtain the target language model of the sensitive information of the predetermined sensitive kind for identification.","['G06F16/355', 'G06F21/6245', 'G06F40/295', 'G06N3/045', 'G06N3/08']"
US20250063140A1,Generating smart topics for video calls using a large language model and a context transformer engine,"The present disclosure relates to systems, non-transitory computer-readable media, and methods for utilizing a context transformer engine, a smart topic agent, and a large language model to generate a smart topic output. In particular, in one or more embodiments, the disclosed systems generate a smart topic output from a transcript of a video call. In some embodiments, the disclosed systems provide a smart topic interface that provides the smart topic output on a client device and receives selections of smart topic elements. In one or more embodiments, the disclosed systems generate a combined smart topic from transcripts of video calls in which client devices that participated are associated with a collaborating user account group.","['G06Q10/101', 'G06F3/0481', 'G06F3/0484', 'G06F40/30', 'G10L15/1815', 'G10L15/183', 'G10L15/22', 'G10L15/30', 'H04L65/1083', 'H04L65/4015', 'H04L65/403', 'H04N7/152', 'H04N7/155', 'G10L15/1822', 'G10L15/26']"
US20240303711A1,Conversational and interactive search using machine learning based language models,"A system, for example, an online system uses a machine learning based language model, for example, a large language model (LLM) to process high-level natural language queries received from users. The system receives a natural language query from a user of a client device. The system determines contextual information associated with the query. Based on this information, the system generates a prompt for the machine learning based language model. The system receives a response from the machine learning based language model. The system uses the response to generate a search query for a database. The system obtains results returned by the database in response to the search query and provides them to the user. The system allows users to specify high level natural language queries to obtain relevant search results, thereby improving the overall user experience.","['G06Q30/0627', 'G06Q30/0635', 'G06F16/9532']"
US20240321387A1,Immunogen selection,"A method is provided for identifying a number of variants of concern of a reference disease associated immunogen. The method uses a language model to perform inference on data representing each of a plurality of variants and data representing the reference immunogen. For each of the plurality of variants and the reference immunogen, a characteristic vector is derived from an output feature map of a hidden layer of the language model. For each of the plurality of variants, a measure of distance is generated for the variant that includes calculating a measure of distance between the characteristic vector of the variant and the characteristic vector of the reference immunogen. A semantic change score is calculated for each variant based on the generated measure of distance for that variant. A variant of the reference immunogen is selected based, at least in part, on the generated the semantic change scores.","['G16B15/30', 'C07K14/005', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N5/01', 'G16B20/20', 'G16B40/20', 'G16H50/80', 'C12N2770/20022']"
US12254170B2,Utilizing a transformer-based generative language model to generate digital design document variations,"The present disclosure relates to systems, non-transitory computer-readable media, and methods for utilizing a design language model and a generative language model to generate digital design documents with design variations. In particular embodiments, the disclosed systems implement the design language model to tokenize the design of a document into a sequence of language tokens. For example, the disclosed systems tokenize visual elements and a layout of the documentâ€”in addition to optional user-added content. The generative language model utilizes the sequence of language tokens to predict a next language token representing a suggested design variation. Based on the predicted language token, the disclosed systems generate a modified digital design document visually portraying the suggested design variation. Further, in one or more embodiments, the disclosed systems perform iterative refinements to the modified digital design document.","['G06F40/186', 'G06F3/04842', 'G06F3/0482', 'G06F3/04845', 'G06F40/106', 'G06F40/151', 'G06F40/284', 'G06N3/04', 'G06T7/60']"
US20220318669A1,Training a machine learning model using structured data,"A computing system may receive a corpus of training data including a plurality of data entity schemas. A first data entity of a first set of data entities corresponding to a first data entity schema is associated with a topic characteristic based on a first set of attributes defined by the first data entity schema, and a first attribute of the first set of attributes is associated with a structural characteristic that is common across each of the first set of data entities. The system may identify a respective attribute type identifier for each attribute of the first set, generate an attribute embedding for each attribute using the attribute value and the identifier, generate an entity embedding based on each attribute embedding and parameterize the topic characteristic for each data entity and the structural characteristic for each attribute.","['G06N20/00', 'G06F18/24133', 'G06F18/214', 'G06K9/6256', 'G06N3/0455', 'G06N3/088']"
US12153884B1,Advanced transformer architecture with epistemic embedding for enhanced natural language processing,"An advanced transformer architecture for an LLM with epistemic embedding for disclosed. The LLM includes a corpus address system for detailed addressing of input data, an input layer configured to create detailed addressing for words and sentences within the input corpus, and an embedding layer that combines epistemic embedding, word embedding, metadata embedding, and speaker tag embedding, and a corpus attention system using attention markers for managing focus. Epistemic embedding for the input corpus is generated using a vignette tableau and the epistemic embeddings are indicative of user sentiment and epistemic evidence values.","['G06N20/00', 'G06F40/284', 'G06F40/295', 'G06F40/30']"
US20240378656A1,Streamlined image to message and action replacement workflow with multi-modality machine-learned large language model,"A computer system receives an image from a picker, which indicates an out-of-stock target item and potential replacements items. The system provides, to a machine learning model, a prompt requesting identification of the target item and the potential replacement items in the image. The system receives identification of the target item and a list of potential replacement items in the image. The system generates a first list of potential replacements items based on the list of potential replacement items identified in the image and a second list of replacement items from the target item by applying one or more replacement models to the identified target item. The system may merge the two lists and assign replacement scores to each item in the merged list to create a list of recommended replacement items. The system generates a message based on the image and the list of recommended replacement items.","['G06Q30/0631', 'G06Q30/0633', 'G06Q30/0639', 'G06V10/774']"
US20240289861A1,Generating queries for users of an online system using large language machine-learned models and presenting the queries on a user interface,"Responsive to an input query from a user, an online system presents a list of recommended items that are related to the input query. The input query may be formulated as a natural language query. The online system performs an inference task in conjunction with the model serving system to generate one or more additional queries that are related to the input query and/or are otherwise related to the recommended items presented in response to the input query. The additional queries may be presented to the user in conjunction with the list of recommended items.","['G06Q30/0635', 'G06Q30/0627', 'G06Q30/0631', 'G06Q30/0643']"
US20250036604A1,Automated correction of attributes using machine-learned large language models (llms),"An online system maintains a product catalog including products from various retailers, from which users can purchase products. Each of the products are associated with attributes such as a size value and a size unit of measure (UOM). The online system identifies products with erroneous product attributes using taxonomy attribute value homogeneity. The online system performs an inference task in conjunction with the model serving system or the interface system to infer a correct size value and size UOM of the product. The online system evaluates the accuracy of the inferred size value and size UOM of the product. Responsive to determining that the inferred data is accurate, the online system updates the product catalog with the corrected product attribute information.","['G06F40/205', 'G06F16/215', 'G06F16/2365']"
US20250086213A1,Machine learned language model based natural language interface for querying databases,"A system, such as an online system, allows users to ask natural language questions requesting information stored in a database. The system receives a natural language question. The system determines database tables and database queries associated with the natural language question. The system generates a prompt for input to a machine learned language model. The prompt specifies the natural language question, information describing database tables, and the example database queries. The system sends the prompt to the machine learned language model for execution and receives a response generated by the machine learned language model. The response includes a database query corresponding to the natural language question. The system sends the database query for execution on a database system and provides the result of execution of the database query to the client device.","['G06F16/3344', 'G06F16/243', 'G06F16/24522', 'G06F16/31', 'G06F16/3329', 'G06F16/90332']"
US20250133037A1,Guided conversation context compression with adversarial hypothetical questions and evaluating relevance of contextual information for llms,A system may smartly edit the context of a conversation to be input into a chatbot LLM by using a conversation compression algorithm to prune and compress redundant elements. The system evaluates the conversation context compression algorithm using both a chatbot LLM and an adversarial LLM. The system retrieves a logged conversation and generates a compressed conversation context from the logged conversation. The system generates a synthetic user response by applying the adversarial LLM and generates a test conversation by replacing a user response in the conversation with the synthetic user response. The system generates a compressed context of the test conversation. The system generates a test chatbot LLM response by prompting the chatbot LLM with the compressed context of the test conversation. The system evaluates the conversation context compression algorithm by comparing the test chatbot response with a benchmark chatbot response.,['H04L51/02']
US20240289632A1,Aligning large language models with specific objectives using reinforcement learning and human preference,"An online system trains a specific-purpose LLM. The online system obtains training examples and divides training examples across batches. The online system generates a specific response by applying parameters of the specific-purpose LLM to a batch of training examples. The online system generates a general response by applying parameters of a general-purpose LLM to the batch of training examples. The online system computes a human readability score representing the difference between the specific response and the general response. The online system computes an objective compliance score by applying an evaluation model to the specific response, the evaluation model trained to score the first response based on a specific objective. The online system updates the parameters of the specific-purpose LLM based on the human readability score and the objective compliance score.","['H04L51/02', 'G06N3/092']"
US12248794B2,Self-supervised system for learning a user interface language,"A computer implemented method includes accessing training data that includes images, associated alternative text, and proximately located text providing instructions describing a user interface. The images are paired with text captions derived from the proximately located text and image's alternative text, training a vision and language model in a self-supervised manner using language masking, region masking, and image-text alignment techniques on respective image region features and tokenized text captions. Fine-tuning of the vision and language model is performed to obtain a specialized model representing user interface elements and associated functions.","['G06F9/453', 'G06F9/451', 'G06F40/284', 'G06F40/40', 'G06N3/045', 'G06N3/0895', 'G06V10/774', 'G06V10/811', 'G06F40/205']"
CN112487786A,Natural language model pre-training method based on disorder rearrangement and electronic equipment,"The invention relates to the technical field of language processing, in particular to a natural language model pre-training method based on disorder rearrangement and electronic equipment, which comprises the following steps: s1, providing a sequence recovery network and a pre-training text; s2, obtaining word-level training texts or sentence-level training texts; s3, converting each word of the word-level training text into an input word vector or an input sentence vector; s4, inputting the input word vector or the input sentence vector into the sequence recovery network to obtain the prediction sequence probability distribution; s5, optimizing the sequence recovery network according to the predicted sequence probability distribution and the real sequence comparison of the word-level training texts or the real sequence comparison of the sentence-level training texts; and S6, repeating the steps S2-S5 until the sequence recovery network reaches the set optimization condition. The natural language model pre-training method based on disorder rearrangement and the sequence recovery network can extract context information on multiple layers, so that downstream tasks are easier to perform.",[]
US20230042305A1,Recommendation method and system,"There is provided a method and system for training and using a transformer language model (TLM) part of a recommendation engine. Natural language discussions about a category of items are received, the discussions comprising tags each indicative of a respective item belonging to the category of item. Information is received for each respective item. Based on the natural language discussions, the tags and the information about the respective item, the TLM is trained to: upon receipt of a user input, determine whether a given item should be recommended based on the user input, if the given item should be recommended, retrieving given information about the given item and generating a response to the user input, the response to the user input comprising the given item to be recommended and the given information, and output the response to the user input. The response is generated in natural language format.","['G06F40/35', 'G06F40/30', 'G06F40/284', 'G06F40/40', 'G06N3/045', 'G06N3/08', 'G06N5/04', 'G06Q30/0631']"
US20240281668A1,Document classification using large language models,"A system and a method for classifying text from a document using an ensemble of close ended questions and a neural network based large language model, which might have been trained for different purposes. The method comprising feeding the language model with queries based on the text and the ensemble and post processing output of the language model using a knowledge representation rule based model or an additional machine language model. The method may be used as spam or phishing filter, detect sensitive contents due to various criteria, and/or the like.","['G06F40/35', 'G06F16/3344', 'G06F18/2415', 'G06F40/166', 'G06F40/247', 'G06F40/295', 'G06F40/30', 'G06F40/58', 'G06N5/022']"
US20240330718A1,Generating knowledge graph databases for online system using large language machine-learned models,"An online system generates a knowledge graph database representing relationships between entities in the online system. The online system generates the knowledge graph database by at least obtaining descriptions for an item. The online system generates one or more prompts to a machine-learned language model, where a prompt includes a request to extract a set of attributes for the item from the description of the item. The online system receives a response generated from executing the machine-learned language model on the prompts. The online system parses the response to extract the set of attributes for the item. For each extracted attribute, the online system generates connections between an item node representing the item and a set of attribute nodes for the extracted set of attributes in the database.","['G06F40/40', 'G06F16/9024', 'G06F40/205', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N5/022']"
US12282744B2,Statistical language models for simulating communication sessions,"A statistical language model may be used to simulate one or more users of a conversation. The statistical language model may be used to train a user to participate in a particular types of conversation by simulating communications by another type of user in the conversation. The communications may be simulated by selecting a simulation context from available simulation contexts and the simulation context may correspond to a difficulty level. Upon receiving a communication from a user, a responsive simulated communication may be generated by processing the received communication and the simulation context with the statistical language model. Upon completion of the simulation, another simulation context may be selected for the next simulation.","['G06F40/284', 'G06F40/35', 'G06F40/56', 'G06N3/0442', 'G06N3/0455', 'G06N3/08', 'G06N7/01', 'G06N3/084']"
CN118194917A,Method and system for improving structure of language model based on mixed expert model,"The invention provides a method and a system for improving a structure of a language model based on a mixed expert model, which belong to the technical field of artificial intelligence, and comprise the steps of obtaining the mixed expert model to be adjusted and configuring hardware resources; preparing a fine-tuning data set; designing a gate control network structure self-adapting according to tasks; introducing a z-loss penalty function, improving a mixed expert model structure by adopting an expert competition mechanism method, and internally arranging a dynamic adjustment expert capacity factor in the fine adjustment process of the mixed expert model, finally optimizing Tokens, distributing the dynamic adjustment expert capacity factor to expert accuracy, reducing the risk of overfitting, and effectively balancing the load of each expert when processing tasks, thereby improving the generalization capability and the resource utilization efficiency of the model in a new scene.","['G06N3/0442', 'G06N3/08']"
WO2021234610A1,Method of and system for training machine learning algorithm to generate text summary,"A method and a system for generating an abstractive summary of a document using an abstractive machine learning algorithm (MLA) and a method and a system for training the abstractive MLA. A document including a plurality of text sequences is received. An extractive summary of the document is generated, the extractive summary including a set of summary text sequences which is a subset of the plurality of text sequences. The abstractive MLA generates, based on the set of summary text sequences and at least a portion of the plurality of text sequences, an abstractive summary of the document including a set of abstractive text sequences, at least one abstractive text sequence not being included in the plurality of text sequences. In some aspects, the extractive summary is generated by an extractive MLA having been trained to generate extractive summaries.","['G06F40/56', 'G06N3/044', 'G06N3/045']"
CN110046248A,"Model training method, file classification method and device for text analyzing","This specification embodiment provides a kind of model training method, file classification method and device for text analyzing, method includes: first with the first bidirectional transducer model, for each word in the first training sentence, initial term vector based on the word, and the information above of the word, obtain the corresponding positive vector of the wordï¼›Followed by the first bidirectional transducer model, for each word in the first training sentence, the context information of initial term vector and the word based on the word obtains the corresponding opposite vector of the wordï¼›Then according to the position of each word in the first training sentence, the opposite vector of the latter word of the positive vector sum position of the previous word of the position is stitched together, as the corresponding target term vector in the positionï¼›It recycles first language model to be trained for the corresponding target term vector in each position to the first bidirectional transducer model and first language model, so that not only the speed of service was fast, but also can guarantee the robustness of model.",['G06F16/35']
US20210365611A1,Path prescriber model simulation for nodes in a time-series network,"A method of creating and executing action pathways for time series data may include accessing a model of a system, where the system is represented by a hierarchy of nodes in a data structure representing time series of data. The method may also include simplifying the model by removing relationships between the nodes that affect parent nodes less than a threshold amount, and simulating the model to identify a node comprising a time series of data that risks missing a predefined target value. The method may further include generating a pathway of actions for changes to driver nodes that cause the time series of data to move within a threshold distance of the predefined target value in the future, and causing the pathway of actions to be executed.","['G06F30/20', 'G06F7/24', 'G06F16/22', 'G06F16/26', 'G06F16/284', 'G06F16/285', 'G06N20/00', 'G06N3/045', 'G06N5/01', 'G06Q30/0201', 'G06F2111/08', 'G06N3/044']"
CN116934593A,Image super-resolution method and system based on semantic reasoning and cross-convolution,"The invention belongs to the technical field of image super-resolution, and provides an image super-resolution method and system based on semantic reasoning and cross convolution, wherein the method comprises the following steps: acquiring a low-resolution scene text image; preprocessing an image to obtain shallow features of the image and an original probability sequence; inputting the acquired shallow features and the original probability sequence into a cross convolution model based on semantic reasoning and multiple scales to obtain a super-resolution image; the model comprises a text semantic reasoning module, a reconstruction module and a text structure retaining module which are connected in sequence; the text semantic reasoning module is a transducer-based encoder-decoder structure for generating text prior features with high character readability. The encoder combines a self-attention mechanism and a language model based on a transducer, and realizes double-branch semantic reasoning. The improved encoder adaptively learns semantic features with high accuracy, exhibiting good generalization ability on scene images with learnable language rules.","['G06T3/4053', 'G06F40/232', 'G06F40/30', 'G06N3/0442', 'G06N3/0455', 'G06N3/0464', 'G06N3/0499', 'G06N3/08', 'G06V10/774', 'G06V10/80', 'Y02D10/00']"
US12093645B2,Inter-training of pre-trained transformer-based language models using partitioning and classification,An example system includes a processor to pre-train a transformer-based language model on a general domain. The processor can inter-train the pre-trained transformer-based language model using partitioning and classification to generate an inter-trained transformer-based pre-trained language model. The processor can then fine-tune the inter-trained transformer-based pre-trained language model on a target task to generate a fine-tuned transformer-based language model.,"['G06F40/30', 'G06F40/279', 'G06N3/088', 'G06N3/09', 'G10L25/30']"
US20250231957A1,Compression-Based Data Instance Search,"A knowledge management system may receive a set of data instances. The system may extract a plurality of entities from the set of data instances. The system may convert the plurality of entities into a plurality of entity embeddings, each entity embedding representing an entity in a latent space. The system may generate a reference embedding that has the same length as the plurality of entity embeddings. The system may compare, for each value in each entity embedding, the value to a corresponding value of the reference embedding. The system may generate a plurality of entity fingerprints, each entity fingerprint corresponding to an entity embedding, each entity fingerprint comprising Boolean values that are generated based on comparing values in each entity embedding to corresponding values of the reference embedding. The system may store the plurality of entity fingerprints to represent the plurality of entities.","['G06F16/254', 'G06F16/3347', 'G06F16/93', 'H04L9/008']"
CN111008517A,A compression method of neural language model based on tensor decomposition technology,"The invention discloses a neural language model compression method based on tensor decomposition technology, which starts from linear representation of an original attention function, then proves that the attention function can be linearly represented by a group of orthonormal basis vectors, and then compresses parameters by sharing the group of basis vectors under the condition of constructing a multi-head mechanism; meanwhile, the neural network model can have stronger discrimination capability through the modeling in a tensor slicing mode; the invention provides a new idea for developing a neural network model with low parameters and high accuracy.",['G06N3/084']
CN113901843A,BERT and word embedding dual-representation fused Hanyue neural machine translation method,"The invention relates to a BERT and word embedding dual-representation fused Hanyue neural machine translation method, and belongs to the technical field of natural language processing. The method comprises the steps of representing and learning a source language sequence by using a pre-training language model and word embedding, carrying out splicing operation after establishing a relation between two representations by an attention mechanism to obtain a dual representation vector, and carrying out linear transformation and a self-attention mechanism to enable the word embedding representation and the pre-training language model representation to be fully self-adaptively fused together to obtain a full representation of an input text so as to improve the performance of a neural machine translation model. The Chinese-Vietnamese neural machine translation method fusing the BERT and the word embedding dual representation, provided by the invention, solves the problem that the performance of the Chinese and Vietnamese neural machine translation is not ideal because Vietnamese is a low-resource language, and obviously improves the quality of a Chinese-Vietnamese neural machine translation model.","['G06F40/58', 'G06F18/25', 'G06F40/242', 'G06F40/49', 'Y02D10/00']"
US20230386610A1,Natural language processing to predict properties of proteins,"A protein language natural language processing (NLP) system is trained to predict binding affinity. Amino acids of proteins are tokenized and masked. A first neural network is trained on TCR sequences and epitope sequences in an unsupervised or self-supervised manner. The information obtained from the first phase of training is applied in a subsequent training operation via transfer learning, to a second neural network. An annotated compact dataset is used to fine-tune the second neural network in a second phase of training, and in a supervised manner, to predict biophysiochemical properties of proteins, including TCR-epitope binding.","['G16B40/20', 'G16B40/00', 'G16B15/30', 'G16B20/00', 'G16B30/00', 'G16B45/00']"
US20250200284A1,System and method for stratified sampling and dynamic token management in adaptive thought object theming,"A system and method for adaptive theming of thought objects comprising stratified sampling to categorize thought objects from communication environments and maintains proportional representation across categories. A dynamic token capacity threshold is determined based on factors including query complexity, quantity of thought objects, and computational resource availability. The system selects thought objects from each category until specific word limits are reached and removes objects when token thresholds are exceeded while preserving proportional representation. A transformer receives these sampled thought objects and a prompt providing context and instructions for theme assignment. An object-theming transformer determines probability scores for mapping thought objects to known themes, assigning themes when scores exceed predefined thresholds. Unthemed objects are processed by a topic identification transformer to generate new theme names. The system displays themed thought objects on a graphical user interface, enabling efficient categorization and analysis of qualitative responses across diverse domains.","['G06F40/117', 'G06F40/242', 'G06F40/284', 'G06F40/30']"
WO2024178710A1,Systems and methods for using neural codec language model for zero-shot cross-lingual text-to-speech synthesis,"Systems and methods are provided for accessing a machine learning model configured as a zero-shot cross-lingual text-to-speech model which has been previously trained on a text-to-speech training dataset comprising different bilingual speech transcription pairs, obtaining a first text prompt in a first language, a second text prompt in a second language, a speech sample comprising audio data from an unseen target speaker, providing the first text prompt in the first language, the second text prompt in the second language, and the speech sample from the target speaker as inputs to the machine learning model, and finally, generating a personalized speech output based on the inputs and by at least converting the second text prompt in the second language using a synthesized voice of the target speaker based on the speech sample from the target speaker.","['G10L13/02', 'G10L13/033', 'G10L13/086']"
CN113761944A,"Corpus processing method, apparatus, device and storage medium for translation model","The application relates to a corpus processing method, a corpus processing device and a corpus processing storage medium of a translation model. The method relates to the technical field of natural language processing, and comprises the following steps: acquiring an original training corpus for training a translation model; acquiring at least two groups of trained universal language models, acquiring quality scores corresponding to parallel sentences in an original training corpus through each group of universal language models, and filtering the original training corpus according to the quality scores to obtain high-quality training corpora, wherein the model structures of each group of universal language models are different; and obtaining the domain scores corresponding to all parallel sentences in the high-quality training corpus through the trained target domain language model and the general domain language model, and screening the high-quality training corpus of the target domain from the high-quality training corpus according to the domain scores. By adopting the method, the corpus of the target field can be screened on the basis of ensuring high quality, so that the translation performance of the translation model can be greatly improved by the obtained corpus.","['G06F40/44', 'G06F40/51', 'G06F40/58', 'G06N3/045', 'G06N3/08']"
CN116258132A,Lightweight language model based on low-rank decomposition and weight sharing,"The invention discloses a lightweight language model based on low-rank decomposition and weight sharing; the method comprises the following steps: step 1: applying an LMF-layer linear layer based on low-rank decomposition to a transducer network to obtain a first lightweight model LMF-transducer, and randomly initializing LMF-transducer weight; step 2: training an original transducer network on a target task data set until convergence to obtain a weight matrix to be decomposed; step 3: performing singular value decomposition on the weight matrix obtained in the step 2 to obtain a low-rank weight matrix of each linear layer of the transducer; step 4: taking the low-rank weight matrix obtained in the step 3 as an initial weight of the first lightweight model LMF-transducer; step 5: performing grouping-based cross-layer parameter sharing on the first light-weight model LMF-Transformer to obtain a second light-weight model LightFormer; step 6: training the second lightweight model LightFormer on a target task dataset until convergence; the invention can greatly reduce the parameters of the model, improve the reasoning speed of the model and has better effect than the latest lightweight transducer language model.","['G06F40/205', 'G06F40/58', 'G06N3/08']"
EP4579489A1,Query intent specificity,"The technology described herein relates to systems, methods, and computer storage media, among other things, for providing search query intent specificity. Embodiments may include identifying a search query performed using a search engine and generating a query vector for the search query by aggregating search result embeddings (e.g., item listing vectors) of search results from the search query. Further, in some embodiments, similarities (e.g., cosine similarities) between the query vector and the item listing vectors can be determined. As such, an intent specificity of the search query can be determined. Further, in some embodiments, the intent specificity can be used to train an intent specificity machine learning model for generating intent specificity scores for other search queries. Based on the intent specificity scores determined using the one or more trained intent specificity machine learning models, determinations can be made with respect to precision and recall, etc.","['G06F16/95', 'G06F16/9535', 'G06F16/2455', 'G06F16/2465', 'G06Q30/0601', 'G06Q30/0629']"
US12260223B2,Generative AI accelerator apparatus using in-memory compute chiplet devices for transformer workloads,"An AI accelerator apparatus using in-memory compute chiplet devices. The apparatus includes one or more chiplets, each of which includes a plurality of tiles. Each tile includes a plurality of slices, a central processing unit (CPU), and a hardware dispatch device. Each slice can include a digital in-memory compute (DIMC) device configured to perform high throughput computations. In particular, the DIMC device can be configured to accelerate the computations of attention functions for transformer-based models (a.k.a. transformers) applied to machine learning applications, including generative AI. A single input multiple data (SIMD) device configured to further process the DIMC output and compute softmax functions for the attention functions. The chiplet can also include die-to-die (D2D) interconnects, a peripheral component interconnect express (PCIe) bus, a dynamic random access memory (DRAM) interface, and a global CPU interface to facilitate communication between the chiplets, memory and a server or host system.","['G06F15/7821', 'G06F9/3001', 'G06F9/3836', 'G06F9/3887']"
WO2021083312A1,"Method for training statement paraphrasing model, and statement paraphrasing method and apparatus","Disclosed are a method for training a statement paraphrasing model, and a statement paraphrasing method and apparatus, relating to a natural language processing technique in the field of artificial intelligence. The method for training a statement paraphrasing model comprises: acquiring training data, wherein the training data comprises a plurality of statements, the languages of the plurality of statements are different, and the plurality of statements have the same meaning (710); and training a statement paraphrasing model according to the training data, wherein the statement paraphrasing model is used for generating a paraphrased statement of an input statement on the basis of the input statement (720). By means of the method for training a statement paraphrasing model, a paraphrased statement can be obtained easily and rapidly.","['G06F16/3329', 'G06F16/33', 'G06F16/332', 'G06F16/3344', 'G06F40/30', 'G06F40/58']"
AU2022283786B2,Neural image compositing with layout transformers,"#$%^&*AU2022283786B220250814.pdf##### ABSTRACT Systems and methods for image processing are described. Embodiments of the present disclosure receive an image depicting an object; generate a sequence of tokens including a set of tokens corresponding to the object and a set of mask tokens corresponding to an additional object to be inserted into the image; generate a placement token value for the set of mask tokens based on the sequence of tokens using a sequence encoder, wherein the placement token value represents position information of the additional object; and insert the additional object into the image based on the position information to obtain a composite image. ABSTRACT Systems and methods for image processing are described. Embodiments of the present 2022283786 09 Dec 2022 disclosure receive an image depicting an object; generate a sequence of tokens including a set of tokens corresponding to the object and a set of mask tokens corresponding to an additional object to be inserted into the image; generate a placement token value for the set of mask tokens based on the sequence of tokens using a sequence encoder, wherein the placement token value represents position information of the additional object; and insert the additional object into the image based on the position information to obtain a composite image. 1/12 FIG. 1 Identify a target object from the query Provide a composite image to the user Provide an image and a query Insert the target object into the image 105105 110110 115115 120120 Query: â€œinsert a chairâ€ â€œchairâ€ 20 22 28 37 86 0 9 D ec 2 02 2 1/12 2022283786 09 Dec 2022 Provide an image and a query 105 Query: ""insert a Identify a target object chair"" from the query 110 ""chair"" Insert the target object into the image 115 Provide a composite image to the user 120 FIG. 1","['G06T11/60', 'G06N3/02', 'G06N3/08', 'G06N3/084', 'G06T11/00', 'G06V10/26', 'G06V10/454', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06V20/35', 'G06V20/36', 'G06V20/70', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2210/12', 'G06T2210/61']"
US20250190454A1,Prompt-based data structure and document retrieval,"A knowledge management system may generate a plurality of prompts based on divisions of documents of unstructured text, each prompt relevant to a division of unstructured text. At least one prompt is generated such that a corresponding division of unstructured text is a response to said at least one prompt. The system may generate prompt embeddings for the plurality of prompts corresponding to the plurality of documents of unstructured text. The system may generate prompt-embedding clusters to group similar prompts from one or more documents of unstructured text. The system may receive a query. The system may convert the query to one or more query embeddings. The system may identify one or more prompts that are relevant to the query based on comparing the one or more query embeddings to the prompt embeddings. The system may identify one or more documents in one or more prompt-embedding clusters.","['G06F16/3332', 'G06F16/3326', 'G06F16/355', 'G06F16/358']"
US20250094817A1,Medical Language Model,"A method of domain knowledge learning by developing a language model from medical data. One application of the invention includes the steps of receiving a medical examination dataset, executing a data processing procedure, and providing an automatic short answer grading mechanism. The method also includes determining a final decision of the grade by aggregating the deciding factors in the final grade and reporting the results' uncertainty.","['G06F16/36', 'G06N3/094', 'G06F40/295', 'G06F40/30', 'G06N3/045', 'G06N3/09', 'G16H70/00', 'G06V30/416']"
US20250086435A1,Anomaly detection and user attribution using machine-learning large language models,"An online system detects an anomaly associated with an item selection made by a picker for fulfilling an order of a user of an online system. The system generates a prompt for execution by a machine-learned model trained as a large language model. The prompt comprises a chat log between the picker and the user. The system provides the prompt to the machine-learned model for execution. The system receives, as output from the machine-learned model and based on the chat log, a description indicating whether the anomaly is attributable to the user. The system determines, based on the output from the machine-learned model, that the item selection is not attributable to the user. Responsive to determining that the item selection is not attributable to the user, the system provides a notification to a client device of the user to confirm whether the item selection is approved by the user.","['G06N3/09', 'G06N3/0455', 'G06Q10/087', 'G06Q30/0633', 'G06Q30/0635']"
US20250086651A1,Leveraging data for platform support using large language machine-learned model-based agents,"An online system provides a support application including a chatbot application. One or more tools may each be configured to access external data. The interface system hosts an agent powered by an underlying large language model. The online system receives a user query via the chatbot application. For at least one or more iterations, the online system performs steps to provide a prompt to the LLM that specifies at least the user query, contextual information, a list of available tools, or a request to output an action. The system parses the response from the LLM to extract a selected action and action inputs for the selected action. The system triggers execution of a respective tool that corresponds to the selected action with the action inputs. The system generates a response to the user query and transmits the response to the client device.","['G06Q30/0635', 'G06F16/316', 'G06F16/3329', 'G06Q30/016', 'G06Q30/0627', 'G06Q30/0631']"
CN118643845B,Translation method combining language model and intelligent body,"The application relates to a translation method combining a language model and an agent, and relates to the field of machine translation. The method comprises the steps of constructing and utilizing a language model of a domain adaptation feedforward network by using a large-scale parallel corpus in a general domain and a parallel corpus in a vertical domain; at the time of translation: determining a vertical domain range to which a target text to be translated belongs, and selecting a plurality of language models adapting to corresponding vertical domains based on the vertical domain range as a translation game intelligent agent for providing a translation scheme; the plurality of translation game agents sequentially translate the original text according to the original text and the original text precursor under the control of the prompt word aiming at any original text in the target text; and for the translations provided by each translation game agent, the pre-trained evaluation agent evaluates the translations according to the original text and the combined translations of the original text and the original text, and obtains the translation with the highest score as the final translation result of the original text.","['G06F40/49', 'G06F40/58', 'G06N3/0455', 'G06N3/084']"
CN110826325B,Language model pre-training method and system based on countermeasure training and electronic equipment,"The invention relates to a language model pre-training method based on countermeasure training, which comprises the following steps: step S1, providing a semantic extraction network, an identification network and an original text; step S2, converting words in the original text into first initial word vectors, inputting the first initial word vectors into a semantic extraction network, and extracting the first vectors of the original text through the semantic extraction network; s3, modifying the original text, converting words in the modified text into second initial word vectors, inputting the second initial word vectors into a semantic extraction network, and extracting the second vectors of the modified text through the semantic extraction network; step S4, inputting the first vector and the second vector into an authentication network to obtain an authentication result of the authentication network; step S5, optimizing the semantic extraction network and the authentication network according to the authentication result of the authentication network; and S6, repeating the steps S2-S5 until the first vector and the second vector can pass through the authentication of the authentication network to obtain an optimized semantic extraction network and an optimized authentication network.","['G06N20/00', 'G06N20/10', 'G06N3/044']"
WO2025058745A1,Correlating security alerts using large language models,"The disclosure focuses on using a context-based insight system to determine security incident reports that include security incident insights and remediation actions based on various combinations of security alerts in cloud computing systems. The context-based insight system uses a security alert generative language model (GLM) to generate security incident reports based on correlated security alerts within a security incident and the attack-type contexts of those security alerts. By using the security alert GLM guided by attack-type contexts to generate security incident reports, the context-based insight system provides understandable text narratives that provide clear and accurate insights into security incidents including remediation actions to address the security incidents as a whole rather than just reporting individual security alerts of the security incident. Further, the context-based insight system dynamically updates the security incident report as additional related security alerts are detected and received.","['H04L63/14', 'H04L63/1416', 'G06F21/55', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'H04L63/20', 'H04W12/12', 'G06N20/00', 'G06N3/02', 'G06N3/04', 'H04L41/16']"
US20250077976A1,Reinforcement learning based optimization of textual artifacts using generative artificial intelligence,A system generates text artifacts using a machine learned language model. The text artifacts may be provided to a search engine for providing to users along with search results. The system iteratively improves the set of text artifacts by performing the following steps. The system updates the prompt used to generate the text artifacts based on the performance of the text artifacts to obtain a new prompt. The system executes the machine learned language model using the new prompt to generate a new set of text artifacts. The system evaluates the new set of text artifacts to determine performance of each of the new set of text artifacts. These steps are repeatedly performed to improve the set of text artifacts.,['G06N20/00']
US20250037323A1,Generating artificial intelligence (ai)-based images using large language machine-learned models,"An online system performs a task in conjunction with the model serving system or the interface system. The system generates a first prompt for input to a machine-learned language model, which specifies contextual information and a first request to generate a theme. The system provides the first prompt to a model serving system for execution by the machine-learned language model, receives a first response, and generates a second prompt. The second prompt specifies the theme and a second request to generate a third prompt for input to an image generation model that includes a third request to generate one or more images of one or more items associated with the theme. The system receives the third prompt by executing the model on the second prompt, provides the third prompt to the image generation model, and receives one or more images for presentation.","['G06T11/00', 'G06T5/70', 'G06V10/764']"
CN114596843A,A fusion method based on end-to-end speech recognition model and language model,"S1, training an end-to-end speech recognition model by using speech and text pairs, and training an external language model by using text data; s2, independently taking out the trained decoder part of the voice recognition model and forming an independent model; s3, training the independent model by using training data to text and obtaining an estimation model of the internal language model after convergence; s4, decoding the score fusion of the speech recognition model, the external language model and the estimation model of the internal language model to obtain a decoding result. The algorithm can improve the recognition accuracy after the speech recognition model and the language model are fused, and has wide application prospect in the field of speech recognition.","['G10L15/063', 'G10L15/183', 'G10L15/26', 'G10L19/16', 'G10L2015/0631']"
WO2025022002A1,Analysis of antigen-binding proteins,"A method for predicting one or more features of an antigen-binding protein is provided, comprising: providing the amino acid sequence of at least part of one or more chains of the antigen-binding protein or an encoded sequence derived therefrom as input to a deep learning model comprising a sequence encoding module configured to take as input an amino acid sequence and to produce as output an embedding corresponding to the input sequence, and predicting the one or more features of the antigen-binding protein using the embedding produced by the sequence encoding module The sequence encoding module has been trained as part of a machine learning model comprising a structure encoding module, and using training sequence data and structure data from a plurality of antigen-binding proteins. Related methods, systems and products are also described.","['G16B40/20', 'G16B15/30', 'G16B30/00']"
US20250131321A1,Efficient Training Mixture Calibration for Training Machine-Learned Models,"Systems and methods are provided for efficiently calibrating a data mixture for training machine-learned models (e.g., machine-learned sequence processing models, such as transformer-based models). For example, machine-learned models can be trained over a broad dataset that can include multiple different categories of data. The mixture of data categories within the dataset can influence model performance. To improve the performance of machine-learned models, example implementations of the present disclosure can learn a distribution of data categories using a lightweight proxy model before initiating training of a large primary model. In this manner, for instance, example implementations can obtain an improved training data distribution with less computational expense and can leverage the learned training data distribution to better train a large primary model.","['G06N20/00', 'G06N5/01']"
EP4503043A1,A computer-implemented method for training a model for use in estimation of at least one property of a chemical compound and/or for generating or optimizing chemical compounds,"The invention relates to a computer-implemented method for training a model (13) for use in estimation of at least one property (16) of a chemical compound and/or for generating or optimizing chemical compounds, the model comprising a language model (13-1), the method comprising the steps of:
a) Training the language model (13-1) using a large number of chemical compounds in a string representation (10) as training data,
b) Performing domain adaption of the trained language model (13-1) of step a) for a target domain using target domain-specific training data of the chemical compounds,
c) Training the domain-adapted trained model (13) of step b) by supervised training for a specific molecular or other property prediction task or a specific chemical compound generation or optimization task.","['G16C20/70', 'G06N3/045', 'G16C20/30', 'G16C20/50', 'G06N3/08']"
US20240087683A1,Classification using a machine learning model trained with triplet loss,"A machine learning model trained with a triplet loss function classifies input strings into one of multiple hierarchical categories. The machine learning model is pre-trained using masking language modeling on a corpus of unlabeled strings. The machine learning module includes an attention-based bi-directional transformer layer. Following initial training, the machine learning model is refined by additional training with a loss function that includes cross-entropy loss and triplet loss. This provides a deep learning solution to classify input strings into one or more hierarchical categories. Embeddings generated from inputs to the machine learning model capture language similarities that can be visualized in a cartesian plane where strings with similar meanings are grouped together.","['G06N3/0455', 'G16B40/00', 'C12Q1/689', 'G06F40/20', 'G06F40/30', 'G06N3/084', 'G16B40/20', 'G16B50/10']"
CN117217318B,Text generation method and device based on Transformer network model,"The invention provides a text generation method and device based on a transform network model. The method comprises the steps of taking absolute values of input features, calculating average values of each column, judging abnormality if the absolute values are larger than or equal to a preset abnormality threshold value, and recording positions; expanding an input feature matrix and a weight matrix; quantization to obtain a set of row and column scaling coefficients; and deploying a transducer network model by using the two scaling factor groups, and inputting a prompt sentence to make reasoning and obtain a new text. The technical scheme provided by the invention can overcome the problem of difficult quantification caused by abnormal values, can be deployed on a graphic processor, can also be designed to be deployed by special hardware, and can reduce the time of kernel calling and improve the overall reasoning efficiency.",[]
EP4471666A1,Sensory transformer method of generating candidate food ingredients and food formulas with the candidate ingredients,"Techniques to suggest one or more sets of ingredients that can be used to recreate or mimic a target sensory description using artificial intelligence are disclosed. An artificial intelligence model includes a transformer inspired neural network architecture that learns from ingredients, recipes, and sensory profiles. The artificial intelligence model includes a sensory transformer model that generates a probability distribution of source ingredients based on an embedding associated with first digital data representing ingredients and the second digital data representing sensory description, and a selector that selects at least one candidate ingredient from the probability distribution of source ingredients for the embedding. A complete set of ingredients generated based on the at least one candidate ingredient when combined become a food product that has or achieves the sensory description.","['G01N33/02', 'G06N3/045', 'G06N3/0455', 'G06N3/088']"
CN114743544A,Pinyin-based two-stage decoupling Chinese speech recognition model,"The invention discloses a pinyin-based two-stage decoupling Chinese speech recognition model, which decomposes a speech recognition process into two steps of speech to pinyin and pinyin to Chinese characters, and independently constructs and trains an acoustic model from speech to pinyin and a language model from pinyin to Chinese characters: constructing a full convolution acoustic model based on a mixed downsampling and multipath cross convolution module, and identifying from audio Mel spectral characteristics to pinyin; and constructing a Transformer language model based on a homophone modeling scheme to perform transcription from pinyin to Chinese characters. In the acoustic model, a hybrid downsampling and multipath cross convolution structure is provided and adopted, the parameter number is greatly reduced, the complexity is reduced, the training time and the calculation resource overhead are saved, and the generalization performance of the model is improved. In the language model, homophone modeling is adopted, the output characteristic dimension is reduced from more than 4000 to 55, the number of parameters is reduced, the model learning difficulty is reduced, and the transcription accuracy is improved.","['G10L15/063', 'G10L15/16', 'G10L25/03', 'G10L2015/0633', 'Y02T10/40']"
US20240320063A1,Integration from large language machine-learned model powered applications to online system,"An online system receives, from a model serving system, an application programming interface (API) request from a plug-in provided by an online system. The API request includes a list of items obtained from a conversation session of a user with a machine-learned language model application of the model serving system. The online system generates a URL to a landing page for the user for creating a purchase list with the online system based on the list of items. Responsive to receiving a request to access the URL, the online system causes display of the landing page on a client device of the user that displays the purchase list including retailer items for one or more retailers corresponding to the list of items in the API request.",['G06F9/541']
CN114925659B,"Dynamic width maximization decoding method, text generation method and storage medium","The invention discloses a dynamic width maximization decoding method, a text generation method and a storage medium. In order to solve the technical problems that the beam search in the prior art cannot meet the requirements, the performance of search results is reduced, the generation efficiency is low, and the like, the invention dynamically adjusts the search width of the beam search through the output probability distribution, reduces search errors, realizes higher-quality text generation under higher search width parameter setting, and has higher coincidence degree with a target text. The invention is suitable for the field of natural language processing.","['G06F40/126', 'G06F16/335', 'G06F40/216', 'G06F40/289', 'G06N3/047', 'G06N3/08', 'Y02D10/00']"
US12360876B2,Computer resource monitoring by processing images using a model comprising large language models,"A system and a method for classifying images pertaining to computer resource usage of a user, using an ensemble of close ended questions and a neural network based language model. The method may be used to as aid to monitor and/or enforce computer usage policies in industrial organizations, government services, academy, education, and/or the like.","['G06F11/3438', 'G06V10/764']"
CN114330290A,Language model training method and device,"A method and apparatus for training a language model are provided. The training method of the language model comprises the following steps: receiving input training data, wherein the training data comprises a given word and a context of the given word; generating a proxy for a context-dependent vector of words of the training data in the target language based on the given word and the context of the given word, and generating a context-dependent word vector of the training data in the source language based on the context of the given word; determining alignment probabilities of a source language and a target language as a loss function based on the agent of context-dependent vectors of words and the context-dependent word vectors; and training is carried out based on the loss function, so that the aim of cross-language alignment is fulfilled, and the quality of a language model is improved.",[]
CN115510812A,Ultra-Long Sequence Processing Method Based on Improved Transformer Model,"The invention provides an ultralong sequence processing method based on an improved Transformer model, which is realized based on a WSformer model constructed by improvement on the basis of the Transformer model and specifically comprises the following steps of: s1, pre-training a WSformer model: s1.1, segmenting a super-long sequence into small sequences by adopting a sequence segmentation method; s1.2, feature extraction based on a double-layer attention mechanism: performing word-level feature extraction and sentence-level feature extraction by adopting a double-layer attention mechanism to obtain feature codes of the whole sequence; s2, improved position vector coding: in the encoding process of step S1.2, the position encoding of the vocabulary is adjusted by using a trigonometric function, so as to implement encoding of the position vector. The method can effectively reduce the time cost of long text sequence coding, more accurately depict the position relation between words, realize effective modeling of long sequence text, and improve the prediction precision and the calculation efficiency.","['G06F40/126', 'G06F40/211', 'G06F40/216', 'G06F40/284', 'G06N3/08']"
US20230297606A1,"Low-resource, multi-lingual transformer models","Generally discussed herein are devices, systems, and methods for multi-lingual model generation. A method can include determining, for low-resource languages, respective a language similarity value indicating language similarity between each of the low-resource languages, clustering the low-resource languages into groups based on the respective language similarity value, aggregating training data of languages corresponding to a given group resulting in aggregated training data, and training a re-ranking language model based on the aggregated training data resulting in a trained re-ranking language model.","['G06F16/35', 'G10L15/183', 'G06F16/335', 'G06F18/214', 'G06F18/23213', 'G06F40/263', 'G06F40/284', 'G06F40/289', 'G06K9/6223', 'G06K9/6256', 'G06N3/04', 'G06N3/08']"
KR20240012047A,Apparatus and method for emotion classification,"According to one aspect of the present invention, a sentiment classification device comprises: a data preprocessing module which performs tokenization based on word pieces on text data and performs preprocessing on the text data by encoding each token generated according to the tokenization to generate preprocessed data; a learning module which trains a sentiment classification model prepared through transfer learning on a pre-trained language model using the preprocessed data generated by the data preprocessing module; and a classification module which inputs target text data into the sentiment classification model whose learning has been completed by the learning module to classify the sentiment of the target text data. Accordingly, the present invention eliminates consumption in terms of time, cost, and technology, while enabling more accurate sentiment analysis of text data.","['G06F40/30', 'G06F16/35', 'G06F40/284', 'G06N20/00', 'G06N3/045', 'G06N3/096']"
EP4332824A1,System and method for translating natural language traffic rules into formal logic for autonomous moving vehicles,The invention relates to a system and method for translating natural language traffic rules into formal logic including a training method for a machine learning system and can be used in the context of advanced driver assistance systems or autonomous driving systems (ADAS or AD) for vehicles or other autonomous moving vehicles like robots and drones.,"['G06F40/42', 'G06F40/289', 'G06N3/042', 'G06N5/025', 'G06N3/0442', 'G06N3/045', 'G06N5/045']"
CN114944148B,Streaming Vietnamese Speech Recognition Method Incorporating External Linguistic Knowledge,"The invention provides a streaming Vietnam voice recognition method integrating external language knowledge, belonging to the field of artificial intelligence. Vietnam language is a low-resource language, and training corpus is difficult to obtain; the streaming end-to-end model has difficulty learning language knowledge in a large amount of external text during training, which limits the performance of the streaming Vietnam speech recognition model. In the invention, the Vietnam speech section is used as a modeling unit of a language model and a streaming Vietnam speech recognition model, and in a training stage, a new loss function is calculated through the output of a pre-training Vietnam language model and a speech recognition model decoder, so that the streaming Vietnam speech recognition model is helped to learn some Vietnam language knowledge, and the model parameters of the streaming Vietnam speech recognition model are optimized. On the Vietnam data set, the word error rate of the streaming Vietnam voice recognition model can be improved by 2.45% by fusing the language model in the training stage. The language model is fused again in the decoding stage, and the word error rate of the model can be respectively improved by 1.35% and 4.75%.","['G10L15/005', 'G10L15/063']"
WO2024129336A1,Long sequence modeling via state space model (ssm)-enhanced transformer,"A computing device is provided including a processor configured to execute a transformer including an encoder having a global layer configured to receive tokenized embeddings for each of a plurality of tokens in a local input sequence and compute a global self-attention vector for each of the tokenized embeddings. The encoder further includes a local layer configured to receive each global self-attention vector from the global layer and compute local self-attention for each local input sequence, and add and normalize the global self-attention vector with the local self-attention vector to thereby produce an encoder representation including a self-attention vector for each local input sequence that includes both global self-attention values and local self-attention values. The transformer is configured to output a prediction for the global input sequence based on the encoder representation of each of the local input sequences of the global input sequence.",['G06N3/0455']
KR20240020519A,"Apparatus, method and computer program for generating sentence using language model based on transformer",The present invention relates to a sentence generating apparatus for generating a sentence using a transformer-based language model. The sentence generating apparatus comprises: a source sentence receiving unit for receiving a source sentence; an incomplete sentence generating unit for generating an incomplete sentence from the source sentence using a pre-trained input encoder and decoder; a condition determining unit for determining whether the incomplete sentence satisfies a preset condition; a final sentence generating unit for generating a final sentence based on a feature vector generated through the condition encoder when the incomplete sentence does not satisfy the preset condition; and a sentence output unit for outputting the final sentence.,"['G06F40/58', 'G06F40/56', 'G06F40/279', 'G06N3/0455', 'G06N3/08']"
US20240119925A1,System and method for post-asr false wake-up suppression,"A method includes obtaining a speech signal. The method also includes predicting a first likelihood of a wake word or phrase being spoken in the speech signal using a first machine learning model trained to receive the speech signal as input. The method further includes, responsive to the first likelihood exceeding a first threshold, performing automatic speech recognition on the speech signal to determine a textual representation of the speech signal. The method also includes predicting a second likelihood of the wake word or phrase being spoken in the speech signal using a second machine learning model trained to receive at least one of the textual representation, audio features associated with the speech signal, and context features associated with the electronic device. In addition, the method includes, responsive to the second likelihood exceeding a second threshold, generating instructions to perform an action requested in the speech signal.","['G10L15/22', 'G10L15/08', 'G10L15/32', 'G10L15/16', 'G10L2015/088', 'G10L2015/223']"
CN116663531A,Training Method of Text Generation Model Based on Uncertainty Estimation of Multi-granularity Data,"The invention provides a training method, a training system, a training storage medium and training electronic equipment of a text generation model based on multi-granularity data uncertainty estimation, and relates to the technical field of text generation. In the invention, the data uncertainty of word and sentence granularity, namely the source word uncertainty, the target word uncertainty and the sentence uncertainty are modeled; and then, three kinds of data are inaccurately fused into the generation model training process based on the transducer in a heterogeneous mode, so that adverse effects of possible misleading of the uncertainty data in the training process are finally reduced, and the robustness of training is increased. In addition, in the prior art, uncertainty is usually estimated only by using simple statistical features such as word frequency, mutual information and the like, and the degree of data uncertainty cannot be truly reflected; the invention simultaneously predicts the data uncertainty of two granularities of words and sentences by means of the powerful language representation capability of the pre-training model BERT, and greatly improves the capability of data uncertainty prediction.","['G06F40/211', 'G06F40/216', 'G06F40/289', 'G06N3/0455', 'G06N3/09', 'Y04S10/50']"
US20240370498A1,Automatic textual document evaluation using large language models,"A system and a method for analyzing conversation text using a flow of query prompts, an ensemble of close ended questions and a neural network based language model. The method may be used as a tutor or examination bot, for mental coherency screening, data mining, clustering groups of trainees or customers according to training needs or interests, and the likes.","['G06F16/90332', 'G06F40/20']"
US20240320523A1,Replacing Online Conversations Using Large Language Machine-Learned Models,"An online system performs an inference task in conjunction with the model serving system or the interface system to continuously monitor conversations between requesting users and fulfillment users to determine whether the online system can intervene to automatically respond to a message sent by a sending party, rather than prompting the receiving party for a manual reply. Upon inferring that a message can be automatically responded to, the online system automatically provides a response to the message without the receiving party's manual involvement. The online system can further be augmented to classify and reroute certain requesting user or fulfillment user queries that impact an order's end state by intercepting the conversation on behalf of either party and performing one or more automated actions. If the message is action-oriented, the online system may perform one or more automated actions in response to the message.","['G06N5/022', 'G06N20/00', 'G06N5/025', 'G06N5/04']"
US20250021766A1,Textual dataset augmentation using large language models,"A system and a method for augmenting a dataset comprising a textual content using instruction to cause a conversational language model to create variations of text items by changing text properties such as length, style, terminology, dialect, rhyming and the like. The method may also be used with combined prompts and iteratively.","['G06F40/279', 'G06F40/30', 'G06F40/40', 'G06F40/56']"
KR20240079507A,Method and apparatus for learning language model using crossmodal information,Provided is a method for generating a language model using crossmodal information. The method comprises the steps of: receiving first modality information based on a language and second modality information based on a non-language; converting the first modality information into a first byte string; converting the second modality information into a second byte string; converting the first and second byte strings into a first embedding vector and a second embedding vector by applying an embedding technique for each modality; generating semantic association information between the first and second modality information by inputting the first and second embedding vectors into a crossmodal transformer; and learning a language model by setting the generated semantic association information as learning data.,"['G06F40/40', 'G06F40/58', 'G06F40/284', 'G06F40/30', 'G06F40/53', 'G06F40/56', 'G06N20/00', 'G06V20/70']"
CN115410555A,Method and device for correcting voice text,"The invention relates to a method and a device for correcting a voice text. The correction method of the voice text comprises the following steps: acquiring a voice text; the phonetic text is corrected using a text correction model. The method for generating the text correction model comprises the following steps: acquiring new words in a specific field and new word texts comprising the new words; generating a training text according to the new word text; and training the language model based on the Transformer by utilizing the training text and the mask language model to generate a text correction model. The text correction model is generated by carrying out unsupervised learning by using the new words and the new word text, so that the text correction model can be quickly and efficiently adapted to the new words continuously appearing in a specific field, and the accuracy of the voice text can be still ensured under the condition of using a common voice recognition model.","['G10L15/063', 'G06F40/232', 'G06F40/289', 'G10L15/04', 'G10L15/26']"
US20240303710A1,Processing crowd-sourced information using machine learning based language models,"A system, for example, an online system uses a machine learning based language model, for example, a large language model (LLM) to process crowd-sourced information provided by users. The crowd-sourced information may include comments from users represented as unstructured text. The system further receives queries from users and answers the queries based on the crowd-sourced information collected by the system. The system generates a prompt for input to a machine-learned language model based on the query. The system provides the prompt to the machine-learned language model for execution and receives a response from the machine-learned language model. The response comprises the insight on the topic and evidence for the insight. The evidence identifies one or more comments used to obtain the insight.","['G06Q30/0201', 'G06Q30/0282', 'G06Q30/0627']"
CN117435933B,Transformer equipment health evaluation method integrating pre-training language model and atlas,"The invention discloses a transformer equipment health evaluation method integrating a pre-training language model and a map, which comprises the following steps: respectively and correspondingly processing the real-time and historical operation evaluation data to obtain real-time and historical alarm texts; constructing a dynamic state evaluation model; constructing a knowledge graph according to the knowledge graph construction framework based on the history warning text; training based on the historical alarm text and the dynamic state evaluation model to obtain a short text matching model; extracting entity and attribute information of a real-time warning text based on a knowledge graph construction framework; carrying out semantic similarity matching on the basis of the real-time warning text matching dynamic state evaluation model and the electric short text matching model so as to obtain a deduction score; inquiring the knowledge graph to determine the deduction value, and calculating and updating the health degree score according to the deduction value and the deduction coefficient; the knowledge graph construction is carried out by adopting the improved mT5, the text matching is carried out by adopting the improved BERT+CoSENT, and the health degree is dynamically evaluated through a quantitative deduction mechanism triggered by an alarm.","['G06F18/22', 'G06F16/367', 'G06F18/214', 'G06F18/25', 'G06N3/0442', 'G06N3/084', 'Y04S10/50']"
US20250061529A1,Ai-assisted subject matter management system,"System and methods herein are configured to receive and ingest a set of course materials, receive user requests from teachers or students, tokenize and generate vector embeddings based on the user requests, and compare generated input vector embeddings vector embeddings associated with workflows to find a matching workflow. The systems and methods are further configured to obtain a subset of the course materials corresponding to the matching workflow, generate a prompt, and submit the prompt to a language model. The systems and methods are further configured to provide a response to the prompt back to the user.","['G06Q10/103', 'G06F40/174', 'G06F40/284', 'G06F40/30', 'G06Q50/205']"
US11836591B1,Scalable systems and methods for curating user experience test results,"Techniques are described herein for selecting, curating, normalizing, enriching, and synthesizing the results of user experience tests. In some embodiments, a system identifies a qualitative element within a result set for a user experience test. The system then selects a machine learning model to apply based on one or more attributes associated with the user experience test and generates a predicted visibility, quality, and/or relevance for the qualitative element. Based on the prediction, the system generates a user interface that curates a set of results of the user experience test.","['G06N3/045', 'G06N20/20', 'G06F11/3692', 'G06F11/3698', 'G06N3/084']"
US12032918B1,Agent based methods for discovering and documenting user expectations,"Techniques are described herein for using artificial intelligence to select, curate, normalize, enrich, and synthesize the results of user experience (UX) tests. In some embodiments, a system identifies a set of unstructured textual elements associated with one or more UX tests. The system may configure agents using generative language model services, including a reviewing agent that reviews and edit outputs of a machine learning classification model applied to the unstructured textual elements and/or a curating agent that selects unstructured textual elements to represent themes within the user experience test classified using the machine learning classification model. The outputs may be used to enhance the scalability, function, and efficiency of applications directed at improving product designs.","['G06F40/40', 'G06F40/30']"
US20250173606A1,Automated machine-learned language model prompt recommendations,An online system may receive a registration of an application for a language model gateway configured as an intermediary between users and a first machine-learned language model. The online system may monitor a conversation associated with the application using the language model gateway. The conversation is between a user of the application and the first machine-learned language model and includes a prompt from the user directed toward the first machine-learned language model. The online system may extract the prompt and compile an input for a second machine-learned language model that is fine-tuned to improve prompts. The input may be the prompt and one or more criteria to improve the prompt. The online system may provide the input to the second machine-learned language model. The online system may determine a suggested improvement to the prompt using the second machine-learned language model and provide the suggested improvement to the user.,"['G06N20/00', 'G06F40/205', 'G06F40/279', 'G06N3/084']"
CN118430519A,A training method for a large language model system based on an improved Transformer network,"The invention relates to a training method of a large language model system based on an improved transducer network, which belongs to the field of text and voice processing and large language model, and comprises the following specific processes: acquiring a data set; dividing the dataset into a text class and a speech class; adopting FlipDA data enhancement to the text data; performing two-stage data enhancement processing on the voice data; then an improved transducer network is constructed, a AdapterTuning fine tuning module is added into the network, and a one-to-one FFN module in an original model is replaced by a one-to-many FFN applying dynamic expansion; and finally, inputting the data into a network for training. According to the invention, flipDA data enhancement algorithm is introduced, so that the accuracy of a large language model can be stably improved in most scenes, and meanwhile, a dynamic expansion one-to-many FFN module is introduced, so that the time of new task learning is reduced, and the problem of mutual interference of incoherent tasks in multi-task learning is solved.","['G10L15/063', 'G10L15/18', 'G10L15/26', 'G10L17/04']"
US12056453B2,Semantic structure identification for document autostyling,"Systems and methods for natural language processing are described. Embodiments of the present disclosure receive plain text comprising a sequence of text entities; generate a sequence of entity embeddings based on the plain text, wherein each entity embedding in the sequence of entity embeddings is generated based on a text entity in the sequence of text entities; generate style information for the text entity based on the sequence of entity embeddings; and generate a document based on the style information.","['G06F40/186', 'G06F40/284', 'G06F40/103', 'G06F40/253', 'G06F40/30', 'G06F40/35']"
US12423521B2,Using unsupervised clustering and language model to normalize attribute tuples of items in a database,"A computer system uses clustering and a large language model (LLM) to normalize attribute tuples for items stored in a database of an online system. The online system collects attribute tuples, each attribute tuple comprising an attribute type and an attribute value for an item. The online system initially clusters the attribute tuples into a first plurality of clusters. The online system generates prompts for input into the LLM, each prompt including a subset of attribute tuples grouped into a respective cluster of the first plurality. Based on the prompts, the LLM generates a second plurality of clusters, each cluster including one or more attribute tuples that have a common attribute type and a common attribute value. The online system maps each attribute tuple to a respective normalized attribute tuple associated with each cluster. The online system rewrites each attribute tuple in the database to a corresponding normalized attribute tuple.","['G06F40/247', 'G06F16/211', 'G06F16/215', 'G06F16/285']"
WO2025029526A2,Explainable adaptable artificial intelligence networks,"Computer-implemented methods and systems make a generative AI system more explainable. A programmed computer system grows a generative AI system by adding one or more explainable network elements to the generative AI system. Each explainable network element can be trained to discriminate two or more explainable sets of training data items for the generative AI system. After adding the one or more explainable network elements, training of the generative AI system can be updated with the one or more explainable network elements added. Then the programmed computer system can determined whether continued growth of the generative AI system is required.","['G06N3/045', 'G06N3/044', 'G06N3/047', 'G06N3/0475', 'G06N3/08']"
US12386774B2,AI accelerator apparatus using full mesh connectivity chiplet devices for transformer workloads,"An AI accelerator apparatus using in-memory compute chiplet devices. The apparatus includes a first semiconductor substrate having a plurality of chiplets, each of which includes a plurality of tiles. Each tile includes a plurality of slices, a central processing unit (CPU), and a hardware dispatch device. Each slice can include a digital in-memory compute (DIMC) device configured to perform high throughput computations. In particular, the DIMC device can be configured to accelerate the computations of attention functions for transformer-based models (a.k.a. transformers) applied to machine learning applications. The chiplets are in a full mesh connectivity configuration such that at least one of the die-to-die (D2D) interconnects of each chiplet is coupled to one of the D2D interconnects of each other chiplet using a non-diagonal link. The chiplets can also include other interfaces to facilitate communication between the chiplets, memory and a server or host system.","['G06F13/4221', 'G06F9/38873']"
US12175372B1,Integration of machine learning models with dialectical logic frameworks,"This disclosure relates to techniques for executing dialectical analyses using large language models and/or other types of deep learning models. A dialectic logic engine can execute various programmatic processes or functions associated with applying dialectic analyses to input strings. The programmatic processes executed by the dialectic logic engine can initiate communication exchanges with one or more generative language models to derive parameters for performing dialectic analyses and/or to derive outputs based on the parameters. The dialectic logic engine also can execute functions for enforcing constraint conditions and/or eliminating bias from responses generated by the one or more generative language models to improve the accuracy, precision, and quality of the parameters and/or outputs derived from the parameters. In some embodiments, a dialectic training system can be configured to execute a training procedure that trains or fine-tunes a language model to extend the capabilities of the language model to dialectic domains.","['G06N3/0895', 'G06N3/045']"
US20240134912A1,System and method for question-based content answering,"A system and method for question-based content answering that can include training a query-content model; indexing a collection of media content data forming indexed content; receiving a query input through a computer implemented computer interface; applying a retrieval model to the query input and indexed content and determining candidate content segment results, which may include: retrieving an initial set of candidate content segments by performing a keyword search of the query input on the indexed content, and ranking, based in part on language modeling using the query-content model, the initial set of candidate content segments into the candidate content segment results; and presenting the candidate content segment results in the computer interface.","['G06F16/90332', 'G06F16/3329', 'G06F16/951', 'G06F16/9535', 'G06F40/205', 'G06F40/284', 'G06F40/30', 'G06N3/045', 'G06N3/08']"
US20230115321A1,Fine-tuning multi-head network from a single transformer layer of pre-trained language model,"Techniques are provided for customizing or fine-tuning a pre-trained version of a machine-learning model that includes multiple layers and is configured to process audio or textual language input. Each of the multiple layers is configured with a plurality of layer-specific pre-trained parameter values corresponding to a plurality of parameters, and each of the multiple layers is configured to implement multi-head attention. An incomplete subset of the multiple layers is identified for which corresponding layer-specific pre-trained parameter values are to be fine-tuned using a client data set. The machine-learning model is fine-tuned using the client data set to generate an updated version of the machine-learning model, where the layer-specific pre-trained parameter values configured for each layer of one of more of the multiple layers not included in the incomplete subset are frozen during the fine-tuning. Use of the updated version of the machine-learning model is facilitated.","['G10L15/063', 'G06N3/084', 'G06F40/20', 'G06F40/30', 'G06N20/00', 'G06N3/045', 'G06N3/0475', 'G06N3/09', 'G06N3/096', 'G10L15/22', 'H04L51/02', 'G10L2015/0635', 'G10L2015/223']"
WO2023242540A1,Methods and systems for transforming and retrieving information from document data using machine learning,"The present disclosure provides methods and systems for querying insight information. The method comprises: providing different input fields on a UI for receiving a query input for searching insight information in a document, the input fields comprise a first input field for receiving one or more questions in natural language and a second input field for receiving one or more search terms; receiving the query comprising a question or a search term; identifying, using one or more machine learning algorithm trained models, one or more sections of the document each comprising a chunk of texts relevant to the query input, and rendering the one or more sections in the UI with the chunk of texts annotated with a visual indicator; receiving a user feedback data indicating an acceptance or rejection of the chunk of texts; and improving at least one of the trained models using the user feedback data.","['G06F16/93', 'G06F16/3329', 'G06F16/338']"
CN110738057B,Text style migration method based on grammar constraint and language model,"The invention provides a text style migration method based on grammar constraint and language model, which comprises extracting grammar relation graph G of input sentence x by Stanford dependency syntax tool package x Then the grammar relation graph G is mapped by a self-grammar-transducer structure x Adding style information S of original input sentence x Style information S of sentence after expected conversion y Obtaining grammar relation graph G' x And G' y Then combining grammar relation graph G of original input sentence x The input sentence x 'is reconstructed by a cross-graph-transformer structure to obtain the sentence y' after style migration. In order to better learn self-graph-transducer structures blended with style information and to learn cross-graph-transducer structures of reconstructed style migration sentences, the method also utilizes a language model to replace a traditional CNN classifier to guide the learning of the latter. Experiments on corresponding data sets in such a way show that the semantic invariance can be better kept under the condition of changing sentence styles compared with the previous text style migration method.","['G06N3/045', 'Y02D10/00']"
CN114168709B,A text classification method based on lightweight pre-trained language model,"The invention discloses a text classification method based on a lightweight pre-training language model, aiming at improving the text classification accuracy on the premise of less occupied resources. The technical scheme is as follows: constructing a text classification model based on a pre-training language model, wherein the text classification model comprises a teacher model and a student model; processing the GLUE data set into a format required by a text classification model for classification; initializing parameters of the teacher model, and finely adjusting the teacher model by adopting the processed GLUE data set; secondly, initializing parameters of the student model, and obtaining network weight parameters of the student model by adopting a GLUE data set and using a teacher model subjected to light weight fine tuning by a knowledge distillation method; initializing a student model by using a student model network weight parameter to obtain a lightweight student model; and finally, carrying out text classification on the text input by the user by using a lightweight student model to obtain a classification result. The invention realizes high accuracy of text classification on the premise of less occupied resources.","['G06F16/35', 'G06F16/3344', 'G06F18/214']"
US20250182858A1,Large Language Model for Unified Text and Point Cloud Molecular Input,"A transformer model architecture is described. The transformer model architecture comprises a point cloud input module, a text input module, a point cloud encoder module operatively coupled with the point cloud input module, a large language model module operatively coupled to the text input module and point cloud encoder module and configured to receive data therefrom, and a text output module operatively coupled to the large language model module. The text output module is configured to output molecular data in line notation format.","['G16B15/00', 'G06N3/0455', 'G06N3/042', 'G06N3/088', 'G06N5/041', 'G06T7/75', 'G16B15/30', 'G16B40/00', 'G16B40/30', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084']"
CN116364055B,"Speech generation method, device, device and medium based on pre-trained language model","The embodiment of the invention relates to a voice generation method, a device, equipment and a medium based on a pre-training language model, which belong to the field of voice synthesis, and a semantic token sequence is obtained by encoding a target text of voice to be generated based on a first pre-training language model; encoding the voice style control information based on the natural language description based on the second pre-training language model to obtain a style token sequence; autoregressive is carried out on the semantic token sequence and the style token sequence based on a third pre-training language model, so that an acoustic token sequence is obtained; decoding the acoustic token sequence based on the trained tricot decoder to generate target voice corresponding to the target text; the voice is generated through each pre-training language model, and the voice style to be generated of the text is conveniently and accurately controlled through natural language, so that the diversity of voice style control is increased, and the voice generation quality is improved.","['G10L13/027', 'G10L13/08', 'G10L25/30', 'Y02T10/40']"
US12314154B2,Code execution trace generation with pre-trained large language model,"A large language model, previously pre-trained on multiple source code modeling tasks, is pre-trained, through curriculum learning, to learn to predict a code execution trace given a source code program. The model is pre-trained using a variety of pre-training datasets consisting of pairs of a source code sample and a corresponding execution trace. The curriculum pre-training starts with a pre-training dataset of single line executions and adds in additional pre-training datasets with more increasing complex behaviors. The pre-training datasets include mutation-augmented source code samples and their corresponding execution traces.","['G06F11/3636', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/084']"
US12223269B2,Language-model pretraining with gradient-disentangled embedding sharing,"A method for training a language model comprises (a) receiving vectorized training data as input to a multitask pretraining problem; (b) generating modified vectorized training data based on the vectorized training data, according to an upstream data embedding; (c) emitting pretraining output based on the modified vectorized training data, according to a downstream data embedding equivalent to the upstream data embedding; and (d) adjusting the upstream data embedding and the downstream data embedding by computing, based on the pretraining output, a gradient of the upstream data embedding disentangled from a gradient of the downstream data embedding, thereby advancing the multitask pretraining problem toward a pretrained state.","['G06F40/30', 'G06F40/284', 'G06F40/295', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N3/084', 'G06N3/088', 'G06N5/04']"
CN114783413A,"Re-scoring language model training and speech recognition method, device, system and device","The application provides a method, a device, a system and equipment for re-scoring language model training and voice recognition. The method comprises the following steps: acquiring first voice training data and a voice test data set; the first voice training data comprises a plurality of voice samples; determining a plurality of candidate texts corresponding to the first voice training data and a plurality of scores corresponding to the candidate texts; determining the weight of a plurality of scores in the current iteration training of the Transformer network to be trained through a Bayesian optimization algorithm according to the voice test data set; and performing iteration training on the transform network in the current round according to the candidate texts and the scores and the weight of the scores of the candidate texts in the current round of iteration training, and repeating the steps until the training of the transform network reaches a convergence condition to obtain a reprinting language model.","['G10L15/005', 'G10L15/063', 'G10L15/22', 'G10L15/26']"
US12260340B2,Extreme language model compression with optimal sub-words and shared projections,"Provided is a knowledge distillation technique for training a student language model that, relative to a larger teacher language model, has a significantly smaller vocabulary, lower embedding dimensions, and/or hidden state dimensions. Specifically, aspects of the present disclosure are directed to a dual-training mechanism that trains the teacher and student language models simultaneously to obtain optimal word embeddings for the student vocabulary. In some implementations, this approach can be combined with learning shared projection matrices that transfer layer-wise knowledge from the teacher language model to the student language model. Example experimental results have also demonstrated higher compression efficiency and accuracy when compared with other state-of-the-art compression techniques, including the ability to compress the BERTBASE model by more than 60Ã—, with only a minor drop in downstream task metrics, resulting in a language model with a footprint of under 7 MB.","['G06F40/284', 'G06N3/045', 'G06N3/088']"
US11526774B2,Method for automatically compressing multitask-oriented pre-trained language model and platform thereof,"Disclosed is a method for automatically compressing multi-task oriented pre-trained language model and a platform thereof. According to the method, a meta-network of a structure generator is designed, a knowledge distillation coding vector is constructed based on a knowledge distillation method of Transformer layer sampling, and a distillation structure model corresponding to a currently input coding vector is generated by using the structure generator; at the same time, a Bernoulli distribution sampling method is provided for training the structure generator; in each iteration, each encoder unit is transferred by Bernoulli distribution sampling to form a corresponding coding vector; by changing the coding vector input to the structure generator and a small batch of training data, the structure generator and the corresponding distillation structure are jointly trained, and a structure generator capable of generating weights for different distillation structures can be acquired.","['G06N5/022', 'G06N3/082', 'G06F18/214', 'G06F18/2415', 'G06K9/6256', 'G06K9/6277', 'G06N3/045', 'G06N3/086', 'G06N3/126', 'G06N5/04', 'G06N3/063']"
CN112232511B,Multitask-oriented automatic compression method and platform of pre-trained language model,"The invention discloses a pre-training language model automatic compression method and platform for multiple tasks. Designing a meta-network of a structure generator, constructing a knowledge distillation coding vector based on a Transformer layer sampling knowledge distillation method, and generating a distillation structure model corresponding to a currently input coding vector by using the structure generator; simultaneously, a Bernoulli distributed sampling method is provided to train a structure generator; in each iteration, each encoder unit is moved by using a Bernoulli distribution sampling mode to form a corresponding encoding vector; by changing the coding vector input into the structure generator and the training data of a small batch, combining the training structure generator and the corresponding distillation structure, the structure generator capable of generating weights for different distillation structures can be learned; and meanwhile, on the basis of the trained meta-learning network, searching for an optimal compression structure through an evolutionary algorithm, thereby obtaining an optimal general compression architecture of the pre-training language model irrelevant to the task.","['G06N3/082', 'G06N5/04']"
US20220237378A1,System and method for natural language processing with pretrained language models,"A computer-implemented system and method and for learning an entity-independent representation are disclosed. The method may include: receiving an input text; identifying named entities in the input text; replacing the named entities in the input text with entity markers; parsing the input text into a plurality of tokens; generating a plurality of token embeddings based on the plurality of tokens; generating a plurality of positional embeddings based on the respective position of each of the plurality of tokens within the input text; generating a plurality of token type embeddings based on the plurality of tokens and the one or more named entities in the input text; and processing the plurality of token embeddings, the plurality of positional embeddings, and the plurality of token type embeddings using a transformer neural network model to generate a hidden state vector for each of the plurality of tokens in the input text.","['G06F40/284', 'G06F40/20', 'G06F40/205', 'G06F40/295', 'G06N20/00', 'G06N3/045', 'G06N3/08']"
US20250156413A1,Context-based prompt generation for automated translations between natural language and query language,A disclosed method facilitates translation of natural language queries into query language statements usable to retrieve data from or write data to a particular database. The method includes obtaining a pool of shots. Each shot in the pool includes a natural language query component and a corresponding database translation component. The method further provides for vectorizing the natural language query component for each of the shots into a common vector space; receiving a natural language query from a user interface; vectorizing the natural language query within the common vector space; identifying a subset of vectorized natural language query components that satisfy a similarity metric when compared to the vectorized natural language query; and generating an LLM prompt that includes shots from the pool corresponding to the subset of the vectorized natural language query.,"['G06F16/243', 'G06F16/24522', 'G06F40/295', 'G06F40/30']"
WO2022095713A1,Answer span correction,"A method of using a computing device to improve an answer generated by a natural language question and answer system includes receiving, by a computing device, multiple questions in a natural language question and answer system. The computing device further generates multiple answers to the multiple questions. The computing device still further constructs a new training set with the generated multiple answers, where each answer is compared with a corresponding question of the multiple questions. The computing device additionally augments the new training set with one or more tokens delimiting a span of one or more of the generated multiple answers. The computing device further trains a new natural language question and answer system with the augmented new training set.","['G06N3/08', 'G06F40/216', 'G06F40/284', 'G06F40/289', 'G06F40/35', 'G06N3/045', 'G06N5/04', 'G06F16/245', 'G06N3/044', 'G06N3/048']"
CN118445379A,Large language model training method and device,"The embodiment of the specification discloses a training method and device for a large language model. The training method comprises the following steps: firstly, in a target attention head of a large language model, processing the output of the upper layer aiming at an input text by utilizing a query transformation matrix, a key transformation matrix and a value transformation matrix respectively to obtain the query matrix, the key matrix and the value matrix; then, compressing the query matrix into a global query vector, and respectively fusing the global query vector and a plurality of key vectors in the key matrix to obtain an attention matrix; then, compressing the attention moment array into a global attention vector, and respectively carrying out fusion processing on the global attention vector and a plurality of value vectors in the value matrix to obtain the output of the target attention head so as to determine the target output of the large language model; the large language model is then trained based on the target output.","['G06F16/3347', 'G06F16/3329', 'G06N3/045', 'G06N3/082', 'G06N5/041']"
CN117892139B,Large language model training and use method based on inter-layer comparison and related device,"The application discloses a large language model training and using method based on interlayer comparison and a related device, which are applied to the field of artificial intelligence. The large language model comprises an N-layer network model and decoding strategy modules respectively connected with the output ends of the N-layer network model. And acquiring first probability distribution respectively output by the N-layer network model through a decoding strategy module. The output error rate of the low-level network model is higher, and the output accuracy of the high-level network model is higher. If the first probability distribution difference between the network models of the Q layer and the N layer is the largest, the accuracy of the output of the network model of the N layer is higher, and the error rate of the output of the network model of the Q layer is higher. The output of the large language model reduces the phantom cognition of the low-level network model, increases the factual answer of the high-level network model, and improves the accuracy.","['G06F18/214', 'G06F18/2411', 'G06F18/2415', 'G06N3/04']"
WO2023091226A1,Language-model pretraining with gradient-disentangled embedding sharing,"A method for training a language model comprises receiving vectorized training data as input to a multitask pretraining problem; generating modified vectorized training data based on the vectorized training data, according to an upstream data embedding; emitting pretraining output based on the modified vectorized training data, according to a downstream data embedding equivalent to the upstream data embedding; and adjusting the upstream data embedding and the downstream data embedding by computing, based on the pretraining output, a gradient of the upstream data embedding disentangled from a gradient of the downstream data embedding, thereby advancing the multitask pretraining problem toward a pretrained state.","['G06F40/30', 'G06N3/0455', 'G06N3/0475', 'G06N3/084', 'G06N3/088', 'G06N3/09', 'G06N3/096']"
CN111144115B,"Pre-training language model acquisition method, device, electronic equipment and storage medium","The application discloses a method and a device for acquiring a pre-training language model, electronic equipment and a storage medium, and relates to the technical field of natural language processing. The specific implementation scheme is as follows: obtaining a first training sentence, determining an actual entity link relation of the first training sentence, inputting the first training sentence into a pre-training language model, obtaining vector representation of each character in the first training sentence through a transducer structure in the pre-training language model, predicting to obtain a predicted entity link relation of an entity in the first training sentence according to the vector representation of each character, and adjusting model parameters of the pre-training language model according to the difference between the actual entity link relation and the predicted entity link relation, thereby determining the entity link relation in combination with the entity in training expectation, enabling the trained pre-training language model to obtain entity knowledge information better, and avoiding knowledge offset phenomenon of the pre-training language model caused by introduction of a knowledge graph.","['G06F16/35', 'G06F16/367']"
US11501171B2,Method and platform for pre-trained language model automatic compression based on multilevel knowledge distillation,"Disclosed are an automatic compression method and platform for a pre-trained language model based on multilevel knowledge distillation. The method includes the following steps: step 1, constructing multilevel knowledge distillation, and distilling a knowledge structure of a large model at three different levels: a self-attention unit, a hidden layer state and an embedded layer; step 2, training a knowledge distillation network of meta-learning to generate a general compression architecture of a plurality of pre-trained language models; and step 3, searching for an optimal compression structure based on an evolutionary algorithm. Firstly, the knowledge distillation based on meta-learning is studied to generate the general compression architecture of the plurality of pre-trained language models; and secondly, on the basis of a trained meta-learning network, the optimal compression structure is searched for via the evolutionary algorithm, so as to obtain an optimal general compression architecture of the pre-trained language model independent of tasks.","['G06F40/30', 'G06F16/35', 'G06F40/40', 'G06N3/045', 'G06N3/082', 'G06N3/086', 'G06N3/047', 'G06N3/084']"
US20240211685A1,Language Model-Based Multi-Objective Optimization,"A computer-implemented method: (A) receives product data identifying a product; (B) (B) receives patent data identifying a patent claim; (C) receives prior art data identifying a set of prior art, (D) executes an optimization process to minimize an objective function, the objective function including: a first sub-objective function and a second sub-objective function, the executing the optimization process comprising: (D)(1) generating a plurality of interpretations of the patent claim, (D)(2) for each particular interpretation in the plurality of interpretations of the patent claim: (D)(2)(a) using a first language model to evaluate the first sub-objective function with the product data and the particular interpretation as inputs to the first sub-objective function, (D)(2)(b) using a second language model to evaluate the second sub-objective function with the prior art data and the particular interpretation as inputs to the second sub-objective function.",['G06F40/20']
CN113360641A,Deep learning-based power grid fault handling plan semantic modeling system and method,"The invention discloses a deep learning-based power grid fault handling plan semantic modeling system and method, wherein a word vector module is adopted to finish the vectorization expression of a fault handling plan text, a plan entity identification module and a plan relation extraction module are adopted to finish the extraction of all entity components in the fault handling plan text, a plan intention understanding module is adopted to finish the intention discrimination of different handling fragments in the fault handling plan, the selection of different equipment type tables is finished based on different intentions, and further the quick mapping of equipment is finished. The modules are mutually matched for use, so that the key information extraction in the fault handling plan can be quickly and accurately realized, the problems of low manual extraction and intelligence level during the electronization of the fault handling plan are solved, the problem of extremely low efficiency of manually browsing the files of the fault handling plan during the fault handling is solved, and compared with a general plan extraction mode, the method is more reasonable and the accuracy is greatly improved.","['G06F16/35', 'G06F40/295', 'G06F40/30', 'G06Q10/20', 'G06Q50/06', 'Y04S10/50']"
US12400124B1,Incremental learning method and apparatus for large vision-language model for autonomous driving,"An incremental learning method and apparatus for a large Vision-Language Model for autonomous driving are provided. The incremental learning method includes: expanding a first training sample set to obtain a second training sample set, wherein the second training sample set includes a plurality of image samples annotated with road scene targets; inserting a plurality of fine-tuning sub-networks respectively into specified positions in a first large Vision-Language Model to generate a second large Vision-Language Model; processing the image samples in the second training sample set using the second large Vision-Language Model to obtain a target prediction result; calculating a loss value using the target prediction result and a target annotation result; and preserving parameters of the first large Vision-Language Model unchanged, and updating parameters of the fine-tuning sub-networks using the loss value.","['G06V10/778', 'G06N3/096', 'G06N3/04', 'G06N3/0455', 'G06N3/082', 'G06V10/774', 'G06V10/82', 'Y02T10/40']"
US11003865B1,Retrieval-augmented language model pre-training and fine-tuning,"Systems and methods for pre-training and fine-tuning of neural-network-based language models are disclosed in which a neural-network-based textual knowledge retriever is trained along with the language model. In some examples, the knowledge retriever obtains documents from an unlabeled pre-training corpus, generates its own training tasks, and learns to retrieve documents relevant to those tasks. In some examples, the knowledge retriever is further refined using supervised open-QA questions. The framework of the present technology provides models that can intelligently retrieve helpful information from a large unlabeled corpus, rather than requiring all potentially relevant information to be stored implicitly in the parameters of the neural network. This framework may thus reduce the storage space and complexity of the neural network, and also enable the model to more effectively handle new tasks that may be different than those on which it was pre-trained.","['G06F40/284', 'G06F18/2155', 'G06F40/49', 'G06F40/56', 'G06K9/6259', 'G06N3/045', 'G06N3/084', 'G06N5/022', 'G06N5/025']"
US20230169075A1,Apparatus and method for processing natural language query about relational database using transformer neural network,"Disclosed is a natural language query processing apparatus comprising, a processor that receives a natural language query input by a user and generates a structured query based on the natural language query, wherein the processor, when generating the structured query based on the natural language query, generates the structured query using a natural language processing result for the natural language query, a schema relationship extracted based on a relationship between subdatabases in a database related to the natural language query, and a cross-attention result generated between the natural language processing result and the schema relationship.","['G06F16/243', 'G06F16/24522', 'G06F16/2433']"
KR102506404B1,Decision-making simulation apparatus and method using pre-trained language model,"The present invention relates to a decision-making simulation apparatus and method using a pre-trained language model. To this end, the present invention is able to provide a decision-making simulation apparatus, comprising: a multi-modal task classification module using a cross attention matrix as input data, and using task information including a task class, which is a preset class for a specific task, and confidence score as output data; a prompt generator module which is a transformer-based artificial neural network module including a decoder block using self-attention, which uses a series of embedding vectors embedding the task information in the cross attention matrix as input data, and uses improved prompt information as output data; and a base artificial neural network module meaning a trained language model which is a pre-trained artificial neural network module including n units of transformer blocks including a self-attention layer, cross attention layer, and feed forward artificial neural network layer, and using improved prompt information as input data and using decision-making text information as output data. Therefore, performance of a trained language model, such as accuracy and speed, can be improved.","['G06N3/082', 'G06F40/279', 'G06N3/04', 'G06N3/0499']"
US11562147B2,Unified vision and dialogue transformer with BERT,"A visual dialogue model receives image input and text input that includes a dialogue history between the model and a current utterance by a human user. The model generates a unified contextualized representation using a transformer encoder network, in which the unified contextualized representation includes a token level encoding of the image input and text input. The model generates an encoded visual dialogue input from the unified contextualized representation using visual dialogue encoding layers. The encoded visual dialogue input includes a position level encoding and a segment type encoding. The model generates an answer prediction from the encoded visual dialogue input using a first self-attention mask associated with discriminative settings or a second self-attention mask associated with generative settings. Dense annotation fine tuning may be performed to increase accuracy of the answer prediction. The model provides the answer prediction as a response to the current utterance of the human user.","['G06F40/35', 'G06F18/21', 'G06F40/284', 'G06K9/6217', 'G06N3/045', 'G06N3/08', 'G06V10/774', 'G06V10/82', 'G06N3/047', 'G06N3/084']"
CN112988785B,SQL conversion method and system based on language model coding and multitask decoding,"The invention discloses a SQL conversion method and a system based on language model coding and multitask decoding, wherein a language model is pre-trained by combining the field of a data set, so that the feature extraction capability in the field is improved; then, the query database is sequentially expanded according to the table names and the column names, the two-dimensional table is converted into a one-dimensional text sequence, and the one-dimensional text sequence is spliced into an input sequence X by combining with user questions; inputting the sequence X into a pre-training language model, and outputting a coding result; respectively decoding and restoring SQL segments by utilizing a multitask decoder consisting of 9 different neural networks, and calculating cross entropy loss; setting different weights for loss values of different neural networks, summing the loss values to obtain the total loss of the model, optimizing a target function by using a gradient descent algorithm, and updating model training parameters; and after training, saving the model parameters, and automatically generating a corresponding SQL sequence according to the user problem and the target database.","['G06F16/2433', 'G06N3/045', 'G06N3/08']"
US20240403341A1,Using large language models to generate search query answers,"A system generates search query responses based on a content repository that include natural language answers and actions performed based on content items in the repository. The system receives a query associated with the content repository. Based on the query, the system retrieves a set of text chunks that are relevant to the query. At least a portion of the relevant text chunks are sent to a large language model (LLM) to cause the LLM to generate an answer description for the user query, based on the text chunks.","['G06F16/3326', 'G06F16/3344', 'G06F16/383']"
CN112613326B,A neural machine translation method of Tibetan-Chinese language with syntactic structure,"The invention relates to a neural machine translation method for a Tibetan Chinese language fused with a syntactic structure, and belongs to the technical field of machine translation and feature fusion application. The method aims to introduce more self-syntax structures of languages in a neural machine translation framework so as to help improve the machine translation quality, and provides a relative position coding method by optimizing the position coding technical problem of a transform so as to blend syntax structure information. The method optimizes the transformer by extracting a structure position coding method based on the dependency relationship, and finally achieves the purpose of improving the translation quality of the Tibetan-Chinese neural machine. The method can effectively improve the efficiency of the self-attention neural network in learning the association between the two languages, relieve the problems caused by different syntactic structures of the two languages, reduce the time complexity of the algorithm, solve the problem of context information loss caused by the adoption of absolute position coding in the traditional model, and reduce the occurrence of wrong translation and missing translation of low-resource neural machine translation.","['G06F40/58', 'G06F40/211', 'G06N3/044']"
US20210183484A1,Hierarchical cnn-transformer based machine learning,"Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, text medical notes may contain information not immediately available in structured variables. Applicants propose a hierarchical CNN-Transformer model with an explicit attention mechanism as an interpretable, multi-task clinical language model.","['G16H10/60', 'G16H50/70', 'G06F17/16', 'G06F40/169', 'G06F40/279', 'G06F40/295', 'G06N20/00', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G16H15/00']"
CN117874171B,"Course related test question generation method, device, equipment, medium and product","The invention discloses a course related test question generation method, device, equipment, medium and product, and relates to the technical field of artificial intelligence. The method comprises the steps of firstly carrying out structuring treatment on course teaching data of a target course to obtain a structured text of the course teaching material, then carrying out knowledge point extraction treatment on the structured text of the course teaching material to obtain a course knowledge point set, then generating at least one relevant test question of the target course according to the course knowledge point set, judging whether the corresponding test question has unavailability errors according to each relevant test question by using a pre-trained artificial intelligent model, finally removing the relevant test question with the unavailability errors to obtain a final relevant test question set, thereby being capable of extracting matched test questions for the target course quickly and efficiently, ensuring that the test questions cannot be out of class, coverage knowledge point incompleteness, test question errors and the like during use, and ensuring high quality and usability of the test questions.","['G06F16/3344', 'G06F16/3343', 'G06F16/367', 'G06F18/214', 'G06F40/205', 'G06F40/253', 'G06F40/30', 'G06N5/022']"
US12182507B2,"Text processing model training method, and text processing method and apparatus","A text processing model training method, and a text processing method and apparatus in the natural language processing field in the artificial intelligence field are disclosed. The training method includes: obtaining training text; separately inputting the training text into a teacher model and a student model to obtain sample data output by the teacher model and prediction data output by the student model; the sample data includes a sample semantic feature and a sample label; the prediction data includes a prediction semantic feature and a prediction label; and the teacher model is a pre-trained language model used for text classification; and training a model parameter of the student model based on the sample data and the prediction data, to obtain a target student model. The method enables the student model to effectively perform knowledge transfer, thereby improving accuracy of a text processing result of the student model.","['G06F40/216', 'G06F16/35', 'G06F16/355', 'G06F18/214', 'G06F18/241', 'G06F18/24133', 'G06F40/166', 'G06F40/279', 'G06F40/30', 'G06N3/042', 'G06N3/045', 'G06N3/0455', 'G06N3/084', 'G06N3/096']"
WO2025007889A1,"Protein language model training method, electronic device, computer-readable medium and program product","Provided are a protein language model training method, a method for extracting an amino acid sequence representation by using a protein language model, a method for obtaining a new amino acid sequence by using a protein language model, a method for obtaining a related amino acid sequence by using a protein language model, a method for predicting an amino acid sequence perplexity by using a protein language model, an electronic device, a computer-readable medium, and a program product. The protein language model training method comprises: during a training process, executing at least one training sub-task amongst two training sub-tasks included in a first type of training task and a second type of training task; and, on the basis of a first loss value corresponding to the first type of training task and a loss value corresponding to the second type of training task, adjusting parameters of a protein language model to obtain a trained protein language model.","['G16B25/10', 'G16B30/00']"
CN118447520A,Classification type government affair document analysis method based on multi-mode large language model,"A classification type government affair document analysis method based on a multi-mode large language model comprises the steps of firstly training a transducer model for understanding and classifying input problems, and gradually executing each step through a preset scheme after obtaining specific types of the problems; firstly, detecting the positions of pictures and characters in a document through document format detection, and dividing the pictures according to the positions; the method comprises the steps that the divided pictures are respectively subjected to picture information acquisition through different visual encoders, and the output of the visual encoders is spliced according to classification categories and preset splicing sequences to obtain visual vectors; after passing through the encoder, the relevant information of the picture is stored in the visual vector, and then the visual vector is connected with a large language model by using a transducer architecture; and finally, fine tuning the large language model. The method and the device improve the accuracy of information acquisition aiming at the government documents.","['G06V30/412', 'G06N3/045', 'G06V10/82', 'G06V30/15', 'G06V30/18057', 'G06V30/19147', 'G06V30/19173', 'G06V30/413']"
US12271696B1,System and method for training and operating large language models using codewords,"This invention presents an optimized approach for training and operating Large Language Models (LLMs) using codewords. By converting traditional token-based LLMs to codeword-based systems, the method achieves significant efficiency gains. The process involves tokenizing training data and assigning codewords to tokens. LLMs are then trained and operated using these compact codewords instead of conventional tokens. During operation, prompts are converted to codewords, processed by the LLM, and the outputs are converted back to text. This approach reduces the overall cost of training and operating LLMs by approximately, offering a more efficient solution for large-scale language processing tasks.","['G06F40/284', 'G06F40/242']"
CN112528669A,"Multi-language model training method and device, electronic equipment and readable storage medium","The application discloses a training method and device of a multi-language model, electronic equipment and a readable storage medium, and relates to the technical field of deep learning and natural language processing. The technical scheme of the application in training the multi-language model is as follows: acquiring a training corpus, wherein the training corpus comprises a plurality of bilingual corpora and a plurality of monolingual corpora; training a first training task on the multi-language model by using a plurality of bilingual corpora; training a second training task on the multi-language model by using a plurality of monolingual corpora; and completing the training of the multi-language model under the condition that the convergence of the loss functions of the first training task and the second training task is determined. According to the method and the device, semantic interaction among different languages can be realized by the multi-language model, and the accuracy of the multi-language model in learning semantic representation of multi-language corpus is improved.","['G06F40/30', 'G06F40/58', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N3/0895', 'Y02D10/00']"
US20240370699A1,Data pre-fetch for large language model (llm) processing,"Examples described herein relate to a processor to process constant weight values and key value entries associated with a first transformer kernel of a large language model (LLM) neural network and a circuitry. The circuitry is to: during processing of the constant weight values and key value entries associated with the first transformer kernel of the LLM neural network, pre-fetch constant weight values and key value entries associated with a second transformer kernel of the LLM neural network into a buffer.","['G06N3/045', 'G06N3/044', 'G06N3/063']"
US20240406210A1,Cyber security training tool that uses a large language model,"The cyber security training tool has a natural language processor and a large language model to be able to analyze both i) a synthetic cyberattack in a mimic network corresponding to a real world network as well as ii) a real cyberattack in the real world network. The cyber security training tool can then provide analysis and an explanation as to why machine learning identified the synthetic cyberattack and/or the real cyberattack as a cyber threat for a purpose of providing cyber security training to at least one of i) an end user of the real world network and ii) a cyber security team member for the real world network. The cyber security training tool further has a user interface component to display security awareness training for the synthetic cyberattack and/or the real cyberattack, and to show the end user and/or the cyber security team member an understanding of the machine learning of the synthetic cyberattack and/or the real cyberattack displayed in the user interface component.","['H04L63/1441', 'G06F21/554', 'G06F21/566', 'G06F21/6245', 'G06F9/45558', 'G06N20/00', 'G06N3/045', 'G06N3/0895', 'H04L63/1433', 'H04L63/1483', 'G06F2009/45587', 'G06F2221/033', 'G06F2221/034']"
US12045592B2,Semi-supervised translation of source code programs using neural transformers,An automated system for translating source code written in one programming language into a different programming language utilizes a neural transformer with attention trained on semi-supervised data. The model is jointly pre-trained with a masked language model objective and an autoregressive objective on a large unsupervised source code corpus to learn to comprehend the syntactic structure and semantics of source code. The pre-trained model is then fine-tuned with a token-type prediction objective and an autoregressive objective on supervised translation tasks and data augmented tasks to learn to translate source code from one programming language into a different programming language.,"['G06F8/51', 'G06N3/045', 'G06N3/063', 'G06N3/084', 'G06N3/088']"
US11769011B2,Universal language segment representations learning with conditional masked language model,"The present disclosure provides a novel sentence-level representation learning method Conditional Masked Language Modeling (CMLM) for training on large scale unlabeled corpora. CMLM outperforms the previous state-of-the-art English sentence embedding models, including those trained with (semi-)supervised signals. For multilingual representations learning, it is shown that co-training CMLM with bitext retrieval and cross-lingual natural language inference (NL) fine-tuning achieves state-of-the-art performance. It is also shown that multilingual representations have the same language bias and principal component removal (PCR) can eliminate the bias by separating language identity information from semantics.","['G06F40/30', 'G06F40/284', 'G06N20/00', 'G06N3/04', 'G06N3/045', 'G06N3/084', 'G06N3/088', 'G06F40/216', 'G06F40/289', 'G06F40/42', 'G06N3/044']"
US11886446B2,Cross-lingual language models and pretraining of cross-lingual language models,"Existing research on cross-lingual retrieval cannot take good advantage of large-scale pretrained language models, such as multilingual BERT and XLM. The absence of cross-lingual passage-level relevance data for finetuning and the lack of query-document style pretraining are some of the key factors of this issue. Accordingly, embodiments of two novel retrieval-oriented pretraining tasks are presented herein to further pretrain cross-lingual language models for downstream retrieval tasks, such as cross-lingual ad-hoc retrieval (CUR) and cross-lingual question answering (CLQA). In one or more embodiments, distant supervision data was constructed from multilingual texts using section alignment to support retrieval-oriented language model pretraining. In one or more embodiments, directly finetuning language models on part of an evaluation collection was performed by making Transformers capable of accepting longer sequences. Experiments show that model embodiments significantly improve upon general multilingual language models in at least the cross-lingual retrieval setting and the cross-lingual transfer setting.","['G06N20/00', 'G06F16/24578', 'G06F16/3329', 'G06F16/3337', 'G06F40/284', 'G06F40/289', 'G06F40/58']"
US11699432B2,Cross-context natural language model generation,Provided is a method including obtaining a corpus and an associated set of domain indicators. The method includes learning a set of vectors in an embedding space based on n-grams of the corpus. The method includes updating ontology graphs comprising a set of vertices and edges associating the set of vertices with each other. The method also includes determining a vector cluster using hierarchical clustering based on distances of the set of vectors with respect to each other in the embedding space and determining a hierarchy of the ontology graphs based on a set of domain indicators of a respective set of vertices corresponding to vectors of the vector cluster. The method also includes updating an index based on the ontology graphs.,"['G10L15/063', 'G06F16/2237', 'G06F16/248', 'G06F16/322', 'G06F16/328', 'G06F16/3323', 'G06F16/3329', 'G06F16/3338', 'G06F16/3344', 'G06F16/3347', 'G06F16/345', 'G06F16/367', 'G06F16/90332', 'G06F40/20', 'G06F40/289', 'G06F40/30', 'G06F40/40', 'G06F9/451', 'G06F9/547', 'G06N20/00', 'G06N3/04', 'G06N5/022', 'G10L15/16', 'G10L15/197', 'G16H50/70', 'G16H70/60', 'G06F9/453', 'G06N3/044', 'G06N3/045', 'G16B50/10', 'G16H10/60', 'G16H40/20', 'G16H70/20', 'Y02A90/10']"
US11941522B2,Address information feature extraction method based on deep neural network model,"The present disclosure discloses an address information feature extraction method based on a deep neural network model. The present disclosure uses a deep neural network architecture, and transforms tasks, such as text feature extraction, address standardization construction and semantic-geospatial fusion, into quantifiable deep neural network model construction and training optimization problems. Taking a character in an address as a basic input unit, the address language model is designed to express it in vectors, then a key technology of standardization construction of Chinese addresses is realized through neural network target tasks. Moreover, considering expression features of the Chinese addresses and geospatial attributes, a fusion scheme of address semantic representations and geographical coordinates is proposed, and a weighted clustering method and a feature fusion model are designed, to extract, from an address text of a natural language, a fusion vector syncretic with semantic features and spatial features.","['G06N3/08', 'G06F16/29', 'G06F16/35', 'G06F18/23213', 'G06F40/126', 'G06F40/30', 'G06N3/045', 'G06N3/084', 'G06N7/01', 'G06N3/048']"
US20250086471A1,Generating small language model via two-phase training,"Systems and methods for generating a small language model are provided. In particular, a computing device may obtain a general dataset including a plurality of general data, annotate a subset of the general dataset based on one or more classifier metrics indicative of a quality of the general dataset, train a classifier based on the annotated subset of the general dataset and the one or more classifier metrics, analyze each general data of the general dataset to determine a score for each of the one or more classifier metrics associated with the respective general data using the trained classifier, generate a filtered general dataset by filtering the general dataset based on one or more filters, train the small language model with the filtered general dataset, generate a synthetic dataset for refining the small language model, and train the small language model with the synthetic dataset.","['G06N3/091', 'G06N3/045', 'G06N3/0475']"
EP4471616A1,Model training method and related device,"This application may be applied to the artificial intelligence field, and specifically discloses a model training method. The method includes: obtaining a second embedding vector input to a decoder in a pre-trained language model, where the second embedding vector corresponds to a second data sequence, the second data sequence includes first sub-data, a masked to-be-predicted data unit, and second sub-data, the first sub-data is located before the to-be-predicted data unit in the second data sequence, and the second sub-data is located after the to-be-predicted data unit in the second data sequence; obtaining a hidden state based on a first embedding vector by using an encoder in the pre-trained language model PLM; and predicting the to-be-predicted data unit based on the first sub-data, the second sub-data, and the hidden state by using the decoder in the PLM and an output layer of the decoder. In this application, a corresponding PLM does not need to be pre-trained for each type of sequence generation task, thereby greatly reducing resources required for training the PLM.","['G06N3/08', 'G06F16/3329', 'G06F16/3344', 'G06N3/044', 'G06N3/045', 'G06N3/0455', 'G06N3/047', 'G06N3/0895']"
US11798529B2,Generation of optimized knowledge-based language model through knowledge graph multi-alignment,"A language module is joint trained with a knowledge module for natural language understanding by aligning a first knowledge graph with a second knowledge graph. The knowledge module is trained on the aligned knowledge graphs. Then, the knowledge module is integrated with the language module to generate an integrated knowledge-language module.","['G10L15/063', 'G10L13/086', 'G06F18/2155', 'G06N3/042', 'G06N3/045', 'G06N3/088', 'G06N5/022', 'G10L13/033', 'G10L13/047', 'G10L15/1815', 'G10L15/16']"
CN118364870A,"Optimization method, device, electronic device and storage medium for large language model","The invention discloses a large language model optimization method, a device, electronic equipment and a storage medium, and relates to the technical field of artificial intelligence, wherein the large language model optimization method specifically comprises the following steps: s10, combining an original LLM model and a large model SFT data set to generate a training data set; s20, embedding a bypass network in a main network of the original LLM model to obtain a new LLM model, and inputting a mask word sequence at the input end of the bypass network; s30, training a new LLM model by adopting a loss function on a training data set, wherein after training, the LLM model can predict a plurality of candidate word sequences in one-time inference; s40, generating the candidate word sequence and verifying the correctness of the candidate word sequence in parallel. The invention has the beneficial effects that: not only saves the resource consumption and time, but also enhances the decoding capability of the LLM model, and also ensures the quality of the output result of the LLM model.","['G06N3/0455', 'G06F40/226', 'G06F40/284', 'G06N3/08']"
US11361571B1,Term extraction in highly technical domains,"A language model is fine-tuned by extracting terminology terms from a text document. The method comprises identifying a text snippet, identifying candidate multi-word expressions using part of speech tags, and determining a specificity score value for each of the candidate multi-word expressions. Moreover, the method comprises determining a topic similarity score value for each of the candidate multi-word expressions, selecting remaining expressions from the candidate multi-word expressions using a function of a specificity value and a topic similarity value of each of the candidate multi-word expressions, adding a noun comprised in the text snippet to the remaining expressions depending on a correlation function, labeling the remaining multi-word expressions, and fine-tuning an existing pre-trained transformer-based language model using as training data the identified text snippet marked with the labeled remaining expressions.","['G06V30/414', 'G06F40/30', 'G06F40/166', 'G06F40/268', 'G06F40/279', 'G06N3/08', 'G06V30/19093', 'G06V30/19147', 'G06V30/1985', 'G06N3/045', 'G06N5/02']"
CN112559556B,Language model pre-training method and system for table mode analysis and sequence mask,"The invention discloses a language model pre-training method and system for table pattern analysis and sequence mask, and belongs to the field of natural language processing. The method comprises the steps of (1) giving a natural language question, an association table and a target SQL sequence, and searching a unit value with the highest overlapping degree with the natural language question; (2) splicing the natural language question sentences, the column names, the column types and the unit values into a long sequence, adding separators and markers, and performing mask processing; (3) according to the mask sequence prediction task, the table mode analysis task and the condition number prediction task, a language model is jointly trained; (4) optimizing the objective function by using a gradient descent algorithm; (5) after training, saving the model weight parameters, and directly adding the model into the existing Text2SQL model to complete the initialization coding of natural language question sentences and database modes.","['G06F16/2433', 'G06F40/30', 'G06N20/00']"
US11900056B2,Stylistic text rewriting for a target author,"Rewriting text in the writing style of a target author is described. A stylistic rewriting system receives input text and an indication of the target author. The system trains a language model to understand the target author's writing style using a corpus of text associated with the target author. The language model may be transformer-based, and is first trained on a different corpus of text associated with a range of different authors to understand linguistic nuances of a particular language. Copies of the language model are then cascaded into an encoder-decoder framework, which is further trained using a masked language modeling objective and a noisy version of the target author corpus. After training, the encoder-decoder framework of the trained language model automatically rewrites input text in the writing style of the target author and outputs the rewritten text as stylized text.","['G06F40/253', 'G06F40/166', 'G06F40/44', 'G06F18/214', 'G06F18/217', 'G06F40/169', 'G06F40/289', 'G06F40/56', 'G06N3/045', 'G06N3/088', 'G06N5/022', 'G06V30/418']"
US20240362286A1,Semantic search and summarization for electronic documents,Techniques for an artificial intelligence (AI) platform to search a document collection are described. Embodiments may use AI and machine learning techniques within a framework of an electronic document management system to perform semantic searching of an electronic document or a collection of electronic documents for certain types of information. The AI platform may summarize the information in a natural language representation of a human language. Other embodiments are described and claimed.,"['G06F16/901', 'G06F16/93', 'G06F16/9538', 'G06N3/045', 'G06N3/08', 'G06N3/088', 'G06N5/022']"
US20220230628A1,Generation of optimized spoken language understanding model through joint training with integrated knowledge-language module,A system is provided for generating an optimized speech model by training a knowledge module on a knowledge graph. A language module is trained on unlabeled text data and a speech module is trained on unlabeled acoustic data. The knowledge module is integrated with the language module to perform semantic analysis using knowledge-graph based information. The speech module is then aligned to the language module of the integrated knowledge-language module. The speech module is then configured as an optimized speech model configured to leverage acoustic and language information in natural language processing tasks.,"['G10L15/063', 'G06F16/90332', 'G06F16/9024', 'G06F40/30', 'G06N3/042', 'G06N3/045', 'G06N3/088', 'G06N5/02', 'G06N5/022', 'G10L15/18', 'G10L15/1822', 'G10L15/183', 'G06F40/216', 'G06F40/284', 'G06F40/35', 'G10L15/16', 'G10L15/1815', 'G10L25/30', 'G10L25/63']"
CN110852086B,"Artificial intelligence based ancient poetry generating method, device, equipment and storage medium","The invention discloses an artificial intelligence based ancient poetry generating method, an artificial intelligence based ancient poetry generating device, artificial intelligence based ancient poetry generating equipment and a storage medium, wherein the method comprises the following steps: obtaining keywords contained in an ancient poetry generation request, determining a target type for generating the ancient poetry according to the keywords, inputting the keywords into a preset transform model, generating a poetry initial draft according to the target type, then performing shielding training by using a pre-trained language model to obtain a temporary poetry initial draft, comparing each predicted character in the temporary poetry initial draft with a corresponding shielded character, if the predicted character is different from the shielded character, continuing performing iterative shielding prediction on the temporary poetry initial draft until each predicted character is the same as the corresponding shielded character, and taking the obtained temporary poetry initial draft as a target poetry. And sequentially carrying out shielding prediction on each character in the temporary poetry initial draft by a loop iteration mode and a pre-trained language model, realizing continuous updating of each character according to the whole context, and improving the quality of generating the ancient poetry.","['G06F40/284', 'G06F40/30']"
US11250839B2,Natural language processing models for conversational computing,"In non-limiting examples of the present disclosure, systems, methods and devices for training conversational language models are presented. An embedding library may be generated and maintained. Exemplary target inputs and associated intent types may be received. The target inputs may be encoded into contextual embeddings. The embeddings may be added to the embedding library. When a conversational entity receives a new natural language input, that new input may be encoded into a contextual embedding. The new embedding may be added to the embedding library. A similarity score model may be applied to the new embedding and one or more embeddings for the exemplary target inputs. Similarity scores may be calculated based on the application of the similarity score model. A response may be generated by the conversational entity for an intent type for which a similarity score exceeds a threshold value.","['G10L15/063', 'G06F40/30', 'G06F40/242', 'G06N3/045', 'G06N3/08', 'G06N5/041', 'G10L15/197', 'G06N3/042', 'G06N3/044']"
CN114547329A,"Method for establishing pre-training language model, semantic analysis method and device","The embodiment of the application discloses a method for establishing a pre-training language model, a semantic parsing method and a semantic parsing device. The technical scheme comprises the following steps: acquiring first training data, wherein the first training data comprises more than one group of multi-turn dialogue sample pairs and mode information of an associated table, and the multi-turn dialogue sample pairs comprise a plurality of sample pairs consisting of natural language texts and corresponding Structured Query Language (SQL) sentences in one dialogue; training by utilizing first training data to obtain a pre-training language model comprising an Embedding (Embedding) layer and a transformation (Transformer) network; in the training process, an input sequence formed by a natural language text, the context of the natural language text and mode information is input into a pre-training language model, and at least one of a session semantic analysis task, a dialogue editing strategy prediction task, a mask prediction task and a column label prediction task is executed. The method and the device are suitable for scenes needing to model the structured table data.","['G06F16/367', 'G06F16/2433', 'G06F40/35']"
US20230274420A1,Method and system for automated generation of text captions from medical images,"Computer implemented method for generating captions for medical images and/or clinical reports are provided. The methods comprise obtaining one or more medical images; using an image processing component to process the one or more images, wherein the image processing component comprises a deep learning model that takes as input the one or more medical images and produces as an output an image feature tensor; and using a natural language processing component to generate a caption for the one or more medical images, wherein the natural language processing component comprises a transformer-based model that takes as input the image feature tensor from the image processing component and produces as output a probability for each word in a vocabulary. Related systems and products are also described.","['G16H50/20', 'G06F40/169', 'G06T7/0012', 'G06F40/30', 'G06F40/56', 'G06N3/045', 'G06N3/08', 'G06V10/469', 'G06V10/761', 'G16H15/00', 'G16H30/40', 'G06N3/0455', 'G06N3/0464', 'G06N3/0499', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004']"
US12198048B2,Modality adaptive information retrieval,"In some embodiments, a multimodal computing system receives a query and identifies, from source documents, text passages and images that are relevant to the query. The multimodal computing system accesses a multimodal question-answering model that includes a textual stream of language models and a visual stream of language models. Each of the textual stream and the visual stream contains a set of transformer-based models and each transformer-based model includes a cross-attention layer using data generated by both the textual stream and visual stream of language models as an input. The multimodal computing system identifies text relevant to the query by applying the textual stream to the text passages and computes, using the visual stream, relevance scores of the images to the query, respectively. The multimodal computing system further generates a response to the query by including the text and/or an image according to the relevance scores.","['G06F16/243', 'G06F16/438', 'G06F18/214', 'G06F18/22', 'G06F40/20', 'G06F40/30', 'G06N3/045', 'G06N3/08', 'G06V10/761', 'G06V20/30', 'G06V30/40']"
WO2024010702A1,Internet of things (iot) systems and methods implementing a data-driven food waste measurement platform and related device(s),"The present application relates generally to method of measuring food waste from recurring food purchase, food donation activities and more specifically to the use of flexible printed electronics integrated to nutritional substance collection bags, boxes or receptacle bins to obtain data from bags or receptacle bins holding nutritional substance waste and the conversion of the data into format suitable for a trained computer vision transformer, a trained machine learning model and machine learning guided discrete event simulation model to obtain the identity of nutritional substance waste, the predicted and estimated economic, social and environmental impact values of nutritional substance waste generative activities.",['G06Q50/26']
US12282411B2,Program improvement using large language models,"Some embodiments generate prompts and submit them in queries to a language model trained on code to perform automated program repair. Some embodiments fix syntactic mistakes and semantic mistakes by combining multimodal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. In some cases, edit distance is minimized between an initial flawed program and the automatically created improved version of that program. The initial flawed program is obtained from a programming student, or from a source code generator.","['G06F11/3608', 'G06F8/42', 'G06F8/71']"
US20240412720A1,Real-time contextually aware artificial intelligence (ai) assistant system and a method for providing a contextualized response to a user using ai,"An artificial intelligence (AI) assistant system and a method for providing a contextualized response to a user using AI are disclosed. The system comprises an audio input device for receiving voice input, an audio output device for providing output, a processor, a wireless communication device, a contextual memory unit for storing conversational context data on a sliding window basis, and a non-volatile system memory unit. The processor executes instructions to receive voice input, determine user identification, update conversational context data with user identification and a tokenized representation of the voice input, process the voice input using a transformer-based language model to generate a response, update the conversational context data with a tokenized representation of the generated response, and output the response via the audio output device. The method comprises receiving voice input, determining user identification, updating conversational context data, processing voice input, and generating and outputting a conversational response.","['G10L13/02', 'G06F40/35', 'G06F16/33295', 'G06F16/90332', 'G06F3/167', 'G10L17/22', 'G10L15/22']"
US20240256792A1,Methods and systems for dynamic generation of personalized text using large language model,"Methods and systems for automatically prompting a LLM to generate a personalized text, such as a personalized textual description, in which portions of the text are customized based on user attributes. In various examples, responsive to a request for a textual description, a user record is retrieved for a user associated with the request and one or more user attributes are obtained based on the user record. In examples, a prompt to a large language model (LLM) for generating a user-specific textual description is generated, the prompt including the one or more user attributes to include in the generated user-specific textual description and a source text. The prompt is provided to the LLM to receive a generated user-specific textual description. The generated user-specific textual description is provided for display via a user device.","['G06F16/337', 'G06F40/40']"
US12373666B2,Convolution-augmented transformer models,"Systems and methods can utilize a conformer model to process a data set for various data processing tasks, including, but not limited to, speech recognition, sound separation, protein synthesis determination, video or other image set analysis, and natural language processing. The conformer model can use feed-forward blocks, a self-attention block, and a convolution block to process data to learn global interactions and relative-offset-based local correlations of the input data.","['G06N3/04', 'G06N20/00', 'G06N3/045', 'G06N3/082', 'G06N3/084', 'G10L15/16', 'G10L21/0272']"
CN116050425A,"Method for establishing pre-trained language model, text prediction method and device","The embodiment of the application discloses a method for establishing a pre-training model, a text prediction method and a device, and relates to the technical field of deep learning. The main technical scheme comprises the following steps: acquiring training data comprising a plurality of first text samples; training a pre-training language model using the training data, the pre-training language model comprising a plurality of hidden layers, the plurality of hidden layers comprising: m transducer network layers and N pooling network layers, wherein M and N are positive integers; the pooling network layer comprises a pooling processing module and a feedforward neural network module, wherein the pooling processing module carries out pooling processing on the characteristic representation input by the pooling network layer, and provides the pooling processed representation for the feedforward neural network module. The semantic understanding method and device can better understand the semantics, and accuracy of the pre-training language model in semantic understanding is improved.","['G06F40/30', 'G06F40/211', 'G06F40/279', 'G06N3/08']"
US20220147838A1,Self-supervised visual-relationship probing,"Methods and systems disclosed herein relate generally to systems and methods for generating visual relationship graphs that identify relationships between objects depicted in an image. A vision-language application uses transformer encoders to generate a graph structure, in which the graph structure represents a dependency between a first region and a second region of an image. The dependency indicates that a contextual representation of the first region was derived, at least in part, by processing the second region. The contextual representation identifies a predicted identity of an image object depicted in the first region. The predicted identity is determined at least in part by identifying a relationship between the first region and other data objects associated with various modalities.","['G06N5/022', 'G06N3/08', 'G06N3/045', 'G06N7/00', 'G06T7/90', 'G06V10/82', 'G06V10/84', 'G06V10/86', 'G06V20/00']"
US20230162481A1,Pre-training of computer vision foundational models,"Examples are provided for pre-training a computer vision foundation model. A representative method comprises curating a pre-training database of image-text pairs from weakly labeled data. Language is encoded of text descriptions from the image-text pairs. The images of the image-text pairs are encoded using a hierarchical vision transformer with shifted windows and convolutional embedding. Based on the encoded images and the encoded language, the computer vision foundation model is pre-trained via unified image-text contrastive learning.","['G06V10/774', 'G06T9/00', 'G06F40/126', 'G06V10/82', 'G06F40/186', 'G06F40/30']"
US20240386015A1,Composite symbolic and non-symbolic artificial intelligence system for advanced reasoning and semantic search,"A semantic search system integrates with an AI platform to provide advanced search capabilities by leveraging automatically generated ontologies and knowledge graphs. The system employs natural language processing, machine learning, and large language models to create, update, and align ontologies from diverse data sources. It supports context-aware query interpretation, personalized results, and complex reasoning by incorporating user context, feedback, and domain knowledge. The system optimizes search performance and efficiency through indexing techniques, distributed computing, and continuous learning. With a modular architecture and scalable infrastructure, the semantic search system enables users to retrieve relevant, meaningful, and context-specific information from vast amounts of structured and unstructured data. The integration of the semantic search system with the AI platform's components, such as knowledge graphs and model blending, enhances the platform's overall reasoning, decision-making, and problem-solving capabilities, empowering users with intelligent and intuitive search experiences across various domains and applications.","['G06F40/30', 'G06F16/245', 'G06F16/248', 'G06F16/9024', 'G06N5/022', 'G06N5/04']"
US12374426B2,Predicting mRNA properties using large language transformer models,"Methods, computer systems, and apparatus, including computer programs encoded on computer storage media, for predicting mRNA properties. The system obtains data representing a codon sequence of the mRNA molecule, generates an input token vector by numerically encoding the codon sequence, and generates an embedded feature vector by processing the input token vector using an embedding machine-learning model having a first set of model parameters.","['G06N3/044', 'G06N3/045', 'G16B20/00', 'G16B30/00', 'G16B40/20']"
US20220382978A1,Training masked language models based on partial sequences of tokens,Embodiments of the present disclosure include systems and methods for training masked language models based on partial sequences of tokens. A sequence of tokens for training a transformer model is received. A defined proportion of the sequence of tokens is selected. Each value of the defined proportion of the sequence of tokens is replaced with a defined value. The transformer model is trained by using the sequence of tokens to train the transformer model during a forward pass and using a subset of the sequence of tokens that includes the defined the proportion of the sequence of tokens to train the transformer model during a backward pass,"['G06F40/284', 'G06N3/045', 'G06N3/084', 'G06N3/063']"
US20240256764A1,Methods and systems for generation of text using large language model with indications of unsubstantiated information,"Methods and systems for prompting a large language model (LLM) to generate a description of an object with indications of any unsubstantiated information are disclosed. A prompt is generated to a LLM to generate a description of an object, where the prompt includes one or more object attributes to include in the generated description. The prompt also includes an instruction for the LLM to annotate any portions of the generated description that are, involve, and/or include unsubstantiated information according to a defined format. The prompt is provided to the LLM and the generated description is received. The generated description is parsed to identify, based on the defined format, one or more annotated portions indicating unsubstantiated information. The generated description is presented for display via a user device.","['G06F40/205', 'G06F16/3328', 'G06F40/169', 'G06F40/40']"
US20240354317A1,Using Fixed-Weight Language Models to Create and Interact with a Retrieval Index,"A technique uses an encoder system to produce an index of target item embeddings. Each target item embedding is input-agnostic and universal in the sense that different expressions of a target concept, produced using different combinations of input modes, map to the same target item embedding in the index. The encoder system throttles the amount of computations it performs based on the assessed capabilities of an execution platform. A retrieval system processes a multimodal input query by first generating a candidate set of target item embeddings in the index that match the input query, and then using a filtering operation to identify those target item embeddings that are most likely to match the input query. The encoder system and the retrieval system rely on language-based components having weights that are held constant during a training operation. Other weights of these systems are updated during the training operation.","['G06F16/313', 'G06F16/3344', 'G06F16/3347']"
CN117954134B,A patient health monitoring and intervention system based on large language model,"The invention relates to a patient health monitoring and intervention system based on a large language model, which comprises a user interaction module, a database module, a user behavior recognition module, a coping strategy selection module and an optimization module, wherein the user interaction module is used for collecting patient descriptions and calling and matching patient information in an electronic medical record system stored in the database module; the user behavior recognition module is used for cleaning and normalizing the collected data; the coping strategy selection module is used for matching the obtained data with the knowledge base, comprehensively evaluating the health state of the patient, converting the matched result into natural language by using a decoder of a large language model, transmitting the evaluation result and response to the user interaction module, and enabling the user interaction module to give feedback to the user so as to assist the attending physician in evaluating the condition of the patient; the main physician evaluates the evaluation results and responses, and the evaluation results are input into an optimization module to help the large language model to further improve the performance.","['G16H80/00', 'G16H40/20', 'G16H50/20', 'G16H50/30']"
US20240256762A1,Methods and systems for prompting large language model to process inputs from multiple user elements,"Methods and systems for prompting a large language model (LLM) to process inputs from multiple user elements to generate a revised block of text are described. One or more text-editing instructions related to respective one or more selected text portions in a block of text are received. A prompt is generated for a LLM to generate a revised block of text, the prompt including at least a portion of an annotated block of text, the annotated block of text including each text-editing instruction inserted into the block of text relative to each respective selected text portion. The prompt is provided to the LLM and a revised block of text is received and outputted.","['G06F40/40', 'G06F40/166', 'G06F40/205']"
CN118761468A,Large-scale language model reasoning optimization method based on token fusion,"The invention discloses a token fusion-based large-scale language model reasoning optimization method, which comprises the steps that a text sequence preprocessed by a word segmentation device enters an embedded layer to be encoded, a word vector and a position vector are generated, and the word vector and the position vector are added to obtain a hidden state as an input matrix of a transform module; performing MHA module calculation, token grouping, token fusion, MLP module calculation and token restoration operation layer by layer aiming at a large-scale language model; repeating the steps until the last layer of the large-scale language model; and the hidden state generated in the last layer enters the tail wiring layer for calculation, and the probability of each vocabulary in the vocabulary is output. The invention uses pruning method to compress LLM, mainly focusing on how to reduce the reasoning delay of large language model. The method has the advantages that the large language model is compressed by using the model compression technology, the calculation amount of model reasoning is reduced, so that the reasoning delay is reduced, the reasoning process is accelerated, and meanwhile, the minimum precision loss can be ensured.","['G06N5/04', 'G06F40/126', 'G06F40/284', 'G06F40/289', 'G06N3/0455', 'G06N3/082']"
CN115631825A,Method and related equipment for automatically generating structured reports using natural language model,"The invention discloses a method for automatically generating a structured report by using a natural language model and related equipment, wherein the method comprises the following steps: acquiring an input sample image, generating a hidden state sequence and a classification identification sequence by an image diagnosis network according to the sample image, and obtaining a prediction result of whether the structure is abnormal; if the prediction result is free of structural abnormality, acquiring an abnormal sample image in the sample image, and generating a medical image report without the abnormal image by a complete report generation network according to the abnormal sample image; and if the prediction result is that structural abnormality exists, acquiring an abnormal sample image in the sample image, and generating a medical image report with the abnormal image by a task perception report generating network according to the abnormal sample image. According to the invention, the medical image report is generated respectively according to whether the sample image has the abnormality, and the accuracy of report diagnosis generated is ensured by respectively generating reports for different structures, so that the workload of doctors is greatly reduced.","['G16H15/00', 'G06T7/0012', 'G06T2207/30004']"
US12346657B2,Transformer assisted joint entity and relation extraction,"Systems and methods are provided for adapting a pretrained language model to perform cybersecurity-specific named entity recognition and relation extraction. The method includes introducing a pretrained language model and a corpus of security text to a model adaptor, and generating a fine-tuned language model through unsupervised training utilizing the security text corpus. The method further includes combining a joint extraction model from a head for joint extraction with the fine-tuned language model to form an adapted joint extraction model that can perform entity and relation label prediction. The method further includes applying distant labels to security text in the corpus of security text to produce security text with distant labels, and performing Distant Supervision Training for joint extraction on the adapted joint extraction model using the security text to transform the adapted joint extraction model into a Security Language Model for name-entity recognition (NER) and relation extraction (RE).","['G06F40/295', 'G06F21/577', 'G06F40/237', 'G06F40/40', 'G06F2221/034']"
CN114944150A,A dual-task-based method for constructing an acoustic model for Conformer land-air communication,"The application relates to the technical field of voice recognition, and discloses a method for constructing a Conformer land-air conversation acoustic model based on double tasks, which comprises the following steps: establishing a land-air communication data set; extracting Fbank characteristics and using the Fbank characteristics as acoustic characteristics of the land-air communication voice signals; carrying out data expansion on the acoustic features; introducing the CNN model into a Transformer model to form a Conformer model; and training the Conformer model by using a CTC model and an LAS model based on the expanded acoustic characteristics to form a Conformer land-air conversation acoustic model based on double tasks of the CTC model and the LAS model. The method and the device have the advantages that the accuracy of voice recognition is high, and the accuracy of land-air communication is improved.","['G10L15/02', 'G06F40/189', 'G06N3/045', 'G06N3/08', 'G10L15/063', 'G10L15/16', 'G10L15/183', 'G10L15/26']"
CN113032545B,Method and system for conversation understanding and answer configuration based on unsupervised conversation pre-training,"The invention provides a dialogue understanding and answer configuration method based on unsupervised dialogue pre-training, which comprises the steps of carrying out dialogue related dialogue information preprocessing, constructing a dialogue input sample with dialogue content, role information and turn information, and carrying out word embedding on the dialogue input information; adding word embedding output results with dialogue text word information, position information, role information and turn information to be used as input of a dialogue pre-training language model encoder; performing pooling processing based on BiLSTM-CNN on the encoding vector output by the model encoder to obtain a dialogue characterization vector; and optimizing the model by utilizing a pre-training learning task generated by any combination of sentence mask modeling at a conversation level, word overall mask modeling and conversation level contrast learning modeling based on reply generation. The invention can effectively solve the problem of multi-turn dialog intention recognition, and can improve the configuration efficiency of answers by a method for generating reply answers.","['G06F16/3329', 'G06F16/3344', 'G06F40/126', 'G06F40/284', 'G06N3/044', 'G06N3/045', 'G06N3/084']"
US11625543B2,Systems and methods for composed variational natural language generation,"Embodiments described herein provide a composed variational natural language generation (CLANG) model that is configured to generate training samples for few-shot intents. Specifically, the CLANG model may build connections between existing training samples of many-shot intents and new training samples of few-shot intents by modeling an intent as a combination of a domain and an action. In this way, the CLANG model transfers knowledge from existing many-shot intents to few-shot intents in natural language generation by learning how to compose utterances with many-shot intents and transferring such knowledge to few-shot intents.","['G06F40/56', 'G06F40/30', 'G06F16/90332', 'G06F40/284', 'G06N20/00', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N7/005', 'G06N7/01']"
US12340571B2,Using intrinsic multimodal features of image for domain generalized,"Various embodiments classify one or more portions of an image based on deriving an â€œintrinsicâ€ modality. Such intrinsic modality acts as a substitute to a â€œtextâ€ modality in a multi-modal network. A text modality in image processing is typically a natural language text that describes one or more portions of an image. However, explicit natural language text may not be available across one or more domains for training a multi-modal network. Accordingly, various embodiments described herein generate an intrinsic modality, which is also a description of one or more portions of an image, except that such description is not an explicit natural language description, but rather a machine learning model representation. Some embodiments additionally leverage a visual modality obtained from a vision-only model or branch, which may learn domain characteristics that are not present in the multi-modal network. Some embodiments additionally fuse or integrate the intrinsic modality with the visual modality for better generalization.","['G06F40/30', 'G06F40/40', 'G06N3/045', 'G06V10/764', 'G06V10/7715', 'G06V10/774', 'G06V10/811', 'G06V10/82', 'G06V10/86']"
US12111751B2,Debugging tool for code generation neural language models,"A debugging tool identifies the smallest subset of an input sequence or rationales that influenced a neural language model to generate an output sequence. The debugging tool uses the rationales to understand why the model made its predictions and in particular, the particular input tokens that had the most impact on the output sequence. In the case of erroneous output, the rationales are used to alter the input sequence to avoid the error or to tailor a new training dataset to retrain the model to improve its performance.","['G06F11/3636', 'G06F8/33', 'G06N3/044']"
US20230207064A1,Inter-model prediction score recalibration during training,"The technology disclosed relates to a system for inter-model prediction score recalibration. The system includes a first model that generates, based on evolutionary conservation summary statistics of amino acids in a reference protein sequence, a first set of pathogenicity scores with rankings for variants that mutate the reference sequence to alternate protein sequences. The system further includes a second model that generates, based on epistasis expressed by amino acid patterns spanning a multiple sequence alignment aligning the reference sequence to non-target sequences, a second set of pathogenicity scores with rankings for the variants. The system further includes a rank loss determination logic that determines a rank loss parameter by comparing the two sets of rankings, a loss function reconfiguration logic that reconfigures a loss function based on the rank loss parameter, and a training logic that uses the reconfigured loss function to train the first model.","['G16B20/20', 'G16B30/00', 'G16B40/00', 'G06F18/2111', 'G06F18/2148', 'G06F18/2155', 'G06N20/00', 'G06N20/20', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N3/084', 'G06N3/126', 'G16B10/00', 'G16B20/00', 'G16B20/40', 'G16B30/10', 'G16B40/20', 'G16B40/30', 'G16B50/10', 'Y02A90/10']"
US20240394477A1,Constructing Prompt Information for Submission to a Language Model by Dynamically Selecting from Context Information,"A technique for interacting with a machine-trained language model uses dynamic prompt management. The technique includes: receiving an input query; accessing a state data store that provides candidate context information; partitioning the candidate context information into plural parts; selecting targeted context information from the candidate context information by determining a semantic relevance of the input query to each of the plural parts by performing vector-based analysis; creating prompt information that includes the input query and the targeted context information; submitting the prompt information to the machine-trained language model; and receiving a response from the machine-trained language model based on the prompt information. The technique has the overall effect of reducing the number of content units submitted to the language model, which, in turn, reduces the amount of resources and time required by the language model to process the input query.","['G06F40/30', 'G06F40/284', 'G06F40/295', 'G06F40/35', 'G06F40/40']"
JP2023047336A,Code search based on multi-class classification,"To provide a method for searching for a source code on the basis of multiple class classification, a storage medium and an electronic device.SOLUTION: A method comprises: a step 402 for receiving a set of natural language (NL) descriptors and a set of corresponding programming language (PL) codes; a step 404 for, on the basis of a first language model, determining a first vector associated with the set of the received NL descriptors; a step 406 for determining a second vector associated with the set of the received PL codes on the basis of the second language model; a step 408 for determining the number of the set of semantic code classes for clustering the set of the PL codes; a step 410 for, on the basis of the determined number, the determined first vector and second vector, clustering the set of the PL codes to the set of the semantic code classes; and a step 412 for training the multiple class classifier model, from the set of the semantic code classes.SELECTED DRAWING: Figure 4","['G06F40/30', 'G06F11/3624', 'G06F16/951', 'G06F18/22', 'G06F18/23213', 'G06F18/285', 'G06F40/166', 'G06F40/211', 'G06F40/216', 'G06F40/242', 'G06F40/40', 'G06F40/44', 'G06F8/30', 'G06F8/36', 'G06F8/42', 'G06F8/65', 'G06F8/73', 'G06N3/04', 'G06N3/0442', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/08', 'G06N3/084', 'G06N7/01', 'G06F8/436', 'G06F8/60']"
CN118070227A,A multimodal data processing method enhanced by large language model,"The invention discloses a large language model enhanced multi-mode data processing method, which comprises the following steps: the fusion model comprises a modal encoder, a modal bridge, a text word segmentation device, a multi-modal large language model and a task head; the modality encoder encodes each modality image data into a token sequence; the mode bridge is used for finishing dimension mapping from each mode to the language mode token; the multi-modal large language model completes analysis of each modal data; designing a specific task head for each task to promote generalization of the model on the task; one to four text prompt words are provided for each mode image data to guide the fusion model to correctly analyze each mode image data. The invention can explain image data of multiple modes by using one model; the large language model is used as a core to construct the universal artificial intelligence, and fusion of image data is promoted to be converted from a model-specific and task-specific paradigm to a universal paradigm.","['G06F18/256', 'G06F40/205', 'G06F40/284', 'G06N3/0455', 'G06N3/08', 'G06V10/764']"
US20250094025A1,Composable low-rank adaptation models for defining large-language model text style,"A computer system maintains low-rank adaptation (LoRA) models, where each LoRA model includes a set of weights configured to modify parameters of a large-language model (LLM) to cause the LLM to generate text having a corresponding property. The computer system presents a set of manipulable user-interface controls that allow configuration of properties of LLM-generated text. Output of the LLM is modified using LoRA models that are selected based on a state of the user-interface controls as manipulated. A preview is provided of LLM output corresponding to the current state of the user-interface controls during presentation and manipulation thereof. To provide this preview, the computer system iteratively provides a prompt to the LLM and outputs the output of the LLM responsive to that prompt for each iteration. For each iteration, the LLM output is modified using the LoRA models selected based on the current state of the user-interface controls as manipulated.","['G06F3/0484', 'H04L51/02', 'G06F3/0482', 'G06F3/04847', 'G06F40/30', 'G06F40/56', 'G06N20/00', 'G06N3/045', 'G06N3/047']"
CN117150145B,Personalized news recommendation method and system based on large language model,"The invention discloses a personalized news recommending method and system based on a large language model, and relates to the technical field of news recommending. The method comprises the steps of obtaining news reading historical data of a user and encrypted account number sequence information data; extracting features of the acquired data to obtain news coding features and user coding features; constructing a mask large language model of a bidirectional coding representation thought paradigm based on a Transformer, and carrying out model training by taking news coding features and user coding features as model inputs; and performing personalized news recommendation according to the user news reading historical data and the encrypted account number sequence information data by using a trained mask large language model which is based on the bidirectional coding representation thought paradigm of the Transformer. According to the invention, the reading habit and the interest of the user can be more completely characterized and understood through the news recommendation large language model based on the mask attention according to the news reading history of the user, the user serial number and the like, and recommendation information with higher accuracy can be predicted in candidate news.","['G06F16/9535', 'G06F21/602', 'G06F40/126', 'G06N3/0455', 'G06N3/0464', 'G06N3/0499', 'G06N3/084', 'Y02D10/00']"
US12182506B2,Systems and methods for dynamic large language model prompt generation,"A website development system automatically generates text for a webpage. The system obtains a prompt template associated with a section of the webpage, where the prompt template includes one or more parameters. Based on the webpage, the prompt template determines a first value for a first one of the one or more parameters. A request to provide input for a second value of a second parameter is sent for display to a user. Using the prompt template, the first value, and the second value, the system generates a prompt to a large language model to generate text for the section of the webpage.","['G06F16/958', 'G06F40/106', 'G06F40/186', 'G06F40/20', 'G06F8/35']"
US20240311546A1,Methods and systems for prompting large language model to generate formatted output,"Methods and systems for prompting a large language model (LLM) to generate a revised text passage with formatting are described. A text-editing instruction is received that is related to at least a portion of a text passage having at least one formatting tag. The text passage is processed to identify the at least one formatting tag in the text passage. A prompt to the LLM is generated, to cause the LLM to generate a revised text passage. The prompt includes the text-editing instruction related to at least the portion of the text passage and also includes a formatting-specific instruction to format the revised text passage using the at least one formatting tag in the revised text passage. The revised text passage is received and caused to be displayed based on the formatting tag.","['G06F40/106', 'G06F40/103', 'G06F40/166', 'G06F40/169', 'G06F40/205']"
US11687835B2,Domain specific pre-training of cross modality transformer model,"A transformer based vision-linguistic (VL) model and training technique uses a number of different image patches covering the same portion of an image, along with a text description of the image to train the model. The model and pre-training techniques may be used in domain specific training of the model. The model can be used for fine-grained image-text tasks in the fashion domain.","['G06V10/82', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06V10/774', 'G06F2218/12', 'G06N3/0442', 'G06N3/0464']"
EP4120286A1,Method of evaluating text similarity for diagnosis or monitoring of a health condition,"The invention relates to a computer implemented method of training a machine learning model to evaluate the similarity of a candidate text to a reference text for determining or monitoring a health condition, where the model takes a text comparison pair comprising a reference text and a candidate text, each comprising data encoding a text sequence, the method comprising: pre-training an edit encoder to learn to generate an edit-space representation of an input text comparison pair, where the edit-space representation encodes information for mapping the reference text to the candidate text, the edit encoder comprising a machine learning model; and performing task-specific training by adding a task-specific network layer and training the task-specific network layer to map an edit-space representation generated by the pre-trained edit encoder to an output associated with a health condition. Edit-space representations learned in this way are able to encode a greater range of changes in language use than known metrics used to evaluate machine translations.","['G16H50/70', 'G06F40/166', 'G06F40/284', 'G06F40/30', 'G06N20/00', 'G06N3/0455', 'G06N3/082', 'G06N3/0895', 'G06N3/096', 'G16H10/60', 'G16H50/20']"
US20230377748A1,A Neural Graph Model for Automated Clinical Assessment Generation,"Embodiments generate medical support text, e.g., assessments and plans, based on patient medical data. One such embodiment begins by receiving medical data for a given patient. Next, a patient knowledge graph for the given patient is generated based on the received medical data and an expanded graph is generated by expanding the patient knowledge graph based upon supplementary data. In turn, the medical support text for the given patient is generated based upon the expanded graph.","['G16H50/20', 'G06F40/40', 'G06N3/045', 'G06N3/08', 'G06N5/022', 'G16H10/60', 'G16H50/70', 'G16H70/60']"
US11487999B2,Spatial-temporal reasoning through pretrained language models for video-grounded dialogues,A system and method for generating a response in a video grounded dialogue are provided. A video-grounded dialogue neural network language model receives video input and text input. The text input includes a dialogue history between the model and a human user and a current utterance by the user. Encoded video input is generated using video encoding layers. Encoded text input is generated using text encoding layers. The encoded video input and the encoded text input are concatenated in to a single input sequence. A generative pre-trained transformer model generates the response to the current utterance from the singe input sequence.,"['H04N19/46', 'G06F40/40', 'G06N3/045', 'G06N3/047', 'G06N3/049', 'G06N3/08', 'G06V10/764', 'G06V10/82', 'G06V20/40', 'G06V20/46', 'H04N19/31', 'H04N19/33', 'H04N19/90', 'G06F40/30']"
US20240370479A1,Semantic search and summarization for electronic documents,Techniques for an artificial intelligence (AI) platform to search a document collection are described. Embodiments may use AI and machine learning techniques within a framework of an electronic document management system to perform semantic searching of an electronic document or a collection of electronic documents for certain types of information. The AI platform may summarize the information in a natural language representation of a human language. Other embodiments are described and claimed.,"['G06V30/416', 'G06F16/316', 'G06F16/3347', 'G06F40/30']"
US20250004918A1,Debugging tool for code generation neural language models,"A debugging tool identifies the smallest subset of an input sequence or rationales that influenced a neural language model to generate an output sequence. The debugging tool uses the rationales to understand why the model made its predictions and in particular, the particular input tokens that had the most impact on the output sequence. In the case of erroneous output, the rationales are used to alter the input sequence to avoid the error or to tailor a new training dataset to retrain the model to improve its performance.","['G06F11/3604', 'G06F11/3636', 'G06N3/042', 'G06N3/044', 'G06N3/0455', 'G06N3/105', 'G06F11/3698', 'G06F8/33', 'G06N3/084', 'G06N3/0985', 'G06N5/045']"
CN118779438A,Data intelligent question answering method and system integrating domain knowledge,"The invention provides a data intelligent question-answering method and system integrating domain knowledge, which relate to the technical field of intelligent question-answering and comprise the steps of obtaining a question in a natural language form proposed by a user, conducting semantic enhancement on the question by utilizing a pre-trained language model to obtain a semantic vector representation integrating key information in the question, conducting multi-hop addressing in a pre-constructed multi-granularity knowledge graph according to the semantic vector representation, retrieving a plurality of candidate answer subgraphs which are most related to the question in terms of semanteme, conducting matching calculation on the semantic vector representation of the question and semantic embedded vectors of each candidate answer subgraph to obtain semantic relevance scoring of each candidate answer subgraph and the question, constructing a ranking model based on reinforcement learning, conducting optimized ranking on the candidate answer subgraphs by utilizing the ranking model according to the semantic relevance scoring to obtain a ranking list.","['G06F16/3329', 'G06F16/3344', 'G06F16/3346', 'G06F16/35', 'G06F16/367', 'G06N3/042', 'G06N3/0442', 'G06N3/045', 'G06N3/0464', 'G06N3/048', 'G06N3/08', 'Y02D10/00']"
US20250157235A1,Semantic labeling of images with generative language model,"A computing system including one or more processing devices configured to receive an image. The processing devices are further configured to compute a segmentation mask that identifies a region of interest included in the image. At a feature extractor, the processing devices are further configured to compute encoded image features based on the image. The processing devices are further configured to receive a text instruction. At a visual resampler, the processing devices are further configured to compute a mask query based on the segmentation mask, the encoded image features, and the text instruction. At a generative language model, the processing devices are further configured to receive a natural language query that includes the mask query and the text instruction. Based on the natural language query, at the generative language model, the processing devices are further configured to generate and output a semantic label associated with the region of interest.","['G06V10/774', 'G06V20/70', 'G06F16/3344', 'G06F40/30', 'G06N3/045', 'G06N3/0455', 'G06N3/0499', 'G06N3/084', 'G06V10/25', 'G06V10/26', 'G06V10/267', 'G06V10/40', 'G06V10/7715', 'G06V10/82', 'G06V20/50']"
CN118035473A,"Multi-mode file retrieval method, system and medium based on large language model","The invention discloses a multi-mode file retrieval method, a system and a medium based on a large language model, wherein the method comprises the following steps: acquiring a multi-modal file comprising an image file and an audio file; processing the image file through a preset Donut model to obtain a first text file, and processing the audio file through a preset Whisper model to obtain a second text file; obtaining a plurality of candidate retrieval results according to the similarity between the query sequence and the first text file and the second text file; slicing the candidate search results to obtain a plurality of candidate search fragments, and inputting the candidate search fragments and a preset prompting template into a large language model to obtain target search results. The invention can realize multi-mode file retrieval on one hand, and can automatically correct errors according to the file context and accurately retrieve according to the user requirement on the other hand, has high reliability and retrieval precision, and can be widely applied to the field of file retrieval technology processing.","['G06F16/48', 'G06F16/68', 'G06F16/7844', 'G06F40/30', 'G06N3/0455', 'G06N3/084']"
CN116757652B,Online recruitment recommendation system and method based on large language model,"The application discloses an online recruitment recommendation system based on a large language model, which comprises an input construction module and a large language model fine tuning and recommendation module, wherein the input construction module extracts heterogeneous graphs of job seekers and recruiters according to an online recruitment platform interaction database, and converts interaction semantic paths existing in the heterogeneous graphs into a natural language form to be input into the large language model; the large language model fine tuning and recommending module adopts online recruitment data to conduct parameter fine tuning training, and the trained recommending model is used for processing input text features to generate corresponding recommending results and decision reasons. The application also discloses an online recruitment recommendation method based on the large language model and a bidirectional reciprocity recommendation system for simultaneously serving the job seeker and the recruiter. The beneficial effects of the application are as follows: the user can intuitively understand the reason of recommending the result, so that the user can trust the recommending result more; at the same time, it is good at handling recommendations of new posts or recruiters.","['G06Q10/105', 'G06F40/279', 'G06F40/30', 'G06N3/0455', 'G06N3/08', 'Y02D10/00']"
CN116797417A,An intelligent assistance system based on large language models,"The invention provides an intelligent auxiliary system based on a large language model, which comprises: vocabulary generation module: generating a vocabulary table through a character pair coding algorithm; an input sequence generation module: word segmentation is carried out according to a vocabulary table to obtain an input sequence; the output result display module: and inputting the input sequence into the language model to obtain an output result, and displaying the output result. The invention utilizes advanced natural language processing technology and machine learning algorithm to realize deep understanding and personalized coaching learning of course content. According to the invention, through real-time answering of student questions, automatic generation of personalized practice problems and automatic summarization and refinement of course contents, the problem of uneven educational resource allocation is greatly improved, the educational quality is improved, and a better learning experience is provided for students.",[]
WO2024193382A1,Knowledge injection and training methods and systems for knowledge-enhanced pre-trained language model,"Knowledge injection and training methods and systems for a knowledge enhanced pre-trained language model are disclosed. The methods comprise: on the basis of a domain knowledge graph, identifying important elements in a pre-training statement, injecting the important elements into training data of a pre-trained language model to obtain knowledge-enhanced training data, and inputting the knowledge-enhanced training data into the pre-trained language model; for each layer of a feed-forward network, on the basis of the inputted training data and a knowledge evaluation target, determining the probability of generating a correct answer to the knowledge evaluation target, then determining a knowledge attribution score in respect of each neuron according to the probability; then, from each layer, taking neurons that have knowledge attribution scores that meet a preset condition as knowledge neurons so as to generate a knowledge path comprising said knowledge neurons; and on the basis of a preset loss function, updating parameters of the knowledge path. Knowledge noise is reduced by means of learning according to the weight of the injected knowledge, robustness is high, and the knowledge path can be identified so as to reduce the amount of updated system parameters, thus reducing resource consumption.","['G06F16/367', 'G06F40/216', 'G06F40/295', 'G06F40/30', 'G06N3/0442', 'G06N3/0455', 'G06N3/0499', 'G06N3/084', 'G06N3/0985']"
US12380880B2,End-to-end automatic speech recognition with transformer,"An end-to-end automatic speech recognition (ASR) system can be constructed by fusing a first ASR model with a transformer. The input of the transformer is a learned layer generated by the first ASR model. The fused ASR model and transformer can be treated as a single end-to-end model and trained as a single model. In some embodiments, the end-to-end speech recognition system can be trained using a teacher-student training technique by selectively truncating portions of the first ASR model and/or the transformer components and selectively freezing various layers during the training passes.","['G10L15/16', 'G06N3/045', 'G10L15/02', 'G10L15/063', 'G10L15/26', 'G10L2015/025', 'G10L2015/0633']"
US20240160902A1,Similarity-based generative ai output filtering,"Methods and systems for generating output content using a generative artificial intelligence (AI) model based on an input. A similarity-assessment layer at the output of the generative AI model determines a similarity measure for the output content vis-Ã -vis pre-existing items in a repository. The similarity measure is compared to a threshold value and, responsive to the comparison indicating excessive similarity, one or both of the input and the generative AI model are adjusted, and the generative AI model is re-run to generate new output content.","['G06N3/0455', 'G06N20/00', 'G06N3/044', 'G06N3/047', 'G06N3/0475', 'G06N3/084', 'G06N3/088', 'G06N3/0895', 'G06N3/09', 'G06N3/0985', 'G06N3/006', 'G06N3/0464', 'G06N3/0499', 'G06N3/092']"
US20230169281A1,Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation,"Representation learning for text and speech has improved many language-related tasks. However, existing methods only learn from one input modality, while a unified representation for both speech and text is needed for tasks such as end-to-end speech translation. Consequently, these methods cannot exploit various large-scale text and speech data and their performance is limited by the scarcity of parallel speech translation data. To address these problems, embodiments of a fused acoustic and text masked language model (FAT-MLM) are disclosed. FAT-MLM embodiments jointly learn a unified representation for both acoustic and text input from various types of corpora including parallel data for speech recognition and machine translation, and pure speech and text data. Within this cross-modal representation learning framework, an end-to-end model is further presented for fused acoustic and text speech translation. Experiments show that by fine-tuning from FAT-MLM, the speech translation model embodiments substantially improve translation quality.","['G06F40/58', 'G06F40/44', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G10L15/005', 'G10L15/02', 'G10L15/063', 'G10L15/16', 'G10L15/28', 'G10L19/16', 'G10L25/24', 'G10L25/27', 'G06F40/216', 'G06F40/284']"
US20230281400A1,Systems and Methods for Pretraining Image Processing Models,"Example embodiments of the present disclosure relate to systems and methods for pretraining image-processing models on weakly-supervised image-text pairs. The pretraining can include receiving a training sequence for the machine-learned image-processing model. The training sequence can include text tokens and image tokens. A prefix sequence can contain the image tokens. A remainder sequence can include a remainder set of the text tokens. The pretraining can include determining, using the prefix sequence as an input to the machine-learned image-processing model, an objective based on recovery of the remainder sequence. The pretraining can include updating one or more learnable parameters of the machine-learned image-processing model based on the objective.","['G06V10/82', 'G06F40/284', 'G06V10/766', 'G06V30/10', 'G06F40/58']"
US12067369B2,System and method for language translation,"Provided are computer implemented systems and methods for providing improved language translation, including translation of input text to an output translated text Selement. This may include providing for rule-based language translation. This may further include a plurality of machine translation models adapted to different translation domains. A machine translation selector is described that selects an in-domain machine translator from the plurality of machine translation models based on an input text element. The selected in-domain machine translator may generate a first translated text element from the input text element. A post-editor module may generate a second translated text element by predicting a post-edit to the first translated text element. The first translated text element and the second translated text element are evaluated by a quality evaluation model that determines a first metric associated with the first translated text element and a second metric associated with the second text element.","['G06F40/47', 'G06F40/211', 'G06F40/42', 'G06F40/44', 'G06F40/51', 'G06F40/55', 'G06N20/00', 'G06N3/04', 'G06N3/044', 'G06N3/082', 'G06N3/086', 'G06N5/025', 'G06N7/01', 'G06F40/20']"
CN111104789B,"Text scoring method, device and system","The invention provides a text scoring method, a text scoring device and a text scoring system; the method comprises the following steps: acquiring a text to be scored; extracting text characteristics of a text; text features include shallow language features, syntactic features, semantic features, and theme features; the semantic features are used for representing semantic consistency in the text; the theme characteristics are used for representing the relativity between the text and a preset text theme; inputting the text characteristics into a preset scoring model to obtain an output result; and determining the score of the text according to the output result. The method comprises the steps of extracting shallow language features, syntax features, semantic features and theme features from a text to be scored to serve as text features, inputting the text features into a preset scoring model, and taking an output result output by the scoring model as a score of the text. In the method, comprehensive evaluation analysis is performed on four aspects of shallow language features, syntax features, semantic features and theme features, so that reliability of a scoring result can be enhanced.","['G06F16/3344', 'Y02D10/00']"
US20230084333A1,Adversarial generation method for training a neural model,"Methods and systems for training a neural language model. Clean sequence pairs are received including clean source and target sequences. For each clean sequence pair, a noisy version is sampled with an adversarial generator to generate a noisy sequence pair. Parameters of the neural language model are optimized on the clean and noisy sequence pairs. Parameters of the adversarial generator are optimized to minimize a modeling loss of the adversarial generator and maximize a neural language loss of the neural language model using backpropagation.","['G06F40/58', 'G06N3/09', 'G06N3/0454', 'G06N3/0455', 'G06N3/0472', 'G06N3/0475', 'G06N3/084', 'G06N3/094', 'G06F40/42']"
CN110941945B,Language model pre-training method and device,"The application discloses a language model pre-training method and device. One embodiment of the method comprises: acquiring an initial language model, wherein the initial language model is used for determining the incidence relation among words contained in text data input to the initial language model; acquiring a training sample set for training an initial language model, wherein training samples in the training sample set comprise sample data, first labeling information and second labeling information; and pre-training the initial language model, the initial denoising self-coding model and the initial sequence-to-sequence model to obtain a trained language model. The implementation mode can display the rhyme information of the captured text, and effectively improves the semantic representation effect of the output text of the language model.",[]
CN114238649A,Common sense concept enhanced language model pre-training method,"The invention discloses a common sense concept enhanced language model pre-training method, which comprises the following steps: step 1) collecting a corpus set, and constructing an unsupervised corpus set based on common sense concepts, wherein the unsupervised corpus set comprises a plurality of sentences, and each sentence comprises a plurality of common sense concepts and the position of each common sense concept in the sentence; step 2) based on the unsupervised corpus, randomly covering the common sense concept to form a training sample, inputting the training sample into a pre-established language model for training, wherein the training target is to predict the covered common sense concept, so as to obtain a pre-training language model with enhanced common sense concept; and 3) obtaining a prediction sequence of language modeling by using the common sense concept enhanced pre-training language model. The invention effectively strengthens the common sense comprehension capability of the pre-training language model, and experiments prove that the pre-training language model with enhanced common sense concepts is finely adjusted on the common sense question-answering task, so that the question-answering accuracy is obviously improved.","['G06F16/367', 'G06F40/211', 'G06N3/045', 'G06N3/088']"
US20250158638A1,Federated latent transformer deep learning core,"A system and method for a federated deep learning platform utilizing homomorphically-compressed and encrypted data. The system comprises multiple client devices, each with a local dataset, and a central server hosting a deep learning core. Client devices convert local data into codewords, which are also homomorphically encrypted. The central server processes these encrypted codewords without decryption, preserving data privacy. The platform supports at least two architectural variants: a conventional Transformer trained on codewords, and a Latent Transformer operating on latent space vectors. Both variants eliminate the need for embedding and positional encoding layers. The system aggregates encrypted model updates from clients, enabling collaborative learning while maintaining data confidentiality. Additional features comprise differential privacy implementation and adaptive federated optimization techniques. This innovative approach allows for efficient, privacy-preserving distributed learning across diverse datasets, addressing key challenges in federated learning such as data heterogeneity, non-IID distributions, and communication efficiency.","['H03M7/3059', 'G06N20/00', 'G06N3/045', 'G06N3/084', 'H03M7/6005']"
US11922947B2,Systems and methods for configuring and using an audio transcript correction machine learning model,"A system, method, and computer-program product includes constructing a transcript correction training data corpus that includes a plurality of labeled audio transcription training data samples, wherein each of the plurality of labeled audio transcription training data samples includes: an incorrect audio transcription of a target piece of audio data; a correct audio transcription of the target piece of audio data; and a transcript correction identifier that, when applied to a model input that includes a likely incorrect audio transcript, defines a text-to-text transformation objective causing an audio transcript correction machine learning model to predict a corrected audio transcript based on the likely incorrect audio transcript; configuring the audio transcript correction machine learning model based on a training of a machine learning text-to-text transformer model using the transcript correction training data corpus; and executing the audio transcript correction machine learning model within a speech-to-text post-processing sequence of a speech-to-text service.","['G06N3/045', 'G10L15/26', 'G10L15/02', 'G10L15/04', 'G10L15/16', 'G10L15/28', 'G10L25/30', 'G10L25/78', 'G06N3/084', 'G10L2025/783']"
WO2024197740A1,Method and apparatus for generating common-sense after-class exercise in low-resource scenario,"The present invention relates to the technical field of artificial intelligence. A method and apparatus for generating a common-sense after-class exercise in a low-annotation resource scenario are disclosed, realizing automatic question generation by a machine. By means of transferring knowledge from a pre-trained language model, a large-scale knowledge base is provided; by means of decoupled learning of a small amount of data, key questioning factors are captured to achieve generalized generation in a manner of drawing inferences about other cases from one instance. The scale of parameter tuning is further reduced by means of prompt learning and the like. In addition, the present invention also constructs an adversarial framework-based validator to provide feedback on common-sense inferability, thereby generating grammatically correct and logically consistent results. The present invention can transfer rich semantic knowledge from external pre-trained semantic models, and by learning key questioning patterns in a decoupled prior dense latent space, effectively alleviates the problem of resource scarcity. The present invention also develops an adversarial framework with a differentiable validator to guide the generator to produce results with good common sense and logical consistency.",['G06F16/33']
US20240143691A1,Universal transformers,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for implementing a sequence-to-sequence model that is recurrent in depth while employing self-attention to combine information from different parts of sequences.","['G06N3/044', 'G06F17/14', 'G06N3/04', 'G06N3/0455', 'G06N3/0895', 'G06N3/0985']"
US20240184829A1,Methods and systems for controlled modeling and optimization of a natural language database interface,"Systems and methods are disclosed for training and deployment of machine learning-based models that dynamically translate natural language to database query language, and include automation of training data generation, query representation language, and adaptive model training. A method for generating datasets for a natural language interface to a database includes a database query builder, which receives insights regarding the database, and based at least in part on the database insights builds a plurality of database queries. The method further includes generating a data distribution of natural language queries paired with corresponding database queries by, for each one of the plurality of database queries, pairing the database query to a natural language query and one or more paraphrases of the natural language query, and projecting the data distribution onto a plurality of segmented text distributions and applying one or more control signals to generate an optimal training data distribution.","['G06F16/243', 'G06F16/90332', 'G06N3/0455', 'G06N3/0895', 'G06N5/02']"
US20230080674A1,Systems and Methods for Automated Generation of Passage-Based Items for Use in Testing or Evaluation,"Systems, apparatuses, and methods for automatically generating text items that may be used on an exam or test. In some embodiments, the text items may take the form of a question or statement. The exam or test may be used for evaluating a test-taker's knowledge, proficiency, reading comprehension, or other similar purpose or goal.","['G09B7/06', 'G06F40/279', 'G06F16/3329', 'G06F40/284', 'G06F40/56', 'G06N3/08', 'G06F40/253', 'G06F40/30', 'G06N3/045', 'G09B5/065']"
CN118113481B,"Operation method, system, product, equipment and medium of large language model","The invention discloses a method, a system, a product, equipment and a medium for operating a large language model, and relates to the technical field of model reasoning. In the method, the pre-filling and decoding in the reasoning process are carried out on different systems, so that the pressure of single equipment for processing reasoning is reduced, and the long-sequence reasoning can be completed as much as possible; all sequences are utilized for prefilling, so that the prefilling precision is improved; the method comprises the steps of determining a pre-filling distributed system according to pre-filling calculation characteristics and determining a decoding system according to decoding calculation characteristics, so that each system meets the requirements of a corresponding reasoning process, efficiency differences of different calculation stages are balanced, waste of resources is reduced, and efficiency of the whole reasoning process is improved; the computing devices in the pre-filling distributed system are interconnected through an infinite bandwidth network, so that the data transmission efficiency is improved when each computing device sends the key value blocks to other computing devices of the pre-filling distributed system, and the pre-filling efficiency is further improved.","['G06F9/5016', 'G06F9/5061', 'G06F9/5083', 'G06N5/04', 'Y02D10/00']"
US12124814B2,Self-attention-based confidence estimation of language models,"A confidence estimation system includes: a neural network including at least one an attention module including N heads configured to: generate attention matrices based on interactions between tokens for words in an input sequence of words, the input sequence of words including a word that is obscured; and determine the word that is obscured in the input sequence; and a confidence module configured to determine a confidence value indicative of a probability of the neural network correctly determining the word that is obscured, the confidence module determining the confidence value of the word that is obscured using a convolutional neural network that projects the attention matrices generated by the attention module over a multi-dimensional space, the attention matrices recording interactions between the tokens in the input sequence of words without information regarding the tokens for the words and the word that is obscured.","['G06F40/58', 'G06F16/3329', 'G06F16/90332', 'G06F40/30', 'G06N3/0455', 'G06N3/0464', 'G06N3/0895', 'G06N3/10', 'G06N3/042']"
CN117829280A,A knowledge graph completion method based on pre-trained language model combined with contrastive learning,"A training language model knowledge graph completion method combining contrast learning comprises the following steps: step1, collecting a knowledge graph data set; step2, converting a data set in the knowledge graph into a text vector suitable for being input by a Bert model, and encoding by using a Bert double encoder; step3, using star-transformer model coding to pay attention to local information of the text; step4, calculating cosine similarity of embedding of a head entity and a relation and embedding of a tail entity; step5, in order to improve the learning efficiency of positive and negative samples, three contrast learning methods are introduced; step6, setting a training target of the model. The training language model knowledge graph complement model combined with contrast learning can effectively capture text information in the knowledge graph, can learn local information of the text more easily by introducing star-transducer, and can achieve good effect by means of the characteristic of distinguishing positive and negative samples by contrast learning.","['G06N5/022', 'G06F16/288', 'G06F16/9024', 'G06F40/126', 'G06F40/30', 'G06N3/045', 'G06N3/0895']"
US12399991B2,Signing large language model prompts to prevent unintended response,"A technique to prevent a prompt injection attack utilizes a security agent to sign a large language model prompt with a secret that is isolated from the user application or device that generates a user prompt. The secret is tailored for a specific user identifier and session identifier. The large language model is instructed to repeat the secret in each response. The security agent retrieves the response from the large language model and checks for the secret. When the secret is not part of the response, an error message is forwarded to the user application instead of the response.","['G06F21/56', 'G06F2221/034']"
WO2024243106A1,Constructing prompt information for submission to a language model by dynamically compressing source,"A technique for interacting with a machine-trained language model uses dynamic prompt management. The technique includes: receiving an input query and creating prompt information that expresses the input query and targeted context information. The targeted context information is selected from candidate context information. Further, a part of the prompt information is formed by compressing source information by reducing a number of content units in the source information (where the source information includes the input query and/or the candidate context information). The method further includes submitting the prompt information to the machine-trained language model, and receiving a response from the machine-trained language model based on the prompt information. The technique has the overall effect of reducing the number of content units submitted to the language model, which, in turn, reduces the amount of resources and time required by the language model to process the input query.","['G06F16/3329', 'G06F40/35']"
US20250094728A1,Summary of reviews generated by a generative language model,"A computer-implemented method is provided. The computer-implemented method may include: assigning relevancy values to reviews of a plurality of reviews; aggregating a subset of reviews from the plurality of reviews into an input prompt, the subset of reviews selected based on the relevancy values assigned to reviews in the plurality of reviews; and inputting the input prompt into a generative language model yielding a summary review of the subset of reviews generated by the generative language model.","['G06F40/166', 'G06F40/284', 'G06F40/40', 'G06F40/56', 'G06Q30/0282', 'G06F40/30']"
CN111444730A,Data-enhanced Uyghur-Chinese machine translation system training method and device based on Transformer model,"The invention discloses a method and a device for training a data-enhanced Wei-Han machine translation system based on a Transformer model, wherein the Transformer model consists of an encoder and a decoder, the left half part of the model is an encoder end and consists of 6 identical layers, and each layer consists of two sub-layers. The right half is the decoder side, which consists of 6 identical layers, each layer consisting of three sub-layers. The problem of poor translation performance of the neural machine translation model under the condition of resource shortage is greatly improved, and the generalization capability of the model is improved. Experimental results show that data are forged by 17 ten thousand pairs of Wei-Han parallel linguistic data and a translation model is trained, and finally the translation quality is improved to a certain extent.",[]
US12149558B1,Cybersecurity architectures for multi-contextual risk quantification,"The present disclosure relates to cybersecurity architectures and systems for assessing and quantifying security threats and risks associated with machine-readable codes, such as quick response codes, barcodes, data matrix codes, and other types of codes. A security application comprises a multi-context threat assessment system configured to analyze a broad spectrum of risk assessment attributes across multiple contexts. These contexts relate to the machine-readable code itself, target network resources identified by the code, entities affiliated with the code, end-users interacting with the code, and enterprise systems policies. The system can evaluate various risk assessment attributes for each of these contexts to more accurately quantify potential security risks associated with the machine-readable codes. The security application further includes an API for extending its threat assessment capabilities to various digital ecosystems and an AI-powered learning network comprising language models and computer vision systems to enhance threat detection and risk quantification capabilities.",['H04L63/1433']
US20240105292A1,Platform for synthesizing high-dimensional longitudinal electronic health records using a deep learning language model,"In one aspect, the present disclosure relates to a platform for creating synthetic electronic health records, the platform being configured to perform operations including receiving EHR data and encoding the received EHR data as a plurality of fixed length vectors to form a fixed-length matrix. The platform provides the fixed-length matrix to a machine learning model as input to produce a plurality of visit history representations. For one or more particular visit history representations, of the plurality of visit history representations, the platform applies code information associated with the particular visit history. One or more appended visit histories are provided to one or more masked linear layers to produce a probability matrix comprising probabilities for each code for each visit. The platform produces one or more synthetic EHRs based on repeated sequential generation of and sampling from the probability matrix.","['G16H10/60', 'G16H40/67', 'G16H50/20', 'G16H50/70']"
US20230100303A1,Fractional inference on gpu and cpu for large scale deployment of customized transformers based language models,"Systems and methods for fractional inference on GPU and CPU for large scale deployment of customized transformers based language models are disclosed herein. The method can include, receiving data for use in generation of a machine learning model output, ingesting the data with a first machine learning model on a Graphic Processing Unit, receiving at least one intermediate output from the first machine learning model at a temporary store, receiving the at least one intermediate output from the temporary store at a Central Processing Unit, ingesting the at least one intermediate output with a second machine learning model on the Central Processing Unit, and outputting a prediction with the second machine learning model.","['G06N3/0454', 'G06N3/045', 'G06K9/6267', 'G06F15/78', 'G06T1/20']"
US20240354176A1,Notification messages generated by a generative language model,"One or more computer systems which provide services to a user may react to at least one event occurring in the systems by sending notification messages to the user. It may be undesirable for the user to receive a multitude of such notification messages over a short period of time, which are caused by a same or related event, and/or which provide conflicting instructions. In some embodiments, a notification server may: aggregate a plurality of event messages to form an input prompt, the plurality of event messages associated with at least one event occurring in at least one computer system; input the input prompt into a generative language model to generate a notification message based on the plurality of event messages; and transmit the notification message to a user device instead of the plurality of event messages.","['G06F9/542', 'G06Q10/08', 'G06F9/546', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N3/088']"
CA3155809A1,Acoustic and natural language processing models for speech-based screening and monitoring of behavioral health conditions,The present disclosure provides acoustic and natural language processing (NLP) models for predicting whether a subject has a behavioral or mental health state of interest based at least in part on input speech from said subject.,"['G10L25/63', 'A61B5/0077', 'A61B5/165', 'A61B5/4803', 'A61B5/7267', 'G10L15/26', 'G10L25/30', 'G10L25/66', 'A61B2562/0204']"
US20250148276A1,All-shot training of large language models,"Embodiments described herein provide systems and techniques for training large language models. In one aspect, a process for performing in-context training of a language model is disclosed. This process may begin by receiving a language model that includes a context window of a predetermined size, as well as receiving a set of in-context prompt/completion pairs prepared for a target task. The process then constructs a first token sequence based on the set of in-context prompt/completion pairs. Next, the process fits the first token sequence into the context window. The process subsequently performs a first in-context training pass using the first token sequence to train the language model to generate a next token in accordance with the target task.","['G06N3/08', 'G06N3/084', 'G06F16/53']"
US20230229960A1,"Systems and methods for facilitating integrative, extensible, composable, and interpretable deep learning","Some disclosed systems are configured to obtain a knowledge module configured to receive one or more knowledge inputs corresponding to one or more different modalities and generate a set of knowledge embeddings to be integrated with a set of multi-modal embeddings generated by a multi-modal main model. The systems receive a knowledge input at the knowledge module, identify a knowledge type associated with the knowledge input, and extract a knowledge unit from the knowledge input. The systems select a representation model that corresponds to the knowledge type and select a grounding type configured to ground the at least one knowledge unit into the representation model. The systems then ground the knowledge unit into the representation model according to the grounding type.","['G06N3/042', 'G06N20/00', 'G06F40/40', 'G06N3/045', 'G06N3/096', 'G06V10/774', 'G10L15/063']"
US12003535B2,Phishing URL detection using transformers,"The technology described herein can identify phishing URLs using transformers. The technology tokenizes useful features from the subject URL. The useful features can include the text of the URL and other data associated with the URL, such as certificate data for the subject URL, a referrer URL, an IP address, etc. The technology may build a joint Byte Pair Encoding for the features. The token encoding may be processed through a transformer, resulting in a transformer output. The transformer output, which may be described as a token embedding, may be input to a classifier to determine whether the URL is a phishing URL. Additional or improved URL training data may be generated by permuting token order, by simulating a homoglyph attack, and by simulating an a compound word attack.","['H04L63/168', 'H04L63/1483', 'G06F21/554', 'G06F21/56', 'G06F21/566', 'G06F40/284', 'G06N3/045', 'G06N3/0455', 'G06N3/08', 'G06N3/0895', 'G06N3/094', 'G06N3/096', 'H04L63/1466', 'G06F2221/2119']"
CN118643470B,A multi-modal large model training optimization method and device in the electric power vertical field,"The invention discloses a multi-mode large model training optimization method and device in the power vertical field, and relates to the technical field of power industry vertical large model training. According to the invention, a pruning method is injected into the multi-mode large model to complete the training of the service scene in the electric power vertical field, the problems of large weight parameter quantity, high display memory requirement, long training time, high computing resource requirement and the like in the large model training are solved, the multi-mode large model after pruning is subjected to fine tuning training by a low-rank fine tuning method, the model precision loss caused by pruning is compensated, the model size can be compressed, the model size is more suitable for training and deployment, the powerful generalization capability of the large model can be maintained, and the multi-mode large model in the electric power field which is easy to deploy is obtained.","['G06F18/25', 'G06N3/0455', 'G06N3/082', 'G06N3/084']"
US12067759B2,Method of constructing transformer model for answering questions about video story and computing apparatus for performing the same,"A method of constructing a transformer model for answering questions about a video story according to an embodiment includes: extracting feature vectors related to each character of a video from video data including vision data and subtitle data and question data for video questions and answers, and generating an input embedding using the feature vectors related to the character; and training a transformer model using the input embedding.","['G06V10/44', 'G06F16/7837', 'G06F16/783', 'G06F16/7844', 'G06F16/7847', 'G06F16/9032', 'G06F16/90332', 'G06F16/906', 'G06N20/00', 'G06N3/0455', 'G06V10/25', 'G06V10/469', 'G06V10/764', 'G06V10/82', 'G06V20/41', 'G06V20/46', 'G06V40/20', 'H04N21/488', 'H04N21/4884']"
CN117994677A,Visual large language model construction method for multi-source ship remote sensing image interpretation,"The invention discloses a visual large language model construction method for remote sensing image interpretation of a multi-source ship, which belongs to the technical field of remote sensing and computer vision intersection and comprises the following steps: under the offshore remote sensing image scene, the visual characteristic and the language characteristic are fused based on a visual large language model, the frozen LLaMA is taken as a starting point, fine adjustment is carried out through optimizing four smaller matrixes in LoRA to strengthen alignment and interactive understanding between the visual and the language, a knowledge migration framework is constructed to be suitable for the ship remote sensing field, and the pixel-level ship segmentation indicated by the language is realized in combination with the SAM model. The invention provides a multi-modal large language model aiming at multi-source ship remote sensing image interpretation, and a novel visual language alignment image interpretation method is constructed, and can understand and uniformly process various ship interpretation tasks, such as ship horizontal frame detection, ship rotating frame detection and ship pixel level segmentation tasks.","['G06V20/13', 'G06N3/045', 'G06N3/0455', 'G06N3/048', 'G06N3/096', 'G06V10/454', 'G06V10/82', 'G06V2201/07']"
US20240330480A1,System and method for triaging vulnerabilities by applying bug reports to a large language model (llm),"A system and method are provided for predicting risks related to software vulnerabilities and thereby triaging said vulnerabilities. Input data (e.g., bug reports) are applied to a prediction engine (e.g., a machine learning (ML) method such as a large language model, a transformer neural network, or a classifier model), which outputs two or more scores for each vulnerability. A first score represents a likelihood of an exploit being developed (a threat), a second score represents a likelihood of being attacked (a greater threat), and a third score represents a likelihood of becoming a published common vulnerability and exposure (an even greater threat). Based on these scores, the vulnerabilities are triaged. Because the prediction engine is trained to make predictions using the unstructured data in bug reports, the vulnerabilities can be triaged soon after discovery, reducing the time to remediate vulnerabilities predicted to be significant threats.","['G06F16/334', 'G06F21/577', 'G06F11/3476', 'G06F16/345', 'G06F16/9024', 'G06F21/31', 'G06F21/552', 'G06F21/563', 'G06F21/566', 'G06N20/00', 'H04L63/1425', 'H04L63/1433', 'H04L63/145', 'H04L63/1483', 'H04L63/1491']"
US12229525B2,Controllable reading guides and natural language generation,"The presently disclosed embodiments may include a computer readable medium including instructions that when executed by one or more processing devices cause the one or more processing devices to perform a method. The method may include: identifying a location in an electronic document for at least one text insertion; automatically generating one or more text insertion options, based on a syntactic or semantic context of text in the electronic document before or after the identified location, and causing the one or more text insertion options to be displayed to the user; receiving, from a user, a selection of a text insertion option from among the one or more text insertion options; and causing the selected text insertion option to be included in the electronic document at a location that includes the identified location.","['G06F3/0482', 'G06F3/04847', 'G06F40/166', 'G06F40/211', 'G06F40/274', 'G06F40/289', 'G06F40/30', 'G06F40/58', 'G06N3/084', 'G06F3/0486', 'G06F40/56', 'G06N3/048']"
CN116776939A,Terminal equipment-oriented sparse large language model deployment method,"The invention relates to the technical field of artificial intelligence and mobile computing, and discloses a deployment method of a sparse large language model for terminal equipment, which is improved by a probability statistics and parallelization technology on the basis of a Switch Transformer network structure, wherein an FFN layer in a T5 model is replaced by a MoE structure in the Switch Transformer network structure, a global expert register is designed, the expert register manages an expert network loaded from a disk, and the expert network in the expert register is directly used for reasoning when reasoning, so that the model far larger than the upper limit of a memory of the terminal equipment is deployed on the terminal equipment at the cost of a small amount of increasing reasoning time, and zero-precision loss reasoning deployment is realized.","['G06N3/0495', 'G06N3/0442', 'G06N3/045', 'G06N3/063']"
US20250005629A1,Personalized machine-learned large language model (llm),A computer system finetunes a machine-learned language model to generate a personalized response to a user request. The system may generate a user representation for each of a plurality of users by applying a transformer model to a sequence of tokens representing a sequence of activities of the user. The system may train an evaluation model coupled to receive a user representation and a response to a user request and generate an estimated evaluation score indicating a level of personalization of the response to the user. The system may finetune a first machine-learned language model to generate a second machine-learned language model. The finetuned machine-learned language model is configured to provide personalized responses for customer services at an online concierge system.,"['G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G06Q30/0271', 'G06Q50/12']"
WO2025019136A1,Geomechanical data interpretation and recommendation system using large language models,"A method may include providing one or more inputs to a hybrid data generator, wherein one of the one or more inputs is based at least in part on a wellsite location, wherein the hybrid data generator comprises a large language model, and wherein the large language model is based at least in part on a machine learning algorithm. The method may further include utilizing an information handling system to generate a drilling program based at least in part on the one or more inputs and the hybrid data generator. The method may further include performing at least a portion of a drilling operation based at least in part on the drilling program and collecting at least one measurement from at least one sensor during the drilling operation.","['E21B44/00', 'E21B2200/20', 'E21B2200/22']"
US20250156682A1,Generating symbolic plans using transformer-based models,"Embodiments of the invention are directed to a computer-implemented method that includes inputting a first input into a plansformer that includes a transformer-based neural network (NN). The first input includes symbols and a problem. The computer-implemented method further includes, in response to the inputting, receiving as output from the plansformer a plan for solving the problem.","['G06N3/0455', 'G06N3/045', 'G06N3/09', 'G06N3/096']"
US20240403569A1,Using large language models to generate electronic messages associated with content items,"A system automatically generates electronic messages based on a plurality of signals received in a content management platform. The system receives, at a user interface generated by a computer system for display by a user device, an input to generate an electronic message. The system processes a plurality of signals retrieved based on the received input and generates a prompt to a large language model (LLM) to cause the LLM to generate content for the electronic message, where the prompt is generated at least in part based on the processing of the plurality of signals. The system populates, into the user interface, content for the electronic message that is generated based on output by the LLM.","['G06F40/174', 'G06F40/40']"
US20230409882A1,Efficient processing of transformer based models,"Facilitating efficient processing of transformer based models is provided herein. A low latency processing system includes a transformer having an embedding layer and a Tensor Streaming Processor (TSP) having a Matrix Multiplication module (MXM) and Vector Calculation module (VXM). The TSP is arranged to deterministically process information arranged by the embedding layer and an encoder layer with the associated self-attention mechanism, the information being further modified according to the transformer using a general matrix multiply (GEMM) mapped directly on the MXM and associated accumulator. Further, at least some set of information is processed to parallelize the execution of GEMMs across all MXM planes.","['G06N3/0455', 'G06F17/16', 'G06N3/0495', 'G06N3/063', 'G06N3/084', 'G06N3/048']"
US12333436B2,Augmenting machine learning language models using search engine results,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for augmenting machine learning language models using search engine results. One of the methods includes obtaining question data representing a question; generating, from the question data, a search engine query for a search engine; obtaining a plurality of documents identified by the search engine in response to processing the search engine query; generating, from the plurality of documents, a plurality of conditioning inputs each representing at least a portion of one or more of the obtained documents; for each of a plurality of the generated conditioning inputs, processing a network input generated from (i) the question data and (ii) the conditioning input using a neural network to generate a network output representing a candidate answer to the question; and generating, from the network outputs representing respective candidate answers, answer data representing a final answer to the question.","['G06N3/08', 'G06F16/3329', 'G06F16/953', 'G06N3/0455', 'G06N3/088', 'G06N3/096', 'G06N20/00', 'G06N3/02', 'G06N5/04']"
EP4485307A1,Computer-assisted troubleshooting of packaging lines using large language models,"Disclosed is a method (600) for computer-assisted troubleshooting of a packaging line (1) configured to produce packages. In one embodiment, the method (600) comprises the following steps: receiving (602), via an input device, a problem statement (402) in natural language from a user indicating a problem that has occurred during operation of the packaging line (1); generating (604), by a trained language model (42) and based at least in part on the problem statement (402), one or more instructions (404) indicating operations to be performed by the user on the packaging line (1) to alleviate the problem; and outputting (606), via an output device, the one or more instructions (404) in natural language to the user. Also disclosed is a method (500) of training a language model (42), a language model (42), data processing apparatus (4), a packaging line (1), and a computer program.",['G06Q10/08']
US20250144795A1,Controlling robots using multi-modal language models,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for controlling an agent interacting with an environment. In one aspect, a method comprises: receiving one or more observations of an environment; receiving an input text sequence that describes a task to be performed by a robot in the environment; generating an encoded representation of the input text sequence in an embedding space; generating a corresponding encoded representation of each of the one or more observations in the embedding space; generating a sequence of input tokens that comprises the encoded representation of the input text sequence and the corresponding encoded representation of each observation; processing the sequence of input tokens using a language model neural network to generate an output text sequence that comprises high-level natural language instructions; and determining, from the high-level natural language instructions, one or more actions to be performed by the robot.","['G06N3/045', 'B25J9/1658', 'G06F40/284', 'G06F40/30', 'G06N3/0464']"
CN119322890B,Concept recommendation method based on large language model structure learning and knowledge characterization,"The invention discloses a concept recommendation method based on large language model structure learning and knowledge representation, and belongs to the field of concept recommendation. The method comprises the steps of firstly obtaining learning data and concept knowledge maps of student accounts in a learning process, preprocessing, carrying out text coding on an enhanced text generated by a large language model by using a pre-training language model to obtain a final hidden vector of each concept, further constructing an enhanced concept knowledge map, pre-training GraphGPS a concept encoder based on the enhanced concept knowledge map and a double-view cross map contrast learning method, synthesizing educational data by using a TabDDPM model, pre-training a depth knowledge tracking model, embedding questions, embedding answers, structurally perceived concept representations and student knowledge state representations, splicing to form a final representation, further obtaining a coding sequence, and then pre-training a sequence coding model and a fine-tuning concept recommendation matrix, and finally forming a concept recommendation model for concept recommendation.","['G06F16/9535', 'G06F16/3329', 'G06F16/3344', 'G06F16/335', 'G06F40/30', 'G06N3/0455', 'G06N3/047', 'G06N3/0499', 'G06N3/08', 'G06N5/022']"
WO2024072001A1,Apparatus and method for sharing and pruning weights for vision and language models,"A method of performing a multimodal tasks by using a multimodal model that includes a text encoder and a vision encoder, may include obtaining a text feature from the query via the text encoder, obtaining an image feature from the one or more input images via the vision encoder, and outputting a response to the query based on similarity between the text feature and the image feature, wherein weights vectors of the text encoder and the vision encoder are pruned and shared according to a sharing vector and a pruning vector that are generated by a hypernetwork, and wherein the hypernetwork and the multimodal model are jointly trained to minimize at least one of a difference between the weight vectors in the text encoder and the vision encoder, a difference between the weight vectors in different layers of the text encoder, and a number of parameters in the multimodal model.","['G06N3/0455', 'G06F16/334', 'G06F16/5846', 'G06N3/082', 'G06N3/084', 'G06N3/096', 'G06N3/0985', 'G06N3/0442', 'G06N3/0464']"
US12141179B2,System and method for generating ontologies and retrieving information using the same,"A system and method for automatically generating organization level ontology for knowledge retrieval, are provided. An input/output unit receives a plurality of documents from document sources and an ontology generation system generates the organization level ontology based on the documents. The ontology generation system extracts one or more nodes and directed relationships from each document and generates an intermediate document ontology for each document. A combination of syntactic, semantic, and pragmatic assessment of intermediate document ontology is performed to assess at least structure and adaptability of the ontology. The ontology generation system further generates a refined document ontology, based on assessment, to satisfy one or more quality metrics. Each of the refined document ontologies is integrated together to generate the organization level ontology. Further, a knowledge retrieval system is operatively coupled to the ontology generation system and processes one or more search queries using the generated organization level ontology.","['G06F16/3335', 'G06F16/322', 'G06F16/3344', 'G06F16/367', 'G06F40/20', 'G06N5/02', 'G06N5/022', 'G06F40/30', 'G06N3/0455', 'G06N3/09']"
US20240320447A1,Dynamic generation of enhanced prompt vectors for language models,"Techniques for dynamic prompt vector generation for improved language model predictions are provided. A textual prompt word and textual context data are received, and an interim vector is generated by encoding the textual prompt word and the textual context data using an encoder machine learning model. An augmented prompt vector is generated by processing the interim vector using a sequence generation machine learning model, the sequence generation machine learning model trained based on at least one sequence of vectors comprising a training prompt word, a training related word, and a plurality of intermediate vectors. Model output is generated by processing the augmented prompt vector using a language machine learning model.","['G06F40/47', 'G06F40/247', 'G06F40/30', 'G06F40/44']"
CN118334489B,"Vision language model field self-adaption method based on countermeasure double-prompt learning, terminal and readable storage medium","The invention provides a visual language model field self-adaptive method, a terminal and a readable storage medium based on countermeasure double-prompt learning, which belong to the technical field of image processing and respectively construct related prompt words; inputting the prompt words into a text encoder together to obtain classification weights; obtaining the classification weight in the target domain; obtaining visual characteristics of the image; generating a pseudo tag for the target domain image by using zero sample inference capability of the CLIP; alternately learning text prompt words and visual prompt words through resistance training, so that the text prompt learning and the visual prompt learning are fused into a cooperative framework; training text prompt words and visual prompt words, fixing the text prompt words, and maximizing the prediction mutual information loss of the target domain image and the domain discrimination loss of any image at the global level and the category level respectively. The method and the device realize the field discrimination and field alignment of the global level and the class level, and promote the field self-adaption effect.","['G06V10/82', 'G06F40/30', 'G06N3/0455', 'G06N3/088', 'G06N3/094', 'G06V10/40', 'G06V10/764', 'G06V20/70']"
US20250104693A1,Natural language generation,"Techniques for using a language model (e.g., a large language model (LLM)) to generate a natural language response to a user input and prosody information (e.g., voice characteristics associated with a synthetic voice to output the natural language response to the user) are described. The prosody information may correspond to a natural language (e.g., text or tokenized) description, a spectrogram, and/or a latent representation of the voice characteristic(s) associated with the natural language response. In some embodiments, the natural language response and the prosody information may be generated by different portions of layers of the language model. In such embodiments, the output of the layer(s) of the language model configured to generate the natural language response may be provided to the layer(s) of the language model configured to generate the prosody information and the output may be used to generate the prosody information, and vice versa.","['G10L13/033', 'G06N3/044', 'G06N3/045', 'G10L13/047', 'G10L25/18', 'G10L13/027', 'G10L13/10']"
CN110413749A,Determine the method and device of typical problem,"This specification embodiment provides a kind of method and device of determining typical problem, method include: get multiple customer problems it is corresponding it is multiple represent text after, each characteristic value for representing text is further obtained, so that any two represent the semantic similarity degree negative correlation that the difference between the characteristic value of text represents text with any twoï¼›Then each text that represents is ranked up according to each characteristic value for representing text, obtain text sequence, mark personnel can be by judging that two neighboring in text sequence to represent text whether semantic similar, quickly discovery is labeled by the similar subsequence for representing text continuous arrangement and being formed of multiple semantemes, and for each subsequence of discoveryï¼›Subsequent each subsequence for mark, is determined to represent the selected text of the subsequence, and then according to the corresponding selected text of each subsequence and is not included in the representative text of each subsequence and determines multiple typical problems.","['G06F16/3329', 'G06F16/35', 'G06F18/22']"
WO2025038091A1,Specificity aware teacher model and student model based on large language model,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for using a large language model (LLM) to generate a specificity labeled query-to-document scored teacher training set for training a teacher model. The teacher model, in turn, in sued to generate a specificity labeled query-to-document scored student training set for training a student model. The student model is ensemble trained, using the student model specificity training set and at least one other student model training set does not quantify the query-to-landing page specificity drift, to predict a performance of a query for a landing page.","['G06F16/95', 'G06N3/045', 'G06N3/096']"
US11990134B2,Method for configuring and using a numeric-to-alphabetic expression machine learning model,"A system, method, and computer-program product includes constructing a transcript adaptation training data corpus that includes a plurality of transcript normalization training data samples, wherein each of the plurality of transcript normalization training data samples includes: a predicted audio transcript that includes at least one numerical expression, an adapted audio transcript that includes an alphabetic representation of the at least one numerical expression, and a transcript normalization identifier that, when applied to a model input comprising a target audio transcript, defines a text-to-text transformation objective causing a numeric-to-alphabetic expression machine learning model to predict an alphabetic-equivalent audio transcript that represents each numerical expression included in the target audio transcript in one or more alphabetic tokens; configuring the numeric-to-alphabetic expression machine learning model based on a training of a machine learning text-to-text transformer model using the transcript adaptation training data corpus; and executing the numeric-to-alphabetic expression machine learning model.","['G06N3/045', 'G10L15/26', 'G06N20/00', 'G06N3/063', 'G06N3/09', 'G10L15/02', 'G10L15/04', 'G10L15/063', 'G10L15/16', 'G10L15/28', 'G10L25/30', 'G10L25/78', 'G06N3/0442', 'G06N3/084', 'G10L2025/783']"
WO2025106210A1,Determining anomalous tool invocations by applications using large generative models,"This disclosure relates to utilizing a threat detection system to detect anomalous actions provided by a compromised large generative language model (LLM). For instance, the threat detection system utilizes a detection-based large generative model to process select communication between an application system and the LLM and determine when the LLM may have been potentially compromised. In various implementations, utilizing the detection-based large generative model, the threat detection system determines when an LLM is improperly instructing an application system to invoke tools to perform unapproved actions. Furthermore, when an LLM becomes compromised, the threat detection system intelligently safeguards the detection-based large generative model against similar threats that seek to evade detection or compromise the detection-based large generative model.","['G06F21/566', 'G06F21/554', 'G06N3/047', 'G06N3/0475']"
CN114625840A,A training method and device for a natural language processing model,"The application provides a training method and a device of a natural language processing model, wherein the method comprises the following steps: the method comprises the steps of obtaining a pre-training language model of which each layer is of a layer structure by adopting a self-attention mechanism, obtaining a first learnable vector matrix and a second learnable vector matrix of which each layer is used for learning a first task in the NLP task, then generating a first splicing key vector matrix and a first splicing value vector matrix of each layer according to the first learnable vector matrix and the second learnable vector matrix, and finally training the first learnable vector matrix and the second learnable vector matrix by utilizing training sample data of the first task. The first learnable vector matrix and the second learnable vector matrix are participated in training through the first splicing key vector matrix and the first splicing value vector matrix, and training parameters are greatly reduced due to the fact that a pre-training language model is fixed; the learnable vector matrix can learn the NLP basic task first and then learn the NLP upper-layer task, and the learning ability of the small sample can be improved.","['G06F16/3344', 'G06F16/35', 'G06F17/16', 'G06F18/214', 'G06F40/211', 'G06F40/30']"
CN119003704B,Business process processing method and system based on large language model,"The application provides a business process processing method and a business process processing system based on a large language model, relates to the technical field of artificial intelligence, and can accurately capture a power grid business session event and a dependency relationship thereof by constructing a multi-element relationship network of power grid business description content, thereby providing a comprehensive data basis for analysis of complex power grid business. Further, through the embedded representation and vector fusion technology, the business event, the linkage event and the session knowledge label are converted into vector representation of a high-dimensional space, so that multidimensional information is effectively integrated, and the understanding capability of the model to the power grid business scene is enhanced. The method has the advantages that the sample network member vector is utilized to carry out linkage parameter adaptation optimization and parameter learning on the initialized large language model, the finally generated target large language model is high in accuracy, the dynamic change of power grid business can be well adapted, and the accuracy, the high efficiency and the intellectualization of business process processing are realized.","['G06F16/3344', 'G06F16/36', 'G06F18/25', 'G06N3/0455', 'G06N3/0499', 'G06N3/084', 'G06N3/0985', 'Y04S10/50']"
US20230350936A1,Language model for processing a multi-mode query input,"A query processing system is described which receives a query input comprising an input token string and also at least one data item having a second, different modality, and generates a corresponding output token string.","['G06F16/432', 'G06F16/438', 'G06F40/284', 'G06N3/045', 'G06N3/0464', 'G06N3/0475', 'G06N3/0499', 'G06N3/08']"
US12373641B2,Methods and apparatus for natural language understanding in conversational systems using machine learning processes,"This application relates to apparatus and methods for natural language understanding in conversational systems using machine learning processes. In some examples, a computing device receives a request that identifies textual data. The computing device applies a natural language model to the textual data to generate first embeddings. In some examples, the natural language model is trained on retail data, such as item descriptions and chat session data. The computing device also applies a dependency based model to the textual data to generate second embeddings. Further, the computing device concatenates the first and second embeddings, and applies an intent and entity classifier to the concatenated embeddings to determine entities, and an intent, for the request. The computing device may generate a response to the request based on the determined intent and entities.","['G06F16/3329', 'G06F16/3347', 'G06F16/90332', 'G06F40/284', 'G06F40/295', 'G06F40/35', 'G06F40/40', 'G06N3/04', 'G06N3/045', 'G06N3/08']"
US11960842B2,Methods and apparatus for natural language understanding in conversational systems using machine learning processes,"This application relates to apparatus and methods for natural language understanding in conversational systems using machine learning processes. In some examples, a computing device receives a request that identifies textual data. The computing device applies a natural language model to the textual data to generate first embeddings. In some examples, the natural language model is trained on retail data, such as item descriptions and chat session data. The computing device also applies a dependency based model to the textual data to generate second embeddings. Further, the computing device concatenates the first and second embeddings, and applies an intent and entity classifier to the concatenated embeddings to determine entities, and an intent, for the request. The computing device may generate a response to the request based on the determined intent and entities.","['G06F40/30', 'G06F40/295', 'G06F40/211', 'G06F40/284', 'G06F40/56', 'G06N20/00', 'G06N3/04', 'G06N3/045', 'G06N3/08']"
US20240311582A1,Conversational unit test generation using large language model,"A large language model, trained on source code and natural language text generates a unit test for a change to a file in a pull request of a code repository. An ordered sequence of prompts is created and each is applied serially to the large language model to perform an individual task that leads to the generation of the unit test. The unit test may be added to an existing file or generated as a newly-created file. Each prompt includes the data from a previously-issued prompt of the ordered sequence in order for the model to retain contextual knowledge learned previously. The model generates the unit test as update commands when the unit test is added to an existing file.","['G06N5/022', 'G06F11/3608', 'G06F11/3684', 'G06F40/35', 'G06F40/40', 'G06N3/044', 'G06N3/08']"
CN118733699A,A retrieval-enhanced intelligent financial large language model system,"A retrieval enhancement type intelligent financial big language model system optimizes the storage and access efficiency of financial small files through a data storage and management end; the system comprises a financial small file management module, a dynamic merging algorithm module, a super file management module and a suffix index construction algorithm, so that the recall ratio and construction speed of the index are improved. The search module ensures accuracy and high efficiency of data indexing. The generating end integrates a meta information extraction and interpretability answer generating module, utilizes a transducer architecture and a multi-head attention mechanism to strengthen semantic understanding and processing capacity, and enhances the accuracy and interpretability of the answer through a fine tuning strategy and a memory network. RAG-FinLLM solves the problems of illusion errors and knowledge updating hysteresis of a large financial model in generating answers, realizes the real-time updating of a knowledge base, ensures timeliness and reliability, and meets the requirements of intellectualization and high-precision knowledge question answering.","['G06F16/316', 'G06F16/3329', 'G06F16/3331', 'G06F40/30']"
CN118798258A,"Large language model construction method, text processing method, system, device and medium","The embodiment of the application provides a large language model construction method, a text processing method, a system, equipment and a medium, belonging to the technical field of artificial intelligence. The method comprises the steps of training all network layers of a text processing basic model through a training data set to obtain a first language model, dividing all network layers of the first language model, wherein a plurality of network layers at the front part are used as a shared network, and a plurality of network layers at the rear part are used as expert networks. And respectively training expert networks of the first language model through the text subsets of each field to obtain the expert networks of each field capable of processing the specific field characteristics, connecting the shared network with the expert networks of each field through the router module to obtain a second language model, and then training the second language model to obtain a large language model for text processing, so that the router module can select the proper expert networks of the field to process the corresponding input characteristics, and the accuracy of the large language model on text processing of each field is improved.","['G06N3/045', 'G06F16/35', 'G06N3/042', 'G06N3/08']"
CN118297136A,"Method, storage medium and apparatus for training large language model","The embodiment of the application provides a method, a storage medium and equipment for training a large language model, relating to the technical field of artificial intelligence, wherein the method comprises the following steps: acquiring neuron activation state data of a neuron set corresponding to the large language model in a historical forward training iteration; determining a subset of significant neurons and a subset of non-significant neurons from the set of neurons based on the neuron activation state data; when training a large language model with a set of data samples, the subset of significant neurons is updated, and updates to the subset of non-significant neurons are ignored. Therefore, the training speed of the large language model is accelerated by analyzing sparsity shown by the neurons activated in each forward iteration when the large language model is trained and identifying and discarding the inactive neurons.","['G06N3/098', 'G06F18/217', 'G06N3/045']"
CN118821730A,Method and device for processing very long text sequences for large-scale language models,"The application relates to the technical field of natural language processing, and discloses a method and a device for processing a very long text sequence oriented to a large-scale language model. The method comprises the following steps: text sequence preprocessing: the method comprises the steps of preprocessing an input super-long text sequence, including dynamic sequence segmentation and adaptive sequence rearrangement, segmenting the super-long text sequence into a plurality of continuous and overlapped blocks, and filtering extraneous noise; building a super-long text sequence processing network: constructing a processing network based on a transducer architecture to process the preprocessed text sequence blocks; blocking parallel processing and fusion: carrying out block parallel processing on the text sequence blocks, and fusing processing results; continuous learning and feedback cycle: and the processing strategy is dynamically adjusted according to the processing result and is fed back to the text sequence preprocessing step.","['G06F40/16', 'G06F40/216', 'G06F40/30', 'G06N3/045']"
WO2022159198A1,Generation of optimized knowledge-based language model through knowledge graph multi-alignment,"A language module is joint trained with a knowledge module for natural language understanding by aligning a first knowledge graph with a second knowledge graph. The knowledge module is trained on the aligned knowledge graphs. Then, the knowledge module is integrated with the language module to generate an integrated knowledge-language module.","['G06F40/30', 'G06F40/56', 'G06F40/58', 'G06N3/045', 'G06N3/08', 'G06N5/022', 'G10L15/063', 'G10L15/18', 'G10L13/00', 'G10L15/16']"
WO2025053923A1,Interpreting and resolving map-related queries using a language model,"A technique for interacting with map-related information integrates the use of a machine-trained language model. Upon submission of a query, the technique uses the machine-trained language model to assess at least one intent associated with the query. The technique then invokes an intent-specific processing flow to provide an output result. Each processing flow invokes the use of at least one processing engine to perform an engine-specific task, such as geocoding, route finding, or image retrieval. A processing flow can also call on the machine-trained language model one or more additional times. In some cases, the technique includes a feedback mechanism for soliciting additional information from a user.","['G01C21/3407', 'G06F16/243', 'G06F16/29', 'G06F16/90332', 'G06F16/9537', 'G06F40/216', 'G06F40/284', 'G06F40/295', 'G06F40/30', 'G06F40/40']"
US20250053616A1,Vision-and-language model training,"Systems and methods for improving training processes for image and text applications are described. A first set of embeddings may be generated based on a text input, and a second set of embeddings may be generated via a convolutional neural network (CNN), based on an input image. The first set of embeddings and the second set of embeddings may be utilized to generate a third set of embeddings including one or more placeholder values to be replaced. The placeholder values may be replaced based on predicted values, to reconstruct the input text and image.","['G06F18/2148', 'G06F18/251', 'G06F40/30', 'G06N3/045', 'G06T9/002', 'G06V30/262', 'G06N3/08']"
US12243513B2,Generation of optimized spoken language understanding model through joint training with integrated acoustic knowledge-speech module,"A speech module is joint trained with a knowledge module by transforming a first knowledge graph into an acoustic knowledge graph. The knowledge module is trained on the acoustic knowledge graph. Then, the knowledge module is integrated with the speech module to generate an integrated knowledge-speech module. In some instances, the speech module included in the integrated knowledge-speech module is aligned with a language module to generate an optimized speech model configured to leverage acoustic information and acoustic-based knowledge information, along with language information.","['G10L15/063', 'G06F40/279', 'G06N3/045', 'G06N3/088', 'G06N5/02', 'G10L13/02', 'G10L13/08', 'G10L15/02', 'G10L15/1815', 'G10L15/1822', 'G10L15/22', 'G06F40/30', 'G10L13/00', 'G10L13/033']"
US20230376725A1,Model customization of transformers for improved efficiency,"Embodiments of the present disclosure include systems and methods for providing model customizations of transformers for improved efficiency. A first set of settings for a transformer model is received. Based on the first set of settings, a second set of settings for the transformer model is determined. The first set of settings and the second set of settings are used to configure and train the transformer model.","['G06N3/0455', 'G06N3/04', 'G06N3/0495', 'G06N3/08', 'G06N3/082', 'G06N3/0985']"
WO2024228666A1,"Text prediction model training method and device, and text prediction method and device","Disclosed in the embodiments of the present disclosure are a text prediction model training method and device, and a text prediction method and device. According to the present disclosure, a training sample set is used for training a large language model (LLM) to obtain a text prediction model. In this mode, labeled samples are substantively used for training the LLM, and in the updating process of a second key matrix and a second value matrix, a second key matrix and a second value matrix obtained by means of a previous round of iteration and a first key matrix and a first value matrix generated by a current input feature matrix are used, so that historical information and information of a current input text are both kept, and the LLM can fully understand and learn the labeled samples, thereby improving an LLM-based text prediction effect in an in-context learning scenario. In addition, the mode of forward optimizing a model greatly reduces model parameters needing to be updated, reduces costs of model training, and improves the efficiency.","['G06F16/3329', 'G06F16/313', 'G06F16/35', 'G06F40/30', 'G06N3/0455', 'G06N3/084', 'Y04S10/50']"
WO2025117036A1,Tuning large language models for next sentence prediction,"A processor-implemented method includes generating a group of single-head attention (SHA) operations based on a number of attention heads in a multi-head attention (MHA) mechanism, each SHA operation corresponding to a respective attention head of a group of attention heads associated with the MHA mechanism. The method also includes parallelly executing each of one of the group of SHA operations independently between hardware blocks of a device associated with a neural network model. The method further includes generating an MHA output based on parallelly executing each of one of the group of SHA operations.","['G06N3/063', 'G06N3/08', 'G06N3/045']"
CN112417139B,A Summary Generation Method Based on Pre-trained Language Model,"The invention discloses a method for generating a abstract based on a pre-training language model, which comprises the following steps: acquiring text information of a summary to be generated, and performing multi-feature weighted language pre-training treatment on the text information to obtain candidate summaries; inputting the candidate abstract into a pre-training language model to obtain output data of the pre-training language model; and inputting the output data of the pre-training language model into a decoder model to obtain a target abstract. According to the method, the text information to be generated into the abstract is subjected to pre-training treatment, and then the target abstract can be generated through the encoder and the decoder.","['G06F16/345', 'G06F40/126', 'G06F40/211', 'G06F40/258', 'G06F40/30', 'G06N3/08', 'Y02D10/00']"
EP3977332A1,Keyphrase extraction beyond language modeling,"A system for extracting a key phrase from a document includes a neural key phrase extraction model (""BLING-KPE"") having a first layer to extract a word sequence from the document, a second layer to represent each word in the word sequence by ELMo embedding, position embedding, and visual features, and a third layer to concatenate the ELMo embedding, the position embedding, and the visual features to produce hybrid word embeddings. A convolutional transformer models the hybrid word embeddings to n-gram embeddings, and a feedforward layer converts the n-gram embeddings into a probability distribution over a set of n-grams and calculates a key phrase score of each n-gram. The neural key phrase extraction model is trained on annotated data based on a labeled loss function to compute cross entropy loss of the key phrase score of each n-gram as compared with a label from the annotated dataset.","['G06F40/258', 'G06F18/214', 'G06F18/2178', 'G06F40/211', 'G06F40/284', 'G06F40/30', 'G06N3/08', 'G10L2015/088']"
US11915690B1,Automatic speech recognition,A multi-channel transformer acoustic model that processes a plurality of audio signals output by microphones of a microphone array and outputs probabilities for acoustic units of an utterance represented in the audio signals. The audio signals represent the individual microphones' respective capturing of the utterance. The multi-channel model may perform self-attention on embeddings of the audio signals and then cross-channel attention across the attended audio signals. The cross-channel attention may involve processing of signals relative to each other to model the relationships across channels within and across time frames. The multi-channel model may include a transducer to perform processing frame-by-frame.,"['G10L15/16', 'G10L15/18', 'G10L15/22', 'G10L19/008', 'H04R3/005', 'G10L13/027', 'G10L13/047', 'G10L2015/088', 'G10L2015/225', 'G10L2021/02166', 'H04R1/406']"
CN118982030B,A method for extracting sentiment from multi-turn conversations using large language model reasoning,"The invention discloses a multi-turn dialogue emotion quadruple extraction method using large language model reasoning, which relates to the technical field of emotion quadruple extraction and comprises the steps of collecting multi-turn dialogue data and context information, preprocessing the data, carrying out word pruning on the context information through a dynamic word pruning model, establishing a knowledge distillation model, combining multi-turn dialogue data according to reasoning results to construct a multi-level teacher model, guiding a student model to learn detailed information in a dialogue from multi-level dialogue knowledge through the multi-level teacher model, constructing a dialogue graph based on the dialogue processed by the LazyLLM and knowledge distillation model, dividing the dialogue into minimum sub-dialogues by using a two-dimensional structure entropy minimization algorithm, combining the sub-dialogues to realize optimal division, forming a sub-dialogue set, constructing a two-dimensional table labeling entity relation, extracting entities in the utterance, matching entities based on the sub-dialogue set, forming emotion quadruple groups, and finally matching the emotion quadruple groups through predicting the relation among the entities.","['G06F40/35', 'G06F40/279', 'G06N3/042', 'G06N3/045', 'G06N3/082', 'G06N3/09', 'G06N3/096', 'G06N5/022', 'G06N5/041']"
US12032919B1,Post-calibration of large language model confidence scoring via combined techniques,"Examples provide a large language model confidence scoring post-calibration based on a combination of temperature scaling, softmax denominator top-k probabilities selection, and polynomial regression. A secure machine learning system receives results generated by a machine learning (ML) model, the results including at least one confidence score. The secure ML system identifies at least one challenge in accuracy of the results generated by the ML model configured to perform document processing and understanding. The secure machine learning system implements confidence scoring recalibration to address at least one challenge, the confidence scoring recalibration including functionality to assess reliability of the results generated by the ML model, and applies post-processing calibration to the at least one confidence score generated by the confidence scoring recalibration to enhance performance of the ML model, the post-processing calibration including adjusting the at least one confidence score generated by the confidence scoring recalibration.","['G06F40/284', 'G06F40/40']"
CN117633707A,Fine-grained multi-mode Chinese large language model construction method and computer storage medium,"The application discloses a fine-grained multi-mode Chinese large language model construction method and a computer storage medium, belonging to the field of computers, wherein the method comprises the following steps: planning the architecture of a fine-grained multi-mode Chinese large language model, wherein the fine-grained multi-mode Chinese large language model comprises a multi-mode information extraction and fusion module, a core large language model and a multi-mode content generation module; the method and the device have the advantages that the fine-grained multi-mode Chinese large language model comprises a multi-mode information extraction and fusion module, a core large language model and a multi-mode content generation module, so that the Chinese large language model is used as a central connection understanding and generation module of a model system, a series of multi-mode content understanding and content generation tasks can be executed according to user instructions, and compared with the current multi-mode large model technology, the method and the device have the advantages of few pseudoscopy problems, multiple extensible functions, low training cost, deep understanding of complex multi-mode scenes and the like.","['G06F18/256', 'G06F18/213', 'G06F18/214', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N5/04', 'Y02D10/00']"
WO2024223621A1,Transitive and commutative multimodal models and uses,"Computer-implemented methods for analysing multimodal data are described. A computer-implemented method comprises generating an output data set by processing an input data set comprising data from one or more of a set of at least three modalities, using a machine learning model comprising a transitive and commutative machine-learning model. Related methods, products and systems are described.","['G06N3/0455', 'G06N3/044', 'G06N3/0464', 'G06N3/0475', 'G06N3/048', 'G06N3/0895', 'G06N3/09', 'G06N3/096', 'G06V10/766', 'G06V10/774', 'G06V10/778', 'G06V10/803', 'G06V10/82', 'G06V10/87', 'G06V2201/03']"
US20220147713A1,Social bias mitigation in textual models,A system for generating text using a trained language model comprises an encoder that includes a debiased language model that penalizes generated text based on an equalization loss that quantifies first and second probabilities of respective first and second tokens occurring at a first point in the generated text. The first and second tokens define respective first and second groups of people. The system further comprises a decoder configured to generate text using the debiased language model. The decoder is further configured to penalize the generated text based on a bias penalization loss that quantifies respective probabilities of the first and second tokens co-occurring with a generated word. The encoder and decoder are trained to produce the generated text using a task-specific training corpus.,"['G06F40/56', 'G06F18/23', 'G06K9/6218', 'G06F40/253', 'G06F40/284', 'G06F40/30']"
CN118246485A,"Layer compression key value caching method, system, equipment and medium for large language model","The application provides a layer compression key value caching method, a system, equipment and a medium for a large language model, wherein the method comprises the following steps: setting zero for diagonal elements of the attention matrix of each neural network layer in the large language model; then dividing a neural network layer in the large language model into a model bottom layer, a model middle layer and a model top layer, and adjusting the network structure of the model bottom layer and the network structure of the model top layer so that the network structure of the model bottom layer and the network structure of the model top layer are the same as the preset network structure; and then adjusting the model middle layer so that the model middle layer only uses the key value of the top layer of the model middle layer when calculating the attention matrix. Therefore, in the large language model, the key value cache is subjected to layer compression, only a small number of layers of key values are needed to be calculated and cached, and the memory consumption of the GPU can be greatly saved under the condition that extra calculation expense is not introduced, so that the reasoning throughput of the large language model is effectively improved.","['G06N3/045', 'G06N3/082', 'G06N3/084', 'G06N5/04']"
WO2024243604A1,Learned image compression with masked vision-language modelling,"A method, implemented by a decoder, includes receiving an adjusted masked sparse visual feature, an adjusted masked sparse text feature, an adjusted masked control latent, and a diffusion latent feature of an original image; generating, based on the received adjusted masked sparse visual feature, a recovered masked visual feature; computing, based on the received adjusted masked sparse text feature, an encoded masked text feature; computing, based on the adjusted masked control latent, the recovered sparse visual feature, and the encoded masked sparse text feature, an encoded masked control feature; reconstructing, based on the recovered masked visual feature, the encoded masked text feature, and the encoded masked control feature, a baseline image output; computing, based on the diffusion latent feature, the masked sparse visual feature, and the masked sparse text feature, a supplementary output; and constructing, based on the supplementary output and the baseline image output, a final decoded image output.","['H04N19/46', 'G06N3/045', 'G06V10/82', 'H04N19/167', 'H04N19/59']"
WO2023236977A1,Data processing method and related device,"Provided is a data processing method, which relates to the field of artificial intelligence. The method comprises: acquiring text to be processed and a pre-training language model, wherein the pre-training language model comprises a feature extraction network and a prediction network; by means of the feature extraction network of the pre-training language model, performing feature extraction on said text, so as to obtain a feature representation of data to be processed, wherein the feature representation is a complex number; and by means of the prediction network of the pre-training language model, performing orthogonal transformation on the feature representation, which has been subjected to length unitization processing, so as to obtain a result which is obtained after orthogonal transformation, and determining a text prediction result according to the result which is obtained after orthogonal transformation. By means of the present application, the operations of a pre-training language model can be performed on a quantum circuit, and the pre-training language model, which is represented by means of a complex value, can be constructed such that the representation capability of the model is improved, and the performance of a network is improved.",['G06F40/166']
US20240311272A1,Test checking of pull request changes using large language model,"A large language model, trained on source code and natural language text, predicts the testworthiness of a change in a pull request of a code repository. A prompt given to the large language model includes a question and an answer format for the model to predict whether a unit test is needed for the change. The question describes the model's task, the changes made to the file, and the contents of the changed file. When the model predicts that the change requires a unit test, a check is made to determine if the test exists. A comment is inserted into the pull request indicating the testworthiness of the change and whether a unit test exits for the test.","['G06F11/3608', 'G06N5/022', 'G06F11/3676', 'G06F11/3696', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/084']"
CN115526332A,Student model training method and text classification system based on pre-training language model,"A student model training method and a text classification system based on a pre-training language model are disclosed. The method comprises the following steps: constructing a prompt training sample; adjusting the pre-training language model by using the prompt training sample to obtain a prompt-adjusted teacher model; and training a student model using the processed training samples, and the student model simultaneously learns the class probability vectors output by the cue-adjusted teacher model and the original teacher model during the training process. The present invention requires student models to learn from two teacher models simultaneously, thereby alleviating the overfitting problem of student models in small sample scenarios by adding a distillation path to learn from the original PLM teacher with unsupervised data. Furthermore, the middle layer of PLM is migrated through the knowledge probe to express, and the high-order dependency relationship can be learned from the middle layer of the teacher model by comparing the performance of learning stable knowledge distillation, so that the accuracy and the efficiency of knowledge distillation are improved.","['G06N20/00', 'G06F16/35']"
CN115730587A,Text feature extraction method based on NGU language model,"The invention relates to the technical field of natural language processing, in particular to a text feature extraction method based on an NGU language model. The method is used for optimizing and improving a GRU model based on a recurrent neural network, provides a new text feature extraction model NGU language model, introduces a gate control unit of the GRU into a normalization mechanism, replaces a hyperbolic tangent function with a saturation region into layer normalization operation, and simultaneously fuses a feedforward layer neural network of a Transformer into an iteration unit to improve the semantic representation capability of the model, namely the data fitting capability of the model.",[]
CN116468298B,GPT network model-based automobile technology planning and decision-making method and system,"The invention provides a method and a system for planning and deciding automobile technology based on a GPT network model, which are characterized in that target text data are obtained, and a non-labeling corpus is constructed according to the target text data; constructing a GPT network model, and controlling the GPT network model to perform unsupervised pre-training on the basis of a corpus so as to learn and obtain a universal language model; constructing a marked target data set, and determining each target input and each target output in the target data set; controlling the universal language model to conduct supervision fine tuning processing on the basis of the target data set so as to learn and obtain a target language model; the actual requirements are input into a target language model, corresponding feedback results are output, and particularly, through pretraining and fine-tuning of a large amount of text data related to the automobile, automatic understanding and response of information such as automobile technical routes and the like are realized, and corresponding results are automatically given out, so that technical planning personnel and decision makers are assisted to improve the technical planning/decision-making efficiency and quality.","['G06Q10/0637', 'G06F18/214', 'G06F40/289', 'G06F40/30', 'G06Q10/067', 'Y02T10/40']"
CN117763107A,Electric power defect image detection method based on image-text question-answering multi-mode model,"Compared with the prior art, the invention relates to a power defect image detection method based on a graph-text question-answering multi-mode model, which solves the defects of a large field Jing Fuza, a large number of defect types, a small deep learning target detection model, a large number of messy, miscellaneous and low interactivity in power defect image detection. The invention comprises the following steps: acquiring a multi-mode data set; acquiring a power defect knowledge data set; constructing a graph-text question-answering multi-mode model; training of a graph-text question-answering multi-mode model; and obtaining a power defect image detection result. The invention increases the graphic knowledge of the model electric power defect field by creating the Chinese data set of the multi-mode electric power defect field; the LoRA and Q-Former method is fused to finely adjust the model, the parameter number of the model is reduced, the extraction capacity of the model for the visual and semantic characteristics of the electric power defects is improved, and the visual and semantic information is aligned.",[]
US12045288B1,Natural language selection of objects in image data,"Devices and techniques are generally described for selection of objects in image data using natural language input. In various examples, first image data representing at least a first object and first natural language data may be received. In some examples, first embedding data representing the first natural language data may be generated. Second embedding data representing the first image data may be generated. Relative location data indicating a location of the first object in the first image data relative to at least one other object may be generated. The first embedding data, the second embedding data, and the relative location data may be input into a multi-modal transformer model. The multi-modal transformer model may determine that the first natural language data relates to the first object.","['G06F16/583', 'G06F16/532', 'G06F16/90332', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06V10/82', 'G06V20/20', 'G06N20/00']"
EP4594897A1,Generative pretraining of multimodal retrieval-augmented visual-language models,"Systems and methods for end-to-end pretraining of multimodal retrieval-augmented visual language models. In some examples, multimodal information may be encoded into key-value pairs and stored in a unified memory, which the model's retriever can access via multimodal query encodings in order to identify relevant information within multiple knowledge sources. The model may include an attentive fusion layer so that automatically-generated retrieval scores for multiple simultaneously-considered documents may b used in calculating attention scores, and gradients from the final task may be used to train the entire model (including the retriever) end-to-end and update the unified memory. In such cases, the retriever may thus be trained with the rest of the model without the need for ground-truth scores indicating which knowledge entries are most helpful in answering a given query, and the model' sparameters may thus be focused on understanding queries and conducting reasoning rather than simply memorizing the training data.","['G06F16/24522', 'G06N3/0455', 'G06N3/0475', 'G06N3/084', 'G06N5/022', 'G06N5/041']"
US12254005B1,Systems and methods for retrieving patient information using large language models,"A system for retrieving patient information using large language models including a computing device configured to receive a natural language query as a function of a user input, input the natural language query into a large language model communicatively connected to the least a processor, receive a computer language query comprising a plurality of nodes from the large language model, map the plurality of nodes to one or more entries in a patient database, receive a database response from the patient database as a function of the mapping, generate a final database query as a function of the database response. query the patient database using the final database query, receive a user response as a function of the final database query, and transmit the user response to a graphical user interface as a function of the final database query.","['G06F16/243', 'G16H10/20', 'G16H10/60', 'G16H50/20', 'G16H50/70']"
CN110489521A,"Text categories detection method, device, electronic equipment and computer-readable medium","The embodiment of the present application discloses text categories detection method, device, electronic equipment and computer-readable medium.The embodiment of this method includes: acquisition sample set, wherein includes the classification logotype of multiple samples of text and each samples of text in sample setï¼›BERT model after obtaining pre-training, wherein BERT model includes multilayer transformer structureï¼›The part transformer structure in BERT model is deleted, target language model is generatedï¼›Based on the samples of text and classification logotype in sample set, target language model is trained, generates text categories detection model.This embodiment offers a kind of for carrying out the model of text categories detection, carries out text categories detection using the model, the accuracy of classification testing result can be improved.","['G06F16/3344', 'G06F16/355', 'G06F18/214']"
US20240411666A1,Localizing vulnerabilities in source code at a token-level,"A vulnerability detection and repair system utilize a classifier model to detect a software vulnerability in a source code snippet and the tokens in the source code snippet attributable to the vulnerability. A large language model is then given the vulnerable source code snippet, its vulnerability type, the vulnerability tokens, and a few-shot examples to determine whether or not the source code snippet includes the identified vulnerability. The few-shot examples include positive and negative samples of the type of vulnerability to guide the large language model towards the correct output.","['G06F21/563', 'G06F11/3624', 'G06F21/577']"
US11856038B2,Cognitively learning to generate scripts that simulate live-agent actions and responses in synchronous conferencing,"Embodiments of the invention are directed to a computer-implemented method of responding to an inquiry received electronically at a synchronous conferencing system (SCS). A non-limiting example of the computer-implemented method includes, based at least in part on the inquiry, using a processor of the SCS to generate a script having one or more script computer instructions. The processor is used to execute the script computer instructions to generate script computer instruction results, wherein the script computer instruction results include inquiry response information that is responsive to the inquiry. Based at least in part on the inquiry and the inquiry response information, the processor is used to generate an inquiry response that is responsive to the inquiry.","['H04L65/403', 'G06F16/3329', 'G06F40/30', 'G06F40/35', 'G06F8/30', 'G06N20/00', 'G06N3/0455', 'G06N3/08', 'G06N3/088', 'H04L12/1822', 'H04L51/02', 'H04L65/4015', 'G06F8/36', 'G06F8/38', 'G06N3/084']"
US20250054322A1,Attribute Recognition with Image-Conditioned Prefix Language Modeling,Systems and methods for attribute recognition can include obtaining an image and a text string. The text string can be processed with a language model to generate a set of candidate attributes based on sequence based prediction. The image and the candidate attributes can be processed with an image-text model to determine a likelihood that the respective candidate attribute is depicted in the image. The likelihood determination can then be utilized to determine a predicted attribute for the object of interest.,"['G06F40/279', 'G06V20/70']"
WO2024191674A1,Test checking of pull request changes using large language model,"A large language model, trained on source code and natural language text, predicts the testworthiness of a change in a pull request of a code repository. A prompt given to the large language model includes a question and an answer format for the model to predict whether a unit test is needed for the change. The question describes the model's task, the changes made to the file, and the contents of the changed file. When the model predicts that the change requires a unit test, a check is made to determine if the test exists. A comment is inserted into the pull request indicating the testworthiness of the change and whether a unit test exits for the test.","['G06F11/3604', 'G06F11/3672', 'G06F11/3688']"
US20240419976A1,Systems and methods for enhancing the performance of a large language model using local execution,"A system and method are configured to receive a pre-trained large learning model via the network interface, access user data stored locally, train the pre-trained large learning model using the locally stored user data to provide an enhanced local large learning model, detect that a browser hosted by the computer system is accessing a webpage, identify a webpage space configured to receive third-party content, examine the webpage to determine if content provided by the enhanced, local large learning model may be rendered at the webpage space, and causing content generated or selected by the enhanced, local large learning model to be rendered at the webpage space. The enhanced local large learning model may comprise a neural network.","['G06N3/0895', 'G06N3/045', 'G06N3/08']"
CN115438183A,Business website monitoring system based on natural language processing,"The application belongs to the technical field of Internet and artificial intelligence, and relates to a service website monitoring system based on natural language processing, which comprises: the data online processing subsystem receives first service website data provided by a monitored service website belonging to a target service field through the Internet and sends the first service website data to the data acquisition and cleaning subsystem, the data acquisition and cleaning subsystem cleans the first service website data to obtain second service website data and sends the second service website data to the data resource management subsystem, the data resource management subsystem transmits the second service website data from the data acquisition and cleaning subsystem to the data analysis application subsystem, the data analysis application subsystem analyzes the second service website data from the data resource management subsystem by using a trained pre-training language model based on a Transformer framework, and the pre-training language model based on the Transformer framework is obtained by using text corpora in the target service field; the scheme can improve the accuracy and efficiency of monitoring and analyzing the data of the service website.","['G06F16/353', 'G06F16/3334', 'G06F16/3344', 'G06N5/025']"
CN116910224B,Method and system for extracting switching operation information based on large language model,"The application discloses a method and a system for extracting switching operation information based on a large language model, relates to the technical field of operation and maintenance of transformer substations, solves the problem that equipment and operation synonymous expression cannot be understood when the switching operation information is extracted by the traditional NLP technology, and has the technical scheme key points that: firstly, recognizing synonymous expressions by analyzing synonymous expressions existing in equipment/operation and pre-training a large language model by means of the existing PLM, and generating a knowledge dialogue set based on the synonymous expressions with wrong model recognition for training the professional understanding capability of the large language model; secondly, generating a task dialogue set based on the work ticket and the manually marked switching operation information, and training a question-answering function of the large language model; finally, the LLM model in the electric power field is obtained, and switching operation information can be extracted from synonymous expressions of understanding equipment and operation and in a question-and-answer mode, so that billing efficiency is improved.","['G06F16/3329', 'G06F16/338', 'G06F16/35', 'G06F40/284', 'G06F40/295', 'G06N3/092', 'G06Q10/20', 'G06Q50/06', 'H02B3/00', 'H02J13/00001', 'H02J13/00034', 'H02J13/00036', 'H02B7/06']"
CN113139054B,A Transformer-based coding programming language classification method,"The invention provides a code programming language classification method based on a transducer, which comprises the following steps: (1) Collecting question and answer posts from Stack Overflow as a data set, and carrying out data preprocessing on data in the original data set; (2) Word embedding is carried out on the data segmented by the BPE so as to convert the words into vectors; (3) Based on the constructed data set, the Roberta model is finely tuned, the generated word vector is input into the Roberta model, and the code semantics are learned through a double-layer transducer encoder to generate a semantic representation vector X semantic The method comprises the steps of carrying out a first treatment on the surface of the (4) Will semantic vector X semantic Mapping the linear layer to a programming language class label, and obtaining a corresponding programming language through a Softmax algorithm. The beneficial effects of the invention are as follows: the code type can be quickly identified according to the code fragments, so that the function of assisting a developer in quickly finding a solution on a question-answering website is achieved.","['G06F16/35', 'G06F40/289', 'G06F40/30', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
US12164550B2,Training a model for performing abstractive text summarization,"Techniques for training for and performing abstractive text summarization are disclosed. Such techniques include, in some embodiments, obtaining textual content, and generating a reconstruction of the textual content using a trained language model, the reconstructed textual content comprising an abstractive summary of the textual content generated based on relative importance parameters associated with respective portions of the textual content. In some cases, the trained language model includes a neural network language model that has been trained by identifying a plurality of discrete portions of training textual content, receiving the plurality of discrete portions of the training textual content as input to the language model, and predicting relative importance parameters associated with respective ones of the plurality of discrete portions of the training textual content, the relative importance parameters each being based at least on one or more linguistic similarity measures with respect to a ground truth.","['G06F16/345', 'G06F16/3334', 'G06F18/217', 'G06F18/22', 'G06N3/04', 'G06N3/045', 'G06N3/08']"
US20230376697A1,Mixture-Of-Expert Approach to Reinforcement Learning-Based Dialogue Management,"Systems and methods for dialogue response prediction can leverage a plurality of machine-learned language models to generate a plurality of candidate outputs, which can be processed by a dialogue management model to determine a predicted dialogue response. The plurality of machine-learned language models can include a plurality of experts trained on different intents, emotions, and/or tasks. The particular candidate output selected may be selected by the dialogue management model based on semantics determined based on a language representation. The language representation can be a representation generated by processing the conversation history of a conversation to determine conversation semantics.","['G06F40/35', 'G06F16/33295', 'G06F16/3334', 'G06F16/90332', 'G06F40/126', 'G06N3/006', 'G06N3/0455', 'G06N3/047', 'G06N3/0475', 'G06N3/084', 'G06N3/09', 'G06N3/092', 'G06N7/01', 'G06N3/044']"
US12158906B2,Systems and methods for generating query responses,"A computer-implemented method is disclosed. The method includes: obtaining at least one query; clustering a set comprising the at least one query into first clusters; for each first cluster, identifying, by a large language model (LLM), queries in the cluster that are semantically dissimilar; clustering the queries identified as semantically dissimilar into one or more second clusters; receiving an incoming query; matching the incoming query to a particular cluster from the first or second clusters; obtaining one or more generated response messages based on providing, to the LLM, data associated with the particular cluster for the incoming query.","['G06F16/338', 'G06F16/35', 'G06F16/355', 'G06F30/27']"
US12411841B2,Systems and methods for automatically generating source code,"A computer-implemented method is disclosed. The method includes: receiving a request for retrieval of data satisfying one or more criteria, the request including at least one data request parameter; searching a database storing example queries based on the request to identify at least one matching query; providing, to a large language model (LLM), an input prompt to generate a query purporting to retrieve data satisfying the one or more criteria, the input prompt including the at least one data request parameter and the at least one matching query as an example; and receiving, from the LLM, a result including the generated query.","['G06F16/2433', 'G06F16/245', 'G06F8/30']"
CN111506702A,"Knowledge distillation-based language model training method, text classification method and device","The disclosure relates to a knowledge distillation-based language model training method, a text classification method, a knowledge distillation-based language model training device, a text classification device, an electronic device, and a non-transitory computer-readable storage medium. The language model training method based on knowledge distillation comprises the following steps: a first word vector layer parameter determining step and a language model training step. The text classification method comprises the following steps: acquiring a text to be classified; obtaining a keyword coding list of the text to be classified through extraction based on the text to be classified; obtaining word vectors of all keywords corresponding to the texts to be classified through a language model according to the keyword coding list; and then obtaining a classification result of the text to be classified through the text classification layer. By adopting the knowledge distillation method, the dependence on the labeled sample is reduced while the model accuracy is kept, and the reasoning speed is facilitated to be accelerated by simplifying the model structure, so that the applicability and the reliability of the text classification method in the intelligent auxiliary secret-fixing system are improved.","['G06F16/35', 'G06F16/3344', 'G06F40/30', 'G06N3/044', 'G06N3/084']"
US20240320444A1,User interface for ai-guided content generation,"A computer-implemented method is disclosed. The method includes: obtaining at least one output of a generative model based on input of a first text prompt; presenting the at least one output via a user interface; receiving, via the user interface, user selection of a desired portion of the at least one output; modifying the first text prompt based on the user selection to obtain a second text prompt; and providing the second text prompt as input to the generative model for obtaining a second output.","['G06F40/166', 'G06F3/0482', 'G06F3/04842', 'G06F3/04845', 'G06F40/284', 'G06F40/40', 'G06N3/045', 'G06N3/08', 'G06T11/60', 'G06F3/0486']"
US11954429B2,Automated notebook completion using sequence-to-sequence transformer,"Generally discussed herein are devices, systems, and methods for generating an automatic interactive digital notebook completion model. A method can include receiving notebook content of an interactive digital notebook, the notebook content including a markdown cell followed by a code cell. The method can include generating input/output examples by, for each input/output example by masking one of (i) content of the markdown cell or (ii) content of the code cell resulting in a masked cell, identifying the masked cell and content of another cell of the markdown cell or the code that is not masked as an input for an input/output example, and identifying the content of the masked cell as an output for the input/output example. The method can include training, based on the input/output examples, a natural language processing model that generates a prediction of the content of a second masked cell as an output.","['G06N3/0455', 'G06F40/186', 'G06F3/04815', 'G06F40/40', 'G06N3/045', 'G06N3/084', 'G06F40/18', 'G06F8/30']"
WO2021100181A1,"Information processing device, information processing method, and program","An information processing device according to an embodiment is characterized by having a learning means for learning, with encoding layers from a first layer to an (N-n)-th layer having parameters learned in advance shared by a first model and a second model while N>n (N and n are integers of 1 or more) is satisfied, a parameter of a third model that encoding layers from a ((N-n)+1)-th to an N-th layer having parameters learned in advance are divided by the first model and the second model by multitasking learning including learning for the first model and re-learning for the second model with respect to a predetermined task.","['G06N3/045', 'G06N20/00', 'G06F40/20', 'G06F40/242', 'G06N3/088', 'G06N3/09', 'G06N3/096']"
US20240427879A1,Connecting natural and security language in the embedding space for better threat hunting and incident response,"Methods and apparatuses for improving the speed, quality, and relevance of automated responses provided by a question answering system for security data are described. The question answering system may generate and utilize a large language model that is trained to combine the language of security data, such as the language found in security logs and alerts, with natural language text. Given an input prompt (or a search query) from an end user of the question answering system, the question answering system may identify relevant content from the security data and display a response based on the relevant content. The question answering system may allow the end user of the question answering system to query security logs using natural language text without requiring the end user to provide a structured query and without requiring the security data be parsed and ingested into a database system.","['H04L63/1425', 'G06F21/552', 'G06F16/24522', 'G06F21/554', 'G06F2221/2101', 'H04L63/1441']"
WO2024199423A1,Data processing method and related device,"A data processing method, applied to the field of natural language processing. The method comprises: acquiring a plurality of first character strings, wherein different first character strings correspond to different user requests; splicing the plurality of first character strings into first input data, wherein the first input data comprises a plurality of vectors having a same length, each vector comprises at least one first character string, the length of each vector is a preset fixed length, and the number of vectors comprised in the first input data is a preset fixed number, wherein at least one vector comprises a character which does not belong to the plurality of first character strings and is used for padding the length of the vector to a fixed size; and inputting the first input data as one piece into a language model. According to the present application, under the condition that the size of input data input into a language model each time is kept static and consistent, the input data is obtained by splicing character strings based on a plurality of user requests, such that the language model can process the plurality of user requests at one time, and the processing efficiency is improved.","['G06F16/3329', 'G06N3/045', 'G06N5/04']"
US20240274286A1,Clinical Outcome Prediction By Application Of Machine Learning Models To Clinical Data,"A method includes receiving a clinical data table for a patient. The clinical data table stores clinical data associated with the patient in tabular form. The method also includes extracting, from the clinical data table, one or more categorical features and one or more continuous features, and determining, using a clinical prediction model, one or more predicted clinical outcomes for the patient based on the one or more categorical features and the one or more continuous features extracted from the clinical data table. The method also includes providing, for output from a client device associated with a user, the one or more predicted clinical outcomes for the patient.","['G06N3/08', 'G06N3/045', 'G06N3/0499', 'G16H10/60', 'G16H50/20', 'G16H50/30', 'G16H50/70']"
CN119782490B,Large language model-based generation type dialogue system,"The invention discloses a large language model-based generation type dialogue system, which relates to the technical field of artificial intelligence and aims at the limitation of the existing generation type dialogue system in multi-round dialogue; the dialogue state tracking module generates prompts by using key information codes, combines context conversion, utilizes transfer learning to finely tune a large language model, builds a sample library real-time retrieval example, realizes accurate tracking of multi-round dialogue states, the thinking chain reasoning module stores and manages intermediate state information, dynamically retrieves, invokes a tool to execute tasks and integrate results, builds reasoning logic to understand user intention, and the dialogue generation module captures context based on a transducer architecture large language model, adjusts generation strategies and realizes personalized reply. The method and the device are used for accurately identifying the user intention in the multi-round dialogue.",[]
CN117194702A,"Content generation method, device, electronic equipment and computer readable storage medium","Embodiments of the present disclosure provide a content generation method, apparatus, electronic device, and computer-readable storage medium, the method including: obtaining a chart image feature vector of a chart image to be processed; acquiring a prompt instruction text vector of a preset prompt instruction; the prompting instruction is used for indicating the content of the chart image to be generated; inputting the characteristic vector of the chart image and the text vector of the prompt instruction into a preset language characteristic extraction model of the chart image, extracting language characteristic information in the characteristic vector of the chart image by the language characteristic extraction model of the chart image according to the prompt instruction information in the text vector of the prompt instruction, and obtaining the language characteristic vector of the chart image according to the language characteristic information; and generating the content of the chart image according to the language feature vector of the chart image and the text vector of the prompt instruction. The embodiment improves the data analysis efficiency.",[]
WO2024191709A1,Conversational unit test generation using large language model,"A large language model, trained on source code and natural language text generates a unit test for a change to a file in a pull request of a code repository. An ordered sequence of prompts is created and each is applied serially to the large language model to perform an individual task that leads to the generation of the unit test. The unit test may be added to an existing file or generated as a newly-created file. Each prompt includes the data from a previously-issued prompt of the ordered sequence in order for the model to retain contextual knowledge learned previously. The model generates the unit test as update commands when the unit test is added to an existing file.","['G06F8/71', 'G06F11/36']"
CN117709461A,"Method, system, equipment and medium for generating auxiliary decision for power grid fault treatment","The invention belongs to the technical field of power grid fault treatment, and discloses a method, a system, equipment and a medium for generating auxiliary decision of power grid fault treatment; the generation method comprises the following steps: acquiring power dispatching fault handling information and inquiring a problem; based on the acquired power dispatching fault handling information and the query problem, query is performed by using a power large language model with human feedback alignment completed, and a power grid fault handling auxiliary decision prompt text is output; the power large language model for completing human feedback alignment is obtained after the human feedback alignment training is completed based on a pre-training power large language model based on a transducer architecture. The invention can realize the generation of the auxiliary decision prompt text for power grid fault treatment based on a transducer architecture and human feedback alignment adjustment, and has the advantages of good application range and accuracy and easy understanding by business personnel in the electric power field.","['G06N5/041', 'G06F16/3329', 'G06F16/35', 'G06F16/367', 'G06F40/295', 'G06N3/042', 'G06N3/045', 'G06N3/084', 'G06N5/022', 'G06Q50/06', 'Y04S10/50']"
US20250094821A1,Multi-task fine-tuning for planning performed by large language model,"Techniques are disclosed for fine-tuning a pre-trained machine learning model to be used by a digital assistant for supporting a user's interactions. In one aspect, a method includes accessing a set of training examples, generating a set of synthesized training examples using an iterative process including accessing a dialog script and corresponding prompt template and response template for a predefined scenario, generating one or more prompts based on the dialog script and corresponding prompt template, generating one or more responses associated with each of the one or more prompts based on the dialog script and the response template, and linking each of the responses with the associated prompts to generate one or more synthesized training examples in the set of synthesized training examples. The pre-trained machine learning model is then fine-tuned using the set of training examples and the set of synthesized training examples.","['G06N3/0475', 'G06N3/096']"
US12001518B2,"Method for predicting matching degree between resume and post, and related device","A method for predicting matching degree between a resume and a post, and a related device are provided in this disclosure. In the method for predicting the matching degree between the resume and the post, and the related device according to this disclosure, firstly the semi-structured keys and values in post information and resume information and their source are obtained. Then, the matching degree between the resume information and the post information is predicted by a prediction model including a cascaded pre-trained language model, a Transformer encoder and a single label classification model, based on the keys and values of a respective post information and resume information attribute, and corresponding source representations. Thus, by comprehensively searching internal interaction and external interaction of semi-structured multivariate attributes in person-post matching, the matching result is more accurate.","['G06F18/241', 'G06Q10/105', 'G06F17/16', 'G06F18/22', 'G06F18/2414', 'G06F18/2433', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N3/084', 'G06N5/01', 'G06N7/01', 'G06Q10/1053', 'G06N20/10']"
CN115329062B,Dialogue model training method under low-data scene and computer equipment,"The invention provides a dialogue model training method under a low-data scene and computer equipment, wherein the dialogue model training method under the low-data scene comprises the following steps: step one, adding a small-sized transformer self-attention layer on the language model 2, and training the additional small-sized transformer self-attention layer, wherein in the step of training, the parameters of the language model are fixed. And step two, forming a new transformer model by the trained additional transformer self-attention layer and the copied embedded layer, and continuing to train the new transformer model. The dialogue model training method under the low-data scene can train and obtain an available retrieval type end-to-end dialogue model through the training method under the condition of lacking dialogue data, the parameter quantity is small, overfitting is not easy to happen, and the reasoning and operation speed is high.","['G06F16/3329', 'G06N3/08']"
WO2024066618A1,Method and system for training large-scale language models,"A computer-implemented method is provided for model training performed by a processing system. The method comprises determining a set of first weights based on a first matrix associated with a source model, determining a set of second weights based on the set of first weights, forming a second matrix associated with a target model based on the set of first weights and the set of second weights, initializing the target model based on the second matrix, and training the target model.","['G06N3/0455', 'G06F40/40', 'G06N3/0499', 'G06N3/08', 'G06N3/088', 'G06N3/0475']"
US20250068903A1,Training and tuning of large language models for generating domain-specific predictions,"Various embodiments of the present disclosure provide methods, apparatus, systems, computing devices, computing entities, and/or the like for generating a plurality of training embeddings based on a pre-training dataset, wherein the plurality of training embeddings comprises one or more of descriptive embeddings, sequential ordering embeddings, age/time embeddings, locale embeddings, or encounter number embeddings; generating one or more initialized weights associated with respective one or more layers of a machine learning model based on the plurality of training embeddings; generating one or more fine-tuned weights for the machine learning model by updating at least a portion of the one or more initialized weights using a fine-tuning dataset associated with a target classification; and generating, using the machine learning model, one or more prediction scores for one or more prediction encounter data elements associated with the target classification, based on one or more input temporal sequence of encounters data records.","['G06N3/08', 'G06N3/045', 'G06N3/0499', 'G06N3/084']"
WO2024017287A1,Model training method and apparatus therefor,"A model training method, which relates to the field of artificial intelligence, and comprises: acquiring a first text and a second text, the first text and the second text being obtained via extraction from within an original code text, wherein the first text is a program code, the second text is an attribute description corresponding to the program code, and the first text and the second text comprise a known text and a prediction text; and according to the known text, by means of a natural language model, predicting a text of a text position corresponding to the prediction text to obtain a text prediction result, wherein the prediction text and the text prediction result are used for updating the natural language model. According to the present application, when a training sample synthesized by a program is constructed, a code text and a corresponding attribute description are extracted from within an original program code and serve as a pair of samples, thus reducing training difficulty and improving the prediction precision of a model.","['G06F40/166', 'G06F40/274', 'G06F40/279', 'G06N20/00']"
AU2020326435B2,Systems and methods of controllable natural language generation,The presently disclosed embodiments may include a computer readable medium including instructions that when executed by one or more processing devices cause the one or more processing devices to perform a method. The method may include: analyzing an electronic document text; identifying in the electronic document text a first drafted text element to be modified; causing the first drafted text element to be highlighted to a user on a display; generating at least one text output option that conveys a meaning associated with the first drafted text element but includes one or more changes relative to the first drafted text element; and causing the at least one text output option to be shown to the user via the display.,"['G06F40/186', 'G06F40/166', 'G06F3/0482', 'G06F40/117', 'G06F40/247', 'G06F40/253', 'G06F40/274', 'G06F40/30', 'G06F40/56', 'G06F40/58', 'G06N3/045', 'G06N3/0475', 'G06N3/088', 'G06N3/0895', 'G06N3/09', 'G06N3/092', 'G06N3/047']"
US20240346256A1,Response generation using a retrieval augmented ai model,"Systems, methods, apparatuses, and computer program products are disclosed for using retrieval augmented artificial intelligence to generate a response to a query. A first feature vector is generated based at least on the query. The first feature vector is compared to a plurality of second feature vectors to determine a subset of the second feature vectors that satisfy a predetermined condition. Augmentation information corresponding to the determined subset of second feature vectors are retrieved. An augmented prompt, generated based on the query and the retrieved augmentation information, is provided to a large language model. A response generated by the large language model is received.","['G06F40/35', 'G06F40/40', 'G06F40/194', 'G06F40/216', 'G06F40/30']"
US12182716B2,Compressing and decompressing data for language models,"Embodiments of the present disclosure include systems and methods for compressing and decompressing data generated by sub-blocks in a neural network. In some embodiment, an input matrix is received at a compression block in the neural network. The compression block compresses the input matrix into a compressed matrix and outputs the compressed matrix. The compressed matrix has a reduced dimensionality relative to a dimensionality of the input matrix. A decompression block retrieves the compressed matrix. The decompression block decompresses compressed matrix into a decompressed matrix and outputs the decompressed matrix. The decompressed matrix has a same dimensionality as the dimensionality of the input matrix. The compression and decompression blocks are optimized based on feedback received from the neural network.","['G06N3/084', 'G06N3/04', 'G06N3/045', 'G06N3/063', 'G06F40/47']"
US20240160858A1,Systems and methods for vision-language model instruction tuning,"Embodiments described herein provide a method of generating a vision-language task output to a text instruction relating to an input image, the method comprising receiving, via a data interface, the input image and the text instruction comprising an instruction relating to the image. The method further includes encoding, via an image encoder, the image into a first image representation. The method further includes generating, by a multimodal encoder, a second image representation based on cross-attending the first image representation to the text instruction. The method further includes generating, by a neural network based language model, a vision-language task output in response to the text instruction based on an input combining the second image representation and the text instruction.","['G06F40/40', 'G06V10/774', 'G06V10/82', 'G06V20/70']"
US20240046067A1,Data processing method and related device,"A data processing method includes: obtaining a first embedding vector for indicating a known data unit and a position of the known data unit and a second embedding vector for indicating a position of a to-be-predicted data unit; processing the first embedding vector by using a target encoder, to obtain an output vector; and processing the output vector and the second embedding vector by using a target prediction network, to obtain a to-be-predicted data unit. According to the method, M pieces of additional position information do not need to be separately set as input of the target encoder, and a quantity of latent variables of intermediate output of the target encoder is also consistent with a quantity of input embedding vectors, thereby reducing a computation amount and memory consumption of the target encoder.","['G06F40/30', 'G06F16/3344', 'G06N3/04', 'G06F40/211', 'G06N3/02', 'G06N3/045', 'G06N3/0455', 'G06V10/82', 'G06V20/56']"
WO2024233332A1,Systems and methods for vision-language model instruction tuning,"Embodiments described herein provide a method of generating a vision-language task output to a text instruction relating to an input image, the method comprising receiving, via a data interface, the input image and the text instruction comprising an instruction relating to the image. The method further includes encoding, via an image encoder, the image into a first image representation. The method further includes generating, by a multimodal encoder, a second image representation based on cross-attending the first image representation to the text instruction. The method further includes generating, by a neural network based language model, a vision-language task output in response to the text instruction based on an input combining the second image representation and the text instruction.","['G06N3/045', 'G06N3/09']"
CN116781341A,A decentralized network DDoS attack identification method based on large language model,"The invention discloses a large language model-based decentralized network DDoS attack identification method. The method comprises the following steps: 1) Collecting network traffic data, including unlabeled network traffic and decentralized network DDoS attack traffic; 2) Converting the collected network traffic into a word vector sequence; 3) Pre-training the large language model by using word vector sequences obtained by the unlabeled network traffic conversion; 4) Inputting a word vector sequence obtained by converting the decentralized network DDoS attack flow into a pre-trained large language model, and learning the characteristics of the decentralized network DDoS attack flow; 5) Inputting the characteristics into a fine tuning model to execute a prediction task, comparing the obtained prediction result with a corresponding labeling result, and optimizing parameters of the fine tuning model according to the comparison result; 6) And converting the network traffic to be identified into a word vector sequence, inputting the word vector sequence into the optimized fine tuning model, and identifying whether the network traffic is attack traffic.",[]
US20250117665A1,Methods and systems for evaluating and optimizing large language models and methods for personalized large language models,"A computer-implemented method for evaluating and optimizing a large language model (LLM) includes receiving input data for a large language model; transforming the input data into a standardized format compatible with a multi-metric evaluation framework; applying a set of evaluation metrics to the transformed data, wherein the set of metrics comprises a hallucination metric, a groundedness metric, a relevance metric, a recall metric, a precision metric, a consistency metric, and a Coherence metric; generating an error function that quantifies an overall performance of the LLM, wherein the error function takes as input parameters of the LLM and incorporates the set of evaluation metrics to produce an error score; minimizing the error function wherein a lower error score indicates better performance of the LLM; and outputting an improved LLM with improved parameters.","['G06N3/0895', 'G06N3/0475']"
WO2021139278A1,"Intelligent interview method and apparatus, and terminal device","Disclosed are an intelligent interview method and apparatus, and a terminal device. The method comprises: acquiring reply information of a candidate during an interview process, wherein the reply information comprises a plurality of reply statements (S101); using a preset language model to respectively convert the plurality of reply statements into corresponding statement vector information (S102); according to the statement vector information of the plurality of reply statements, determining statement set vector information corresponding to the reply information (S103); using the statement set vector information to calculate the relationship probability among a plurality of entities contained in the reply information (S104); extracting target relationship information from the plurality of entities according to the relationship probability (S105); and generating interview questions for the candidate on the basis of the target relationship information (S106). An important part in answer content of the candidate can be quickly extracted, thereby facilitating the asking of necessary and rational follow-up questions by an artificial intelligence interviewer, and thus generating an interview evaluation report. In addition, the interview evaluation report may be uploaded to a blockchain so as to guarantee the security, fairness and transparency thereof.","['G06F40/211', 'G06F16/27', 'G06F18/2415', 'G06F40/289', 'G06Q10/0639', 'G06Q10/1053']"
US20240419726A1,Learning to Personalize Vision-Language Models through Meta-Personalization,"Techniques for learning to personalize vision-language models through meta-personalization are described. In one embodiment, one or more processing devices lock a pre-trained vision-language model (VLM) during a training phase. The processing devices train the pre-trained VLM to augment a text encoder of the pre-trained VLM with a set of general named video instances to form a meta-personalized VLM, the meta-personalized VLM to include global category features. The processing devices test the meta-personalized VLM to adapt the text encoder with a set of personal named video instances to form a personal VLM, the personal VLM comprising the global category features personalized with a set of personal instance weights to form a personal instance token associated with the user. Other embodiments are described and claimed.","['G06F16/538', 'G06F16/535', 'G06F16/5866']"
CN118520912A,Sparse processing method and device for sparse attention network and electronic equipment,"The disclosure provides a sparse processing method and device of a sparse attention network and electronic equipment. The present disclosure relates to the field of artificial intelligence, and in particular, to the technical fields of large models, deep learning, natural language processing, and the like. The specific scheme is as follows: constructing mask sparse representation in the memory unit by adopting constraint conditions; wherein, the constraint condition is: the dimensions of the mask matrix are [ B, a, S ], B represents the batch size, a represents the number of heads, S represents the sequence length, and each element in the S dimension represents the first row of each column in the mask matrix to be masked; the computing unit acquires mask sparse representation from the memory unit, performs sparse processing on input data by using the mask sparse representation, and stores a sparse processing result into the memory unit. According to the scheme, the mask sparse representation is constructed by adopting the constraint condition, so that the memory consumption can be reduced from the square magnitude of the sequence length to the linear magnitude of the sequence length, the memory requirement during large model training is remarkably reduced, and the training efficiency is improved.","['G06N3/0495', 'G06N3/0455', 'G06N3/096']"
CN114757169A,Self-adaptive small sample learning intelligent error correction method based on ALBERT model,"The invention discloses an ALBERT model-based adaptive small sample learning intelligent error correction method, which has strong performability, can save computing resources and obviously improve the training effect of a small sample set, and is realized by the following technical scheme: based on an ALBERT model, performing entity extraction based on small sample learning and entity extraction under a zero sample condition, collecting a historical full data set, preprocessing historical full corpus data, performing unsupervised pre-training learning by taking ALBERT _ TINY as a baseline model, and generating a pre-training language model; taking the manual labeling data set as a training set to generate an entity extraction model to be online; taking the screened small sample test set as a prediction corpus, and generating a result set after self-adaptive prediction error correction; and performing iterative training by taking the generated result set as a training set of the next round of pre-training language model training, and stopping iteration when the result data set reaches a set threshold standard to finish the self-adaptive learning of the test set of the batch.","['G06F40/232', 'G06F40/295', 'G06N3/045']"
US11874127B1,Language models and machine learning frameworks for optimizing vehicle navigation routes and vehicle operator sessions,"This disclosure relates to improved techniques for personalizing vehicle routes and operator sessions using pre-trained machine learning language models. In certain embodiments, a language model is trained on operator interaction data to learn operator route preferences for vehicle operators. These learned operator route preferences can be leveraged to optimize and personalize vehicle routes and operator sessions in various ways. Other embodiments are disclosed herein as well.","['G01C21/3608', 'G01C21/3438', 'G01C21/3484', 'G01C21/3617', 'G06Q30/0206', 'G06Q50/30', 'G06Q50/40', 'G10L15/183']"
WO2022222231A1,"Drug-target interaction prediction method and apparatus, device, and storage medium","The present application relates to the technical field of intelligent decision of artificial intelligence, and provides a drug-target interaction prediction method and apparatus, a device, and a storage medium. The method comprises: according to a first pre-training set that is established on the basis of the SMILES sequence data of a label-free drug molecule, performing pre-training to obtain a pre-trained molecular graph model, and according to a second pre-training set that is established on the basis of label-free protein sequence data, performing training to obtain a pre-trained protein sequence language model; obtaining a training set of labeled drug-target pairs, and according to the drug-target pairs in the training set, an information label representing the interaction information of each drug-target pair, the pre-trained molecular graph model, and the pre-trained protein sequence language model, training a prediction model of a drug-target interaction to obtain a target prediction model; and by means of the target prediction model, predicting an interaction between a drug molecule to be predicted and a targeted target. The present application can improve the efficiency and accuracy of the drug-target interaction prediction.","['G16B15/30', 'G06F18/214', 'G16B40/00', 'Y02A90/10']"
US12142371B1,Low-latency conversational artificial intelligence (AI) architecture with a parallelized in-depth analysis feedback loop,"In some aspects, a multi-turn conversational system includes: an artificial intelligence to provide a conversation interface configured to execute multiple turns of human-like conversation with a user and a control logic, in communication with the conversation interface, and configured to generate one or more control signals based on evaluating multiple turns of upstream human-like conversation between the conversation interface and the user. The control signals contribute in part to construction of multiple turns of downstream human-like conversation between the conversation interface and the user.","['G16H40/20', 'G10L15/183', 'G10L15/22', 'G10L25/66', 'G16H10/20', 'G16H10/60', 'G10L15/18', 'G10L15/1822', 'G10L15/19', 'G16H20/10', 'G16H20/60', 'G16H50/20', 'G16H50/70', 'G16H70/40', 'H04L51/02']"
CN117352049A,A parameter-efficient protein language model design method based on self-supervised learning and Kronecker integral decomposition,"The invention discloses a parameter efficient protein language model design method based on self-supervision learning and Kronecker product decomposition. Based on the transfomer model, useful biological information is learned from large-scale protein sequence data using self-supervised learning techniques. In order to reduce the demand of a protein language model on a video memory for deployment, the invention firstly represents the weight matrix of a full connection layer as the sum of Kronecker products of a plurality of small matrices, and then designs a singular value decomposition method based on the Eckhart-Young theorem to initialize a small matrix parameter set. The design method provided by the invention can remarkably reduce the number of parameters while maintaining the modeling capability of the protein language model. In addition, the pre-trained protein language model obtained based on the invention can provide embedded features rich in biological information for tasks such as protein structure prediction, protein folding recognition, protein function prediction and the like.","['G16B20/00', 'G06F18/214', 'G16B40/20']"
CN118115847A,A multimodal large language model for multi-sensor remote sensing image interpretation,"The invention discloses a multi-mode large language model for realizing multi-sensor remote sensing image interpretation, which comprises the following steps: the visual enhancement module is used for extracting and fusing coarse-granularity semantic perception information and fine-granularity detail perception information in the remote sensing image; the cross-modal interaction understanding module is used for fusing visual information and semantic perception information and enhancing interaction and understanding between visual perception and language understanding; and the unified instruction fine tuning module is used for enhancing the capability of the model to follow instructions in downstream tasks and expanding the applicability of the model from the natural field to the remote sensing field. The invention can unify various multi-sensor remote sensing interpretation tasks understood by the remote sensing image.","['G06V10/806', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06V10/52']"
KR20240066745A,Korean Light Weight Language Model,"Disclosed is a method for lightening a korean mass language model. in the method, a teacher model and a student model are divided into left and right sides and are composed of a transformer architecture. query, key, and value values can be obtained from each transformer layer. a self-concentration map composed of a square matrix is created through a query-key value. a value relationship map is obtained through self-internalization using the value value in each layer. after obtaining the self-concentration map and the value relationship map from each transformer layer of the teacher model and the student model, the student model learns to resemble the map distribution of the teacher model.","['G06N20/00', 'G06F40/40', 'G06N7/01']"
CN117314692A,Network video teaching interaction method based on artificial intelligent model,"With the continuous development of network science and technology and society, network video teaching gradually becomes a popular learning mode, the network video teaching has certain limitation in the aspect of interaction between teachers and students, students can encounter the problem of untimely reply when asking questions to teachers, and teachers are relatively less in the aspect of obtaining teaching feedback, so that teaching quality is influenced. Therefore, the invention provides a video teaching interaction method based on an artificial intelligence model. The method comprises the following steps: firstly, an existing artificial intelligent large model is utilized to build an artificial intelligent model for teaching and is deployed on a server, secondly, a training set is built by analyzing teaching videos and teaching schematics uploaded by teachers, and finally, virtual teachers formed by answer randomness parameters of API interfaces are adjusted. The virtual teacher quickly replies questions of the students, and meanwhile, the questions of each student are collected and organized into classroom teaching feedback, and the classroom teaching feedback is fed back to the teacher, so that the teaching quality is improved.","['G06Q50/205', 'G06N3/0455', 'G06N3/096', 'G06Q10/06395', 'G09B7/02']"
WO2021217935A1,"Method for training question generation model, question generation method, and related device","A method for training a question generation model, a question generation method, and a related device. The method comprises: pre-training an initial model to obtain a pre-trained language model, and adjusting a mask matrix during pre-training so as to realize three language models; acquiring question-and-answer information that comprises a question text and an answer text; extracting, from the answer text, a key entity related to the question text; configuring a network in the pre-trained language model such that same adapts to the generation of a Chinese text; inputting the key entity and the answer text into the pre-trained language model, so as to obtain a predicted question text; according to the predicted question text and the question text, determining a prediction error; and adjusting the model according to the prediction error, so as to obtain a question generation model. The method does not need to rely on manual data labeling. The method belongs to the field of artificial intelligence and further relates to blockchain technology, and the predicted question text can be stored in a blockchain node.","['G06F16/3329', 'G06F16/951', 'G06F18/214', 'G06F40/295']"
CN111914553B,Financial information negative main body judging method based on machine learning,"The invention discloses a financial information negative main body judging method based on machine learning, and belongs to the field of machine learning. The method comprises the following steps: extracting financial information corpus and performing data cleaning and enhancement; converting the entity sentence and the content description sentence into an entity sentence vector and a content description sentence vector respectively by adopting a Bert model; splicing is completed according to the dimension, and spliced sentence vectors are formed; respectively inputting the spliced sentence vectors into a support vector machine model, a long-short-term memory model and a distributed gradient enhancement library model, and judging the positive and negative surfaces of the entity; fusing the support vector machine model judgment result, the long-short-period memory model judgment result and the XGBoost model judgment result through voting model fusion to obtain a positive and negative entity prediction result; and carrying out post-processing on the entity positive and negative prediction results, and extracting all negative entities corresponding to the financial text. The method can be used for the financial text to carry out the information mining process.","['G06F40/289', 'G06F16/35', 'G06F18/2411', 'G06F18/24323', 'G06F40/126', 'G06F40/211', 'G06F40/295', 'G06N20/10', 'G06N3/045', 'G06N3/049', 'Y02D10/00']"
CN117829819B,"Fault processing method, device and computer readable storage medium","The application discloses a fault processing method, equipment and a computer readable storage medium, which are applied to the technical field of data processing, wherein the method comprises the following steps: analyzing the repair voice information and/or repair image information currently sent by the user terminal to obtain an understanding text, and determining fault information corresponding to the fault equipment based on the understanding text; determining a fault processing scheme corresponding to the fault equipment according to the fault information; when the fault processing scheme is an assistance processing scheme, generating a repair work order according to the fault information; and determining a target processing object corresponding to the repair work order, so that the target processing object performs fault processing based on the repair work order. According to the technical scheme, the fault processing cost of the fault equipment is reduced, and the fault processing efficiency is improved.","['G06Q10/20', 'G06F16/3343', 'G06F16/3344', 'G06F16/35', 'G06F16/55', 'G06N3/0464', 'G06N3/08', 'G06Q10/063', 'G06V10/764', 'G06V10/82']"
WO2023129955A1,Inter-model prediction score recalibration,"The technology disclosed relates to inter-model prediction score recalibration. In one implementation, the technology disclosed relates to a system including a first model that generates, based on evolutionary conservation summary statistics of amino acids in a target protein sequence, a first pathogenicity score-to-rank mapping for a set of variants in the target protein sequence; and a second model that generates, based on epistasis expressed by amino acid patterns spanning the target protein sequence and a plurality of non-target protein sequences aligned in multiple sequence alignment, a second pathogenicity score-to-rank mapping for the set of variants. The system also includes a reassignment logic that reassigns pathogenicity scores from the first set of pathogenicity scores to the set of variants based on the first and second score-to-rank mappings, and an output logic to generate a ranking of the set of variants based on the reassigned scores.","['G16B20/20', 'G16B40/20', 'G16B40/30', 'G16B15/00', 'G16B30/10']"
US20240403438A1,Code-change based prompt for code repair,"A source code repair system generates a prompt including a few-shot examples of code changes made previously to correct a particular source code vulnerability. The prompt is given to a large language model to generate repair code for a source code snippet having the same vulnerability. Code changes made for a particular source code vulnerability, in the form of code diffs, are clustered into groups of closely related code change embeddings. A select few of the code diffs in each group having a closest mean of the cluster are used as the few-shot examples for the corresponding source code vulnerability.","['G06F11/0793', 'G06F21/577', 'G06F8/51', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06F11/3604', 'G06F2221/033']"
WO2024073087A1,Revision of and attribution for output of text generation models,"Existing language models (LMs) can excel at some tasks such as question answering, reasoning, and dialog. However, they can sometimes generate unsupported or inaccurate content. Therefore, in the present disclosure, systems and methods are provided for improving the reliability of LMs' generated output. First, systems and methods are provided for editing LMs' generated content based on a machine-learned comparison between the generated content and related evidence snippets, which can be retrieved and extracted using a machine-learned query generation model and a machine-learned relevance model. Second, systems and methods are provided for attributing parts of LM-generated content (e.g. factual claims) to related evidence snippets. Thus, the present disclosure can improve the reliability of LM output, both by increasing the factual accuracy of edited content and by allowing a user or computing system to know whether parts of the generated content are supported or contradicted by external evidence.","['G06F16/3329', 'G06F16/90332', 'G06F40/169', 'G06F40/216', 'G06F40/30']"
US12181844B2,Building management system with natural language model-based data structure generation,"Systems and methods are disclosed relating to building management systems with language model-based data structure generation. For example, a method can include receiving a query to select, from a plurality of data sources of a building management system, a selected one or more data sources according to a characteristic indicated by the query in at least one of a natural language representation or a semantic representation. The method can further include applying the query as input to a machine learning model to cause the machine learning model to generate an output indicating the selected one or more data sources, the machine learning model configured using training data comprising sample data and metadata from the plurality of data sources. The method can further include presenting, using at least one of a display device or an audio output device, the output.","['G05B13/027', 'G06F16/248', 'G06N3/0895']"
US20240311617A1,Controlling agents using sub-goals generated by language model neural networks,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for controlling agents using a language model neural network and a vision-language model (VLM) neural network.","['G06N3/045', 'G06N3/0455', 'G06F40/40', 'B25J9/163', 'G06F40/30', 'G06F40/56', 'G06N3/08', 'G06V10/82']"
CN115858812A,Embedded alignment method constructed by computer,"The invention designs an embedded alignment method constructed by a computer, and through the method, the invention constructs the entity name and attribute information of a knowledge graph through a large-scale pre-training language model by the computer, so that the semantic information on the knowledge graph can be comprehensively utilized. Experimental results on different data sets show that the method has stronger robustness. Meanwhile, an ablation experiment is set to verify the effectiveness of the invention.","['G06F16/367', 'G06F17/16', 'G06F18/22', 'G06F40/295']"
CN112364639B,Context-sensitive paraphrasing generation method and system based on pre-training language model,"The invention discloses a context sensitive paraphrase generating method and system based on a pre-training language model, wherein the method comprises the following steps: acquiring a training data set; wherein, the training data set comprises the paraphrased words, the context where the paraphrased words are located, and the paraphrases corresponding to the paraphrased words; constructing a paraphrase model for generating paraphrases for paraphrased words; wherein the paraphrase model is based on an encoder-decoder framework, and an encoder of the paraphrase model is a pre-trained language model; training the paraphrase model based on the training dataset; generating paraphrases of paraphrased words to be paraphrased based on the paraphrased words to be paraphrased and the distributed vector representation of the contexts of the paraphrased words to be paraphrased through a trained paraphrasing model. The invention has the advantages of clear logic, high efficiency and high accuracy, and solves the problem that the prior art can not accurately generate paraphrases for paraphrased words.","['G06F40/284', 'G06F18/214', 'G06F40/205', 'G06N3/045', 'G06N3/08']"
EP4457823A1,Inter-model prediction score recalibration,"The technology disclosed relates to inter-model prediction score recalibration. In one implementation, the technology disclosed relates to a system including a first model that generates, based on evolutionary conservation summary statistics of amino acids in a target protein sequence, a first pathogenicity score-to-rank mapping for a set of variants in the target protein sequence; and a second model that generates, based on epistasis expressed by amino acid patterns spanning the target protein sequence and a plurality of non-target protein sequences aligned in multiple sequence alignment, a second pathogenicity score-to-rank mapping for the set of variants. The system also includes a reassignment logic that reassigns pathogenicity scores from the first set of pathogenicity scores to the set of variants based on the first and second score-to-rank mappings, and an output logic to generate a ranking of the set of variants based on the reassigned scores.","['G16B30/10', 'G16B5/20', 'G16B20/00', 'G16B20/20', 'G16B40/00', 'G16B40/20', 'G16B40/30', 'G16B15/00']"
US12118513B1,Providing generative artificial intelligence (AI) content based on existing in-page content in a workspace,A method for creating in-block content presented in a block on a page of a workspace. The block is configured to initiate a generative process to create in-block content of a particular type. The method includes determining a selection of in-page content based on a location of the block relative to the in-page content and the particular type of in-block content. The method can include causing a generative function to create generative content of the particular type based on the selection of the in-page content. The method can further include populating a block area to present the generative content.,"['G06Q10/101', 'G06F40/40', 'G06Q10/103', 'G06F16/955', 'G06F3/0482', 'G06N3/04']"
WO2024076445A1,Transformer-based text encoder for passage retrieval,"A computing system includes a logic subsystem and a storage subsystem holding instructions executable by the logic subsystem to implement a transformer-based text encoder. The transformer-based text encoder includes a plurality of transformer blocks previously-trained to apply encoding operations to computer-readable text representations of input text strings, the computer-readable text representations including computer-readable question representations of input text questions, and computer-readable passage representations of input text passages. The plurality of transformer blocks include a shared transformer block trained for both the computer-readable question representations and the computer-readable passage representations and a specialized transformer block including two or more input-specific subnetworks, and a routing function to select an input-specific subnetwork of the two or more input-specific subnetworks for each of the computer-readable text representations.","['G06F40/30', 'G06N3/045', 'G06N3/09', 'G06F40/216', 'G06F40/289']"
US20250141899A1,Large language model based intelligent malicious packet detection,"A method and system for detecting malicious network packets via an intelligent large language model are described. In addition, a method for fine-tuning a pretrained large language model to detect malicious network packets is described. The training process generates a plurality of embeddings from input network packet data, generates clusters for those embeddings, performs an entropy analysis, and calculates a loss value. Contrastive learning is used to further fine-tune the large language model based embedder.","['G06N3/0455', 'H04L63/1416', 'H04L63/1425', 'H04L63/1458']"
US20240385814A1,Hybrid inference system for cogs reduction,A hybrid inference system for a coding assistant utilizes a routing model to predict whether output generated by a large language model for a given prompt would be accepted by a user of the coding assistant. The routing model routes the prompt when the routing model indicates that the output generated by the large language model is likely to be accepted. The routing model routes the prompt to a local model when the output generated by the large language model is not likely to be accepted. The routing model is trained on the historical output generated by the large language model for various prompts and the acceptance or rejection of the output by users of the coding assistant.,"['G06F8/35', 'G06F11/3608']"
CN116166688A,"Business data retrieval method, system and processing equipment based on natural language interaction","The invention discloses a service data retrieval method, a system and processing equipment based on natural language interaction, relates to the technical field of natural language processing, and solves the technical problem that the existing natural language interaction data retrieval system can only input the content to be queried at one time generally, and influences the accuracy and fluency of query. The method comprises the following steps: sorting dialogue scenes, and setting dialogue scenes with query probability exceeding a set threshold as independent intentions; based on the independent intention data, generating a training sample after processing to obtain a dialogue process management module; the user inputs the query intention, the dialogue process management module judges whether the minimum query condition is met, if yes, the query result is output, and if not, the user is prompted to increase and/or modify the query condition. According to the invention, the user can supplement and/or modify the query conditions, the accuracy and fluency of the query are greatly improved, and the use experience of the user is better.","['G06F16/243', 'G06F16/242', 'Y02D10/00']"
WO2023235346A1,Prompting machine-learned models using chains of thought,"Example embodiments of aspects of the present disclosure provide an example computer-implemented method for improved prompting of a machine-learned model. The example method can include obtaining an instructive sequence descriptive of an instructive query, an instructive response, and an instructive trace of intermediate states from the instructive query to the instructive response. The example method can include inputting, to a machine-learned model, the instructive sequence and an operative query, wherein the machine-learned model is configured to process the operative query with attention over the instructive sequence. The example method can include generating, using the machine-learned model and responsive to the operative query, an operative response.","['G06N3/0455', 'G06N5/022', 'G06N3/096', 'G06N3/042']"
WO2022241190A2,Machine learning-based systems and methods for extracting information from pathology reports,"Described herein are machine learning-based systems and methods for extracting information from pathology reports are related machine learning model training methods. Also described herein are machine learning-based systems and methods for predicting site and histology codes for a disease. An example system for automatically extracting information from pathology reports includes a transformer-based machine learning model, and a computing device. The computing device includes a processor and a memory operably coupled to the processor, and the memory having computer-executable instructions stored thereon. The computing device is configured to receive a pathology report, transmit the pathology report to the transformer-based machine learning model, and receive at least one of a site description or a histology description for a disease. The transformer-based machine learning model is configured to extract the at least one of the site description or the histology description from the pathology report.","['G16H50/70', 'G16H10/20', 'G16H20/10', 'G16H50/20']"
CN112507628B,Risk prediction method and device based on deep bidirectional language model and electronic equipment,"The invention provides a risk prediction method and device based on a deep two-way language model and electronic equipment. The method comprises the following steps: acquiring position text information of a historical user, and extracting address text information of the historical user at least one specific time point; pre-training a deep bidirectional language model by using a Bert model based on a self-attention mechanism for semantic vector conversion; splicing the address text information by using the deep bidirectional language model, and converting word vectors and sentence vectors to generate user address characteristic data; constructing a risk prediction model, and training the risk prediction model by using a training data set; and calculating the risk assessment value of the current user by using the risk prediction model so as to predict the risk of the current user. According to the method, the risk prediction model is obtained by adding the Sigmoid layer to the depth bidirectional language model, effective characteristic data can be mined, risk users can be identified more accurately, and the model precision can be further improved.","['G06F30/27', 'G06F18/214', 'G06F18/23213', 'G06F40/205', 'G06F40/284', 'G06F40/289', 'G06N3/044', 'G06N3/045', 'G06Q10/04', 'G06Q10/0635']"
WO2023168601A1,"Method and apparatus for training natural language processing model, and storage medium","The present disclosure relates to a method for training a natural language processing model, comprising: obtaining a sample text of a natural language; determining a triple in the sample text, wherein the triple comprises a relationship between two entities in the sample text and the two entities; processing the sample text on the basis of the triple to obtain a knowledge fusion vector; and inputting the knowledge fusion vector into a natural language processing model for training to obtain a target model. According to the present disclosure, the sample text can be processed on the basis of the triple in the sample text to obtain the knowledge fusion vector; compared with the sample text, the knowledge fusion vector comprises other knowledge information other than sample text information, such as entity information in the text and relationship information between entities, so that a computer can obtain the real meaning of the sample text more accurately; therefore, the target model obtained by training the natural language processing model on the basis of the knowledge fusion vector achieves a better natural language processing effect.","['G06F40/58', 'G06F40/20', 'G06F40/30']"
CN117854595A,Large-scale protein language model specific to DNA binding protein field,"The invention discloses a large-scale Protein Language Model (PLM) specific to the field of DNA Binding Proteins (DBP). Comprising the following steps: the UniProtKB data set annotated as a DBP sequence builds a non-redundant DBP sequence based on a full-space multitasking model (ESM 2), and fine-tunes downstream tasks based on domain-adaptive pre-training of ESM 2; experimental results of the invention on four downstream tasks related to DBP (namely DNA binding protein, DNA binding residue, transcription factor and DNA binding Cys2His2 zinc finger prediction) show that compared with the original ESM2, ESM-DBP provides better DBP characteristic characterization, thereby improving prediction performance and being superior to other most advanced prediction methods in accuracy. The salient manifestation of ESM-DBP in transcription factor prediction is mainly due to the high sensitivity to various DNA binding domains by an interpretability analysis of the integrated gradient algorithm. In addition, the invention also has good performance on DBPs with a small amount of similar homologous sequences, and the generalization effect is better than ESM2.","['G16B40/00', 'G06N3/0442', 'G06N3/045', 'G06N3/0499', 'G06N3/088', 'G16B30/10', 'G16B50/00']"
US20240394705A1,Smart contract creation and management using generative artificial intelligence with model merging,"Methods and systems for smart contract creation, validation, and monitoring using generative artificial intelligence models are described. In example implementations, such models may be constructed as merged models from base models and secondary models. The secondary models may be fine-tuned and task-specific. By way of example, the merged models may be used for generation of smart contract code, generation of synthetic data used to validate or test smart contract code, or in cooperation with agents to monitor execution of smart contract code once published on the blockchain.","['G06Q20/401', 'G06Q20/0655', 'G06Q20/381', 'G06Q20/389', 'G06Q20/405', 'G06Q20/407']"
US20240428052A1,Methods and systems for training medical machine-learning models,"A generative machine-learning model may be trained using communication session data to tailor a generative model on particular classes of communications. A set of communications sessions may be received with each communication session including, for example, communications between a doctor and a patient. A set of features may be generated by a natural language model from communications extracted from the communication sessions. A subset of the set of communication sessions may be defined based on the set of features. A training dataset may be defined using the subset of the set of communication sessions and used to train a machine-learning model. The machine-learning model can be configured to generate one or more contexts associated with a feature of the set of features. Upon receiving a request associated with a particular, a response may be generated using the machine-learning model and a feature vector derived from the request.","['G06N3/0475', 'G06F16/635', 'G06F21/32', 'G06F40/35', 'G06F40/58', 'G06N20/00', 'G06N3/045', 'G06N3/0455', 'G06N3/047', 'G06T7/50', 'G06V10/44', 'G06V10/82', 'G06V40/168', 'G06V40/172', 'G16H10/60', 'G16H40/67', 'G16H80/00', 'H04L12/1831', 'H04L65/1069', 'H04L65/1083', 'H04L67/306', 'G06F3/0484', 'G06T2207/20084']"
CN117668177A,"Processing method, device, equipment and storage medium of financial question-answering text","The application belongs to the technical field of natural language processing, and provides a processing method, a device, equipment and a storage medium of a financial question-answering text, wherein the method comprises the following steps: acquiring financial question-answering text training data; the training data is utilized to conduct fine tuning on the pre-training language model, so that a parameter variation matrix of each layer of transformers is obtained according to the financial question-answering text training data, and low-rank decomposition is conducted on the parameter variation matrix, so that two low-rank matrixes of each layer of transformers are obtained; updating the two low-rank matrixes until each layer of convertors converges; acquiring a word vector matrix of a financial question-answering text to be predicted; and processing the word vector matrix of the financial question-answering text to be predicted through the pre-training language model and the fine-tuned language model to obtain a prediction vector matrix. The application expressive force of the model in the intelligent question-answering vertical scene of the financial service is improved, and the accuracy of the model prediction result is improved.","['G06F16/3329', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
US20240220722A1,Multi-modal prompt learning for representation transfer on image recognition tasks,"A method and system for multi-modal prompt learning of vision-language models. Encodings of image-text pairs can be combined with image prompts and text prompts before being input into an image encoder and text encoder of a vision-language model respectively. The image prompt can be generated using the text prompt using a vision-language coupling function to encourage synergy between the two prompts. The combination of encodings and prompts can be fed through the transformer layers of the encoders, and the output of each layer can be combined with a new prompt before entering the next layer, up until a specific depth. The subsequent transformer layers can process the output and generate a final representation for the image and text which can then be used for downstream tasks.","['G06F40/284', 'G06F40/126']"
US20240404632A1,Method for Sequence-Based Prediction of Controlled Terms and Generating Protein Sequences from Controlled Terms using Enhanced Large Language Models,"The present invention relates to a method for enhancing the creativity of a generative pre-trained Large Language Model (LLM) in protein sequence generation and predicting controlled terms from protein sequences. The method includes incorporating 22 novel names representing the 22 amino acids into the vocabulary of the pre-trained LLM, conducting self-supervised learning using protein sequences encoded with the novel names to improve the LLM's comprehension and generation of coherent protein sequences, performing supervised learning using protein sequences to refine the LLM's ability to predict controlled terms based on protein sequences, and performing supervised learning to refine the LLM's ability to generate protein sequences based on controlled terms. The method includes generating the novel names either through a computer program or manually and utilizing datasets of protein sequences and their corresponding controlled terms from a protein database. The self-supervised learning employed is a masked language model (MLM). Additionally, an alternative method is disclosed, which involves identifying a set of 22 amino acid names from the original vocabulary of the pre-trained LLM and proceeding with self-supervised and supervised learning steps using the selected names. The two methods can be used independently or in combination to enhance the creativity of the LLM in achieving protein sequence generation and predicting controlled terms from protein sequences.","['G06N3/09', 'G06N3/0455', 'G06N3/0475', 'G06N3/0895', 'G16B20/30', 'G16B30/00', 'G16B40/20']"
CN110795549A,"Short text dialogue method, device, device and storage medium","The application discloses a short text dialogue method, a device, equipment and a storage medium, wherein the method comprises the following steps: acquiring a query text; calling a short dialog generation model based on a language model to predict the query text to obtain a reply text; a mixed attention layer is arranged in the short conversation generation model based on the language model, and the mixed attention layer simultaneously comprises a self-attention mechanism and an attention mechanism for supervising the query text; and outputting the reply text. According to the method and the device, the S2S framework is replaced by the short text dialogue model based on the language model, and meanwhile the hidden layer representation of the query text is supervised by the mixed attention layer in the model, so that the short text dialogue model can capture the dependence of long and short distances, and can also give an important consideration to the information of the query text during prediction, and the reply text which is semantically closely related to the query text and is rich in form can be generated.","['G06F16/3329', 'G06N20/00']"
CN118536087A,Large language modeling method for hierarchical concept prediction,"The invention provides a large language modeling method for hierarchical concept prediction, which aims to improve the efficiency and reasoning performance of a large language model. Comprising the following steps: a sequential multi-scale discrete quantization automatic encoder (Seq-MultiScale VQVAE) is used for converting sequential data represented by texts into a hierarchical concept token tree, a probability language model based on the hierarchical concept token tree is provided, and an autoregressive modeling method based on the probability language model of the token tree is provided. Compared with the traditional method for predicting the next word, the large language modeling method for hierarchical concept prediction provided by the invention has better data efficiency and can obtain better reasoning performance by predicting the next concept; in addition, the probability model based on the hierarchical concept token tree can greatly reduce the attention length of the corresponding autoregressive method, so that training and reasoning efficiency is improved.","['G06F18/27', 'G06F16/35', 'G06F40/126', 'G06F40/289', 'G06F40/30']"
CN119396975A,A knowledge question answering system based on large language model,"A knowledge question-answering system based on a large language model. The method comprises the steps of acquiring data from a data source, preprocessing the data, constructing a knowledge graph in the power operation and maintenance field, converting text contents into semantic vectors, integrating the semantic vectors to form a power operation and maintenance knowledge base, carrying out problem recognition analysis, converting user problems into semantic vectors, carrying out semantic retrieval in the power operation and maintenance knowledge base, recalling knowledge information related to the problem semantics, sequencing retrieval results based on the semantic relevance, knowledge confidence and information importance, and integrating the knowledge information and the user problems into input of a large language model to generate prompt information, adopting the large language model finely tuned based on the data in the power operation and maintenance field, and generating answers in a natural language form according to the prompt information. The scheme of the invention utilizes the deep learning technology to understand the semantics of the user problem and carries out accurate retrieval in a knowledge base.","['G06F16/3329', 'G06F16/3344', 'G06F16/3347', 'G06F16/367', 'G06N5/022', 'G06N5/04', 'G06Q50/06', 'Y04S10/50']"
WO2024091359A1,Entity extraction based on edge computing,"The present disclosure proposes a method, an apparatus and a computer program product for entity extraction based on edge computing. A web document may be obtained. A text feature of the web document may be identified. A visual feature corresponding to the text feature may be identified. An entity type sequence corresponding to the web document may be extracted based on the text feature and the visual feature.","['G06F16/957', 'G06F40/295', 'G06F40/143', 'G06F40/279', 'G06F40/30', 'G06N3/08']"
WO2024191902A1,Digital intelligence system,A question answering system is configured to extract answers to queries from a collection of data. The system includes an extractive stage configured to receive a query and to determine one or more answers to the query from the collection of data and one or more generative large language model stages configured to pre-process the query before it is provided to the extractive stage and/or to post-process the answers to the query.,"['G06F16/3331', 'G06N20/00', 'G06N3/045', 'G06N3/0475', 'G06N3/08', 'G06N3/096', 'G06N5/022', 'G06N5/04', 'G06N5/041', 'G06N7/01']"
US12050599B1,Intelligent knowledge-based question answering system,"A question answering system is provided that can be used to supply workers at a jobsite with relevant answers or information for situations occurring at the jobsite. The question answering system can include a knowledge base with different types of content that are related to situations that may occur at the jobsite. When a user submits a query to the question answering system, the system can then determine whether an appropriate response to the query is contained in the knowledge base. If the knowledge base does not have an appropriate response, then the system submits the query to a third party information source such as a generative artificial intelligence tool for an answer. The system then analyzes the response from the third party information source and selects either the response from the third party information source or the response from the knowledge base to provide to the user.","['G06F16/243', 'G06F16/24578']"
WO2023177108A1,Method and system for learning to share weights across transformer backbones in vision and language tasks,"A method of training a model includes configuring a first transformer for visual learning with a first set of weights, configuring a second transformer for textual learning with a second set of weights, adjusting at least the second set of weights based on minimizing a weight difference between the first set of weights and the second set of weights, replacing the first set of weights for the first transformer with the adjusted second set of weights, and updating the first transformer based on the adjusted second set of weights.","['G06N3/0464', 'G06N3/08', 'G06N3/045', 'G06N3/0495', 'G06N3/063', 'G10L15/26', 'G10L25/54']"
CN114678011A,"Speech recognition method, speech recognition apparatus, electronic device, storage medium, and program product","The embodiment of the invention provides a voice recognition method, a voice recognition device, electronic equipment, a storage medium and a program product, wherein the method comprises the following steps: acquiring a voice signal to be subjected to voice recognition; performing voice recognition on the voice signals based on the acoustic model and the language model; the acoustic model and the language model are constructed based on a Transformer structure of a modified self-attention module, and the self-attention value of the self-attention module in the Transformer structure of the modified self-attention module is obtained based on a weight matrix of a neural network. The embodiment of the invention can improve the calculation speed of the self-attention value, further improve the speed of the improved Transformer structure self-attention module for outputting the result, and improve the processing speed of the acoustic model and the language model constructed by the improved Transformer structure, thereby improving the processing speed of the voice recognition as a whole.","['G10L15/063', 'G10L15/02', 'G10L15/16', 'G10L15/18', 'G10L2015/027']"
WO2022159211A1,Generation of optimized spoken language understanding model through joint training with integrated knowledge-language module,A system is provided for generating an optimized speech model by training a knowledge module on a knowledge graph. A language module is trained on unlabeled text data and a speech module is trained on unlabeled acoustic data. The knowledge module is integrated with the language module to perform semantic analysis using knowledge-graph based information. The speech module is then aligned to the language module of the integrated knowledge-language module. The speech module is then configured as an optimized speech model configured to leverage acoustic and language information in natural language processing tasks.,"['G10L15/063', 'G06F40/30', 'G06N3/045', 'G06N3/088', 'G06N5/022', 'G10L15/18', 'G06F40/284', 'G10L15/16', 'G10L15/1815', 'G10L15/183', 'G10L25/30', 'G10L25/63']"
US12238188B2,Offensive chat filtering using machine learning models,A server system is provided that includes one or more processors configured to execute a platform for an online multi-user chat service that communicates with a plurality of client devices of users of the online multi-user chat service that exchanges user chat data between the plurality of client devices. The one or more processors are configured to execute a user chat filtering program that performs filter actions for user chat data exchanged on the platform for the online multi-user chat service. The user chat filtering program includes a plurality of trained machine learning models and a filter decision service that determines a filter action to be performed for target portions of user chat data based on output of the plurality of trained machine learning models for those target portions of user chat data.,"['H04L67/535', 'H04L51/046', 'A63F13/35', 'A63F13/67', 'A63F13/75', 'A63F13/87', 'G06F40/216', 'G06F40/242', 'G06F40/284', 'G06F40/30', 'G06F40/35', 'G06N20/20', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/0464', 'G06N3/0475', 'G06N3/08', 'G06N3/091', 'H04L51/212', 'G10L15/26']"
US20240404514A1,Dialogue system and a dialogue method,"A dialogue system, comprising: an input configured to receive input data relating to speech or text provided by a user; an output configured to provide output data relating to speech or text to a user; and one or more processors, configured to: receive, by way of the input, input data relating to speech or text provided by a user; provide the input data to a subject safety module, the subject safety module configured to receive the input data and evaluate the input data before a system response is output, evaluating the input data comprising performing a first determination on the input data using a matching module and a second determination on the input data using a trained model, wherein the matching module performs the first determination to determine whether the input data matches one or more items from a pre-determined set of one or more items; generate a system response using a first process or using a second process, wherein the first process uses at least one trained language model to generate a dynamically determined system response and wherein the second process retrieves a pre-determined system response, wherein a selection between the first process and the second process is made based on the evaluation of the input data; output, by way of the output, the system response.","['G10L15/183', 'A61B5/4803', 'A61B5/7267', 'G10L15/22', 'G10L25/66', 'G16H10/20', 'G16H20/70', 'G16H50/20', 'G10L15/1815']"
US20230281963A1,Single stream multi-level alignment for vision-language pretraining,"A method is provided for pretraining vision and language models that includes receiving image-text pairs, each including an image and a text describing the image. The method encodes an image into a set of feature vectors corresponding to input image patches and a CLS token which represents a global image feature. The method parses, by a text tokenizer, the text into a set of feature vectors as tokens for each word in the text. The method encodes the CLS token from the NN based visual encoder and the tokens from the text tokenizer into a set of features by a NN based text and multimodal encoder that shares weights for encoding both the CLS token and the tokens. The method accumulates the weights from multiple iterations as an exponential moving average of the weights during the pretraining until a predetermined error threshold is reduced to be under a threshold amount.","['G06V10/82', 'G06F40/205', 'G06F40/284', 'G06F40/30', 'G06V10/7715', 'G06V10/774', 'G06V10/776', 'G06V10/806']"
CN119939863A,A substation AI-assisted design method and system based on LLM and RAG,"The application provides a substation AI aided design method based on LLM and RAG, which comprises the steps of carrying out demand analysis by utilizing a large language model LLM, extracting key information, carrying out knowledge retrieval by utilizing a RAG technology, including substation aided design knowledge base construction, substation aided design knowledge retrieval and knowledge reordering, and generating a primary design scheme of a substation by utilizing the LLM according to the demand analysis and the knowledge retrieval result, wherein the design scheme comprises an electric main wiring diagram, equipment type selection and layout. By adopting the combination of the LLM, the RAG and the AI auxiliary design, the application can realize the comprehensive improvement of the design efficiency, the standardization and the innovation of the transformer substation, and overcomes the problems of low efficiency, insufficient standardization and lack of innovation in the prior art.",[]
US20250165444A1,System and method for identifying and determining a content source,"A computer-implemented method for processing input data and determining a source of model-generated data in the input data. The method can be performed by at least one computer processor executing computer-readable instructions tangibly stored on at least one computer-readable medium comprising the steps of preprocessing the input data to clean and normalize the input data and generating in response normalized data, detecting if the model-generated data is present within the normalized data, and determining, based on a final score data, the source of the model-generated data.",['G06F16/215']
US20230135179A1,Systems and Methods for Implementing Smart Assistant Systems,"In one embodiment, a system includes an automatic speech recognition (ASR) module, a natural-language understanding (NLU) module, a dialog manager, one or more agents, an arbitrator, a delivery system, one or more processors, and a non-transitory memory coupled to the processors comprising instructions executable by the processors, the processors operable when executing the instructions to receive a user input, process the user input using the ASR module, the NLU module, the dialog manager, one or more of the agents, the arbitrator, and the delivery system, and provide a response to the user input.","['G06F16/90332', 'G10L15/22', 'G06F16/3329', 'G06F16/367', 'G06F40/30', 'G06N3/006', 'G06N3/0455', 'G06N3/0464', 'G06N5/022', 'G10L15/063', 'G10L15/16', 'G10L15/1815', 'G10L15/30', 'G06N3/084', 'G10L2015/088']"
GB2613581A,Systems and methods for speech recognition,"A computer implemented method for speech recognition, the method comprising: receiving a frame of speech audio 210; encoding the received frame 230; determining a context vector from the encoding of the received frame 240a; deriving an action 240b from the context vector; responsive to the action satisfying a predefined condition 250, deriving a token from the context vector 260; and; executing a function 270 based on the token, wherein the function comprises at least one of text output or command performance. The context vector relates to acoustic content present in a received frame and historic acoustic information. From the derived token a function is executed for example outputting text or performing a command.","['G10L15/00', 'G10L15/063', 'G06N3/006', 'G06N3/04', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/088', 'G06N3/092', 'G10L15/06', 'G10L15/16', 'G10L15/183', 'G10L15/22']"
US20250190866A1,Multimodal data processing and generation system using vq-vae and latent transformer,"A system and method for processing and generating multimodal data using a combination of Vector Quantized Variational Autoencoder (VQ-VAE) and Latent Transformer architectures. The system efficiently handles diverse data types including time-series, textual, sentiment, and structured tabular data through specialized encoding modules. A novel fusion module integrates these encodings, capturing cross-modal relationships. The fused representation is compressed into a discrete latent space, processed by a latent transformer, and then reconstructed and enhanced. This approach enables superior data compression, reconstruction, analysis, and generation of synthetic scenarios.","['G06N20/00', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N3/088']"
CN114819148B,Language model compression method based on uncertainty estimation knowledge distillation,"The invention discloses a language model compression method based on uncertainty estimation knowledge distillation, which mainly solves the problems of high training cost, low speed and noise interference in the knowledge distillation process in the existing network compression technology. The implementation scheme is as follows: 1) Performing half-compression on the original language model to obtain a compressed neural network; 2) Reasonably initializing parameters of the compressed neural network by using an original language model; 3) Adding a parameter distillation loss function of a feedforward network structure, and designing an uncertainty estimation loss function and a cross entropy loss function of a natural language processing task; 4) Training the compressed neural network model by using the designed loss function. The invention reduces the calculated amount of the network compression training process, improves the network compression rate, accelerates the network reasoning speed, can be widely applied to model deployment and model compression tasks, and provides a new model compression solution for application scenes with shortage of hardware resources.","['G06N3/082', 'G06N3/045', 'G06N3/048']"
WO2022046199A1,Multi-token embedding and classifier for masked language models,"Embodiments of the present disclosure include systems and methods for training transformer models. In some embodiments, a set of input data are received. The input data comprises a plurality of tokens including masked tokens. The plurality of tokens in an embedding layer are processed. The embedding layer is coupled to a transformer layer. The plurality of tokens are processed in the transformer layer, which is coupled to a classifier layer. The plurality of tokens are processed in the classifier layer. The classifier layer is coupled to a loss layer. At least one of the embedding layer and the classifier layer combine masked tokens at a current position with tokens at one or more of a previous position and a subsequent position.","['G06F40/284', 'G06N3/084', 'G06F40/30', 'G06N3/045', 'G06N3/08']"
US11803710B1,Multi-modal machine learning architectures integrating language models and computer vision systems,"Improved multi-modal machine learning networks integrate computer vision systems with language models. In certain embodiments, a computer vision system analyzes at least one image to generate a computer vision output. The language model generates an output based, at least in part, on a consideration of the computer vision output. The outputs of the language model can be generated by jointly considering textual information learned by the language model and visual content extracted by the computer vision system, thereby significantly improving the accuracy, breadth, and comprehensiveness of the outputs.","['G06F16/583', 'G06F40/35', 'G06F16/532', 'G06F40/40', 'G06V10/764', 'G06V10/774', 'G06V10/803', 'G06V10/82', 'G06V10/96', 'G06V20/70', 'G06F40/30']"
US20240386037A1,Systems and methods for providing user interfaces to converse with a corpus of electronic documents via a large language model,"Systems and methods for providing user interfaces to converse with a corpus of electronic documents via a large language model are disclosed. Exemplary implementations may: present a user interface configured to obtain entry of user input from a user to select one or more documents to be provided as input to a large language model for an individual conversation; responsive to selection of the individual conversation, provide an individual query as a prompt to the large language model; obtain and present an individual reply from the large language model; determine an individual document from the one or more documents that is relevant to the individual reply; present the individual document in a particular portion of the user interface; and/or perform other steps.","['G06V30/412', 'G06F16/3328', 'G06F40/40', 'G06F40/56']"
US20240289361A1,User interface for chat-guided searches,"A computer-implemented method is disclosed. The method includes: receiving, via a first user interface, a selection associated with an object; determining a first set of object attributes based on the selection; presenting, via a second user interface, a text prompt for a user to identify a subset of the first set of object attributes; receiving, via the second user interface, an indication of one or more preferred object attributes of the identified subset; and updating the first user interface to display content relating to objects associated with the one or more preferred object attributes.","['G06F40/40', 'G06Q30/0627', 'G06F16/3328', 'G06F40/284']"
US20240370718A1,Systems and methods for multi-modal language models,"Embodiments described herein provide a method of generating a multi-modal task output to a text instruction relating to inputs of multiple different modalities (e.g., text, audio, video, 3D). The method comprises receiving, via a data interface, a first input of a first modality, a second input of a second modality and the text instruction relating to the first and the second inputs; encoding, by a first multimodal encoder adapted for the first modality, the first input of the first modality into a first encoded representation conditioned on the text instruction; encoding, by a second multimodal encoder adapted for the second modality, the second input of the second modality into a second encoded representation conditioned on the text instruction; and generating, by a neural network based language model, the multi-modal task output based on an input combining the first encoded representation, the second encoded representation, and the text instruction.","['G06N3/0455', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
US20240289365A1,Systems and methods for performing vector search,"A computer-implemented is disclosed. The method includes: receiving a search query input; generating input enhancement data based on the search query input, the generating comprising processing the search query input using a large language model (LLM); causing to transform at least one of the search query input or the input enhancement data into a first vector embedding; and performing a search of an embedding space based on the first vector embedding.","['G06F16/3347', 'G06F16/3329', 'G06F16/335']"
US12211598B1,Configuring a generative machine learning model using a syntactic interface,"Described herein are a system, method, and device for configuring a generative machine learning model using a syntactic interface. A system may include a user interface, a memory, and a processor configured to, using a syntactic interface displayed using the user interface, receive a syntactic interface input from a user; identify an electronic medical record (EMR) by generating an EMR database query as a function of the syntactic interface input, querying an EMR database using the EMR database query, and receiving, from the EMR database, an EMR database response; generate a prompt as a function of the syntactic interface input; generate a first generative model output as a function of the prompt and the EMR using a trained generative machine learning model; and using a conversational interface displayed using the user interface, display the first generative model output to the user.","['G16H10/60', 'G06F16/2455', 'G16H10/20']"
US20240265205A1,Methods and systems for parsing a mix of features and instructions into a prompt,"A computer system and computer-implemented method, the method including receiving text input, the text input including feature inputs and prompt instructions; analyzing the text input to identify the feature inputs and the prompt instructions; and generating a prompt to be provided to a Large Language Model (LLM) based on a prompt template, the feature inputs, and the prompt instructions.","['G06F40/289', 'G06F40/205']"
US12353469B1,Verification and citation for language model outputs,"A user provides a question to be answered from detailed, dense or otherwise complex documents to a processing system that converts the question to a structured query language query and generates an embedding from the question, augmented by temporal data, synopses, themes, or other relevant information or data. The embedding is compared to embeddings generated from documents of a knowledge base to identify documents that are relevant to the question, and to rank such documents for their relevance. Highly ranking documents are combined with the query and provided to a language model that returns an answer to the question. A source for the answer is identified in at least one of the documents. The answer and the identified documents are presented to the user.","['G06F16/332', 'G06F16/3329', 'G06F16/3344', 'G06F16/383', 'G06Q50/18']"
CN119991168A,Marketing strategy optimization method based on large language model analysis and evaluation drive,"The invention provides a marketing strategy optimization method based on large language model analysis and evaluation driving, which comprises the steps of obtaining historical data comprising user behavior data, label data and statistical data, analyzing and constructing a basic marketing strategy, collecting user content data in real time through Kafka, carrying out deep analysis on the user content by utilizing a large language model, matching personalized marketing schemes from a knowledge base based on the deep analysis result and basic rule matching of the large language model, collecting feedback of a user on the marketing activity through a feedback circulation mechanism, storing the feedback result into the knowledge base and being used for optimizing the marketing strategy and the large language model, logging, textualizing and vectorizing decision processes into a vector library, and optimizing a decision flow and a marketing scheme by combining the continuously optimized large language model. The invention improves the accuracy and effect of the marketing strategy by using the large language model.",[]
CN118315012A,Method and device for generating medical examination conclusion based on large language model,"One or more embodiments of the present application provide a medical examination conclusion generation method and apparatus based on a large language model, the method including: acquiring a target expression text corresponding to the target medical examination; extracting medical examination reasoning knowledge matched with the target expression text from a medical examination reasoning knowledge base; wherein the medical examination reasoning knowledge comprises a presentation text and a conclusion text corresponding to the medical examination; constructing a sample based on the extracted medical examination reasoning knowledge, and constructing a prompt text based on the sample and the target expression text; and inputting the prompt text into a large language model, and outputting a target conclusion text corresponding to the target medical examination, which is obtained by reasoning based on the target expression text under the guidance of the sample, by the large language model.","['G16H15/00', 'G06N3/0455', 'G06N3/08', 'G06N5/022', 'G06N5/04', 'G16H50/70']"
US20240419917A1,Customized prompt generation service for software engineering tasks,"A customized prompt generation service automates prompts to a large language model to perform a specified software engineering task. The service stores the custom data of a client that includes code diff hunks, source code segments, code reviews, repaired code, and unit tests from a code base or repository of the client. Prompt templates are associated with each software engineering task that include the requisite information needed for the large language model to perform the target task. A prompt to a large language model includes examples of the software engineering task from the custom data of the client of the service.","['G06F8/30', 'G06F40/40', 'G06F8/36', 'G06F8/77', 'G06N20/00', 'G06N3/045']"
US12271791B2,Attention free transformer,"Attention-free transformers are disclosed. Various implementations of attention-free transformers include a gating and pooling operation that allows the attention-free transformers to provide comparable or better results to those of a standard attention-based transformer, with improved efficiency and reduced computational complexity with respect to space and time.","['G06N20/00', 'G06F17/14', 'G06F17/16', 'G06F40/58', 'G06N3/048', 'G06N3/08', 'G06N5/04', 'G06T3/4046', 'G06T3/4053']"
US12165650B2,Systems and methods for enhanced speaker diarization,"A system, method, and computer-program product includes receiving speech audio of a multi-turn conversation, generating, via a speech-to-text process, a transcript of the speech audio, wherein the transcript of the speech audio textually segments speech spoken during the multi-turn conversation into a plurality of utterances, generating a speaker diarization prompt that includes contextual information about a plurality of speakers participating in the multi-turn conversation, inputting, to a large language model, the speaker diarization prompt and the transcript of the speech audio, and obtaining, from the large language model, an output comprising an enhanced transcript of the speech audio, wherein the enhanced transcript of the speech audio textually segments the speech spoken during the multi-turn conversation into a plurality of refined utterances and associates a speaker identification value with each of the plurality of refined utterances.","['G06N3/045', 'G06F40/35', 'G06N20/00', 'G06N3/063', 'G06N3/09', 'G10L15/02', 'G10L15/04', 'G10L15/26', 'G10L15/28', 'G10L25/30', 'G10L25/78', 'G06N3/0442', 'G06N3/084', 'G10L17/06', 'G10L2025/783']"
US12380288B2,Multilingual unsupervised neural machine translation with denoising adapters,Methods and systems for unsupervised training for a neural multilingual sequence-to-sequence (seq2seq) model. Denoising adapters for each of one or more languages is inserted into an encoder and/or a decoder of the seq2seq model. Parameters of the one or more denoising adapters are trained on a language-specific denoising task using monolingual text for each of the one or more languages. Cross-attention weights of the seq2seq model with the trained denoising adapter layers are fine-tuned on a translation task in at least one of the one or more languages with parallel data.,"['G06F40/42', 'G06F40/58', 'G06F40/126', 'G06F40/47', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N3/084', 'G06N3/088']"
US12314301B2,Code search for examples to augment model prompt,A user query for information regarding data of a codebase is answered by a large language model given a prompt that includes examples of code segments from the codebase that are similar to the user query. The code segments from the codebase are associated with metadata that includes both natural language text and source code. The search for the examples of code segments from the codebase is based on embeddings of code segments and associated metadata that are closely similar to an embedding of the user query and context.,"['G06F16/3347', 'G06F16/338']"
US11354515B2,Discovery and recommendation of online learning resources,"A system models web content including learning resources available on a website, and makes suggestions of potentially useful learning resources when a user highlights text of interest within the website. In order to facilitate these suggestions, a neural network-based system is trained on learning resources and other content available on the website to create a common word embedding for learning resources and other website text. A student model may then be created to facilitate real time or near real time suggestions of relevant learning resources in response to selections of text from the website.","['G06F40/169', 'G06F3/0481', 'G06F40/284', 'G06F40/30', 'G06N3/045', 'G06N3/0454', 'G06N3/088', 'G06F2207/4824', 'G06N7/01']"
EP4434011A1,Pre-training of computer vision foundational models,"Examples are provided for pre-training a computer vision foundation model. A representative method comprises curating a pre-training database of image-text pairs from weakly labeled data. Language is encoded of text descriptions from the image-text pairs. The images of the image-text pairs are encoded using a hierarchical vision transformer with shifted windows and convolutional embedding. Based on the encoded images and the encoded language, the computer vision foundation model is pre-trained via unified image-text contrastive learning.","['G06V20/70', 'G06F18/214', 'G06F18/217', 'G06V10/82']"
US20250061634A1,Audio-driven facial animation using machine learning,"Systems and methods of the present disclosure include animating virtual avatars or agents according to input audio and one or more selected or determined emotions and/or styles. For example, a deep neural network can be trained to output motion or deformation information for a character that is representative of the character uttering speech contained in audio input. The character can have different facial components or regions (e.g., head, skin, eyes, tongue) modeled separately, such that the network can output motion or deformation information for each of these different facial components. During training, the network can use a transformer-based audio encoder with locked parameters to train an associated decoder using a weighted feature vector. The network output can be provided to a renderer to generate audio-driven facial animation that is emotion-accurate.","['G10L21/10', 'G06T13/205', 'G06N20/00', 'G06N3/0464', 'G06T13/40', 'G10L15/16', 'G10L2021/105']"
CN112559702B,Transformer-based method for generating natural language questions in the field of civil and architectural information,"The invention discloses a natural language problem generation method in the field of civil construction information based on a Transformer. And then, fine adjustment of downstream tasks is carried out on the Bert by adopting a UniLM idea, and the natural language text generation capability of the model in the field of civil construction information is improved. The model provided by the method has higher feasibility and effectiveness in the problem generation in the field of civil construction information, and reaches a higher natural language problem generation level.","['G06F16/3329', 'G06F16/353', 'G06N3/08']"
US20240135611A1,Neural compositing by embedding generative technologies into non-destructive document editing workflows,"One or more aspects of the method, apparatus, and non-transitory computer readable medium include obtaining an original image, a scene graph describing elements of the original image, and a description of a modification to the original image. The one or more aspects further include updating the scene graph based on the description of the modification. The one or more aspects further include generating a modified image using an image generation neural network based on the updated scene graph, wherein the modified image incorporates content based on the original image and the description of the modification.","['G06T11/60', 'G06T3/40', 'G06T5/50', 'G06V10/82', 'G06T2207/20081', 'G06T2207/20221']"
US12361213B2,Systems and methods for data parsing,"Systems and methods for data parsing are disclosed. In one aspect, a method of parsing raw data associated with one or more transactions involves receiving a text string including raw data for a transaction, matching the text string to a plurality of locations within a location corpus to extract location information from the text string, and identifying a candidate entity from the text string based on a similarity score with respect to a plurality of entities within an entity corpus. The method further involves in response to the similarity score of the identified candidate entity being less than a threshold score, generating entity information using the tokens indicative of entity information, and generating normalized transaction data including the extracted location information and one of the identified candidate entity or the generated entity information.","['G06F40/205', 'G06F40/126', 'G06F40/284', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G06Q10/101', 'G06Q30/018', 'G06Q30/04', 'G06Q30/0609', 'G06Q40/02', 'G06Q40/12']"
US11790171B2,Computer-implemented natural language understanding of medical reports,"A natural language understanding method begins with a radiological report text containing clinical findings. Errors in the text are corrected by analyzing character-level optical transformation costs weighted by a frequency analysis over a corpus corresponding to the report text. For each word within the report text, a word embedding is obtained, character-level embeddings are determined, and the word and character-level embeddings are concatenated to a neural network which generates a plurality of NER tagged spans for the report text. A set of linked relationships are calculated for the NER tagged spans by generating masked text sequences based on the report text and determined pairs of potentially linked NER spans. A dense adjacency matrix is calculated based on attention weights obtained from providing the one or more masked text sequences to a Transformer deep learning network, and graph convolutions are then performed over the calculated dense adjacency matrix.","['G06F40/295', 'G06F18/2413', 'G06F40/10', 'G06F40/117', 'G06N3/042', 'G06N3/044', 'G06N3/045', 'G06N3/088', 'G06N7/01', 'G06V10/764', 'G06V10/82', 'G06V30/153', 'G06V30/262', 'G06V30/40', 'G16H15/00', 'G16H50/70', 'G06N3/08', 'G06V30/10']"
US12393768B2,Layout-aware multimodal pretraining for multimodal document understanding,"Systems and methods for document processing that can process and understand the layout, text size, text style, and multimedia of a document can generate more accurate and informed document representations. The layout of a document paired with text size and style can indicate what portions of a document are possibly more important, and the understanding of that importance can help with understanding of the document. Systems and methods utilizing a hierarchical framework that processes the block-level and the document-level of a document can capitalize on these indicators to generate a better document representation.","['G06N3/084', 'G06F40/166', 'G06F16/483', 'G06F16/93', 'G06F40/109', 'G06F40/284', 'G06N3/045', 'G06V30/413']"
US20230325725A1,Parameter Efficient Prompt Tuning for Efficient Models at Scale,"Systems and methods for natural language processing can leverage trained prompts to condition a large pre-trained machine-learned model to generate an output for a specific task. For example, a subset of parameters may be trained for the particular task to then be input with a set of input data into the pre-trained machine-learned model to generate the task-specific output. During the training of the prompt, the parameters of the pre-trained machine-learned model can be frozen, which can reduce the computational resources used during training while still leveraging the previously learned data from the pre-trained machine-learned model.","['G06N20/20', 'G06V10/764', 'G06V10/7747', 'G06F16/2455', 'G06F16/55', 'G06F40/30', 'G06N3/0455', 'G06N3/08', 'G06N3/084', 'G06V10/454', 'G06V10/776', 'G06V10/82']"
US20250028900A1,"Method for data analysis by large language model, and graphic user interface system thereof","A method and Graphic User Interface (GUI) system of data analysis for training a discriminative machine learning model are provided. One or more sets of data are provided to a large language model (LLM) for a predefined task with or without prompts provided by a user; the results output by the LLM is compared with a set of initial human-supplied ground truth data generated by the user to produce performance metrics, the user may provide or update one or more prompts for the LLM based on the performance metrics till the performance metrics reach a threshold. The set of initial human-supplied ground truth data and the results of the LLM can be used to train a discriminative machine learning model.",['G06F40/20']
US11748577B1,"Computer-generated content based on text classification, semantic relevance, and activation of deep learning large language models","The disclosure relates to systems and methods of automatically generating unique content including natural language text based on a corpus of previously generated response documents and discrete requirements defined in a requirements specification. The system may use generative stitching that includes multi-layer processes that execute to influence the generation of unique content including natural language text through an artificial intelligence (AI) language transformer model trained to output the content based on previously written material that is semantically relevant to the discrete requirements and is weighted against labeled attributes. The labeled attributes may determine the influence asserted against the language transformer, thereby generating unique on-target content that may be combined to create a computer-generated response document.","['G06F40/40', 'G06F40/30', 'G06N20/00', 'G06N3/045', 'G06N3/09', 'G06F40/169', 'G06F40/289', 'G06F40/56']"
US11663404B2,"Text recognition method, electronic device, and storage medium","The disclosure provides a text recognition method, an electronic device, and a storage medium. The method includes: obtaining N segments of a sample text; inputting each of the N segments into a preset initial language model in sequence, to obtain first text vector information corresponding to the N segments; inputting each of the N segments into the initial language model in sequence again, to obtain second text vector information corresponding to a currently input segment; in response to determining that the currently input segment has the mask, predicting the mask according to the second text vector information and the first text vector information to obtain a predicted word at a target position corresponding to the mask; training the initial language model according to an original word and the predicted word to generate a long text language model; and recognizing an input text through the long text language model.","['G06F40/30', 'G06F40/279', 'G06F40/166', 'G06N20/00']"
CN116822616A,A device for softmax function training in large language models,"The invention provides a device for training a Softmax function in a large language model, wherein the upper half part of the device is a forward propagation path, and the lower half part of the device is a reverse propagation path; the forward propagation path includes e x Exponential function unit, adder and divider, at e x Registers are inserted among the exponent function unit, the adder and the divider; the backward propagation path comprises two multipliers and an adder A 1 And a multiplexer MUX for reconstructing the data path; the two multipliers are respectively left multiplier B 1 Right multiplier B 2 The method comprises the steps of carrying out a first treatment on the surface of the The multiplexer MUX is used for changing the data flow inside the device; the forward propagation path and the backward propagation path share two random access memories RAM1 and RAM2. The invention can be applied to SoftmaThe x-function trains the computation at various stages, thereby better utilizing the computation and storage resources to achieve higher performance and energy efficiency.","['G06N3/084', 'G06F7/575', 'G06N3/063']"
CN110399798B,Discrete picture file information extraction system and method based on deep learning,"The invention discloses a discrete picture file information extraction system and method based on deep learning.A terminal-to-terminal OCR model combines character detection and text recognition, can realize the sharing of a convolutional neural network image processing layer, and improves the calculation speed and recognition effect; the bidirectional LSTM is used in the aspect of character detection, and a text area is accurately segmented according to the file layout when characters are recognized, so that the problems that noise is recognized as characters, and a chart generates messy codes, wrong lines and the like in the conventional OCR technology can be solved; in the aspect of text recognition, ResNet is combined with a transform coder-decoder, so that the book face body and the handwriting body can be accurately recognized; by combining the end-to-end OCR model with the information extraction model, the information loss can be reduced, and the accuracy of information extraction is improved; the Bayesian optimization algorithm is used for realizing the training and parameter adjustment automation of the information extraction model, so that a user can customize the information extraction model desired by the user even if the user does not understand machine learning.","['G06F18/214', 'G06V20/62', 'G06V30/413', 'G06V30/10']"
US12136413B1,Domain-specific parameter pre-fixes for tuning automatic speech recognition,Domain-specific parameters may be used for tuning speech processing. A pre-trained transformer-based language model may train domain-specific parameters using domain-specific unlabeled text data. This domain-specific parameters can then be appended to candidate texts produced by a speech model on received speech data and input to the transformer-based language model to score the candidate texts. The scores of the candidate texts determined using the pre-trained transformer-based language model can then be used to select a candidate text for further speech processing.,"['G10L15/16', 'G10L15/063', 'G10L15/183']"
WO2024226360A1,Semantic search and summarization for electronic documents,Techniques for an artificial intelligence (AI) platform to search a document collection are described. Embodiments may use AI and machine learning techniques within a framework of an electronic document management system to perform semantic searching of an electronic document or a collection of electronic documents for certain types of information. The AI platform may summarize the information in a natural language representation of a human language. Other embodiments are described and claimed.,"['G06F16/345', 'G06F16/93']"
WO2024055694A1,Method and device for compressing generative pre-trained language models via quantization,"A method is provided for quantizing a neural network model performed by a processing system. The method comprises determining a scaling factor based on a distribution of weights associated with the neural network model, determining quantized weights based on the scaling factor and the weights associated with the distribution, determining a training loss of the neural network model based on the quantized weights during training of the neural network model, and determining an updated scaling factor for the neural network model based on a gradient of the training loss.","['G06N3/084', 'G06N3/045']"
CN117577252A,"Methods, devices and storage media for multi-modal fusion medical rare event prediction","The invention relates to a method, a device and a storage medium for predicting medical rare events by multi-mode fusion, which are applied to the technical field of medical rare event prediction and comprise the following steps: on the basis of the existing model of the transducer, the data are preprocessed, the defect of the data is filled, meanwhile, the strong modeling capability of the transducer model in the fields of natural language processing, computer vision and the like is utilized to extract semantic features of the structured data, and then the semantic features of the unstructured data, namely the text data, are extracted to fuse the semantic features of the structured data and the unstructured data, so that the prediction of medical rare events is performed based on the fused features, and meanwhile, the structured data and the unstructured data are utilized, the prediction of medical rare events of the multi-mode data is realized, and the prediction accuracy is improved.","['G16H10/60', 'G06F40/30', 'G06N3/0464', 'G06N3/08', 'G16H50/30', 'Y02A90/10']"
CA3129721C,Pre-trained contextual embedding models for named entity recognition and confidence prediction,"At least one processor may obtain a document comprising text tokens. The at least one processor may determine, based on a pre-trained language model, word embeddings corresponding to the text tokens. The at least one processor may determine, based on the word embeddings, named entities corresponding to the text tokens; and one or more accuracy predictions corresponding to the named entities. The at least one processor may compare the one or more accuracy predictions with at least one threshold. The at least one processor may associate, based on the comparing, the named entities with one or more confidence levels. The at last one processor may deliver the named entities and the one or more confidence levels.","['G06F40/295', 'G06F40/284', 'G06N3/045', 'G06N3/0455', 'G06N3/0499', 'G06N3/08', 'G06N3/0895', 'G06N3/09', 'G06V10/40', 'G06V30/10']"
AU2021371022B2,Systems and methods for the automatic classification of documents,"Systems and computer implemented methods for classifying documents are provided that include: pretraining and then fine tuning a machine learning model with a domain specific dataset that includes a plurality of documents each annotated with at one label selected from a plurality of predefined labels for a given domain; and predicting using the trained/fine tuned machine learning model, at least one label from the plurality of labels for at least one other document. The machine learning model is preferably fine tuned using a label attention multi-task learning process that includes: a first task for training the machine learning model with respect to all labels used for the plurality of documents in the dataset, and a second task for training the machine learning model with respect to a subset of all of the labels used for the plurality of documents in the dataset.","['G06N3/08', 'G06F40/30', 'G06F9/4881', 'G06N3/045', 'G06N3/0499', 'G06N3/0895', 'G06N3/09', 'G06N3/096']"
US20240153590A1,Natural language processing to predict properties of proteins,"A protein language natural language processing (NLP) system is trained to predict specific biophysiochemical properties. Amino acids of proteins are tokenized and masked. A first neural network is trained on a library of amino acid sequences in an unsupervised or self-supervised manner. The information obtained from the first phase of training is applied in a subsequent training operation via transfer learning, to a second neural network. In aspects, an annotated compact dataset is used to fine-tune the second neural network in a second phase of training, and in a supervised manner, to predict biophysiochemical properties of proteins, including TCR-epitope binding.","['G16B15/30', 'G16B40/20', 'G06F40/284', 'G06F40/30', 'G06F40/40', 'G06N3/044', 'G06N3/045', 'G06N3/088', 'G16B15/20', 'G16B35/10']"
US20240020116A1,Systems and methods for generating natural language using language models trained on computer code,"Disclosed herein are methods, systems, and computer-readable media for generating natural language based on computer code input. In an embodiment, a method may comprise one or more of: accessing a docstring generation model configured to generate docstrings from computer code; receiving one or more computer code samples; generating, using the docstring generation model and based on the received one or more computer code samples, one or more candidate docstrings representing natural language text, each of the one or more candidate docstrings being associated with at least a portion of the one or more computer code samples; identifying at least one of the one or more candidate docstrings that provides an intent of the at least a portion of the one or more computer code samples; and/or outputting, via a user interface, the at least one identified docstring with the at least a portion of the one or more computer code samples.","['G06F8/30', 'G06F8/73', 'G06F40/284', 'G06F40/30', 'G06F8/33', 'G06N3/045', 'G06N3/08', 'G06F11/3684', 'G06F40/186']"
US12190072B2,Profile-based natural language message generation and selection,"In some embodiments, text for user consumption may be generated based on an intended user action category and a user profile. In some embodiments, an action category, a plurality of text seeds, and a profile comprising feature values may be obtained. Context values may be generated based on the feature values, and text generation models may be obtained based on the text seeds. In some embodiments, messages may be generated using the text generation models based on the action category and the context values. Weights associated with the messages may be determined, and a first text message of the messages may be sent to an address associated with the profile based on the weights. Based on a reaction value obtained in response to the first message, a first expected allocation value may be updated based on the reaction value.","['G06F40/42', 'H04L51/48', 'H04L67/306']"
US20220059200A1,Deep-learning systems and methods for medical report generation and anomaly detection,Systems and methods for generating and detecting anomalies in medical reports using an attention-based machine learning model are disclosed. The attention-based machine learning model for generating medical reports includes at least one decoder layer in which each decoder layer includes an attention sublayer operatively coupled to a feed-forward sublayer. The attention-based machine learning model for detecting anomalies in medical reports includes at least one bidirectional encoder layer in which each encoder layer includes an attention sublayer operatively coupled to a feed-forward sublayer.,"['G16H40/67', 'G16H15/00', 'G16H40/63', 'G16H50/20', 'G16H50/70']"
US11798534B2,Systems and methods for a multilingual speech recognition framework,"Embodiments described herein provide an Adapt-and-Adjust (A2) mechanism for multilingual speech recognition model that combines both adaptation and adjustment methods as an integrated end-to-end training to improve the models' generalization and mitigate the long-tailed issue. Specifically, a multilingual language model mBERT is utilized, and converted into an autoregressive transformer decoder. In addition, a cross-attention module is added to the encoder on top of the mBERT's self-attention layer in order to explore the acoustic space in addition to the text space. The joint training of the encoder and mBERT decoder can bridge the semantic gap between the speech and the text.","['G10L15/16', 'G06N3/04', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G10L15/063', 'G10L15/065', 'G10L15/183', 'G10L2015/0631']"
US11327960B1,Systems and methods for data parsing,"Systems and methods for data parsing are disclosed. In one aspect, a method of parsing raw data associated with one or more transactions involves receiving a text string including raw data for a transaction, matching the text string to a plurality of locations within a location corpus to extract location information from the text string, and identifying a candidate entity from the text string based on a similarity score with respect to a plurality of entities within an entity corpus. The method further involves in response to the similarity score of the identified candidate entity being less than a threshold score, generating entity information using the tokens indicative of entity information, and generating normalized transaction data including the extracted location information and one of the identified candidate entity or the generated entity information.","['G06F16/2379', 'G06F40/295', 'G06F40/205', 'G06F16/2468', 'G06F16/29', 'G06F40/279', 'G06F40/284', 'G06F9/466', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06Q20/38', 'G06Q40/02', 'G06F16/243', 'G06F40/216', 'G06F40/30', 'G06N20/00', 'G06N3/02', 'G06N7/01']"
CN116703337B,Project document examination system and method based on artificial intelligence technology,"The invention discloses a project document examination system and method based on an artificial intelligence technology, and relates to the technical field of document examination. The system comprises: configuration module, management module and service module. According to the invention, the problems and defects in the project document can be rapidly identified by adopting an artificial intelligence algorithm and a natural language processing technology training model, so that a user can be helped to better find and solve potential risks, the quality and reliability of the project are improved, the automatic examination of the project document is realized, the examination efficiency can be effectively improved, and the examination quality and accuracy are improved. The document inspection system also has the characteristics of standardization and standardization, and can perform intelligent inspection according to inspection rules and standards defined by users, thereby helping to ensure the compliance of documents and meet industry standards. This helps the enterprise to build a unified project audit flow, improves the level of job standardization, and reduces errors and omissions.","['G06Q10/103', 'G06N5/02']"
US12288380B2,Systems and methods for unified vision-language understanding and generation,"Embodiments described herein provide systems, methods, and devices for generating enhanced vison-language training data. A method may include: receiving, from a communication interface, a first training dataset of image-text pairs and a second training dataset of annotated image-text pairs; fine-tuning an image-grounded text decoder and an image-grounded text encoder using the second training dataset of annotated image-text pairs; generating, by the fine-tuned image-grounded text decoder, a predicted text based on a training image from the first training dataset; generating, by the fine-tuned image-grounded text encoder, a filtering decision based on the training image and the predicted text; adding the training image and the predicted text to form a third training dataset of image-text pairs depending on the filter decision; and training a vision-language model using the third training dataset of image-text pairs.","['G06V10/774', 'G06F40/126', 'G06F40/284', 'G06F40/30', 'G06T9/00', 'G06V10/764', 'G06V10/803', 'G06F40/216', 'G06F40/279']"
US12164873B2,System and method for hybrid question answering over knowledge graph,"Aspects of the subject disclosure may include, for example, identifying an entity of a natural language question, locating a node of a knowledge graph corresponding to the entity, and generating a candidate answer set including a group of other entities located a predetermined proximity to the node. Contextual information for the group of other entities is determined from the knowledge graph, and the natural language question and contextual information are separately encoded to obtain separate encoded vectorial representations of the natural language question and members of the candidate answer set. The encoding uses pre-trained language model embeddings obtained via a bidirectional encoder representations from transformer encoding process. The encoded vectorial representations of the question under an influence of aspects of the contextual information are scored and a member of the candidate answer set selected according to the score to obtain an answer to the original question. Other embodiments are disclosed.","['G06F16/9024', 'G06F16/3347', 'G06F16/90332', 'G06F40/295', 'G06N3/042', 'G06N3/045', 'G06N3/08', 'G06N3/0895', 'G06N5/02', 'G06N5/04', 'G06N5/041', 'G06N3/0464']"
US11797591B2,Abstractive multi-document summarization through self-supervision and control,"A method for generating enriched training data for a multi-source transformer neural network for generation of a summary of one or more passages of input text comprises creating, from a plurality of input text sets, training points each comprising an input text subset of the input text set and a corresponding reference input text from the input text set, wherein the size of the input text subset is a predetermined number. Control codes are selected based on reference features corresponding to categorical labels of reference texts in the created training points. The input text is enriched with the selected control codes to generate enriched training data.","['G06N20/10', 'G06F16/345', 'G06F16/35', 'G06F40/58', 'G06N3/0455', 'G06N3/088', 'G06N3/0895', 'G06N7/01']"
US11580975B2,Systems and methods for response selection in multi-party conversations with dynamic topic tracking,"Embodiments described herein provide a dynamic topic tracking mechanism that tracks how the conversation topics change from one utterance to another and use the tracking information to rank candidate responses. A pre-trained language model may be used for response selection in the multi-party conversations, which consists of two steps: (1) a topic-based pre-training to embed topic information into the language model with self-supervised learning, and (2) a multi-task learning on the pretrained model by jointly training response selection and dynamic topic prediction and disentanglement tasks.","['G10L15/22', 'G06F18/2185', 'G06F40/216', 'G06F40/284', 'G06F40/289', 'G06F40/30', 'G06F40/35', 'G06K9/6264', 'G10L13/027', 'G10L15/02', 'G10L15/063', 'G10L15/16', 'G10L15/1822', 'G10L2015/221', 'G10L2015/228']"
US12314318B2,Enhanced searching using fine-tuned machine learning models,"An advanced search system leverages a pre-trained large language model to enhance user query responses. The system, equipped with hardware processors, a search query via an interface and accesses a pre-trained large language model designed to respond to the search query. The system fine-tunes the model to generate a task-specific generative model. The system employs the task-specific generative model to generate a search result to the search query and analyzes the search result based on a performance metric associated with the task-specific generative model. The system refines the task-specific generative model based on the analyzing of the search result.","['G06F16/24575', 'G06F16/248', 'G06F16/345', 'G06F16/90328', 'G06F16/93', 'G06F16/9538', 'G06F16/9558']"
US11516158B1,Neural network-facilitated linguistically complex message generation systems and methods,"Provided are methods and systems for automated or semi-automated generation of complex messages. Provided systems include neural network(s) that are trained with at least an initial training set including message records having specific characteristics, such as size and form characteristics, and recognize certain user inputted content as â€œinstructional prompts.â€ The neural network(s) use the instructional prompts, training set, and other prompts to generate a distribution of semantic element options for each semantic element the system determines to include in system drafted message(s). The system selects from among such options to generate a plurality of draft messages which are presented to users for evaluation, editing, or transmission, with the instructional prompts treated as priority content. The systems and methods include mechanisms for reviewing and changing the instructional prompts based on factors that can include the content of the system-generated draft messages before further iterations to enhance the accuracy of future messages.","['G06F40/30', 'G06F40/253', 'G06F40/35', 'G06N3/045', 'G06N3/0454', 'G06N3/047', 'G06N3/0475', 'G06N3/084', 'G06N3/09', 'H04L51/02']"
WO2022121251A1,"Method and apparatus for training text processing model, computer device and storage medium","A method for training a text processing model, relating to the technical field of artificial intelligence, comprising: acquiring a first text sample set to be trained (202); performing model training respectively on the basis of said first text sample set to obtain a five-stroke word vector model and a pinyin word vector model corresponding to different input methods (204); acquiring a second text sample set to be trained and a pre-trained language model (206); on the basis of the language model, the five-stroke word vector model and the pinyin word vector model, respectively extracting encoded data corresponding to said second text sample set (208); and performing model training according to the encoded data to obtain a text processing model (210).","['G06F40/274', 'G06F3/0237']"
US20250139369A1,Training multi-modal foundation model,A method is provided that includes: obtaining first urban data of a first sample urban region; inputting the first urban data into a multi-modal foundation model to obtain respective predicted vector representations of a plurality of first data segments; obtaining a plurality of general-purpose foundation models that are pre-trained; for each general-purpose foundation model: generating a vector representation label of a first data segment of a corresponding data modality by using the general-purpose foundation model; and determining a knowledge distillation loss of the general-purpose foundation model based on the vector representation label and a predicted vector representation of the first data segment; and adjusting parameters of the multi-modal foundation model based on at least respective knowledge distillation losses of the plurality of general-purpose foundation models.,"['G06F40/284', 'G06F18/22', 'G06F18/256', 'G06F40/126', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N3/096', 'G06N5/02']"
US20240371082A1,Three-dimensional reasoning using multi-stage inference for autonomous systems and applications,"In various examples, an autonomous system may use a multi-stage process to solve three-dimensional (3D) manipulation tasks from a minimal number of demonstrations and predict key-frame poses with higher precision. In a first stage of the process, for example, the disclosed systems and methods may predict an area of interest in an environment using a virtual environment. The area of interest may correspond to a predicted location of an object in the environment, such as an object that an autonomous machine is instructed to manipulate. In a second stage, the systems may magnify the area of interest and render images of the virtual environment using a 3D representation of the environment that magnifies the area of interest. The systems may then use the rendered images to make predictions related to key-frame poses associated with a future (e.g., next) state of the autonomous machine.","['C10G59/02', 'C10G11/02', 'B01J21/02', 'B01J29/46', 'B25J9/1697', 'C10G11/04', 'C10G50/00', 'C10G57/02', 'G06T15/20', 'G06T19/20', 'G06T7/73', 'G06T2207/20081', 'G06T2207/20084', 'G06T2219/2016']"
US12032910B2,Joint intent and entity recognition using transformer models,"Systems described herein may use transformer-based machine classifiers to perform a variety of natural language understanding tasks including, but not limited to sentence classification, named entity recognition, sentence similarity, and question answering. The exceptional performance of transformer-based language models is due to their ability to capture long-term temporal dependencies in input sequences. Machine classifiers may be trained using training data sets for multiple tasks, such as but not limited to sentence classification tasks and sequence labeling tasks. Loss masking may be employed in the machine classifier to jointly train the machine classifier on multiple tasks simultaneously. The user of transformer encoders in the machine classifiers, which treat each output sequence independently of other output sequences, in accordance with aspects of the invention do not require joint labeling to model tasks.","['G06F40/284', 'G06F18/24', 'G06F40/295', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
US11875590B2,Self-supervised document-to-document similarity system,"Examples provide a self-supervised language model for document-to-document similarity scoring and ranking long documents of arbitrary length in an absence of similarity labels. In a first stage of a two-staged hierarchical scoring, a sentence similarity matrix is created for each paragraph in the candidate document. A sentence similarity score is calculated based on the sentence similarity matrix. In the second stage, a paragraph similarity matrix is constructed based on aggregated sentence similarity scores associated with the first candidate document. A total similarity score for the document is calculated based on the normalize the paragraph similarity matrix for each candidate document in a collection of documents. The model is trained using a masked language model and intra-and-inter document sampling. The documents are ranked based on the similarity scores for the documents.","['G06F16/337', 'G06V30/418', 'G06F16/338', 'G06F16/93', 'G06F18/2113', 'G06F18/2178', 'G06V10/751']"
US12079602B2,Systems and methods for a conversational framework of program synthesis,"Embodiments described herein provide a program synthesis framework that generates code programs through a multi-turn conversation between a user and a system. Specifically, the description to solve a target problem is factorized into multiple steps, each of which includes a description in natural language (prompt) to be input into the generation model as a user utterance. The model in turn synthesizes functionally correct subprograms following the current user utterance and considering descriptions and synthesized subprograms at previous steps. The subprograms generated at the multiple steps are then combined to form an output of program in response to the target problem.","['G06F8/30', 'G06F8/35', 'G06F8/10', 'G06F8/75', 'G06N20/00', 'G06N3/045', 'G06N3/0455', 'G06N3/09', 'G06N3/098', 'G06N3/063']"
US20250022615A1,Method and system for identifying biological entities for drug discovery,"A computer-implemented method of training a machine learning model to identify biological entities for drug discovery is disclosed. The method comprises providing a training data set comprising a plurality of entity-linked text sequences, each text sequence including a mention of a biological entity, where the biological entity is linked to a corresponding biological entity identifier from a set of possible biological entity identifiers; masking the mention of the biological entity within each text sequence; encoding each masked text sequence into an input representation for a machine learning model; and training a machine learning model to predict the unique entity identifier of the masked biological entity based on the input representation. The described method is able to utilise the full breadth of the rich contextual information available in the biomedical text corpus to predict new biological targets for drug discovery and avoids the restrictions intrinsic to relationship prediction using knowledge graphs. The ability to identify more promising, biologically relevant targets in an automated manner, significantly reduces the requirement of human input and reduces the failure rate in targets that are progressed in the drug delivery pipeline.","['G06N3/042', 'G16H50/70', 'G06F40/279', 'G06F40/295', 'G06N3/045', 'G16B15/30', 'G16B40/20', 'G16H70/40']"
US12411699B2,Dynamic generation of user interface controls,"Disclosed here are methods and systems for dynamically generating user interface controls. In one embodiment, a method comprises receiving, via a search input element, an input; generating an input vector corresponding to the input; identifying a set of user interface controls matching the input, the identifying including comparing the input vector to a set of user interface control vectors; and providing the matching set of interactive user interface controls for presentation on a single user interface page.","['G06F9/451', 'G06F3/0481', 'G06N20/00']"
CN118967365A,A method for power demand forecasting based on large language model,"The invention discloses a power demand prediction method based on a large language model, which belongs to the technical field of power prediction and specifically comprises the following steps: collecting and filtering power demand data; dividing the power demand data into a training set, a verification set and a test set; setting seasonal parameters to integrate the power demand data according to the time sequence; constructing a large language power model based on the combination of CNN, BERT and LSTM, setting model parameters, training data of a training set, taking numerical power data as a label, calculating root mean square error between power demand trend data output by the model and the label as a loss function, calculating gradient of the loss function relative to the model parameters, updating the parameters, and repeating the above process for a plurality of periods until convergence; verifying the performance of the model by using a verification set, evaluating the final performance of the model by using an independent test set, and calculating a model error; the invention provides effective technical support for the optimal scheduling of the power system.","['G06Q50/06', 'G06N3/0442', 'G06N3/0455', 'G06N3/0464', 'G06Q30/0202']"
CN118377933B,Optimization method of text video retrieval based on text-generated image technology,"The invention provides a text video retrieval optimization method based on text generated image technology, which generates an image model by using Stable diffration text, and generating images of the text information in the data set, and adding the generated image information as a video frame into the training set, so that the data scale is effectively expanded. The invention also considers that the key frame information is reversely generated into text information, and further enriches the text data set. Based on the expanded data set, the invention designs a new loss function, comprehensively considers the fine granularity and coarse granularity characteristics of the video, optimizes the training process of the text video retrieval model, and improves the retrieval effect. The invention can effectively solve the problems of data deficiency, insufficient model training and the like in the existing text video retrieval research by using the Stable diffration driven data enhancement and optimized loss function design, and provides a new technical support for multimedia content analysis and retrieval application.","['G06F16/7844', 'G06V10/761', 'G06V10/80', 'G06V20/40', 'G06V20/635', 'Y02T10/40']"
CN118606447A,"Model training method, text generation method, device, equipment and medium","The embodiment of the application provides a model training method, a text generation method, a device, equipment and a medium, wherein the model training method comprises the following steps: acquiring an input matrix corresponding to a word element of text data; the embedding layer performs first mapping processing on the input matrix to obtain a first embedded representation; the embedding normalization layer normalizes the modular length of the row vectors in the first embedded representation and normalizes the modular length of different row vectors in the first embedded representation; the module length adjusting layer adjusts the module length of the second embedded representation to obtain a third embedded representation; the processing layer processes the third embedded representation to output text information of a large language model corresponding to the text data acquired at the present time; loss information is determined from the text information. The embodiment of the application can improve the training full degree and the overall training effect of the parameters of the large language model, reduce the data transmission quantity of the large language model in the training process, and improve the hardware processing speed corresponding to the large language model.","['G06F16/3329', 'G06F16/3344', 'G06F16/3346', 'G06F40/216', 'G06F40/30', 'G06N3/0455', 'G06N3/0499', 'G06N3/084']"
WO2023285688A1,Method of evaluating text similarity for diagnosis or monitoring of a health condition,"The invention relates to a computer implemented method of training a machine learning model to evaluate the similarity of a candidate text to a reference text for determining or monitoring a health condition, where the model takes a text comparison pair comprising a reference text and a candidate text, each comprising data encoding a text sequence, the method comprising: pre-training an edit encoder to learn to generate an edit-space representation of an input text comparison pair, where the edit-space representation encodes information for mapping the reference text to the candidate text, the edit encoder comprising a machine learning model; and performing task-specific training by adding a task-specific network layer and training the task-specific network layer to map an edit- space representation generated by the pre-trained edit encoder to an output associated with a health condition. Edit-space representations learned in this way are able to encode a greater range of changes in language use than known metrics used to evaluate machine translations.","['G16H50/70', 'G06N3/045', 'G06N3/0455', 'G06N3/088', 'G06N3/096', 'G16H50/20']"
US20240330446A1,Finding semantically related security information,"Methods and apparatuses for improving the performance and energy efficiency of machine learning systems that generate security specific machine learning models and generate security related information using security specific machine learning models are described. A security specific machine learning model may comprise a security specific large language model (LLM). The security specific LLM may be trained and deployed to generate semantically related security information. The security specific LLM may be pretrained with a security specific data set that was generated using similarity deduplication and long line handling, and with security specific objectives, such as next log line prediction based on host, system, application, and cyber attacker behavior. The security specific large language model may be fine-tuned using a security specific similarity dataset that may be generated to align the security specific LLM to capture similarity between different security events.","['G06N3/0895', 'G06F21/554', 'G06F2221/034', 'G06N3/045']"
US11808594B1,Language models and machine learning frameworks for optimizing vehicle navigation routes and vehicle operator sessions,"This disclosure relates to improved techniques for personalizing vehicle routes and operator sessions using pre-trained machine learning language models. In certain embodiments, a language model is trained on operator interaction data to learn operator route preferences for vehicle operators. These learned operator route preferences can be leveraged to optimize and personalize vehicle routes and operator sessions in various ways. Other embodiments are disclosed herein as well.","['G06N3/045', 'G01C21/3484', 'G01C21/3608', 'G06N3/0895', 'G06N3/096']"
CN110032730A,"A kind of processing method of text data, device and equipment","This specification embodiment discloses processing method, device and the equipment of a kind of text data, which comprises obtains the target text data for being directed to specified servicesï¼›Word segmentation processing is carried out to the target text data, determines the participle position in the target text dataï¼›Scheduled separator is inserted at the participle position in the target text dataï¼›Target text data inserted with the separator are input in scheduled language model, result data relevant to the specified services is obtained.","['G06F40/289', 'G06F40/30']"
CN118734250A,"Multimodal data fusion control method, device, equipment and medium","The invention relates to the technical field of computers and discloses a multi-modal data fusion control method, device, equipment and medium, wherein the method introduces cross-modal attention at Embedding layers of a pre-training language model and a transducer encoder layer to realize the full fusion of information among modes on different semantic layers so as to form multi-modal representation with layering and semantic interactivity; the importance weight of different modal information in the multi-modal fusion process is adaptively adjusted, flexible and dynamic cross-modal information interaction is realized, and the adaptability and generalization capability of the model in different scenes are improved; the consistency of the generated content and the source input in the semantic layer is explicitly converted, the correlation between the generated content and one of the modal data is evaluated in real time, and the accuracy, the correlation and the reliability of the output content of the model are improved, so that the performance of the model in the cross-modal understanding and generating task is remarkably improved, and the hardware processing efficiency is improved.","['G06F18/253', 'G06F18/213', 'G06F18/22', 'G06N3/045', 'G06N3/0464']"
CN112580373A,High-quality Mongolian unsupervised neural machine translation method,"A high-quality Mongolian Chinese unsupervised neural machine translation method includes pre-segmenting large-scale Mongolian Chinese monolingual corpus, pre-training the segmented corpus with a monolingual language model by using Bert to obtain Mongolian and Chinese language models, training unsupervised Mongolian Chinese segmenter by combining a sub-word-segment correlation matrix generation method, scoring the correlation of any two sub-words in a Mongolian sentence to be segmented to complete segmentation, embedding segmented Mongolian Chinese bilinguals into a shared potential space, optimally aligning a Mongolian Chinese bilingual word vector space by using an unsupervised countermeasures autonomous learning method, training a Mongolian language model on the Mongolian Chinese monolingual corpus segmented in the space, searching nearest neighbor by using a CSLS method to obtain a Mongolian Chinese bilingual dictionary based on a GAS framework, training an initial Mongolian Chinese translation model by combining the pre-training model, training a high-quality Mongolian Chinese monolingual corpus training method, training a high-quality Mongolian Chinese bilingual learning strategy, and a Chinese language, Hanmeng bidirectional dual unsupervised translation model.","['G06F40/58', 'G06F40/205', 'G06F40/242', 'G06F40/284', 'G06F40/289', 'G06F40/30', 'Y02D10/00']"
US20230316003A1,Natural Language Processing for Identifying Bias in a Span of Text,"A computing machine accesses text from a record. The computing machine identifies, using a natural language processing engine, an entity mapped to a first span of the text. The first span includes a contiguous sequence of one or more words or subwords in the text. The computing machine determines a bias category for the entity. The bias category is selected from a predefined list of bias categories. The determined bias category for the entity depends on a second span of the text. The second span includes a contiguous sequence of one or more words or subwords in the text. The second span is different from the first span.","['G06F40/279', 'G06F40/40', 'G06F40/166', 'G06F40/284', 'G06F40/30', 'G06F40/58', 'G06N3/0442', 'G06N3/045', 'G06N3/06', 'G06N3/08', 'G06N5/022', 'G06F40/205', 'G06N3/0464', 'G06N3/048']"
US20240256793A1,Methods and systems for generating text with tone or diction corresponding to stylistic attributes of images,Methods and systems for prompting a large language model (LLM) to generate a stylistic description of an image are disclosed. One or more visual attributes are extracted from an image using a first trained machine learning model. The visual attributes are mapped to one or more emotion attributes using a second trained machine learning model. A LLM prompt is generated based on the one or more emotion attributes and provided to the LLM. A generated description of the image is obtained from the LLM and displayed with the image.,"['G06F40/284', 'G06F40/30', 'G06F40/40', 'G06V10/40', 'G06V10/82', 'G06V20/70']"
CN111694924B,Event extraction method and system,"The invention provides an event extraction method, which comprises the steps of obtaining original corpus, labeling elements in sentences in a form of [ tag-element ], dividing the labeled corpus into a training set and a test set, mapping the training set into vectors by a pre-training language model to obtain word embedded vectors, inputting the word embedded vectors into a neural network model, outputting sequence tag information by the neural network model, establishing a loss function based on the sequence tag information, evaluating an event extraction model by using the test set, finally obtaining the event extraction model, adjusting the structures of the training set and the test set for a plurality of times, selecting the event extraction model with the best evaluation result as an optimal model, and inputting new original corpus into the optimal model to obtain the event extraction result; the event extraction method provided by the invention belongs to a supervised neural network extraction method, and is combined with fine-grained contextualized word vectors in the field, so that the event extraction method is more in line with the scene of event extraction in the professional field.","['G06F16/313', 'G06N3/045', 'G06N3/08']"
US20220094713A1,Malicious message detection,"In a natural language processing model such as a Bidirectional Encoder Representations from Transformers (BERT) model, transformer layers can be replaced with simplified adapters without significant loss of predictive ability. This compressed model may in turn be trained to perform security classification tasks such as detection of new phishing attacks in electronic mail communications.","['H04L63/1483', 'H04L63/1425', 'G06F18/214', 'G06K9/6256', 'H04L63/1416', 'H04L63/145']"
CN113139575B,An Image Caption Generation Method Based on Conditional Embedding Pre-trained Language Model,"The invention discloses an image title generation method based on a condition-embedded pre-training language model. The invention proposes a network based on a pre-training language model, called CE-UNILM. And constructing a KEN at the input end of the pre-training language model UNILM, carrying out target detection on the image by using a target detection method by the KEN, and inputting the result serving as key text information in a keyword embedding mode. Extracting image features by constructing the VEN, encoding the image, and inputting the image in a condition embedding mode. Meanwhile, the CELN provided by the invention is an effective mechanism for adjusting the pre-training language model to perform feature selection through visual embedding, and is applied to a transformer in a unified pre-training language model. The result shows that the method has better robustness and adaptive capacity.","['G06F18/214', 'G06F18/2411', 'G06N3/044', 'G06N3/08', 'G06V10/40']"
CN111723547A,An automatic text summarization method based on pre-trained language model,"The invention relates to a text automatic summarization method based on a pre-training language model, belonging to the technical field of natural language processing. The method comprises the following steps: encoding the source text information by using a pre-training language model BERT network; the digest is then automatically generated for the source text by the LSTM joint attention mechanism. In the automatic Chinese text summarization task, the generated Chinese summary achieves good readability, the generated summary is high in quality, meanwhile, the model training speed is high, and due to the fact that the pre-training language model is used as the encoder, even under the condition that training data are few, the summary with relatively high quality can be generated.","['G06F40/126', 'G06F40/258', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/049', 'G06N3/08']"
US20240256582A1,Search with Generative Artificial Intelligence,"Methods and apparatuses for utilizing generative artificial intelligence (AI) techniques to automatically generate and display summaries of search results are described. A search and knowledge management system may generate a set of search results for a given search query and provide the set of search results (e.g., a set of verified documents that are the most relevant verified documents for the search query) as part of an input prompt to guide a generative AI model in generating a summary response of the set of search results. The generative AI model may comprise a Generative Pre-trained Transformer (GPT) model. The summary response may comprise a natural language text response and the set of search results may comprise electronic documents and messages and/or portions thereof.","['G06F16/338', 'G06F16/3329', 'G06F16/345', 'G06N20/00', 'G06N3/045']"
US20240354436A1,Data permissioned language model document search,"Computer-implemented systems and methods are disclosed, including systems and methods utilizing language models for searching a large corpus of data. A computer-implemented method may include: receiving a first user input comprising a natural language query; vectorizing the first user input into a query vector; executing, using the query vector, a similarity search in a document search model to identify one or more similar document portions, where the document search model includes a plurality of vectors corresponding to a plurality of portions of a set of documents; generating a first prompt for a large language model (â€œLLMâ€), the first prompt including at least the first user input, and the one or more similar document portions; transmitting the first prompt to the LLM; receiving a first output from the LLM in response to the first prompt; and providing, via a user interface, the first output from the LLM.","['G06F21/6227', 'G06F16/3344', 'G06F16/3347']"
US20240312558A1,Machine learning systems and methods for deep learning of genomic contexts,"Some aspects provide for a method for generating a contextual embedding of a gene. In some embodiments, the method comprises: using at least one computer hardware processor to perform: obtaining information specifying genomic context of the gene, the genomic context containing a plurality of genes including the gene, the information containing gene sequences for the plurality of genes; encoding the information specifying the genomic context to obtain an initial encoding of the genomic context, the encoding comprising: mapping the gene sequences to protein sequences; and encoding the protein sequences using a trained protein language model (pLM) to obtain the initial encoding of the genomic context; and processing the initial encoding of the genomic context with a genomic language model (gLM) to obtain the contextual embedding of the gene.","['G06N3/08', 'G06N3/045', 'G16B20/00', 'G16B40/20', 'G16B45/00']"
US20220067304A1,Energy-Based Language Models,"Systems and methods are provided for training and using energy-based language models such as cloze language models. In particular, one aspect of the present disclosure is directed to an energy-based cloze language model for representation learning over text. In some instances, the models provided herein can be referred to as the â€œElectricâ€ model. Similar to the BERT model, example models proposed herein can be a conditional generative model of tokens given their contexts. However, example models proposed herein do not mask text or output a full distribution over tokens that could occur in a context. Instead, the example proposed models assign a scalar energy score to each input token. Another aspect of the present disclosure provides techniques to train the proposed models to assign low energies to data tokens and high energies to other ones using an algorithm based on noise-contrastive estimation.","['G06F40/40', 'G06F40/20', 'G06F40/279', 'G06F40/284', 'G06N20/00']"
CN115688748A,"Question error correction method, device, electronic equipment and storage medium","The invention provides a question error correction method, a question error correction device, electronic equipment and a storage medium. A question text sample used by the question error correction model during training and a corrected question text sample corresponding to the question text sample can be selected according to the accuracy of the question error correction model required by actual training, so that the problem that the model parameter space is too large when N in the N-gram model is too large is solved. Moreover, because the question error correction model applies word vectors, the word vectors are represented by distributed features, and the semantics of words can be considered through the word vectors, so that the characteristic of high sparsity represented by traditional statistical words can be well relieved.",[]
CN117580227A,"Control method, device, equipment and storage medium for vehicle ambient light","The application discloses a control method, a device, equipment and a storage medium of a vehicle atmosphere lamp, which are applied to the technical field of light control, wherein the method comprises the following steps: determining a description text corresponding to the current driver state; determining multimedia data matched with the current driver state according to the description text and a prompt template corresponding to the description text; determining control parameters of each lamp bead based on the coding positions of each lamp bead of the vehicle atmosphere lamp and the multimedia data; and controlling each lamp bead of the vehicle atmosphere lamp to work according to the control parameters. The control requirement of the vehicle atmosphere lamp under different scenes is met, the linkage of the vehicle atmosphere lamp under different scenes and the emotion change of a driver is realized, and the control effect of the vehicle atmosphere lamp is improved.","['H05B47/165', 'H05B47/105', 'H05B47/155', 'Y02B20/40']"
CN117668174A,"Method for constructing medical question-answering language model, and related method and equipment","The application discloses a method for constructing a medical question-answering language model, which comprises the following steps: determining a transducer module structure for a preset medical question-answering language model, wherein the transducer module structure comprises a feedforward layer module, and the feedforward layer module comprises N feedforward layers; the Transformer module structure is used for constructing the preset medical question-answering language model; and in the feedforward layer module, k feedforward layers are selected from the N feedforward layers through a parameterized router submodule so as to respectively input the inputs of the feedforward layer module into the k feedforward layers for calculation and output the k feedforward layers with the weighted summation, wherein k is smaller than N, thereby realizing the study of the preset medical question-answering language model on a plurality of different medical scenes. Related methods and apparatus are also disclosed. The method and the device improve the question-answering accuracy and stability of the medical question-answering language model.","['G06F16/3329', 'G06F16/35', 'G06N3/045', 'G06N3/0499', 'G06N3/09', 'G06N3/096', 'G16H50/20']"
CN112862662A,Method and equipment for distributed training of transform-xl language model,"The invention relates to a method and equipment for distributed training of a transform-xl language model, which are applied to a process of training the transform-xl language model by DPP, and the method comprises the following steps: acquiring text corpus data for training a tranformer xl language model; sequencing all the text corpus data according to a context sequence; partitioning the sequenced text corpus data into blocks according to the number of GPUs (graphics processing units) so as to divide the text corpus data into a plurality of subdata; distributing different sub-data to different GPUs, wherein the GPU trains the sequence of the distributed sub-data to be consistent with the context sequence; and sequentially training the sub data through the GPUs to realize training of the transform-xl language model. According to the scheme, the sampling method in the DDP is reconstructed, the reconstructed DDP is used for training the tranformer xl, the training speed of massive text corpora is accelerated, the efficiency problem is solved, and the historical information of the tranformer xl model is kept.","['G06T1/20', 'G06F9/5027']"
CN117690598A,Label-free medical table data learning method and device based on large language model,"The invention discloses a method and a device for learning unlabeled medical table data based on a large language model, comprising the following steps: acquiring non-tag medical table data, and preprocessing the non-tag medical table data to obtain processed medical table data; generating a labeling task prompt word to form a prompt word data set for each record in the medical table data, wherein the labeling task prompt word comprises a general prompt word, a unique prompt word and a required prompt word; labeling a pseudo tag for each record based on the prompt word data set by using a large language model to obtain a medical table data set with the pseudo tag; the classification model parameters are optimized by noisy data learning of the classification model using a Divideo mix algorithm and based on the medical form dataset with the pseudo-labels. Therefore, the unlabeled medical table data can be fully utilized for classification task learning.","['G16H50/70', 'G06F16/353', 'G06F18/10', 'G06F18/24', 'G06N3/0455', 'G06N3/09']"
US12387056B2,Emoji sanitization for natural language model processing,"In some implementations, a device may obtain a natural language input including an emoji. The device may identify one or more appearance modifiers associated with the emoji. The device may generate a token associated with the emoji that removes the one or more appearance modifiers, wherein the token is associated with multiple emojis including the emoji, and wherein the token is a modified code associated with the emoji or is associated with a cluster that is associated with the multiple emojis. The device may provide, to a natural language processing (NLP) model, the token associated with the emoji. The device may obtain, from the NLP model, an output that indicates an interpretation of the natural language input based on providing the token to the NLP model.","['G06F40/284', 'G06F40/30', 'G06F40/40']"
WO2024228873A1,Structure aware transformers for natural language processing,"Disclosed is a machine learning model architecture that can incorporate structure information from multiple types of structured text into a single unified machine learning model. For example, a single unified model may be trained with structure information from XML files, tabular data, and/or flat text files. A structure-aware attention mechanism builds on the attention mechanism of the transformer architecture. Specifically, values computed for a traditional transformer attention mechanism are used to compute structure-aware attention scores. In some configurations, the location of a token in the structured text is incorporated into that token's embedding. Similarly, metadata about a token, such as whether the token is a key or a value of a key/value pair, may be incorporated into the token's embedding. This enables the model to reason over token metadata and the location of the token in the structured text in addition to the meaning of the token itself.","['G06F40/30', 'G06N3/0499', 'G06F40/14', 'G06F40/216', 'G06F40/221', 'G06F40/284']"
US20250111239A1,Test Case Generation Through Reinforcement Learning With Static Code Quality Rewards,"A deep learning model is trained to learn to generate a better-quality unit test case for a focal method through reinforcement learning using a reward score that considers static code quality properties of a best coding standard. The static code quality properties include an assertion in the predicted unit test case, an invocation of the focal method in the predicted unit test case, and a descriptive name for the predicted unit test case. A reward model is trained to compute a reward score for a model-predicted unit test case based on the static code quality properties. The reward score is used in a proximal policy optimization method to produce a policy loss that updates the parameters of the deep learning model towards generating a better-quality unit test case.","['G06N3/092', 'G06N3/045']"
US12147497B2,Systems and methods for cross-lingual cross-modal training for multimodal retrieval,"Current pretrained vision-language models for cross-modal retrieval tasks in English depend upon on the availability of many annotated image-caption datasets for pretraining to have English text. However, the texts are not necessarily in English. Although machine translation (MT) tools may be used to translate text to English, the performance largely relies on MT's quality and may suffer from high latency problems in real-world applications. Embodiments herein address these problems by learning cross-lingual cross-modal representations for matching images and their relevant captions in multiple languages. Embodiments seamlessly combine cross-lingual pretraining objectives and cross-modal pretraining objectives in a unified framework to learn image and text in a joint embedding space from available English image-caption data, monolingual corpus, and parallel corpus. Embodiments are shown to achieve state-of-the-art performance in retrieval tasks on multimodal multilingual image caption datasets.","['G06F18/2148', 'G06F40/30', 'G06F40/58', 'G06F40/20', 'G06F40/44', 'G06F40/51', 'G06N3/045', 'G06N3/084', 'G06N5/022', 'G06V10/774', 'G06V10/82', 'G06F40/216', 'G06F40/284']"
US12243290B2,Video transformer for deepfake detection with incremental learning,"A method, apparatus, and system for detecting DeepFake videos, includes an input device for inputting a potential DeepFake video, the input device inputs a sequence of video frames of the video, and processing circuitry. The processing circuitry detects faces frame by frame in the video to obtain consecutive face images, creates UV texture maps from the face images, inputs both face images and corresponding UV texture maps, extracts image feature maps, by a convolution neural network (CNN) backbone, from the input face images and corresponding UV texture maps and forms an input data structure, receives the input data structure, by a video transformer model that includes multiple encoders, and computes, by the video transformer model, a classification of the video as being Real or Fake. A display device plays back the potential DeepFake video and an indication that the video is Real or Fake.","['G06V10/7715', 'G06T7/40', 'G06T7/50', 'G06V10/54', 'G06V10/776', 'G06V10/82', 'G06V20/41', 'G06V20/46', 'G06V20/49', 'G06V20/70', 'G06V40/161', 'G06V40/40', 'H04N21/44008', 'H04N21/637', 'G06T2207/10016', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30201']"
CN113987179B,"Dialogue emotion recognition network model, construction method, electronic device and storage medium based on knowledge enhancement and retroactive loss","The application discloses a dialogue emotion recognition network model based on knowledge enhancement and backtracking loss, a construction method, electronic equipment and a storage medium, and belongs to the technical field of natural language processing. According to the method, firstly, the encoder is utilized, interaction of all parties in a conversation is achieved through designing different mask matrixes, conversation history information is introduced, and the influence of emotion dynamics is considered only in a word encoding stage. Then, the application realizes the utilization of the external structured emotion knowledge by utilizing the graph attention network, thereby expanding the semantic information of the words and making up for the lack of knowledge of the word encoder module in certain specific dimensions. Finally, a backtracking loss is added on the basis of the task loss function to achieve the effect of utilizing prior experience of the past model state in the training process. The aim is to use previous training experience to guide weight updates, ensuring that model decisions are closer to true values than would be the case in the previous training step.","['G06F16/353', 'G06F18/214', 'G06F18/24', 'G06N3/08']"
US12147775B2,Content augmentation with machine generated content to meet content gaps during interaction with target entities,"A content generator system receives a request to generate content for a target entity, and one or more keywords. The content generator system retrieves, for the target entity, a current stage identifier linking the target entity to a current stage within a multi-stage objective. The content generator system generates an input vector including the current stage identifier, a target stage identifier, a token embedding comprising the one or more keywords, and a position embedding for each of the one or more keywords, the target stage identifier associated with a target stage within the multi-stage objective different from the current stage. The content generator system generates output text content for the target entity by applying a generative transformer network to the input vector. The content generator system transmits the output text content to a computing device associated with the target entity.","['G06F40/40', 'G06F40/216', 'G06F40/279', 'G06F40/30', 'G06F40/44', 'G06F40/56', 'G06N3/08', 'G06N3/09']"
US20240202464A1,Iterative context-based generative artificial intelligence,"Systems and methods managing a plurality of agents to generate a response to a query using a multimodal model. An example method uses the plurality of agents to iteratively determine subsequent outputs of the multimodal model satisfies the query. It can generate a respective context associated with a respective output of the multimodal model. And determine, by the multimodal model based on the respective context, whether the respective subsequent output satisfies the query.","['G06F16/3347', 'G06F16/3326', 'G06F16/334', 'G06F16/335', 'G06F16/338', 'G06F16/345', 'G06F40/20', 'G06F40/40', 'G06N20/00', 'G06N3/045', 'G06N3/0475', 'G06N3/092', 'G06N5/04', 'G06N3/063']"
US20220122689A1,Systems and methods for alignment-based pre-training of protein prediction models,"Embodiments described herein provide an alignment-based pre-training mechanism for protein prediction. Specifically, the protein prediction model takes as input features derived from multiple sequence alignments (MSAs), which cluster proteins with related sequences. Features derived from MSAs, such as position specific scoring matrices and hidden Markov model (HMM) profiles, have long known to be useful features for predicting the structure of a protein. Thus, in order to predict profiles derived from MSAs from a single protein in the alignment, the neural network learns information about that protein's structure using HMM profiles derived from MSAs as labels during pre-training (rather than as input features in a downstream task).","['G16B5/20', 'G16B40/20', 'G16B30/10']"
US20240414191A1,Interactive cyber-security user-interface for cybersecurity components that cooperates with a set of llms,"An interactive cyber-security user-interface for cybersecurity components can receive a voice input from a user as well as ii) a text input as a user input. The interactive cyber-security user-interface works with a set of differently trained LLMs to carry out tasks on behalf of the user input. The interactive cyber-security user-interface cooperates with the set of differently trained LLMs, which are grouped together to operate as an orchestrated system to provide different tasks. The tasks can include a collection of supplementary information, a summarization of cyber security information, translating a query in the natural human speech format into the required search syntax, how to integrate with an API, acting as a first line of support to user inquiries, a suggested response to a cyber security issue, etc. The interactive cyber-security user-interface for the cybersecurity components acts as the user interface for one or more of the cybersecurity components.","['G06F40/58', 'H04L63/1416', 'H04L63/1425', 'H04L63/1433', 'H04L63/1441', 'H04L63/205']"
US20250117666A1,Data generation and retraining techniques for fine-tuning of embedding models for efficient data retrieval,"A method includes obtaining chunks of information and generating training samples for an embedding model using the chunks of information. Generating the training samples includes using at least one large language model to generate training samples in which different ones of the chunks of information are and are not relevant to different potential queries. The method also includes training the embedding model using the training samples. In some cases, the embedding model may represent a retriever model. For example, an input query may be obtained at the retriever model, and the retriever model may be configured to identify a specified number of chunks of information relevant to the input query. One or more of the chunks of information may be provided from the retriever model to a generative model, and the generative model may be used to create a response to the input query.","['G06N3/091', 'G06N20/00']"
US12386874B2,Database generation from natural language text documents,"Some embodiments may perform operations of a process that includes obtaining a natural language text document and use a machine learning model to generate a set of attributes based on a set of machine-learning-model-generated classifications in the document. The process may include performing hierarchical data extraction operations to populate the attributes, where different machine learning models may be used in sequence. The process may include using a pre-trained Bidirectional Encoder Representations from Transformers (BERT) model augmented with a pooling operation to determine a BERT output via a multi-channel transformer model to generate vectors on a per-sentence level or other per-text-section level. The process may include using a finer-grain model to extract quantitative or categorical values of interest, where the context of the per-sentence level may be retained for the finer-grain model.","['G06F16/3344', 'G06F16/31', 'G06F16/3347', 'G06F16/355', 'G06F40/186', 'G06F40/216', 'G06F40/279', 'G06F40/295', 'G06F40/30', 'G06F40/35', 'G06F40/44', 'G06N20/20', 'G06N3/0442', 'G06N3/0455', 'G06Q50/18', 'G06N5/04']"
US11651211B2,Training of neural network based natural language processing models using dense knowledge distillation,"Techniques for training a first neural network (NN) model using a pre-trained second NN model are disclosed. In an example, training data is input to the first and second models. The training data includes masked tokens and unmasked tokens. In response, the first model generates a first prediction associated with a masked token and a second prediction associated with an unmasked token, and the second model generates a third prediction associated with the masked token and a fourth prediction associated with the unmasked token. The first model is trained, based at least in part on the first, second, third, and fourth predictions. In another example, a prediction associated with a masked token, a prediction associated with an unmasked token, and a prediction associated with whether two sentences of training data are adjacent sentences are received from each of the first and second models. The first model is trained using the predictions.","['G06F40/284', 'G06N3/045', 'G06N3/08', 'G10L15/16', 'G10L25/30']"
EP4498262A1,Tuning a generative artificial intelligence model,"Systems and methods are disclosed for tuning a generative artificial intelligence (AI) model based on a knowledge base. Instead of manually generating questions relevant to the knowledge base, providing those questions to the generative AI model, and manually reviewing the answers generated by the generative AI model in order to tune the generative AI model over many iterations, a natural language processing model may be configured to leverage the knowledge base to automatically generate questions and answers based on the knowledge base. In this manner, the natural language processing model is able to generate tuning data that may be used to automatically tune the generative AI model. The systems and methods also disclose automatic tuning of the generative AI model, including testing and feedback that may be used to improve tuning of the generative AI model.","['G06F40/30', 'G06F16/3329', 'G06F40/40', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N5/04', 'G06F40/216', 'G06F40/289']"
CN113823272B,"Voice processing method, device, electronic device and storage medium","The application discloses a voice processing method, a device, an electronic device and a storage medium, comprising the steps of obtaining a marked first voice sample, an unmarked second voice sample, a pre-trained basic voice model, a streaming voice processing model and a language model; the method comprises the steps of adjusting a streaming voice processing model according to a first voice sample to obtain an adjusted streaming voice processing model, training the streaming voice processing model based on the first voice sample, a second voice sample and a teacher model corresponding to the streaming voice processing model to obtain a trained streaming voice processing model, marking the second voice sample according to the first voice sample, a basic voice model and a language model to obtain a marked second voice sample, and training the trained streaming voice processing model based on the first voice sample and the marked second voice sample to obtain a target voice processing model so as to process voice to be processed through the target voice processing model.","['G10L15/063', 'Y02T10/40']"
CN115017911B,Cross-modal processing for vision and language,"According to an implementation of the present disclosure, a scheme for cross-modal processing is presented. In this approach, a set of visual features of the training image is extracted from the visual feature extraction sub-model in the target model. Each visual feature corresponds to a block of pixels in the training image. A set of visual semantic features corresponding to the set of visual features is determined based on the visual semantic dictionary. And extracting a group of text features of the training text corresponding to the training image according to the text feature extraction sub-model in the target model. Each text feature corresponds to at least one word in the training text. Based on the set of visual semantic features and the set of text features, a target model is trained for determining association information between the input text and the input image. In this way, the trained target model may be facilitated to accurately and quickly provide results in a variety of vision-language tasks.","['G06F40/30', 'G06N3/08', 'G06V20/41', 'G06F40/40', 'G06N3/045', 'G06N3/0464', 'G06N3/084', 'G06N3/0895', 'G06N3/09']"
US12399892B2,System and method for transferable natural language interface,"A computer system and method for answering a natural language question is provided. The system comprises at least one processor and a memory storing instructions which when executed by the processor configure the processor to perform the method. The method comprises receiving a natural language question, generating a SQL query based on the natural language question, generating an explanation regarding a solution to the natural language question as answered by the SQL query, and presenting the solution and the explanation.","['G06F16/2433', 'G06F16/243']"
GB2618883A,Methods and apparatus for augmenting training data using large language models,"A method of giving a command to a management system involves a processor receiving a natural language request for performing an identified task, e.g. a cybersecurity task such as malware detection. A set of features are extracted from the request, based on the context in which the management system is operating. A first machine learning (ML) model infers a template command based on the features. The template command is presented in editable form to the user, for approval. The final command approved by a user is executed by the management system. The final command can also be provided to a second machine learning model which uses it to generate a set of natural language phrases semantically related to the final command. These phrases can then be used to create new training data which is used to retrain the first ML model. The natural language request can be formed via an auto-complete process by receiving a first portion of a request from a user, using a machine learning model to predict a number of second portions completing the request, and having the user select one of the options.","['G06F16/24522', 'G06F21/57', 'G06F16/243', 'G06F40/30', 'G06F40/35', 'G06N3/045', 'G06N3/08', 'G06F2221/034', 'G06F40/205', 'G06F40/274', 'G06F40/289']"
US20250139681A1,Contextual bandit model for query result ranking optimization,"A system uses a contextual bandit model for query processing. The system receives, from a client device, a user query for identifying one or more items by the system and described by query feature(s). The system obtains contextual feature(s) describing the query's context. The system applies a query processing model to the user query to determine a relevance score for each query result. The system applies a contextual bandit model to the query features and the contextual features to determine a weight vector for ranking parameters. The ranking parameters include relevance of a query result to the user query and dependability of the query result. The system determines, for each query result, a ranking score based on the weight vector and ranking parameter values of the query result. The system transmits the query results ranked according to the ranking scores for display on the client device.","['G06Q30/0629', 'G06Q30/0633']"
US12299579B2,Adversarial pretraining of machine learning models,This document relates to training of machine learning models. One example method involves providing a machine learning model having one or more mapping layers. The one or more mapping layers can include at least a first mapping layer configured to map components of pretraining examples into first representations in a space. The example method also includes performing a pretraining stage on the one or more mapping layers using the pretraining examples. The pretraining stage can include adding noise to the first representations of the components of the pretraining examples to obtain noise-adjusted first representations. The pretraining stage can also include performing a self-supervised learning process to pretrain the one or more mapping layers using at least the first representations of the training data items and the noise-adjusted first representations of the training data items.,"['G06N3/084', 'G06F18/24', 'G06F40/20', 'G06F40/242', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N3/088', 'G06V10/7784', 'G06V10/82', 'G06F40/284', 'G06N3/082', 'G06T2207/20081']"
WO2024236317A1,Method and system for predicting biological entities,"A computer-implemented method predicting a biological entity meeting a user- defined biological requirement using a knowledge base, the method comprising: providing an inference knowledge base comprising a corpus of textual data; receiving a user query defining a biological requirement for which a biological entity is to be predicted; obtaining, based on the query, a query sentence text describing the biological requirement and including mention of a biological entity, in which the biological entity itself is masked for prediction; selecting a candidate biological entity for the masked biological entity and retrieving a plurality of evidence sentences from the knowledge base, each evidence sentence including mention of the candidate biological entity, wherein the evidence sentences are retrieved based on computing a similarity of the query sentence to sentences within the knowledge base; inputting each training query sentence and a plurality of retrieved evidence sentences into a reasoner model, where mention of the candidate biological entity is masked in the query sentence and evidence sentences, the reasoner model trained to predict a probability that the candidate biological entity is the masked biological entity based on the retrieved evidence sentences.","['G06N5/022', 'G06N3/045', 'G06N3/08', 'G16H50/20', 'G16H50/70']"
WO2024059334A1,System and method of fine-tuning large language models using differential privacy,"A system and method of fine-tuning a large language model including differential privacy includes pretraining a large language model using a non-private dataset to generate a pretrained output. The pretrained output is used as an input to a fine-tuning model. The pretrained output is fine-tuned using a private dataset to generate a differentially private large language model. A set of privacy parameters including a privacy budget and a failure probability are determined. A privacy analysis agent calculates a desired amount of privacy based on the privacy budget, and calculates a noise multiplier based on the desired amount of privacy. The pretrained model is transformed using the noise multiplier to add an amount of noise to the pretrained output. A randomized differentially private stochastic gradient descent model fine-tunes the transformed pretrained output by reparametrizing a weight matrix associated with each layer of the transformed pretrained output.","['G06N3/096', 'G06N3/045', 'G06N3/084', 'G06N3/09']"
CN112328849B,"User portrait construction method, user portrait-based dialogue method and device","The application discloses a user portrait construction method, a user portrait-based dialogue method and a user portrait-based dialogue device, and relates to the field of robot question and answer. The method comprises the following steps: acquiring historical dialogue data of a user account; extracting features of the historical dialogue data to obtain historical dialogue features; classifying historical dialogue characteristics and user information corresponding to the user account to obtain user labels with at least two dimensions corresponding to the user account, wherein the user labels are used for classifying user behavior data corresponding to the user account according to the historical dialogue data; a user representation is constructed from the user tags in at least two dimensions. The user labels with at least two dimensions are generated by acquiring the historical data corresponding to the user account, and the accurate and comprehensive user labels can be acquired based on the accurate semantic intention, so that the accurate and comprehensive user portrait is constructed.","['G06F16/906', 'G06F16/3329']"
US12182684B2,Sequence model processing method and apparatus,"Embodiments of this application disclose a sequence model processing method and apparatus, to improve a task execution effect of a sequence model. The method includes: inputting a source sequence into an encoder side of a sequence model, the encoder side including a self-attention encoder and a temporal encoder; encoding the source sequence by using the temporal encoder, to obtain a first encoding result, the first encoding result including time series information obtained by performing time series modeling on the source sequence; and encoding the source sequence by using the self-attention encoder, to obtain a second encoding result; inputting a target sequence, the first encoding result, and the second encoding result into a decoder side of the sequence model; and decoding the target sequence, the first encoding result, and the second encoding result by using the decoder side, and outputting a decoding result obtained after the decoding.","['G06F40/44', 'G06F40/58', 'G06N3/044', 'G06N3/045', 'G06N3/049', 'G06N3/08']"
CN112966712B,"Language model training method and device, electronic equipment and computer readable medium","The embodiment of the application discloses a language model training method and device, electronic equipment and a computer readable medium. An embodiment of the method comprises: a method for language model training, the method comprising: selecting a first text sample set based on the natural language processing task, and training a first pre-training model by using the first text sample set to obtain an initial language model; testing the test texts in the preset test text set by using the initial language model, and generating a second text sample set based on the test result and the test texts; training the first pre-training model by using a second text sample set to obtain a second pre-training model; and training the second pre-training model by using the first text sample set to obtain a target language model. The embodiment can obtain a model applied to a natural language processing task, and the model can improve the accuracy of a processing result.","['G06F18/214', 'G06F16/35', 'G06N20/00']"
CN117093696B,"Question text generation method, device, equipment and medium of large language model","The invention discloses a method, a device, equipment and a medium for generating a question text of a large language model, which relate to the technical field of computers and comprise the following steps: performing similar text retrieval on an initial question text input by a user by using a preset semantic similarity retrieval algorithm to obtain a corresponding similar text set; training a strategy network by using a preset strategy gradient algorithm; determining a target question text which is most relevant to the semantics corresponding to the initial question text from a similar text set by using a trained strategy network; inputting the target question text into the large language model to obtain the answer information which is generated by the large language model according to the semantics corresponding to the target question text and meets the user intention. According to the invention, through optimizing the selection process and the generation quality of the promts, the problem of unstable output sensitivity and performance of the large language model to different promts can be effectively relieved, and the large language model can generate reply information meeting the user intention more accurately.","['G06F16/3329', 'G06F16/3344', 'G06F40/30', 'G06N3/045', 'G06N3/092', 'Y02D10/00']"
US12093294B1,Edge computing units for operating conversational tools at local sites,"Computing units provided at local sites or edge locations are programmed to execute conversational tools that generate pertinent, domain-specific responses to queries received from workers at such sites or locations. The conversational tools are large language models that are trained with domain-specific knowledge documents. Data representing queries are received from workers at such sites or locations and provided as inputs to the conversational tools along with text representing nearest knowledge documents from a knowledge base associated with the domain, as well as contextual data. Responses identified based on outputs received from the conversational tools in response to the inputs are provided to the workers that generated the queries. Where subsequent queries are received from the workers, responses to the subsequent queries are identified based on the subsequent queries, nearest knowledge documents, contextual data, and conversational histories including previously received queries and responses to such queries.","['G06F16/3329', 'G06F16/3344', 'G06F40/30']"
US12131123B2,Grounded text generation,"A controllable grounded response generation framework includes a machine learning model, a grounding interface, and a control interface. The machine learning model is trained to output computer-generated text based on input text. The grounding interface is useable by the machine learning model to access a grounding source including information related to the input text. The control interface is useable by the machine learning model to recognize a control signal. The machine learning model is configured to include information from the grounding source in the computer-generated text and focus the computer-generated text based on the control signal.","['G06F3/167', 'G06F40/30', 'G06F40/35', 'G06F40/56', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/088', 'H04L51/02', 'H04L51/42', 'G06N3/006']"
WO2022088444A1,Multi-task language model-oriented meta-knowledge fine tuning method and platform,"A multi-task language model-oriented meta-knowledge fine tuning method and a platform. On the basis of cross-domain typical fractional learning, the method obtains highly transferable common knowledge on different data sets of similar tasks, i.e. meta-knowledge, mutually correlates and mutually strengthens learning processes of similar tasks in different domains corresponding to different data sets, thereby improving fine tuning effects of similar downstream tasks on data sets in different domains in language model applications, and improving parameter initialization abilities and generalization abilities of universal language models for similar tasks. The method is to fine tune a cross-domain data set of downstream tasks. Effects of a compression model obtained by means of fine tuning are not limited to a specific data set of such tasks. On the basis of a pre-training language model, the downstream tasks are fine tuned by means of using a meta-knowledge fine tuning network to obtain a language model of similar downstream tasks independent of data sets.","['G06F16/355', 'G06F40/30', 'G06N5/04']"
CN111883110B,"Acoustic model training method, system, equipment and medium for speech recognition","The invention discloses an acoustic model training method, system, equipment and medium for voice recognition, which are characterized in that training samples are obtained, first voice fragments are respectively input into a plurality of preset different reference voice recognition models for recognition to obtain a plurality of recognition texts, similarity values between every two recognition texts are calculated to determine text scores corresponding to the first voice fragments, whether the text scores are larger than a preset threshold value is judged, if yes, the first voice fragments corresponding to the text scores are screened out to serve as third voice fragments, and the voice recognition models are generated based on the training of the third voice fragments and the second voice fragments. The invention solves the defects of high word error rate of voice recognition caused by the fact that a large number of training data sets cannot be formed in a short time because of time and labor waste of manual marking of the training data sets, and improves the recognition accuracy of a voice recognition model.","['G10L15/063', 'G10L15/04', 'G10L15/06', 'G10L15/26', 'G10L2015/0631']"
US11074412B1,Machine learning classification system,"A system trains a classification model. Text windows are defined from tokens based on a window size. A network model including a transformer network is trained with the text windows to define classification information. A first accuracy value is computed. (A) The window size is reduced using a predefined reduction factor value. (B) Second text windows are defined based on the reduced window size. (C) Retrain the network model with the second text windows to define classification information. (D) A second accuracy value is computed. (E) An accuracy reduction value is computed from the second accuracy value relative to the first accuracy value. When the computed accuracy reduction value is â‰¥an accuracy reduction tolerance value, repeat (A)-(E) until the accuracy reduction value is <the accuracy reduction tolerance value. Otherwise, increase the window size, define final text windows based on the increased window size, and retrain the network model.","['G06F40/284', 'G06F40/30', 'G06N3/044', 'G06N3/0445', 'G06N3/045', 'G06N3/08']"
US11068663B2,Session embeddings for summarizing activity,"The disclosed embodiments provide a system for processing data. During operation, the system obtains a first sentence representing a first sequence of actions between a user and a set of jobs. Next, the system applies a language model to token embeddings of a first set of tokens in the first sentence and position embeddings of token positions in the first sentence to produce a first set of output embeddings. The system then combines the first set of output embeddings into a first session embedding that encodes the first sequence of actions. Finally, the system outputs the first session embedding for use in characterizing job-seeking activity of the user.","['G06Q10/1053', 'G06F16/24578', 'G06F16/9535', 'G06F40/284', 'G06F40/30', 'G06N20/00', 'G06N3/044', 'G06N7/01']"
CN110389996B,Implementing a full-sentence recurrent neural network language model for natural language processing,The present disclosure relates to implementing a full sentence recurrent neural network language model for natural language processing. A full sentence Recurrent Neural Network (RNN) Language Model (LM) is provided for estimating likelihood probabilities that each full sentence processed by natural language processing is correct. A noise contrast estimation sampler is applied to at least one complete sentence in a corpus of multiple sentences to generate at least one incorrect sentence. The full sentence RNN LN is trained using the at least one full sentence and the at least one incorrect sentence in the corpus to discern the at least one full sentence as correct. A full sentence recurrent neural network language model is applied to estimate likelihood probabilities that each full sentence processed by natural language processing is correct.,"['G06F40/216', 'G06F16/3344', 'G06F40/20', 'G06F40/289', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G10L15/063', 'G10L15/197', 'G10L15/22', 'G06N7/01', 'G10L15/16']"
CN111554268B,"Language identification method based on language model, text classification method and device","The application relates to a language identification method based on a language model, a text classification method and device, computer equipment and a storage medium, wherein the method comprises the following steps: acquiring training word vectors corresponding to training sentences, and inputting the training word vectors into a first model to be trained and a trained second model respectively to obtain a feature matrix output by each first network layer of the first model and a feature matrix output by each second network layer of the second model; the first network layer and the second network layer are in one-to-one correspondence, and the number of the network layer layers of the first model is smaller than that of the second model; and performing similarity calculation on the feature matrix output by each first network layer and the feature matrix output by the second network layer corresponding to each first network layer to obtain each similarity, adjusting the model parameters of the first model based on each similarity until the updated target similarity meets the convergence condition to obtain a trained first model, and performing language identification through the first model. By adopting the method, the model training efficiency can be improved.","['G10L15/005', 'G06F16/35', 'G06N3/045', 'G06N3/08', 'G10L15/063', 'G10L15/18']"
CN118861257A,A method and system for answering questions about cultural data based on a large model,"The invention discloses a text data question-answering method and system based on a big model, comprising the following steps: s1, acquiring a query text input by a user, and preprocessing; s2, acquiring an intention analysis result; s3, judging whether the user inquiry is a service scene, if not, carrying out non-service scene processing, and updating the current scene; s4, extracting key entities and relations through Few-shot Learning; s5, constructing a dynamic extensible enhanced knowledge graph by using the extracted key entities and relations, and dynamically updating and optimizing the knowledge graph by using a technology combining a transducer model and GRAPHSAGE; s6, converting the index in the knowledge graph into a numerical vector for representation by using an m3e vectorization technology; s7, optimizing system performance by adopting reinforcement learning and Bayesian optimization technology based on near-end strategy optimization; s8, generating answers and data analysis reports based on the user query. The invention combines a large language model, few-shot Learning and knowledge graph technology to realize an intelligent question-answering system, and has the advantages of high response efficiency and the like.","['G06F16/3329', 'G06F16/3344', 'G06F40/295', 'G06F40/30']"
US20250086427A1,A Method and System for Generating Optimal Machine Learning Model Architectures,"A system and method capable of learning connections between machine learning (ML) datasets and optimal ML models to perform model selection for many different dataset types, ML task frameworks, and ML applications. A system and method for generating a desired machine learning model representation by providing as input a dataset representation and one or more target requirements representations for the desired machine learning model representation, training a transformer using a system dataset including a number of machine learning experiments, each machine learning experiment having an associated dataset representation, target requirements representation, model representation, and performance representation, and using the transformer to generate the desired machine learning model representation having a performance representation which equals or exceeds the target requirements representation.","['G06N3/092', 'G06N3/045', 'G06N3/084']"
CN115600675A,A Model Compression and Inference Acceleration Method Based on Lightweight Multi-exit Network,"The invention discloses a model compression and inference acceleration method based on a light-weight multi-outlet network, which comprises the following steps of: 1: training a pre-training language model based on a converter on a data set given by a user to obtain a teacher model and initialize a student model; 2: building volume requirements of a lightweight model, and setting word embedding matrix middle dimensionality, self-attention head number and feedforward network middle dimensionality according to the model volume requirements; 3: training a target light-weight multi-outlet network model by using a combined optimization method integrating static compression and dynamic acceleration; 4: before inference, the confidence threshold value of the light-weight multi-outlet network model is set or changed according to actual needs, and variable acceleration is achieved. The invention designs a width-compressed multi-outlet model to optimize time and space efficiency, greatly reduces the storage calculation overhead, reduces the performance reduction of the compressed multi-outlet model caused by the inconsistency of each layer, and makes up the problem of the great reduction of the model performance caused by the combination of static compression and dynamic acceleration.","['G06N5/022', 'G06F40/30', 'G06N5/04']"
US20250171017A1,Scene modeling using trajectory predictions and tokenized features,"In various examples, systems and methods are disclosed relating to generating scene mode conditioned trajectory predictions that are usable for interfacing with an LLM. A system can obtain traffic scene data associated with movement of one or more agents relative to a vehicle navigating through an environment. The system can encode the traffic scene data to determine latent representations of the movement of the one or more agents relative to the vehicle navigating through the environment. Then the system can determine a joint scene mode distribution based at least on the latent representations. The system can then decode the joint scene mode distribution into one or more trajectory predictions and one or more categorical predictions for each agent of the one or more agents.","['B60W30/095', 'B60W30/0956', 'B60W30/18163', 'B60W60/00276']"
US20250148258A1,Large language model configured to direct domain-specific queries to domain-specific edge models,"Described herein are systems and techniques for implementing a petrophysics assistant. An example method can include receiving, by a control language model configured to perform natural language processing, a query related to one or more subject areas; based on the one or more subject areas associated with the query and a respective domain-specific knowledge of each domain-specific language model from a plurality of domain-specific language models, selecting one or more domain-specific language models from the plurality of domain-specific language models to answer the query; sending, to the one or more domain-specific language models, a request to answer the query; and generating, by the control language model, a response to the query based on one or more responses to the query received from the one or more domain-specific language models.","['G06N3/006', 'G06N3/042', 'G06N3/045', 'G06N3/0455', 'G06N3/08']"
US20210374361A1,Removing undesirable signals from language models using negative data,"A method for training a language model using negative data may include accessing a first training corpus comprising positive training data and accessing a second training corpus comprising negative training data. The method may further include training a first language model using at least the first training corpus, the second training corpus, and a maximum likelihood function. The maximum likelihood function may maximize the likelihood of the first language model predicting the positive training data while minimizing the likelihood of the first language model predicting the negative training data.",['G06F40/58']
US20240112088A1,Vector-Quantized Image Modeling,"Systems and methods are provided for vector-quantized image modeling using vision transformers and improved codebook handling. In particular, the present disclosure provides a Vector-quantized Image Modeling (VIM) approach that involves pretraining a machine learning model (e.g., Transformer model) to predict rasterized image tokens autoregressively. The discrete image tokens can be encoded from a learned Vision-Transformer-based VQGAN (example implementations of which can be referred to as ViT-VQGAN). The present disclosure proposes multiple improvements over vanilla VQGAN from architecture to codebook learning, yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN further improves vector-quantized image modeling tasks, including unconditional image generation, conditioned image generation (e.g., class-conditioned image generation), and unsupervised representation learning.","['G06N3/0495', 'H04N19/94', 'G06N20/00', 'G06F40/284', 'G06N3/045', 'G06N3/0455', 'G06N3/0475', 'G06N3/084', 'G06N3/088', 'G06N3/094', 'G06T9/002', 'G06V10/28', 'G06V10/764', 'G06V10/766', 'G06V10/82', 'H04N19/46', 'H04N19/61', 'G06N3/0464', 'H04N19/12', 'H04N19/124', 'H04N19/17', 'H04N19/463', 'H04N19/467']"
CN112214599B,Multi-label text classification method based on statistics and pre-trained language models,"The invention discloses a multi-label text classification method based on statistics and a pre-training language model, which comprises the following steps of: s1, preprocessing the training corpora to be classified; s2, establishing a label obtaining model based on a statistical method and a language model; s3, processing the acquired label data; s4, establishing a multi-label classification model based on a pre-training language model, and performing model training by using the obtained label data; and S5, performing multi-label classification on the text data to be classified by using the trained multi-label text classification model. The invention provides a label acquisition method combining a statistical method and a pre-training language model, semantic coding information of a text is acquired by using an ALBERT language model, a data set does not need to be marked manually, and the label acquisition accuracy can be improved.","['G06F16/355', 'G06F18/2415', 'G06F40/126', 'G06F40/216', 'G06F40/30', 'G06N3/045']"
US20240135103A1,Training language models and preserving privacy,"In implementations of systems for training language models and preserving privacy, a computing device implements a privacy system to predict a next word after a last word in a sequence of words by processing input data using a machine learning model trained on training data to predict next words after last words in sequences of words. The training data describes a corpus of text associated with clients and including sensitive samples and non-sensitive samples. The machine learning model is trained by sampling a client of the clients and using a subset of the sensitive samples associated with the client and a subset of the non-sensitive samples associated with the client to update parameters of the machine learning model. The privacy system generates an indication of the next word after the last word in the sequence of words for display in a user interface.","['G06F40/30', 'G06F40/295', 'G06F40/274']"
CN110674639B,Natural language understanding method based on pre-training model,"The invention discloses a natural language understanding method based on a pre-training model. The method comprises the following steps: establishing a pre-training model based on a bidirectional depth Transformer; performing word segmentation on the sentence to be understood, and adding special labels at the beginning and the end of the sentence to be understood respectively to obtain a text vector of the sentence to be understood; taking the text vector of the sentence to be understood as input, and calling the pre-training model to obtain the text semantic vector of the sentence to be understood; performing intention recognition; and carrying out entity identification. The invention can accurately and comprehensively understand the intention and identify the entity, and provides a solid foundation for subsequent conversation; the quality and the user experience of the man-machine conversation system can be remarkably improved.","['G06F16/3329', 'G06N3/045', 'G06N3/08']"
CN111444709B,"Text classification method, device, storage medium and equipment","The application discloses a text classification method, a text classification device, a storage medium and a text classification device, and belongs to the technical field of artificial intelligence. The method comprises the following steps: acquiring a text to be processed; calling a target language model in the emotion analysis model to encode the text to be processed to obtain a feature vector sequence, wherein the target language model is a BERT model, and the feature vector sequence represents the context relationship between words in the text to be processed; vectorizing the determined target words to obtain target feature vectors; performing first feature fusion processing on the feature vector sequence and the target feature vector, and acquiring a second feature vector according to the obtained first feature vector and the feature vector sequence; performing second feature fusion processing on the second feature vector and the target feature vector; and predicting the emotion polarity of the target word in the text to be processed based on the obtained third feature vector. According to the method and the device, the emotion polarity of the target word in the text can be accurately predicted, and the target word can be accurately classified in the text.",[]
CN118278521A,Electric power field question-answering method and device based on local text embedded retrieval,"The invention relates to the technical field of artificial intelligence, in particular to a power field question-answering method and device based on local text embedded retrieval. The invention provides assistance and guidance in various aspects of operation, maintenance, planning, safety and the like of the power system, and improves the intelligent and efficient level of the operation of the power system. The invention stores the knowledge base content of the power system into a local vector database by using a vector embedding embedding technology, searches matched knowledge base fragments by using a vector correlation algorithm, takes the knowledge base fragments as contexts, and submits the knowledge base fragments and the related problems of the power system as Prompt promt to a large language model for auxiliary reasoning response.","['G06N5/041', 'G06F16/3329', 'G06F16/35', 'G06F21/6245', 'G06F40/253', 'G06F40/289', 'G06F40/30', 'G06N3/0455', 'G06N3/084', 'G06N5/025', 'G06Q50/06', 'Y02D10/00']"
US12315249B2,Remote sensing interpretation,"Implementations are described herein for obtaining a sequence of high-elevation images capturing a particular geographic area during a particular time period in a plurality of spectral bands; applying the sequence of high-elevation images as input to upstream machine learning model(s) to generate remote sensing embeddings indicating terrain feature(s) of the particular geographic area; applying agricultural data obtained from a local agricultural knowledge graph as input to additional upstream machine learning model(s) to generate agricultural knowledge embedding(s); inferring a natural-language description of a status of the particular geographic area based on generating: an aggregate representative embedding that semantically represents a plurality of agricultural conditions of the particular geographic area, and a natural-language description of one or more of the plurality of agricultural conditions of the particular area using a large language model; and causing a user device associated with an agricultural entity to present the natural-language description of the status.","['G06V20/188', 'G06N20/00', 'G06N3/045', 'G06N5/04', 'G06Q50/02', 'G06V20/70']"
US20250173762A1,Content delivery responsive to electric vehicle charging session,"A system can detect that a charging session to charge a vehicle started at a charging station. In response, the system can identify a user associated with the vehicle and cause a user device of the user to present content that is targeted for the user. The targeted content is configured to promote having the user visit a nearby merchant while the vehicle is being charged at the charging station.","['G06Q30/0266', 'B60L53/305', 'B60L53/65', 'B60L53/67', 'B60L53/68', 'B60L2250/20', 'B60L2260/46']"
US20230029196A1,Method and apparatus related to sentence generation,"A method and an apparatus related to sentence generation are provided. In the method, a known token is determined based on a first sentence. A second sentence is determined based on the known token and a first masked token through a language model. The first masked token and the known token are inputted into the language model, to determine a first predicted token corresponding to the first masked token. The language model is trained based on an encoder of a bidirectional transformer. A second masked token is inserted when the determined result of the first predicted token is determined. The second masked token is inputted into the language model, to determine a second predicted token corresponding to the second masked token. The second sentence includes the first predicted token, the second predicted token and the known token. The second sentence is a sentence to respond to the first sentence.","['G06F40/284', 'G06F40/166', 'G06F40/56']"
US20250200210A1,Quick access content search in a workspace,"A system provides a quick access content search page presented on an interface of a workspace. The quick access content search page includes control items for filtering pages of the workspace based on external sharing and internet sharing. The multiple pages can include parent pages and child pages hierarchically organized within the workspace. The child page can inherit an access permission of the parent page. The access permission can correspond to the external sharing or the internet sharing. In response to a user input on the control item for filtering content based on the external sharing, the system can provide a list of content entries of pages that have been shared outside the workspace. In response to a user input on the control item for filtering content based on the internet sharing, the system can provide a list of content entries of pages that have been shared on the internet.","['G06F21/6218', 'G06F21/6209', 'G06F21/6227', 'G06F3/0482', 'G06F9/451']"
US20240202454A1,Method of domain-adapting large-capacity pre-trained language model using semantic chunk dynamic weight masking,"A domain adaptation procedure, such as fine-tuning training, is required to utilize a large-capacity PLM for a specific domain. Attempts in existing research have been made to improve performance of a PLM through domain adaptor technology based on an N-gram in order to reduce errors on the basis of the results of domain text error analysis of the PLM. Proposed is a method of selecting a semantic chunk through a domain semantic chunk graph and PageRank based on the existing domain adaptor research, with an N-gram as the semantic chunk. Proposed is also a method of domain-adapting a large-capacity PLM using semantic chunk dynamic weight masking, which reflects an output value of a PLM rather than simply integrating embedding values of semantic chunks, in a semantic chunk domain adaptor technology.","['G06F40/30', 'G06N3/096', 'G06F40/284', 'G06F40/289', 'G06N3/0455']"
US12026459B2,Conciseness reconstruction of a content presentation via natural language processing,"A method may include obtaining a document and using a first prediction model to generate text block scores for text blocks in the document, where a first text block of the text blocks is associated with a first text block score of the plurality of text block scores. The method also includes updating, in response to the first text block score for the first text block failing to satisfy a criterion, a modified version of the document with an indicator to set the first text block as a hidden text block in a presentation of the modified version. The method also includes generating a summarization of the first text block based on the words in the first text block and updating the modified version of the document to include the summarization. The method also includes providing the modified version of the document to a user device.","['G06F40/197', 'G06F16/345', 'G06F40/117', 'G06F40/169', 'G06F40/30', 'G06F40/40', 'G06N20/00', 'G06N3/00', 'G06N3/044', 'G06N3/084', 'H04L67/02']"
US11875123B1,Advice generation system,"The one or more embodiments provide for a method, system, and computer program product, an intent, generated by a large language model from a text, is received from a user device as a first input to an advice planner. A state of an account is received as a second input to the advice planner. The advice planner classifieds the intent into a domain corresponding to the intent, and generates, as output, a plan comprising a first set of action logic associated with the domain. Each action logic is a discrete step in an ordered sequence for achieving a desired state of the account. The advice planner forwards the plan to the large language model (LLM). The large language model receives the plan as input and generates advice in a natural language format as output. The advice is then forwarded to the user device.","['G06F40/30', 'G06F40/103', 'G06F40/216', 'G06F40/40', 'G06N20/00', 'G06N3/006', 'G06N3/042', 'G06N3/0442', 'G06N3/0455', 'G06N3/0475', 'G06N3/092', 'G06N5/022']"
US20250117597A1,Query of video subject matter,"Disclosed are systems and methods that convert digital video data, such as two-dimensional digital video data, into a natural language text description describing the subject matter represented in the video. For example, the disclosed implementations may process video data in real-time, near real-time, or after the video data is created and generate a text-based video narrative describing the subject matter of the video. In addition, the disclosed implementations may also support a question and answer session in which a user may submit queries about the subject matter of one or more videos and the disclosed implementations will present natural language responses based on the subject matter of the video and any corresponding context.","['G06F16/7343', 'G06F16/735', 'G06F40/40', 'G06V10/77', 'G06V10/82', 'G06V20/41', 'G06V20/46', 'G06V20/49', 'G06V20/70']"
US12288036B2,Reform input in flow execution,"Systems and processes for operating an intelligent automated assistant are provided. An example method includes, at an electronic device having one or more processors and memory, receiving an utterance including a user request, determining a natural language representation of the user request, determining a first software process associated with the natural language representation, determining whether the natural language representation can be executed by a task flow of the first software process, and in accordance with a determination that the natural language representation cannot be executed by the task flow of the first software process: determining a set of transformation instructions, determining a revised natural language representation using the set of transformation instructions, and providing the revised natural language representation to a second software process.","['G06F18/2113', 'G06F18/214', 'G06F3/165', 'G06F40/35', 'G06F9/4881', 'G06V30/274', 'G10L15/1815', 'G10L15/1822', 'G10L15/22', 'G10L2015/223']"
US20220180056A1,Method and apparatus for translation of a natural language query to a service execution language,"An approach is provided for directly translating a natural language query into machine executable commands. The approach involves parsing the natural language query into one or more phrases comprising one or more words of the natural language query. The approach further involves processing the one or more phrases using a machine learning model that extracts semantic relationship information between the one or more words and uses the semantic relationship information to translate the one or more phrases directly into one or more machine executable commands, one or more parameters of the one or more machine executable commands, or a combination thereof. The approach further involves providing one or more machine executable commands, the one or more parameters, or a combination thereof as an output.","['G06F40/211', 'G06N3/08', 'G06F40/30', 'G06F9/547', 'G06N20/00', 'G06N3/044', 'G06N3/045']"
US12412044B2,Methods for reinforcement document transformer for multimodal conversations and devices thereof,"A computer-implemented method and system for enrichment of responses in a multimodal conversation environment are disclosed. A Question Answer (QA) engine, such as a reinforcement document transformer exploits a document template structure or layout, adapts the information extraction using a domain ontology, stores the enriched contents in a hierarchical form, and learns context and query patterns based on the intent and utterances of one or more queries. The region of enriched content for preparing a response to a given query is expanded or collapsed by navigating upwards or downwards in the hierarchy. The QA engine returns the most relevant answer with the proper context for one or more questions. The responses are provided to the user in one or more modalities.","['G06F40/30', 'G06F16/3329', 'G06F16/3334', 'G06F16/3344', 'G06F16/90332', 'G06F40/137', 'G06F40/186', 'G06F40/205', 'G06F40/35', 'G06F40/40', 'G06N3/042', 'G06N3/0442', 'G06N3/045', 'G06N3/0475', 'G06N3/09', 'G06N5/022', 'G06N5/04', 'G06N5/041']"
US20230320642A1,"Systems and methods for techniques to process, analyze and model interactive verbal data for multiple individuals","Disclosed are methods, systems, and other implementations for processing, analyzing, and modelling psychotherapy data. The implementations include a method for analyzing psychotherapy data that includes obtaining transcript data representative of spoken dialog in one or more psychotherapy sessions conducted between a patient and a therapist, extracting speech segments from the transcript data related to one or more of the patient or the therapist, applying a trained machine learning topic model process to the extracted speech segments to determine weighted topic labels representative of semantic psychiatric content of the extracted speech segments, and processing the weighted topic labels to derive a psychiatric assessment for the patient.","['A61B5/165', 'A61B5/4803', 'A61B5/7267', 'G06F40/279', 'G06F40/30', 'G10L15/063', 'G10L15/26', 'G10L17/02', 'G10L17/04', 'G10L17/14', 'G10L17/18', 'G10L17/22', 'G10L21/028', 'G10L25/66', 'G16H10/20', 'G16H20/70', 'G16H50/20', 'G10L21/0272']"
US11862146B2,Multistream acoustic models with dilations,"Audio signals of speech may be processed using an acoustic model. An acoustic model may be implemented with multiple streams of processing where different streams perform processing using different dilation rates. For example, a first stream may process features of the audio signal with one or more convolutional neural network layers having a first dilation rate, and a second stream may process features of the audio signal with one or more convolutional neural network layers having a second dilation rate. Each stream may compute a stream vector, and the stream vectors may be combined to a vector of speech unit scores, where the vector of speech unit scores provides information about the acoustic content of the audio signal. The vector of speech unit scores may be used for any appropriate application of speech, such as automatic speech recognition.","['G10L15/16', 'G06N3/045', 'G10L25/24', 'G06N3/048', 'G06N3/08', 'G10L15/063', 'G10L2015/223']"
WO2025059185A1,Reliability assessment analysis and calibration for artificial intelligence classification,Disclosed herein is a system and method for the calibration of image classification from machine learning models and interactive artificial intelligence systems based thereon. An example of an interactive AI system using the same is disclosed. The system and method may provide a means for user interaction with the system and method.,"['G06V10/82', 'G06N20/00', 'G06N20/10', 'G06N3/045', 'G06N3/08', 'G06V2201/03']"
WO2023161630A1,"Computer implemented methods for the automated analysis or use of data, including use of a large language model","Methods are provided, such as a method of interacting with a large language model (LLM), including the step of a processing system using a structured, machine- readable representation of data that conforms to a machine-readable language, such as a universal language, to provide new context data for the LLM, in order to improve the output, such as continuation text output, generated by the LLM in response to a prompt; and such as a method of interacting with a LLM, including the step of providing continuation data generated by the LLM to a processing system that uses a structured, machine-readable representation of data that conforms to a machine- readable language, such as a universal language, in which the processing system is configured to analyse the continuation output (e.g. text output) generated by the LLM in response to a prompt to enable an improved version of that continuation output to be provided to a user. Related computer systems are provided.","['G06F40/44', 'G06F40/30']"
US20240266074A1,"Cognitive Communications, Collaboration, Consultation and Instruction with Multimodal Media and Augmented Generative Intelligence","The invention integrates emerging applications, tools and techniques for machine learning in medicine with videoconference networking technology in novel business methods that support rapid adaptive learning for medical minds and machines. These methods can leverage domain knowledge and clinical expertise with networked cognitive collaboration, augmented clinical intelligence and cybernetic workflow streams for learning health care systems. The invention enables multimodal clinical communications, collaboration, consultation and instruction between and among heterogeneous networked teams of persons, machines, devices, neural networks, robots and algorithms, including augmented generative AI algorithms, models and systems. The invention enables cognitively-enriched, annotation and tagging, as well as encapsulation, saving and sharing of collaborated imagery data streams as packetized clinical intelligence.","['A61B34/30', 'G06F40/169', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G16H10/60', 'G16H15/00', 'G16H20/10', 'G16H20/40', 'G16H30/20', 'G16H30/40', 'G16H40/20', 'G16H40/63', 'G16H40/67', 'G16H50/20', 'G16H50/70', 'G16H80/00', 'H04L12/1822', 'H04L65/1069', 'H04L65/4015', 'H04L65/403', 'H04L65/80', 'H04N7/152', 'A61B2090/365', 'A61B2090/373', 'A61B2090/376', 'A61B2090/378', 'A61B34/25', 'A61B34/73', 'A61B90/361', 'H04L51/10', 'H04M3/561', 'H04M3/567', 'H04M7/0027']"
CN114444479B,"End-to-end Chinese speech text error correction method, device and storage medium","The invention provides an end-to-end Chinese speech text error correction method, a device and a storage medium, wherein the method comprises the following steps: constructing a voice error correction data set based on the voice recognition data set, wherein the voice error correction data set comprises a sample to be error corrected and a correct sample; acquiring plain text data, and preprocessing the plain text data; carrying out model training by adopting the preprocessed plain text data to obtain a pre-training language model integrated with voice information; extracting pinyin and characters from sentences of a sample to be corrected, inputting the pinyin and characters into a pre-training language model integrated with voice information, and obtaining semantic sequence representation; performing model training by adopting semantic sequence representation to obtain an end-to-end speech text error correction model; and inputting the text to be corrected into an end-to-end voice text error correction model to obtain the text after error correction. The invention realizes the integration of voice information, the double coding of semantic and pinyin information, and the real-time error correction aiming at the voice text, and simultaneously improves the error correction effect aiming at the voice text.","['G06F40/232', 'G06F16/35', 'G06F40/30', 'G10L15/063']"
US11829720B2,Analysis and validation of language models,"Systems and methods for analysis and validation of language models trained using data that is unavailable or inaccessible are provided. One example method includes, at an electronic device with one or more processors and memory, obtaining a first set of data corresponding to one or more tokens predicted based on one or more previous tokens. The method determines a probability that the first set of data corresponds to a prediction generated by a first language model trained using a user privacy preserving training process. In accordance with a determination that the probability is within a predetermined range, the method determines that the one or more tokens correspond to a prediction associated with the user privacy preserving training process and outputs a predicted token sequence including the one or more tokens and the one or more previous tokens.","['G06F40/284', 'G06F40/40', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/063', 'G06N3/08']"
US11694042B2,Cross-lingual unsupervised classification with multi-view transfer learning,"Presented herein are embodiments of an unsupervised cross-lingual sentiment classification model (which may be referred to as multi-view encoder-classifier (MVEC)) that leverages an unsupervised machine translation (UMT) system and a language discriminator. Unlike previous language model (LM)-based fine-tuning approaches that adjust parameters solely based on the classification error on training data, embodiments employ an encoder-decoder framework of an UMT as a regularization component on the shared network parameters. In one or more embodiments, the cross-lingual encoder of embodiments learns a shared representation, which is effective for both reconstructing input sentences of two languages and generating more representative views from the input for classification. Experiments on five language pairs verify that an MVEC embodiment significantly outperforms other models for 8/11 sentiment classification tasks.","['G06F40/58', 'G06F40/30', 'G06F16/35', 'G06F16/353', 'G06F40/126', 'G06F40/151', 'G06F40/197', 'G06N3/02', 'G06N3/045', 'G06N3/08', 'G06N3/088', 'G06F40/216', 'G06F40/284', 'G06N3/044']"
CN113435652B,Primary equipment defect diagnosis and prediction method,"The invention discloses a method for diagnosing and predicting the defect of primary equipment, which comprises the following steps: treating the defect filling data; based on the well-managed data, a natural language processing technology is adopted to construct a defect standard library for defect description, defect appearance, defect reasons, processing measures and operation data when defects occur; based on a defect standard library, adopting a neural network and a semantic analysis technology to judge and diagnose the position, reason and severity of the new defect data; according to the defect diagnosis result, the risk level of the equipment is evaluated by combining data such as equipment defect aging factors, alarm factors, insulation performance factors, equipment importance degree, defect levels, voltage levels and the like through algorithms of correlation analysis and comprehensive evaluation; and (4) adopting a time series algorithm and Markov correlation analysis prediction. The method can improve the precision and the depth of the model, make up for the deficiency of the existing research, ensure the usability of the model and solve the problem of uncontrollable cost loss caused by the defects to equipment risks.","['G06Q10/04', 'G06F16/215', 'G06N3/045', 'G06N3/08', 'G06Q10/0635', 'G06Q10/06393', 'G06Q50/06']"
US20250117232A1,"Machine-Learned Models for User Interface Prediction, Generation, and Interaction Understanding","Generally, the present disclosure is directed to user interface understanding. More particularly, the present disclosure relates to training and utilization of machine-learned models for user interface prediction and/or generation. A machine-learned interface prediction model can be pre-trained using a variety of pre-training tasks for eventual downstream task training and utilization (e.g., interface prediction, interface generation, etc.).","['G06F9/451', 'G06F18/21355', 'G06F18/214', 'G06F18/24133', 'G06N20/00', 'G06N3/045', 'G06N3/084', 'G06N3/09']"
CN112380343B,"Problem analysis method, device, electronic equipment and storage medium","The invention relates to artificial intelligence technology, and discloses a problem analysis method, which comprises the following steps: training an original problem set and a preset sentence classification template to obtain a problem distribution model and a semantic extraction model, classifying a problem to be analyzed by using the problem distribution model to obtain a classification result, carrying out semantic analysis on the problem to be analyzed by using the semantic extraction model to obtain a semantic analysis result, searching a matched query sentence template according to the classification result and the semantic analysis result to obtain a problem query sentence according to the query sentence template, querying database contents by using the problem query sentence to obtain a question-answer analysis result, and submitting the question-answer analysis result to a query terminal. In addition, the invention also relates to a blockchain technology, and the question and answer analysis result can be stored in a node of the blockchain. The invention also provides a problem analysis device, electronic equipment and a computer readable storage medium. The invention can solve the problem of low problem analysis efficiency.","['G06F16/35', 'G06F16/3344', 'G06F16/335', 'G06F16/338', 'G06F16/367', 'G06F40/30']"
US11900086B2,Systems and methods for architecture-independent binary code analysis,"Binaries configured for execution within respective computing environments may be disassembled into architecture-specific intermediate (AIL) representations. The AIL representations may be converted into canonical intermediate language (CIL) representations. The CIL representations may comprise normalized, architecture-independent code configured to characterize functionality of respective components of a binary (e.g., respective functions or the like). Feature vectors may be extracted from the CIL representations. The feature vectors may be used to identify components of respective binaries, assign security classifications to the binaries, and/or the like.","['G06F8/53', 'G06F8/44', 'G06F21/566', 'G06F8/427', 'G06F8/43', 'G06F8/52', 'G06N3/045', 'G06N3/08', 'G06N3/044']"
US20220300711A1,System and method for natural language processing for document sequences,"A system and method for natural language processing for document sequences comprises a computing device configured to train a neural network as a function of a corpus of documents, wherein training comprises receiving the corpus of documents, identifying significant terms, and tuning, as a function of the corpus of documents, the neural network to generate a plurality of vectors for each significant term of the plurality of significant terms, a vector in a vector space representing semantic relationships between the significant terms and semantic units in the corpus of documents, receive a current document sequence including a plurality of documents in a sequential order, map a plurality of mapped terms of the plurality of significant terms to the plurality of documents as a function of the neural network and the plurality of vectors, and generate a plurality of timelines as a function of the sequential order and the mapped terms.","['G06F40/30', 'G06F16/93', 'G06F40/12', 'G06F40/205', 'G06F40/216', 'G06F40/295', 'G06N3/08', 'G06N3/0895', 'G06N3/096', 'G06N3/0455', 'G06N3/0464', 'G06N3/0499', 'G06N7/01']"
US11475225B2,"Method, system, electronic device and storage medium for clarification question generation","A method, a device and electronic device for clarification question generation are provided in one or more embodiments of this disclosure. The method includes: extracting entity information from a fuzzy context input by a user; inputting the fuzzy context into a template generating module of a pre-built CQG neural network model so as to obtain a clarification question template; inputting the entity information into an entity rendering module of the CQG neural network model so as to obtain at least one entity phrase; and generating a clarification question for a fuzzy question based on the clarification question template and the at least one entity phrase for presenting to the user.","['G06F40/40', 'G06F40/186', 'G06F40/216', 'G06F40/253', 'G06F40/295', 'G06F40/35', 'G06F40/56', 'G06N3/043', 'G06N3/0436', 'G06N3/0455', 'G06N3/047', 'G06N3/08', 'G06N3/0895', 'G06N3/048']"
CN115906831A,Distance perception-based Transformer visual language navigation algorithm,"The invention discloses a distance perception-based visual language navigation algorithm of a Transformer, and belongs to the technical field of visual language cross-modal. The algorithm is implemented by the following way: visual information, instruction information and a memory structure of a sensible area of the intelligent agent are initialized, and then exploration information in a navigation process is fused by providing a scene memory updating module based on a graph data structure and combining a language vision multi-mode pre-training model, so that the perception capability of the intelligent agent on the environment is enhanced; the motion space of each decision in the navigation process is compressed by providing a progress monitor based on distance, so that the operation resources are reduced, and the model training is accelerated; by providing the dynamic distance fusion-based module, distance information is fused into action decision, so that the algorithm gives consideration to the exploration path length while carrying out global exploration, and the efficiency of a navigation task is improved. The visual language navigation algorithm of the transform based on distance perception provided by the invention ensures better navigation success rate and obviously improves the exploration efficiency based on the scene memory algorithm.",['Y02D10/00']
US11809834B2,Machine translation using neural network models,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for machine translation using neural networks. In some implementations, a text in one language is translated into a second language using a neural network model. The model can include an encoder neural network comprising a plurality of bidirectional recurrent neural network layers. The encoding vectors are processed using a multi-headed attention module configured to generate multiple attention context vectors for each encoding vector. A decoder neural network generates a sequence of decoder output vectors using the attention context vectors. The decoder output vectors can represent distributions over various language elements of the second language, allowing a translation of the text into the second language to be determined based on the sequence of decoder output vectors.","['G06F40/44', 'G06F40/58', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/082', 'G06N3/084']"
US11875787B2,Synthetic data generation for training of natural language understanding models,This document relates to machine learning. One example includes a method or technique that can be performed on a computing device. The method or technique can include obtaining a task-semantically-conditioned generative model that has been pretrained based at least on a first training data set having unlabeled training examples and semantically conditioned based at least on a second training data set having dialog act-labeled utterances. The method or technique can also include inputting dialog acts into the semantically-conditioned generative model and obtaining synthetic utterances that are output by the semantically-conditioned generative model. The method or technique can also include outputting the synthetic utterances.,"['G10L15/063', 'G10L15/18', 'G06F40/216', 'G06F40/284', 'G06F40/30', 'G06F40/44', 'G06F40/56', 'G10L15/083', 'G10L15/22', 'G10L15/1822']"
US11816442B2,Multi-turn dialogue response generation with autoregressive transformer models,"Machine classifiers in accordance with embodiments of the invention capture long-term temporal dependencies in the dialogue data better than the existing RNN-based architectures. Additionally, machine classifiers may model the joint distribution of the context and response as opposed to the conditional distribution of the response given the context as employed in sequence-to-sequence frameworks. Machine classifiers in accordance with embodiments further append random paddings before and/or after the input data to reduce the syntactic redundancy in the input data, thereby improving the performance of the machine classifiers for a variety of dialogue-related tasks. The random padding of the input data may further provide regularization during the training of the machine classifier and/or reduce exposure bias. In a variety of embodiments, the input data may be encoded based on subword tokenization.","['G06F40/30', 'G06F18/2148', 'G06F18/217', 'G06F18/2413', 'G06F40/284', 'G06F40/35', 'G06F40/56', 'G06N20/00', 'G06N3/045', 'G06N3/048', 'G06N3/049', 'G06N3/08', 'G10L15/063', 'G10L15/16', 'G10L15/22', 'G10L2015/0631', 'G10L2015/228']"
US11868895B2,Dynamic processing element array expansion,"A computer-implemented method includes receiving a neural network model that includes a tensor operation, dividing the tensor operation into a set of sub-operations, and generating instructions for performing a plurality of sub-operations of the set of sub-operations on respective computing engines of a plurality of computing engines on a same integrated circuit device or on different integrated circuit devices. Each sub-operation of the set of sub-operations generates a portion of a final output of the tensor operation. An inference is made based on a result of a sub-operation of the plurality of sub-operations, or based on results of the plurality of sub-operations.","['G06N3/063', 'G06N3/045', 'G06N3/08', 'G06F17/153', 'G06N3/04', 'G06F7/5443']"
US12198432B2,Systems and methods for video and language pre-training,"Embodiments described a method of video-text pre-learning to effectively learn cross-modal representations from sparse video frames and text. Specifically, an align and prompt framework provides a video and language pre-training framework that encodes the frames and text independently using a transformer-based video encoder and a text encoder. A multi-modal encoder is then employed to capture cross-modal interaction between a plurality of video frames and a plurality of texts. The pre-training includes a prompting entity modeling that enables the model to capture fine-grained region-entity alignment.","['G06V20/41', 'G06F40/279', 'G06F40/284', 'G06V10/26', 'G06V10/761', 'G06V10/774', 'G06V10/776', 'G06V10/806', 'G06V20/46', 'G06V20/47', 'G06F40/30']"
US20220383078A1,Data processing method and related device,"In a data processing method, a processing device obtains a first neural network model and an available resource state of a terminal device, and determines a second neural network model based on the first neural network model and the available resource state. An appropriate model size is determined based on the available resource state, and a part of the first neural network model is selected, based on the determined model size, as the second neural network model on which data processing is to be performed.","['G06N3/06', 'G06N3/045', 'G06N3/0455', 'G06N3/0499', 'G06N3/082', 'G06N3/09', 'G06N3/096', 'G06N3/10', 'G06N5/022', 'G06N3/0495', 'G06N5/04']"
US20220129556A1,Systems and Methods for Implementing Smart Assistant Systems,"In one embodiment, a system includes an automatic speech recognition (ASR) module, a natural-language understanding (NLU) module, a dialog manager, one or more agents, an arbitrator, a delivery system, one or more processors, and a non-transitory memory coupled to the processors comprising instructions executable by the processors, the processors operable when executing the instructions to receive a user input, process the user input using the ASR module, the NLU module, the dialog manager, one or more of the agents, the arbitrator, and the delivery system, and provide a response to the user input.","['G06F21/57', 'G06F21/6245', 'G06F18/217', 'G06F21/577', 'G06F21/602', 'G06F21/74', 'G06F3/167', 'G06K9/46', 'G06K9/6262', 'G06V10/25', 'G06V10/40', 'G06V10/82', 'G06F2221/033']"
CN117992578B,"Method for processing data based on large language model, large language model and electronic equipment","The application discloses a method for processing data based on a large language model, the large language model, electronic equipment, a computer readable storage medium and a computer program product, which are applied to a user terminal, wherein the large language model is deployed on the user terminal, and weight parameters of each linear calculation layer of the large language model are quantized into format data of integer data types in advance; the method comprises the following steps: acquiring input data; vector conversion is carried out on input data through an embedding layer of the large language model, and floating point query vectors of floating point data types corresponding to the input data are obtained; converting the floating point query vector into an integer query vector of an integer data type; and calculating the weight parameters of the linear calculation layer and the integer query vector to obtain a query result corresponding to the input data. The scheme provided by the application can smoothly run the large language model at the user terminal, so that the user terminal can provide service for the user without networking, and the privacy of the user can be better ensured.","['G06F16/3344', 'G06N3/0455', 'G06N3/063', 'G06N3/08']"
CN117043859A,Lookup table cyclic language model,"A computer-implemented method (400) comprising: audio data (120) corresponding to an utterance (119) spoken by a user (10) and captured by a user device (102) is received. The method further includes processing the audio data to determine candidate transcriptions (132) of a sequence of tokens (133) comprising the spoken utterance. For each token in the sequence of tokens, the method includes determining a token insert for the corresponding token (312), determining an n-gram token insert for a preceding sequence of n-gram tokens (322), and concatenating the token insert and the n-gram token insert to generate a concatenated output for the corresponding token (335). The method further includes rescaling candidate transcriptions of the spoken utterance by processing the generated join outputs for each corresponding token in the sequence of tokens.","['G10L15/083', 'G10L15/16', 'G06F40/216', 'G06F40/237', 'G06F40/284', 'G06F40/289', 'G06F40/295', 'G06F40/30', 'G06N3/04', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/0495', 'G06N3/09', 'G10L15/187', 'G10L15/197', 'G10L15/26', 'G10L2015/088']"
CN117637189B,"Training and reasoning method, device, equipment and medium of multi-mode large language model","The invention provides a training and reasoning method, device, equipment and medium of a multi-mode large language model. The training method includes obtaining a first data set associated with a medical image of a user and a second text data set associated with a medical record of the user, wherein the first data set includes a tensor representation of the medical image of the user and first text data generated based on the medical image regarding a description and judgment of the medical image; performing a cluster analysis on the first data set and the second text data set based on the first text data included in the first data set to associate one or more second text data in the second text data set with the first data set; maintaining the first data set stored in a cached first partition of the multimodal large language model and storing at least a first portion of the one or more second text data in a cached second partition; a multi-modal large language model is trained based on data in the first partition and in the second partition to update parameters of the model.","['G16H70/60', 'G06F16/35', 'G06F40/216', 'G06F40/30', 'G06N5/022', 'G06N5/041', 'G16H50/20', 'G16H50/70']"
US12261631B2,Deep learning using large codeword model with homomorphically compressed data,"A system and method for deep learning using a large codeword model with homomorphically compressed and dyadically encrypted data is disclosed. The system preprocesses input data, applies homomorphic-dyadic compression and encryption, tokenizes the compressed data into sourceblocks, and assigns codewords using a codebook. These codewords are processed through a machine learning core, which can be either a conventional transformer-based architecture or a latent transformer core utilizing a variational autoencoder. The system enables secure operations on encrypted data, preserving privacy while allowing complex computations. The processed output is decrypted, decompressed, and translated to match the input modality. A neural upsampler may further enhance the output. The machine learning core is continuously trained using the processed data and additional training data, improving performance over time.","['G06N20/00', 'H03M7/3059', 'H03M7/6005', 'H03M7/6035', 'H03M7/3097']"
CN117743516A,"Language model training method and device, equipment and medium thereof","The application relates to a language model training method, a device, equipment and a medium thereof in the technical field of electronic commerce, wherein the method comprises the following steps: and a single training sample and a supervision tag thereof in a training set are called, a predicted text is deduced by a language model according to the training sample, a literal consistency loss value between the predicted text and the supervision tag is determined, the supervision tag is an original text, the training sample is a covering text obtained by replacing part of text in the original text, predicted semantic features of the predicted text and actual semantic features of the supervision tag are represented by the language model, a semantic correlation loss value between the two features is determined, weight parameters of the language model are updated according to the two loss values, and other training samples and supervision tags thereof in the training set are called for iterative training until the two loss values meet preset conditions, so that the language model trained to be converged is obtained. The method ensures that the language model can accurately understand the semantics of the text.",[]
US20240061998A1,Concept-conditioned and pretrained language models based on time series to free-form text description generation,"A computer-implemented method for employing a time-series-to-text generation model to generate accurate description texts is provided. The method includes passing time series data through a time series encoder and a multilayer perceptron (MLP) classifier to obtain predicted concept labels, converting the predicted concept labels, by a serializer, to a text token sequence by concatenating an aspect term and an option term of every aspect, inputting the text token sequence into a pretrained language model including a bidirectional encoder and an autoregressive decoder, and using adapter layers to fine-tune the pretrained language model to generate description texts.","['G06F40/242', 'G06F40/205', 'G06F40/216', 'G06F40/284', 'G06F40/30', 'G06F40/44', 'G06F40/56']"
CN118211175A,A Transformer-based multimodal sentiment analysis method,"The invention relates to a multimode emotion analysis method based on a transducer. Comprising the following steps: s1, acquiring a group of multi-modes including a text mode T i, a visual mode V i and an audio mode A i, and carrying out joint embedding treatment on the multi-modes; s2, outputting a single-flow transducer model as an H token, wherein in the H token, H [CLS] uses a downstream task by at Pooler, pooler uses a full-connection layer and a tanh activation function, and processing three pairs of T i,Vâ€² i and A' i data through Pooler to obtainS3, performing a pre-training task through multi-mode mask image-language modeling and image text alignment; s4, deriving H [CLS] for each group of modality pairsAndApplying the self-focusing layer and the common focusing layer, and sending to a full-connection layer for result prediction; s5, useRepresenting a joint loss function, consisting of two pre-training tasks in S3,Representing a task loss function, the objective function being formed by the average of three joint losses andAnd (5) summing to obtain the product. The method has remarkable superiority and effectiveness, and the application of the multi-mode pre-training to emotion analysis has development potential and application value.","['G06F18/253', 'G06F18/2431', 'G06F18/27', 'G06F40/30', 'G06N3/0455', 'G06N3/084', 'G06V10/82', 'G06V40/174', 'G10L25/30', 'G10L25/63', 'G06N3/048']"
WO2022187034A1,Phishing url detection using transformers,"The technology described herein can identify phishing URLs using transformers. The technology tokenizes useful features from the subject URL. The useful features can include the text of the URL and other data associated with the URL, such as certificate data for the subject URL, a referrer URL, an IP address, etc. The technology may build a joint Byte Pair Encoding for the features. The token encoding may be processed through a transformer, resulting in a transformer output. The transformer output, which may be described as a token embedding, may be input to a classifier to determine whether the URL is a phishing URL. Additional or improved URL training data may be generated by permuting token order, by simulating a homoglyph attack, and by simulating a compound word attack.","['G06F21/56', 'H04L63/1483', 'G06F2221/2119']"
US20240303496A1,Exploiting domain-specific language characteristics for language model pretraining,"A method, apparatus, non-transitory computer readable medium, and system of training a domain-specific language model are described. One or more aspects of the method, apparatus, non-transitory computer readable medium, and system include obtaining domain-specific training data including a plurality of domain-specific documents having a document structure corresponding to a domain, and obtaining domain-agnostic training data including a plurality of documents outside of the domain. The domain-specific training data and the domain-agnostic training data are used to train a language model to perform a domain-specific task based on the domain-specific training data and to perform a domain agnostic task based on the domain-agnostic training data.","['G06F40/279', 'G06F40/30', 'G06N3/0895']"
CN110334361B,Neural machine translation method for Chinese language,"The invention relates to the technical field of neural machine translation, and discloses a neural machine translation method for a small language, which solves the problem of neural machine translation under the condition of deficient parallel linguistic data. The method comprises the following steps of constructing a neural machine translation model and training: 1. acquiring and preprocessing monolingual corpus; 2. respectively training language models of a source language and a target language by utilizing monolingual corpora; 3. respectively training a mapper for mapping the coding result of one language to the space of the other language by using the coding results of bilingual parallel corpora in the parallel corpus of the small language in the language models of the source language and the target language; 4. training a discriminator model by utilizing monolingual corpus; 5. and training a translation model by utilizing a language model, a mapper, a discriminator model, bilingual parallel corpora and monolingual corpora. The invention is suitable for translation between languages of small languages with only small-scale parallel corpora.","['G06F40/58', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
CN112288075B,Data processing method and related equipment,"The application relates to the field of artificial intelligence and discloses a data processing method, which comprises the following steps: acquiring data to be processed and a target neural network model, wherein the target neural network model comprises a first transducer layer, the first transducer layer comprises a first residual branch and a second residual branch, the first residual branch comprises a first attention head, and the second residual branch comprises a target feedforward layer FFN; and performing target task related processing on the data to be processed according to a target neural network model to obtain a data processing result, wherein the target neural network model is used for performing target operation on the output of the first attention head and the first weight value to obtain the output of the first residual branch, and/or the target neural network model is used for performing target operation on the output of the target FFN and the second weight value to obtain the output of the second residual branch. Aiming at different tasks, the embodiment sets the weight value for controlling the output of the residual branch, and reduces the calculation resource requirement of the terminal equipment for running the target neural network model.","['G06N3/0499', 'G06F7/544', 'G06N3/045', 'G06F40/40', 'G06F7/501', 'G06F7/523', 'G06N3/08', 'G06N3/0464', 'G06N3/048']"
US12271792B2,Systems and methods for vision-and-language representation learning,"Embodiments described herein provide visual-and-language (V+L) systems and methods for learning vision and language representations. Specifically, a method may comprise receiving a training dataset comprising a plurality of image samples and a plurality of text samples; encoding the plurality of image samples into a plurality of encoded image samples and the plurality of text samples into a plurality of encoded text samples; computing a first loss objective based on the plurality of encoded image samples and the plurality of encoded text samples; encoding a first subset of the plurality of encoded image samples and a second subset of the plurality of encoded text samples into a plurality of encoded image-text samples; computing a second loss objective based on the plurality of encoded image-text samples; and updating the V+L model based at least in part on the first loss objective and the second loss objective.","['G06N3/08', 'G06N20/00', 'G06F18/214', 'G06F40/20', 'G06N3/02', 'G06N3/045', 'G06V30/153']"
US11915701B2,Automatic summarization of financial earnings call transcripts,"Computer-readable media, systems and methods may improve automatic summarization of transcripts of financial earnings calls. For example, a system may generate segments, such as by disambiguating sentences, from a transcript to be summarized. The system may use an estimator that assesses whether or not the segment should be included in the summary. Different types of estimators may be used. For example, the estimator may be rule-based, trained based on machine-learning techniques, or trained on based on machine-learning with language modeling using natural language processing to fine-tune language models specific to financial earnings calls.","['G06F16/35', 'G10L15/26', 'G06F40/20', 'G06F40/30', 'G06N20/00', 'G06N3/047', 'G06N3/048', 'G06N5/01', 'G10L15/16', 'G06N3/045', 'G06N7/01']"
US11593556B2,Methods and systems for generating domain-specific text summarizations,"Embodiments provide methods and systems for generating domain-specific text summary. Method performed by processor includes receiving request to generate text summary of textual content from user device of user and applying pre-trained language generation model over textual content for encoding textual content into word embedding vectors. Method includes predicting current word of the text summary, by iteratively performing: generating first probability distribution of first set of words using first decoder based on word embedding vectors, generating second probability distribution of second set of words using second decoder based on word embedding vectors, and ensembling first and second probability distributions using configurable weight parameter for determining current word. First probability distribution indicates selection probability of each word being selected as current word. Method includes providing custom reward score as feedback to second decoder based on custom reward model and modifying second probability distribution of words for text summary based on feedback.","['G06F40/30', 'G06F40/56', 'G10L15/22', 'G10L15/26', 'G10L17/20', 'G10L25/48', 'H04L12/282', 'G06F40/216', 'G10L2015/228']"
US10664381B2,Method and system for synthetic generation of time series data,"Systems and methods for generating synthetic data are disclosed. For example, a system may include one or more memory units storing instructions and one or more processors configured to execute the instructions to perform operations. The operations may include receiving a dataset that includes time series data having a plurality of dimensions and generating a transformed dataset by performing a first data transformation. The first data transformation may include a time-based data processing method. The operations may include generating a synthetic transformed-dataset by implementing a data model using the transformed dataset. The data model may be configured to generate synthetic transformed-data based on a relationship between data of at least two dimensions of the transformed dataset. The operations may include generating a synthetic dataset by performing a second data transformation on the synthetic transformed-dataset. The second data transformation may include an inverse of the first data transformation.","['G06F11/3608', 'G06F9/541', 'G06F11/3628', 'G06F11/3636', 'G06F11/3684', 'G06F11/3688', 'G06F16/215', 'G06F16/2237', 'G06F16/2264', 'G06F16/2423', 'G06F16/24568', 'G06F16/248', 'G06F16/254', 'G06F16/258', 'G06F16/283', 'G06F16/285', 'G06F16/288', 'G06F16/335', 'G06F16/35', 'G06F16/90332', 'G06F16/90335', 'G06F16/9038', 'G06F16/906', 'G06F16/93', 'G06F17/15', 'G06F17/16', 'G06F17/18', 'G06F18/2115', 'G06F18/213', 'G06F18/214', 'G06F18/2148', 'G06F18/217', 'G06F18/2193', 'G06F18/22', 'G06F18/23', 'G06F18/24', 'G06F18/2411', 'G06F18/2415', 'G06F18/285', 'G06F18/40', 'G06F21/552', 'G06F21/60', 'G06F21/6245', 'G06F21/6254', 'G06F30/20', 'G06F40/117', 'G06F40/166', 'G06F40/20', 'G06F8/71', 'G06F9/54', 'G06F9/547', 'G06K9/036', 'G06K9/6215', 'G06K9/6218', 'G06K9/6231', 'G06K9/6232', 'G06K9/6253', 'G06K9/6256', 'G06K9/6257', 'G06K9/6262', 'G06K9/6265', 'G06K9/6267', 'G06K9/6269', 'G06K9/6277', 'G06K9/66', 'G06K9/6885', 'G06K9/72', 'G06N20/00', 'G06N20/10', 'G06N20/20', 'G06N3/04', 'G06N3/044', 'G06N3/0445', 'G06N3/045', 'G06N3/0454', 'G06N3/047', 'G06N3/06', 'G06N3/08', 'G06N3/084', 'G06N3/088', 'G06N3/094', 'G06N5/00', 'G06N5/01', 'G06N5/02', 'G06N5/022', 'G06N5/04', 'G06N7/00', 'G06N7/005', 'G06N7/01', 'G06Q10/04', 'G06T11/001', 'G06T7/194', 'G06T7/246', 'G06T7/248', 'G06T7/254', 'G06V10/768', 'G06V10/993', 'G06V30/194', 'G06V30/1985', 'H04L63/1416', 'H04L63/1491', 'H04L67/306', 'H04L67/34', 'H04N21/23412', 'H04N21/8153', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084']"
US11645479B1,Method for AI language self-improvement agent using language modeling and tree search techniques,"A novel method provides an AI language virtual agent having self-improvement features and which uses language modeling and tree search techniques. The AI language virtual agent exchanges textual discussion with users and other simulated agents. The method includes receiving a current situational description depicting natural language user input, temperament qualities and textual tendencies of the virtual agent, and indicia regarding subject matter context of a present conversation. The indicia regarding subject matter context include textual logs from recent conversational exchanges. The current situational description includes audio, visual, and tactile inputs collected proximate to the virtual agent. The method preferably utilizes an MCTS tree search in combination with self-moving modules, one or more language models, tree search techniques outputting textual responses to the current situation description, and the virtual agent responding with textual expression to verbal input in combination with the audio, visual, tactile, and other sensory inputs.","['G06F40/35', 'G06F40/58']"
CN115310425B,Policy text analysis method based on policy text classification and key information identification,"The invention discloses a policy text analysis method based on policy text classification and key information identification, and belongs to the technical field of natural language processing. The policy text classifier provided by the invention is used for classifying the text through the original paragraph
The method has the advantages that the prompt language of the classification task is added, the prompt language comprises the mask positions needing to be predicted and filled with labels, the paragraph classification problem is converted into the type completion shape filling classification prediction problem, the paragraph classification prediction process is simplified, the policy document text can be accurately analyzed from the aspects of content composition and document structure based on the constructed complete policy document element system, and deeper information is mined. The policy information recognizer provided also simplifies the recognition difficulty of the text entity by predicting the vacant content labels under the constructed policy text element system, and has better performance when the training data is small in scale.","['G06F40/205', 'G06F40/211', 'G06F40/279']"
US11990132B2,Automated meeting minutes generator,"A transcription of audio speech included in electronic content associated with a meeting is created by an ASR model trained on speech-to-text data. The transcription is post-processed by modifying text included in the transcription, for example, by modifying punctuation, grammar, or formatting introduced by the ASR model and by changing or omitting one or more words that were included in both the audio speech and the transcription. After the transcription is post-processed, output based on the post-processed transcription is generated in the form of a meeting summary and/or template.","['G10L15/26', 'G06F16/345', 'G06F16/383', 'G06F40/117', 'G06F40/134', 'G06F40/174', 'G06F40/186', 'G06F40/253', 'G06F40/56', 'G06N3/08', 'G06Q10/063118', 'G06Q10/103', 'G06Q10/109', 'G10L13/08', 'G10L15/22', 'H04L12/1831']"
EP3992859A1,Machine learning system for digital assistants,"A machine learning system for a digital assistant is described, together with a method of training such a system. The machine learning system is based on an encoder-decoder sequence-to-sequence neural network architecture that is trained to map input sequence data to output sequence data, where the input sequence data relates to an initial query and the output sequence data represents canonical data representation for the query. The method of training involves generating a training dataset for the machine learning system based on original query data samples. The method involves clustering vector representations of the query data samples to generate canonical-query original-query pairs for use in training the machine learning system.","['G06N3/08', 'G06F16/3329', 'G06F16/2425', 'G06F16/3337', 'G06F16/3343', 'G06F16/3344', 'G06F16/35', 'G06F40/295', 'G06F40/58', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/088', 'G06N7/01']"
CN118430816A,Digital structured interview processing method based on multimodal fusion and large language model,"The invention provides a digital structuring interview processing method based on multi-mode fusion and a large language model, and belongs to the technical field of data processing. Acquiring multi-modal data of interview objects under a preset questioning paradigm; respectively extracting features of each mode data in the multi-mode data, converting the extracted features into a low-rank feature matrix, and further obtaining a fusion feature matrix; text-converting answer audio of interviewed objects and preprocessing the converted text; the obtained fusion feature matrix and the preprocessed text description are spatially embedded, and a prompt word is generated; generating a new questioning pattern according to the prompt words and the large language model finely tuned by interview corpus, questioning the interviewed objects according to the new questioning pattern, generating new prompt words, and repeating the steps until the interviews are finished; the present invention improves interview authenticity.","['G16H50/30', 'A61B5/0205', 'A61B5/165', 'A61B5/4803', 'G06F16/367', 'G06F40/16', 'G06F40/30', 'G10L15/26']"
US12284147B2,Techniques for optimizing the display of videos,"The disclosed embodiments disclose techniques for optimizing the display of videos. During operation, a computing device receives a video stream to be displayed. The computing device determines a preferred orientation for the video stream, determines a present orientation for the computing device, and determines a mismatch between the preferred orientation and the present orientation. The computing device adjusts the video stream while displaying the video stream on the display. As the video stream plays, the computing device detects any rotation of the computing device, and if so, re-adjusts how the video stream is displayed.","['H04L51/10', 'G06F1/1626', 'G06F1/1694', 'G06F3/0346', 'G06F3/04845', 'G06F3/04883', 'G06V10/25', 'G06V20/41', 'H04L12/1822', 'H04L65/60', 'H04L65/61', 'G06F2203/04806', 'G06V10/462', 'G06V40/16']"
US11880655B2,Fact correction of natural language sentences using data tables,"Embodiments are disclosed for performing fact correction of natural language sentences using data tables. In particular, in one or more embodiments, the disclosed systems and methods comprise receiving an input sentence, tokenizing elements of the input sentence, and identifying, by a first machine learning model, a data table associated with the input sentence. The systems and methods further comprise a second machine learning model identifying a tokenized element of the input sentence that renders the input sentence false based on the data table and masking the tokenized element of the tokenized input sentence that renders the input sentence false. The systems and method further includes a third machine learning model predicting a new value for the masked tokenized element based on the input sentence with the masked tokenized element and the identified data table and providing an output including a modified input sentence with the new value.","['G06F40/284', 'G06F16/24535', 'G06F40/226', 'G06F40/30', 'G06N20/20', 'G06N3/042', 'G06N3/045', 'G06N3/0455', 'G06N3/084', 'G06N3/09']"
WO2024246535A1,A dialogue system and a dialogue method,"A dialogue system, comprising: an input configured to receive input data relating to speech or text provided by a user; an output configured to provide output data relating to speech or text to a user; and one or more processors, configured to: receive, by way of the input, input data relating to speech or text provided by a user; provide the input data to a subject safety module, the subject safety module configured to receive the input data and evaluate the input data before a system response is output, evaluating the input data comprising performing a first determination on the input data using a matching module and a second determination on the input data using a trained model, wherein the matching module performs the first determination to determine whether the input data matches one or more items from a pre-determined set of one or more items; generate a system response using a first process or using a second process, wherein the first process uses at least one trained language model to generate a dynamically determined system response and wherein the second process retrieves a pre-determined system response, wherein a selection between the first process and the second process is made based on the evaluation of the input data; output, by way of the output, the system response.",['G06F40/30']
US20240386477A1,Methods and systems for providing context for generating an online store,"A computer method including fetching, from a first address, first domain content; analysing the first domain content to determine character information and content information; selecting a template from amongst a plurality of templates associated with a Software-as-a-Service (SaaS) application based on at least one of the character information and the content information; and configuring the SaaS application based on the character information and content information using the selected template.",['G06Q30/0641']
US11978271B1,Instance level scene recognition with a vision language model,Systems and methods for image understanding can include one or more object recognition systems and one or more vision language models to generate an augmented language output that can be both scene-aware and object-aware. The systems and methods can process an input image with an object recognition model to generate an object recognition output descriptive of identification details for an object depicted in the input image. The systems and methods can include processing the input image with a vision language model to generate a language output descriptive of a predicted scene description. The object recognition output can then be utilized to augment the language output to generate an augmented language output that includes the scene understanding of the language output with the specificity of the object recognition output.,"['G06V20/20', 'G06V20/70', 'G06V20/30', 'G06N20/00', 'G06N3/0455', 'G06N3/0475', 'G06V10/764', 'G06V10/806', 'G06V10/82', 'G06V20/41']"
CN112269901B,Fault distinguishing and reasoning method based on knowledge graph,"The invention provides a fault distinguishing and reasoning method based on a knowledge graph, which comprises the steps of obtaining equipment data, fault data and disposal scheme data; establishing an equipment map based on the equipment data, establishing a fault map based on the fault data, and establishing a disposal scheme map based on the disposal scheme data; performing map fusion, map completion and map inference on the equipment map, the fault map and the disposal scheme map based on an event extraction algorithm and a TranSE algorithm to obtain a knowledge map; carrying out fault discrimination reasoning by using a knowledge graph of a graph neural network; the method comprises the steps of establishing maps by utilizing a large amount of equipment data, historical fault data and disposal scheme data, fusing and complementing the maps, carrying out fault distinguishing and reasoning by utilizing a final knowledge map, and automatically providing methods for distinguishing, diagnosing and overhauling faults of the transformer.","['G06F16/83', 'G06F40/295', 'G06N3/044', 'G06N3/045', 'G06N5/04', 'G06Q10/20', 'G06Q50/06']"
US12399920B2,Method and system for multi-level artificial intelligence supercomputer design featuring sequencing of large language models,"A method for assigning tasks to LLMs using h-including receiving documents relevant to embeddings, generating context-aware prompts responsive to at least one of a received prompt, derived prompts, and the received knowledge documents, each context-aware prompt corresponding to a specialized task of the one or more specialized tasks, transmitting each context-aware prompt to a respective h- that is configured to specialize in processing prompts having a specialty corresponding to the specialized task of the context-aware prompt, and receiving results from the h-LLMs.","['G06F16/3329', 'G06F40/284', 'G06F40/30']"
US11045271B1,Robotic medical system,"A system includes a camera; an AI visual processor to classify and recognize human anatomical features, and a processor to control robot movement to reach a selected anatomical target.","['G16H40/67', 'A61B34/70', 'A61B18/1492', 'A61B34/30', 'A61B34/32', 'A61B34/37', 'A61B90/361', 'G06N20/00', 'G06N3/045', 'G16H20/40', 'G16H40/63', 'G16H50/20', 'G16H50/80', 'A61B2018/00351', 'A61B2018/00482', 'A61B2018/00577', 'A61B2018/126', 'A61B2018/143', 'A61B2034/2065', 'A61B2034/301', 'A61B2034/302', 'A61B2034/303', 'A61B2090/306', 'A61B2090/3612', 'A61B2090/3614', 'A61B2090/365', 'A61B2090/3735', 'A61B2090/378', 'G06N3/008', 'G06N3/047', 'G06N3/08', 'G06N5/022']"
US20240242487A1,Transfer learning in image recognition systems,Visual Prompt Tuning provides fine-tuning for transformer-based vision models. Prompt Vectors are added as additional inputs to Vision Transformer models. alongside image patches which have been linearly projected and combined with a positional embedding. The transformer architecture allows prompts to be optimized using gradient descent. without modifying or removing any of the Vision Transformer parameters. A Image Recognition System with Visual Prompt Tuning improves a pre-trained vision model by adapting the pre-trained vision model to downstream tasks by tuning the pretrained vision model using a visual prompt.,"['G06V10/7747', 'G06V10/82', 'G06N3/0455', 'G06N3/0464', 'G06N3/096', 'G06V10/469', 'G06V10/764', 'G06V10/774']"
US12406138B2,System for providing intelligent part of speech processing of complex natural language,"A system for providing intelligent part of speech processing of complex natural language is disclosed. The system identifies a multiword concept from an input and replaces the multiword concept with a token to be tagged as a desired part of speech. The system passes the modified text including the token to a part-of-speech tagger to tag each word in the text with the appropriate part-of-speech. The system may replace the token with the original text that the token was utilized to replace so that the original intent of the text is evident. The system may analyze the tagged text to generate analyses and interpretations associated with the input. When multiple multiword concepts are identified, the system may evaluate them by computing scores for each of the multiword concepts that may be replaced with tokens, for each of the modified texts including the tokens, or for any interpretations and analyses thereof.","['G06F40/253', 'G06F40/242', 'G06F40/268', 'G06F40/284']"
US12197896B2,Code generation with reinforcement learning,"A code generation system uses a non-terminal expansion model and a non-terminal selector model to generate a code sketch to complete a partially-formed source code snippet. The non-terminal expansion model is a neural transformer model trained on a supervised dataset through reinforcement learning to learn to predict the production rule to expand for a given non-terminal symbol. The non-terminal selector model is trained through reinforcement learning to predict the non-terminal symbol to expand given a partial-code state. The models are used in a two-step beam search to generate the top candidate code sketches, where a candidate code sketch may contain a hole that represents an unexpanded non-terminal symbol.","['G06F8/33', 'G06F8/36', 'G06N3/045', 'G06N3/0455', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06N3/092', 'G06N3/0985']"
US20240296352A1,Artificial intelligence enhanced knowledge framework,"A computer-implemented system, a method, and computer products for development and use of a knowledge framework are provided. The system comprises one or more processors and a memory including computer program code. The computer program code is configured to, when executed, cause the one or more processors to perform various tasks. These tasks include receive session data related to responses received from a participant in a session, receive machine learning data, create or enhance the knowledge framework based on the machine learning data and the session data, and create additional machine learning data using the knowledge framework as a source of information. The method performs these tasks, and the computer readable medium contains similar computer program code. The method can perform these tasks with computer synergistic generative artificial intelligence, machine learning, and knowledge framework subsystems.","['G06N5/02', 'G06N5/022', 'G06N5/046']"
CN113392209B,"Text clustering method based on artificial intelligence, related equipment and storage medium","The embodiment of the application provides a text clustering method based on artificial intelligence, related equipment and a storage medium, wherein the method comprises the following steps: acquiring a plurality of texts to be clustered; inputting each text in a plurality of texts to be clustered into a vector extraction model to obtain an output vector corresponding to each text, wherein the vector extraction model is obtained by pre-training and fine-tuning a bi-directional coding representation BERT model based on a transformer by utilizing a training sample and keywords in the training sample; determining a representation vector of each text according to a word frequency-inverse document frequency TF-IDF algorithm and the output vector corresponding to each text; and clustering a plurality of representation vectors corresponding to the texts by using a clustering algorithm to obtain at least one class cluster, so that the vector representation of the text fully learns key information and context information of the text, and clustering is performed based on the vector representation, thereby being beneficial to improving the accuracy and efficiency of text clustering.","['G06F16/35', 'G06F16/951', 'G06F40/289']"
US20220036153A1,Ultra large language models as ai agent controllers for improved ai agent performance in an environment,"Methods and artificial intelligence agents are provided to train or guide an artificial intelligence agent. Visual data and/or text data are received from the artificial intelligence agent and/or an environment of the artificial intelligence agent. A text prompt is generated based on the visual information and/or the text data. The text prompt is provided to an ultra-large language model. Text output of the ultra-large language model is received in response to the text prompt. The artificial intelligence agent is supplied with the text output of the ultra-large language model and/or the text output converted into an alternative format. The artificial intelligence agent is configured to select an action, a series of actions, and/or the policy based on the state of an environment of the artificial intelligence agent and on the text output of the ultra-large language model and/or the text output converted into the alternative format.","['G06N3/0427', 'G06F40/40', 'G06N3/042', 'G06N3/006', 'G06N3/045', 'G06N3/08', 'G06N5/02']"
US20250200031A1,Methods and apparatus for natural language interface for constructing complex database queries,"In some embodiments, a processor receives, via an interface, natural language data associated with a user request for performing an identified computational task associated with a cybersecurity management system. The processor is configured to provide the natural language data as input to a machine learning (ML) model. The ML model is configured to automatically infer a template query based on the natural language data. The processor is further configured to cause the template query to be displayed, via the interface. The processor is further configured to receive, via the interface, user input indicating a finalized query associated with the identified computational task, and to provide the finalized query as input to a system configured to perform the identified computational task. The processor is further configured to modify a security setting in the cybersecurity management system based on the performance of the identified computational task.","['G06F16/3322', 'G06F16/2428', 'G06F16/243', 'G06F16/90324', 'G06N20/00', 'H04L63/20']"
US12299405B2,Intent-based query and response routing between users and backend services,"For a seamless and robust artificial intelligence-based assistant experience, an intent-based query and response router has been designed to operate as an intelligent layer between a user and multiple backend services that may respond to one or more queries over the course of a conversation with the user. The query router interacts with an intent classification service to obtain an intent classification for a prompt that is based on a user query. The query router uses the intent classification, which is used as an identifier of a backend service, to route the user query to an appropriate one (or more) of the backend services. When a response is detected, the query router determines a corresponding conversation and provides the response for the conversation.","['G06F40/30', 'G06F40/40', 'G06F16/3325', 'G06F16/3329', 'G06N20/00']"
CN110442684B,A text content-based recommendation method for similar cases,"The invention relates to a class case recommendation method based on text content, which comprises a pre-training part and a fine-tuning part, wherein the pre-training part adopts a transformer encoder as a main structure, trains a Chinese language model, learns Chinese language knowledge from other corpora and obtains a high-quality language model. The fine-tuning part three-tuple model is taken as a frame, preprocessed judicial documents are taken as training data, more knowledge about judgment is learned from the judicial field, and a better text vector representation is obtained. Compared with the traditional case recommendation method based on keywords and the case recommendation method based on the single-task neural network, the case recommendation method based on the content has better effect, and has better robustness based on the semantic training model, which shows that the method provided by the invention is effective and practical.","['G06F16/3344', 'G06F16/335', 'G06F16/35', 'G06F18/22', 'G06N3/045', 'G06N3/084']"
CN115309879A,A Multi-task Semantic Parsing Model Based on BART,"The invention provides a multi-task semantic parsing model based on BART, which belongs to the technical field of natural language processing and comprises a word embedding layer, a BART coding layer, a domain classifier, a BART decoding layer, a probability Decoder, a SPARQL Decoder and a grammar checker. The invention directly converts natural language into knowledge map query language SPARQL, simplifies the question answering steps to play a role in reducing error accumulation, performs field identification on the problems, and queries a corresponding professional field knowledge base according to the field, thereby improving the question answering accuracy.","['G06F16/3329', 'G06F16/367', 'G06N3/04']"
US11545156B2,Automated meeting minutes generation service,"Attributes of electronic content from a meeting are identified and evaluated to determine whether sub-portions of the electronic content should or should not be attributed to a user profile. Upon determining that the sub-portion should be attributed to a user profile, attributes of the sub-portion of electronic content are compared to attributes of stored user profiles. A probability that the sub-portion corresponds to at least one stored user profile is calculated. Based on the calculated probability, the sub-portion is attributed to a stored user profile or a guest user profile.","['G06Q10/10', 'G10L17/06', 'G06F16/345', 'G06F16/535', 'G06F16/635', 'G06F16/9535', 'G06N20/00', 'G06V40/172', 'G06V40/50', 'G10L17/00', 'G10L17/04', 'H04L67/306']"
US20250139140A1,"Method and system for context-aware telecommunications, cellular, and radio based generative pre-trained transformer","This disclosure relates to methods, systems, and devices for AI/ML assisted management of a radio access network (RAN). In particular, an LLM is employed to generate answer to user queries, for example in the form of code that can be executed upon streaming data from the RAN, including performance management and control management databases. To provide prompts the LLM can understand, an LLM agent employs a knowledge base and experience base to transform the language of the queries from the highly technical telecommunications domain to a general domain. Frequently asked questions and their answers may be stored in the experience base and used instead of using the LLM.","['G06F16/3344', 'G06F16/243', 'G06F16/24522', 'G06F16/3329']"
WO2023024412A1,"Visual question answering method and apparatus based on deep learning model, and medium and device","A visual question answering method and apparatus based on a deep learning model, and a medium and a device. The method comprises: establishing a visual question answering model by using a pre-trained language model T5 framework, wherein the visual question answering model comprises an encoder sub-model and a decoder sub-model (S101); acquiring image data and question data, inputting same into the visual question answering model, and performing matching in a preset classification category by using the encoder sub-model in the visual question answering model, so as to obtain a classified first answer corresponding to the question data (S102); obtaining a generated second answer by using the decoder sub-model in the visual question answering model in combination with a common word list (S103); and calculating predicted probabilities corresponding to the first answer and the second answer, so as to select the first answer and/or the second answer as a target answer, and outputting the target answer (S104). By means of the method, a final answer of visual question answering can be matched from a common category, and can also be automatically generated; and an output answer is selected according to the magnitude of a predicted probability, thereby improving the accuracy of a result.","['G06F16/3329', 'G06F16/3344', 'G06F16/3346', 'G06F16/35', 'G06F40/279', 'G06N3/04', 'G06N3/08']"
US12314865B2,Transfer learning system for automated software engineering tasks,A transfer learning system is used for the development of neural transformer models pertaining to software engineering tasks. The transfer learning system trains source code domain neural transformer models with attention in various configurations on a large corpus of unsupervised training dataset of source code programs and/or source code-related natural language text. A web service provides the trained models for use in developing a model that may be fine-tuned on a supervised training dataset associated with a software engineering task thereby generating a tool to perform the software engineering task.,"['G06F8/20', 'G06F40/30', 'G06F40/40', 'G06F8/30', 'G06F8/35', 'G06F8/51', 'G06N3/045', 'G06N3/084', 'G06N3/088', 'G06F8/427', 'G06F8/71', 'G06F8/73', 'G06N20/00', 'G06N3/04', 'G06N3/063', 'G06N3/08']"
US12242971B2,Adversarial training of machine learning models,This document relates to training of machine learning models such as neural networks. One example method involves providing a machine learning model having one or more layers and associated parameters and performing a pretraining stage on the parameters of the machine learning model to obtain pretrained parameters. The example method also involves performing a tuning stage on the machine learning model by using labeled training samples to tune the pretrained parameters. The tuning stage can include performing noise adjustment of the labeled training examples to obtain noise-adjusted training samples. The tuning stage can also include adjusting the pretrained parameters based at least on the labeled training examples and the noise-adjusted training examples to obtain adapted parameters. The example method can also include outputting a tuned machine learning model having the adapted parameters.,"['G06N3/08', 'G06N3/088', 'G06N3/045']"
US20240248897A1,Document Processing and Response Generation System,"Disclosed are methods, systems, devices, apparatus, media, design structures, and other implementations, including a method that includes receiving, at a local device from a remote device, query data representative of a question relating to source content of a source document of a repository of a plurality of source documents, with the source content being associated with transformed content accessible from the local device. The method further includes generating, in response to determination, at the local device, of a match between the query data and at least one portion of the transformed content, output data comprising one or more of a pointer to access, in the source document accessible from the remote device, at least one portion of the source document corresponding to the at least one portion of the transformed content, or a copy of the at least one portion of the source document.","['G06F16/332', 'G06F16/24522', 'G06F16/33', 'G06F16/3328', 'G06F16/3329', 'G06F16/3349', 'G06F16/335', 'G06F16/338', 'G06F16/9032', 'G06F16/93', 'G06F16/9574', 'G06F40/131', 'G06F40/137', 'G06F40/151', 'G06F40/20', 'G06F40/247', 'G06F40/284', 'G06F40/30', 'G06N3/0464', 'G06N5/022', 'G06N5/04', 'G06N3/006', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
US11868730B2,Method and system for aspect-level sentiment classification by graph diffusion transformer,"System and method for aspect-level sentiment classification. The system includes a computing device, the computing device has a processer and a storage device storing computer executable code. The computer executable code is configured to: receive a sentence having a labeled aspect term and context; convert the sentence into a dependency tree graph; calculate an attention matrix of the dependency tree graph based on one-hop attention between any two nodes of the graph; calculate multi-head attention diffusion for any two nodes from the attention matrix; obtain updated embedding of the graph using the multi-head diffusion attention; classify the aspect term based on the updated embedding of the graph to obtain predicted classification of the aspect term; calculate loss function based on the predicted classification and the ground truth label of the aspect term; and adjust parameters of models in the computer executable code based on the loss function.","['G06F40/30', 'G06F16/353', 'G06F40/205', 'G06F40/279']"
CN113157965B,"Audio visual model training and audio visual method, device and equipment","The invention provides a method, a device and equipment for providing audio visual model training and audio visual, comprising the following steps: acquiring a training sample including a relation label indicating whether user information, a user history playing video, a target audio, a target video and a target audio are related or not; inputting the training samples into an audio visual model, and performing feature extraction on the target audio to obtain a first feature representation of the target audio; carrying out feature extraction on user information and a user history playing video to obtain user features and user interest expression features, carrying out feature extraction on a target video to obtain second feature representation, and carrying out combined processing on the user features, the user interest expression features and the second feature representation to obtain third feature representation; determining a similarity between the first feature representation and the third feature representation; and updating the parameters of the audio visual model according to the similarity and the relation labels in the training samples. The invention can carry out personalized video collocation on the same audio frequency, and meets diversified user requirements.","['G06F16/64', 'G06F16/61']"
US11704307B2,Intelligent query editor using neural network based machine learning,"Techniques are described herein for generating, editing, and optimizing queries using neural networks. In some embodiments, the techniques include training a neural network using a set of performant database queries to automatically learn patterns between different sequences of tokens in performant queries. Once trained, the neural network may receive an incomplete query as input, where the incomplete query includes one or more query tokens. The trained neural network may then perform next token prediction to project a set of one or more additional query tokens that may follow the one or more query tokens in the incomplete query to form a completed, performant query.","['G06F16/242', 'G06F16/2423', 'G06F16/90324', 'G06N3/08']"
CN113822078A,XLM-R model fused machine translation model training method,"The present disclosure provides a training method of a machine translation model fused with an XLM-R model, comprising: acquiring a training set, wherein the training set comprises a plurality of groups of bilingual sentence pairs; carrying out standardization processing on the training set; at a coding end of the XLM-R model, abstract representation is carried out on a text sentence at a source end in a training set, the text sentence is input into the XLM-R model, and the code output at the topmost layer of the XLM-R model is taken out; at a decoding end of the XLM-R model, abstract representation is carried out on a target end text sentence in a training set, the target end text sentence is input into the XLM-R model, and decoding output of the topmost layer of the XLM-R model is taken out; inputting the coding output and the decoding output to a coder-decoder fusion layer of a neural translation model, and decoding the output of the fusion layer to obtain a target end translation sentence of a source end text sentence; a trained machine translation model is obtained. The disclosure also provides a training device, a translation device, an electronic device and a readable storage medium.","['G06F40/42', 'G06F40/58', 'Y02D10/00']"
US11409752B1,Dimensional reduction of complex vectors in artificially intelligent solutions to compare similarity of natural language text,"A web-based tool performs records matching in response to a freeform text input, to find highly contextually-related sentences in a corpus of records. Each sentence in the corpus is converted into a full-size vector representation, and each vector's angle within space is measured. Each full-size vector is compressed to a smaller vector and a loss function is used to preserve for each vector the angle within the lower-dimensional space that existed for the higher-dimensional vector. Full-size and reduced vector representations are generated from the freeform text input. The reduced-size vector of the input is compared to those of the corpus of text to identify, in real-time, a set of vector nearest neighbors that includes, with high accuracy, representations of all records in the corpus similar to the input. Full-size vectors for the nearest neighbors are in turn retrieved and compared to the input, and ranked results are generated.","['G06F40/279', 'G06F16/2237', 'G06F16/243', 'G06F16/24578', 'G06F16/248', 'G06F40/126', 'G06F40/20', 'G06N20/00', 'G06N3/045', 'G06N3/088']"
CN112115238B,Question-answering method and system based on BERT and knowledge base,"The invention discloses a question-answering method and a question-answering system based on BERT and a knowledge base, which are applied to the field of information retrieval and aim at the defects of the existing question-answering system based on the knowledge base; and training the two models, processing the question corpus to be answered by adopting the two trained models, obtaining the correct answer of the question, and automatically rewriting the answer.","['G06F16/3329', 'G06F16/3344', 'G06F40/295', 'G06N3/044', 'G06N3/045', 'G06N5/02']"
US20240054233A1,"Device, System, and Method for Protecting Machine Learning (ML) Units, Artificial Intelligence (AI) Units, Large Language Model (LLM) Units, and Deep Learning (DL) Units","Systems and methods for protecting machine learning engines, artificial intelligence engines, large language models, and deep learning engines. An Offline Protection Unit is configured to analyze one or more characteristics of a Protected Engine, and to perform offline fortification of the Protected Engine against attacks by changing operational properties or operational parameters of the Protected Engine to reduce its vulnerability to attacks. An Online Protection Unit is configured to perform analysis of at least one of: (i) inputs that are directed to be inputs of the Protected Engine, (ii) outputs that are generated by the Protected Engine; and based on the analysis, to dynamically perform online fortification of the Protected Engine against attacks by dynamically changing operational properties or operational parameters of the Protected Engine to reduce its vulnerability to attacks.","['G06F21/577', 'G06F21/54', 'G06F21/552', 'G06F21/554', 'G06F21/566', 'G06N3/094', 'G06F2221/032', 'G06N20/20', 'G06N3/126', 'G06N5/01']"
US20210326742A1,Using a Multi-Task-Trained Neural Network to Guide Interaction with a Query-Processing System via Useful Suggestions,"A computer-implemented technique is described herein for assisting a user in advancing a task objective. The technique uses a suggestion-generating system (SGS) to provide one or more suggestions to a user in response to at least a last-submitted query provided by the user. The SGS may correspond to a classification-type or generative-type neural network. The SGS uses a machine-trained model that is trained using a multi-task training framework based on plural groups of training examples, which, in turn, are produced using different respective example-generating methods. One such example-generating method constructs a training example from queries in a search session. It operates by identifying the task-related intent the queries, and then identifying at least one sequence of queries in the search session that exhibits a coherent task-related intent. A training example is constructed based on queries in such a sequence.","['G06F16/90324', 'G06F17/16', 'G06F18/24', 'G06K9/6267', 'G06N20/00', 'G06N5/04']"
US11756551B2,System and method for producing metadata of an audio signal,"An audio processing system is provided. The audio processing system comprises an input interface configured to accept an audio signal. Further, the audio processing system comprises a memory configured to store a neural network trained to determine different types of attributes of multiple concurrent audio events of different origins, wherein the types of attributes include time-dependent and time-agnostic attributes of speech and non-speech audio events. Further, the audio processing system comprises a processor configured to process the audio signal with the neural network to produce metadata of the audio signal, the metadata including one or multiple attributes of one or multiple audio events in the audio signal.","['G10L15/26', 'G10L15/16', 'G05B23/024', 'G10L15/04', 'G10L15/063', 'G10L25/30', 'G10L25/51', 'G10L25/78', 'G05B19/4184', 'G05B2219/37337']"
CN115688937A,Model training method and device,"A model training method is applied to multi-modal data processing, relates to the field of artificial intelligence, and comprises the following steps: acquiring a first characteristic representation and a second characteristic representation, wherein first input data is data of a first mode; the second input data is data of a second mode; mapping the first feature representation to a first discrete label through a first mapping network; mapping the second feature representation to a second discrete label through a second mapping network; the difference between the second discrete label and the first discrete label is used to update the second mapping network; executing a first target task through a first task network according to the first discrete label to obtain a first result; and executing a second target task through a second task network according to the second discrete label to obtain a second result. According to the method and the device, the feature representations of the data in different modes are mapped into the same discrete space, modeling can be performed on the feature representations of the multiple modes based on the discrete space, and a model compatible with the input data of the multiple modes is obtained.",[]
US20230385085A1,"Determining sequences of interactions, process extraction, and robot generation using generative artificial intelligence / machine learning models","Use of generative artificial intelligence (AI)/machine learning (ML) models is disclosed to determine sequences of user interactions with computing systems, extract common processes, and generate robotic process automation (RPA) robots. The generative AI/ML model may be trained to recognize matching n-grams of user interactions and/or a beneficial end state. Recorded real user interactions may be analyzed, and matching sequences may be implemented as corresponding activities in an RPA workflow.","['G06F9/451', 'G06F17/18', 'G06N20/00', 'G06N3/006', 'G06N3/04', 'G06N3/084', 'G06N3/09', 'G06N7/01']"
CN111985240B,"Named entity recognition model training method, named entity recognition method and named entity recognition device","The application discloses a training method of a named entity recognition model, a named entity recognition method and a named entity recognition device, relates to natural language processing of artificial intelligence, and is suitable for the medical field. The method comprises the following steps: invoking a first NER model to identify a first unlabeled dataset to obtain a first model identification result set; invoking a second NER model to identify the first unlabeled dataset to obtain a second model identification result set; correcting the first model recognition result set according to the second model recognition result set to obtain a third model recognition result set; training and updating the first NER model according to the third model identification result set; and calling the updated first NER model to identify the second unlabeled data set to obtain a first updated identification result set, and updating a dictionary of the second NER model according to the first updated identification result set. The method can save the labor cost of model training and improve the model training efficiency.","['G06F40/295', 'G06F40/205', 'G06F40/242']"
CN116757199A,Understanding architecture model for large language model and answer set programming of robots,"The invention discloses a large language model for a robot and an understanding architecture model for answer set programming, which comprises the following components: s1, extracting a group of predicates in a text through a semantic analysis converter and a special large language model; s2, comparing the extracted predicates with common sense knowledge related to the predicates and pre-coded by an answer set programming system through an answer set programming and S (CASP) system, reasoning and obtaining a conclusion reaction, effectively extracting knowledge expressed in the form of predicates from the language through using a large language model, and reasonably reasoning the knowledge through an ASP with target guidance, thereby reasonably reasoning and explaining the knowledge.","['G06F40/289', 'G06F40/205', 'G06F40/30', 'G06N5/04']"
US11900071B2,Generating customized digital documents using artificial intelligence,"Methods and apparatuses are described in which unstructured computer text is analyzed for generation of customized digital documents. A server tokenizes and encodes historical user interactions and historical digital documents into multidimensional vectors. The server trains an interaction classification model using the multidimensional vectors as input to generate a classification for an input user interaction, and trains a language generation model using the multidimensional vectors as input to generate a customized digital document based upon an input user interaction. The server receives a new user interaction and encodes the new user interaction into a new multidimensional vector. The server executes the trained interaction classification model using the new vector as input to generate a digital document classification. The server executes the trained language generation model using the new vector and the classification as input to generate a customized digital document.","['G06F40/56', 'G06F40/284', 'G06N3/045', 'G06N3/08', 'G06N3/084']"
US12259864B1,Apparatus and method for training a machine learning model,"Described herein is an apparatus and method for training a machine learning model. An apparatus may include a computing device configured to receive a corpus of data containing entries corresponding to a plurality of subjects; identify a plurality of entries within the corpus, corresponding to a first subject of the plurality of subjects and representing medical history of the first subject; determine a plurality of temporal attributes of the plurality of entries; generate a plurality of tokens as a function of the plurality of entries; generate a chronological data structure segment ordering one or more of the plurality of entries and the plurality of tokens, as a function of the plurality of temporal attributes; and train a multimodal machine learning model on a training dataset including the chronological data structure segment.","['G06F16/22', 'G06F16/256', 'G16H10/60', 'G16H50/20', 'G16H50/70']"
US12175204B2,Aspect prompting framework for language modeling,"Techniques for dynamically developing a contextual set of prompts based on relevant aspects extracted from s set of training data. One technique includes obtaining training data comprising text examples and associated labels, extracting aspects from the training data, generating prompting templates based on the training data and the extracted aspects, concatenating each of the text examples with the respective generated prompting template to create prompting functions, training a machine learning language model on the prompting functions to predict a solution for a task, where the training is formulated as a masked language modeling problem with blanks of the prompting templates being set as text labels and expected output for the task being set as specified solution labels, and the training learns or updates model parameters of the machine learning language model for performing the task. The machine learning language model is provided with the learned or updated model parameters.","['G06F40/169', 'G06F40/186', 'G06F40/284', 'G06F40/40', 'G06N3/0455', 'G06N3/0475', 'G06N3/096', 'G06V30/19147']"
CN118919056A,Hypergraph Transformer-based multi-modal social network depression detection method,"The invention discloses a multimode social network depression detection method based on hypergraph Transformer, which belongs to the technical field of natural language processing and graph neural network and comprises the following specific steps: preprocessing the text data and the picture data, extracting text features from the text data by utilizing a pre-training language model, constructing a text hypergraph by combining a topic analysis method, and fully capturing semantic association in the text data; for image data, the correlation of feature representation is calculated to construct an image hypergraph, a visual hypergraph convolution network is introduced, the image features are further extracted, and the feature representation capability of the hypergraph is enhanced. Aiming at the characteristics of social network depression detection, the method constructs a hypergraph model capable of reflecting the complexity of multi-modal data, fully utilizes the multi-modal data such as texts, images and the like in the social network, and comprehensively digs the complex association among cross-modalities.","['G16H50/20', 'G06F17/16', 'G06F17/18', 'G06F18/22', 'G06F18/24', 'G06N3/042', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06Q50/01']"
WO2024254051A1,Autonomous visual information seeking with machine-learned language models,"Systems and methods for generating a response to a visual information query can include obtaining input data, processing the input data with one or more machine-learned models, transmitting data to one or more data processing tools, and generating a response based on the input data and the one or more outputs of the one or more data processing tools. The one or more machine-learned models may iteratively process data to generate planning data that may include application programming interface calls to the one or more data processing tools. The one or more machine-learned models may be utilized for planning, reasoning, and response generation when obtaining and generating a response to a visual information query.","['G06F16/532', 'G06F16/53', 'G06N20/00', 'G06V10/74', 'G06V10/82']"
US20240177047A1,Knowledge grap pre-training method based on structural context infor,"Disclosed in the present invention is a knowledge graph pre-training method based on structural context information, the method comprising: for a target triple, constructing an instance comprising context triples, and adopting a triple integration module to encode each of the context triples in the instance to obtain an integration vector; combining the integration vectors for all context triples in the instance into a context vector sequence, and adopting a structural information module to encode the context vector sequence to obtain a structural representation vector for the triple; adopting a general task module to calculate the structural representation vector for the triple, and obtaining a label prediction value for the triples, updating the structural representation vector for the triple based on cross-entropy loss of the label prediction value for the triple and a label truth value for the triple until the completion of the training, so as to obtain an optimized structural representation vector for the target triple. The structural representation vector for the triple obtained by this method incorporates the context information.","['G06N20/00', 'G06F16/36', 'G06F16/367', 'G06F40/30', 'G06N3/042', 'G06N3/045', 'G06N3/08', 'G06N5/022']"
US12406418B2,Personalized text-to-image generation,"One or more aspects of a method, apparatus, and non-transitory computer readable medium include obtaining an input description and an input image depicting a subject, encoding the input description using a text encoder of an image generation model to obtain a text embedding, and encoding the input image using a subject encoder of the image generation model to obtain a subject embedding. A guidance embedding is generated by combining the subject embedding and the text embedding, and then an output image is generated based on the guidance embedding using a diffusion model of the image generation model. The output image depicts aspects of the subject and the input description.","['G06T11/60', 'G06T11/00', 'G06T7/194', 'G06T9/00', 'G06T2207/20081']"
US11983553B2,Conversational interface for content creation and editing using large language models,"Example embodiments of the present disclosure provide for an example method. The example method includes generating an initial user interface including a content assistant component. The example method include obtaining user input data. The example method includes processing, by a machine learned model interfacing with the content assistant component, the data indicative of the input received from the user. The method includes obtaining output data, from the machine learned model interfacing with the content assistant component, indicative of one or more content item components. The method includes transmitting data which causes the content item components to be provided for display via an updated user interface. The method includes obtaining data indicative of user selection of approval of the content item components. The method includes generating, in response to obtaining the data indicative of the user selection of the approval of the content item components, content items.","['G06F9/453', 'G06F40/166', 'G06F40/30', 'G06Q30/0276']"
WO2025101496A1,Model fine-tuning for automated augmented reality,"A second input image is generated by applying a target augmented reality (AR) effect to a first input image. The first input image and the second input image are provided to a first visual-semantic machine learning model to obtain output describing at least one feature of the target AR effect. The first visual-semantic machine learning model is fine- tuned from a second visual-semantic machine learning model by using training samples. Each training sample comprises a first training image, a second training image, and a training description of a given AR effect. The second training image is generated by applying the given AR effect to the first training image. A description of the target AR effect is selected based on the output of the visual-semantic machine learning model. The description of the target AR effect is stored in association with an identifier of the target AR effect.","['G06V10/82', 'G06V20/70', 'G06N3/0455', 'G06T11/00', 'G06V10/774', 'G06V20/20', 'G06V20/46', 'G06F40/40']"
US20240296425A1,Automated description generation for job posting,"Embodiments of the described technologies receive, via a user interface, an input associated with a first user of a user connection network. The input identifies first position data related to a position capable of being filled by a hiring of a person. In response to validating the first position data, second position data different from the first position data is extracted from the user connection network, based on the first position data. A first prompt is formulated based on the first position data and the second position data. The first prompt is sent to a generative language model. A first piece of writing is received from the generative language model. The first piece of writing includes a position description output by the generative language model based on the first prompt. The position description is sent to the user interface in response to the input.","['G06F40/166', 'G06F16/9535', 'G06F40/194', 'G06F40/197', 'G06F40/40', 'G06Q10/1053']"
CN111597327B,Public opinion analysis-oriented unsupervised multi-document abstract generation method,"The invention discloses an unsupervised multi-document abstract generation method for public sentiment analysis. Step 1: collecting network public opinion news in real time, and automatically dividing a news set according to network hotspots; step 2: unsupervised extraction of a single document abstract is carried out on each public opinion news in the set; and step 3: and analyzing all the extracted single-document abstracts in the set to obtain an unsupervised multi-document abstract. The invention solves the problems of low effect, poor practicability of the generated abstract and shortage of Chinese public opinion abstract training corpus in the existing multi-document abstract method so as to realize the monitoring of public opinion news.","['G06F16/345', 'G06F16/36', 'G06F16/9532', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
WO2025042780A1,Systems for controllable summarization of content,A method of generating summaries of content items using one or more large language models (LLMs) is disclosed. A first content item is identified. The first content item includes a set of sub-content items. A level of abstraction is determined for the content item. A prompt is automatically engineered for providing to the one or more LLMs. The prompt includes a reference to the first content item and the level of the abstraction for the first content item. A response to the prompt is received from the LLM. The response includes a second content item. The second content item includes a representation of the first content item that is generated by the LLM. The representation omits or simplifies one or more of the set of sub-content items based on the level of abstraction. The representation is used to control an output that is communicated to a target device.,"['G06F40/56', 'G06F16/345']"
US12282696B2,Method and system for semantic appearance transfer using splicing ViT features,"Using a pre-trained and fixed Vision Transformer (ViT) model as an external semantic prior, a generator is trained given only a single structure/appearance image pair as input. Given two input images, a source structure image and a target appearance image, a new image is generated by the generator in which the structure of the source image is preserved, while the visual appearance of the target image is transferred in a semantically aware manner, so that objects in the structure image are â€œpaintedâ€ with the visual appearance of semantically related objects in the appearance image. A self-supervised, pre-trained ViT model, such as a DINO-VIT model, is leveraged as an external semantic prior, allowing for training of the generator only on a single input image pair, without any additional information (e.g., segmentation/correspondences), and without adversarial training. The method may generate high quality results in high resolution (e.g., HD).","['G06V10/82', 'G06F3/14', 'G06F18/2415', 'G06T7/11', 'G06T7/143', 'G06V10/422', 'G06V10/54', 'G06V10/56', 'G06V10/761', 'G06V20/70', 'G06T2207/20084']"
WO2025034343A1,Compressing dialog information provided to a machine-trained model using abstract tokens,"A technique uses a machine-trained model to generate a response based on a prompt which expresses current input information and abstract token information. The abstract token information summarizes a full dialogue history of a dialogue, and is generated by the model itself. The technique reduces the size of the prompt by incorporating the abstract summary information in lieu of the full dialogue history. A training system trains the machine-trained model by successively improving the predictive accuracy of the machine-trained model, while rewarding the machine-trained model based on an extent to which the machine-trained model compresses instances of abstract token information.","['G06F40/35', 'G06F16/3329', 'G06F16/345', 'G06F40/284', 'G06N20/00', 'G06N3/08', 'G06F40/216']"
US11941373B2,Code generation through reinforcement learning using code-quality rewards,"A deep learning model trained to learn to predict source code is tuned for a target source code generation task through reinforcement learning using a reward score that considers the quality of the source code predicted during the tuning process. The reward score is adjusted to consider code-quality factors and source code metrics. The code-quality factors account for the predicted source code having syntactic correctness, successful compilation, successful execution, successful invocation, readability, functional correctness, and coverage. The source code metrics generate a score based on how close the predicted source code is to a ground truth code.","['G06N3/006', 'G06F8/33', 'G06F18/217', 'G06F8/447', 'G06F8/77', 'G06N3/04', 'G06N3/045', 'G06N3/084', 'G06N3/09', 'G06N3/092', 'G06N3/096', 'G06N3/088']"
US20250095185A1,Artificial intelligence smoothed object detection and tracking in video,Methods and systems for object detection and tracking in video that use at least two different AI-assisted object detection algorithms. A first AI-assisted object detection algorithm selected to be used to detect an object in a video frame and determine a mask defining location of the object on the basis that the video frame is a keyframe. A second AI-assisted object detection algorithm may be used to track location of the mask in temporally subsequent frames until the next keyframe is detected.,"['H04N19/159', 'G06T7/20', 'G06T7/70', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084']"
CN118396126B,"Text processing method, product, equipment and medium","The invention discloses a text processing method, a product, equipment and a medium, and relates to the technical field of large language models. In order to integrate new knowledge into the model, the method uses the injected new knowledge sample in the new knowledge adapter to train, and keeps the parameters in the large language model unchanged in the training process, namely only trains the parameters of the new knowledge adapter on the basis of the existing large language model. Because the parameters of the new knowledge adapter are far smaller than those in the large language model, the new knowledge can be injected into the large language model with high efficiency, low calculation power and low cost, the original capability of the large model can be ensured not to be forgotten, and the efficiency of model training is improved, so that the efficiency of the model in text reasoning is improved; and secondly, a new knowledge adapter and a large language model are used for reasoning the text, so that the new knowledge can be ensured to be inferred.","['G06N5/04', 'G06F40/166', 'G06F40/205', 'G06N3/0499', 'G06N3/08']"
CN118626612A,"Model training method, text generation method, device, equipment and medium","The embodiment of the application provides a model training method, a text generation method, a device, equipment and a medium, wherein a large language model comprises the following steps: an embedding layer and a handling layer, the embedding layer comprising: the first embedding layer corresponds to a first embedding weight matrix, and the second embedding layer corresponds to a second embedding weight matrix; the model training method comprises the following steps: mapping the input matrix by using the first embedded layer to obtain an activation matrix; multiplying the activation matrix with a second embedding weight matrix to obtain an embedded representation; processing the embedded representation; loss information is determined from the text information. The embodiment of the application can improve the training full degree and the overall training effect of the parameters of the large language model, reduce the data transmission quantity of the large language model in the training process, further improve the hardware processing speed and the response speed corresponding to the large language model, and save the memory space occupied by the training of the large language model.","['G06F16/3329', 'G06F16/3344', 'G06F16/3346', 'G06F40/216', 'G06F40/30', 'G06N3/0455', 'G06N3/0499', 'G06N3/084']"
US20240354830A1,System and Method for Identifying Complementary Objects from Images,"A system and method are provided for identifying complementary objects from images and, in particular, to using images containing objects determined based on an input, to identify such complementary objects. The method includes determining one or more objects based on an input, using an input embedding associated with the input, and object embeddings associated with the one or more objects; determining a plurality of images containing the one or more objects; identifying complementary objects in the images; and providing an output collection of identified complementary objects.","['G06V10/761', 'G06Q30/0631', 'G06Q30/0643', 'G06V10/762']"
US11610061B2,Modifying text according to a specified attribute,"Text may be modified according to a specified attribute value, such as changing a 1-star review to a 5-star review. To modify the text, an input sequence of tokens may be obtained corresponding to the text. Mask scores may be computed for the tokens by processing the input sequence of tokens with masking neural network. One or more tokens may be selected using the mask scores, and a masked sequence of tokens may be generated by replacing the selected tokens with a mask token. The masked sequence of tokens may be processed by a language model neural network to select a replacement token for each of the mask tokens. Modified text may then be generated using the selected replacement tokens. The modified text may be used for any appropriate application, such as suggesting messages to users participating in a conversation.","['G06F40/30', 'G06F40/253', 'G06F40/295', 'G06N3/045', 'G06N3/0454', 'G06F40/284', 'G06N3/08', 'G06Q30/016']"
CN117149989B,"Training method for large language model, text processing method and device","The embodiment of the application provides a large language model training method, a text processing method and a device, and relates to the fields of artificial intelligence, cloud technology, natural language processing, machine learning and the like. The method comprises the following steps: acquiring a training sample set; the training sample set comprises a plurality of training samples; the plurality of training samples includes a plurality of first training samples and a plurality of second training samples; the first training sample is a training sample with prediction accuracy greater than a preset threshold value; the second training sample is a training sample with prediction accuracy smaller than a preset threshold value; training the initial rewarding model based on the training sample set to obtain a trained rewarding model; training the pre-trained large language model based on the reward model to obtain a trained large language model. The embodiment of the application provides a richer data base with better training effect for training to obtain the large language model with better performance, and better meets the actual application demands.","['G06F16/3329', 'G06F16/3344', 'G06F16/35', 'Y02D10/00']"
WO2022105181A1,"Error correction method and apparatus for structured triple, computer device, and storage medium","An error correction method and apparatus for a structured triple, a computer device, and a storage medium. Said method comprises: constructing a triple sample set (S101); inputting samples in the triple sample set into a preset GPT2 language model, identifying the samples in the triple sample set by using the GPT2 language model, and outputting a spliced triple knowledge set Y, the triple knowledge set comprising several pieces of sub-triple knowledge (S102); and segmenting the triple knowledge set Y according to a preset rule, to obtain a plurality of single piece of target triple knowledge and output same (S103). A correct triple is generated by means of the GPT2 language model, having high accuracy and high efficiency, the amount of data requiring to be annotated is small, and the cost of manual annotation can be reduced.",['G06F40/295']
CN111104512B,Game comment processing method and related equipment,"The embodiment of the disclosure provides a game comment processing method and related equipment, and belongs to the technical field of computers. The method comprises the following steps: obtaining annotation data obtained by annotating the game comments, wherein the annotation data comprises the game comments and annotation categories thereof; processing the game comments in the annotation data through a pre-training model to obtain semantic expression vectors of the game comments; the pre-training model is obtained by pre-training the unmarked game comments; processing the semantic expression vector of the game comment through a full connection layer to obtain the probability of the target category of the game comment; determining a predicted category of the game comment according to the probability of the target category of the game comment; and training the pre-training model and the full connection layer according to the labeling category and the prediction category to obtain a game comment classification model.",['G06F16/35']
CN113158665B,Method for improving dialog text generation based on text abstract generation and bidirectional corpus generation,"The invention discloses a method for generating a dialogue text based on text abstract generation and bidirectional corpus improvement, which comprises the following steps: 1) Constructing a pre-training text abstract generation model; 2) Constructing a dependency relationship discriminator of upper and lower sentences: taking a BERT model as an encoder, then connecting a classifier, respectively obtaining dialogue corpora with strong one-way dependency relationship in the forward direction or the reverse direction according to the judgment result, and forming a two-way dialogue corpora; 3) Constructing a short text dialogue generating model: the model structure is the same as the pre-training text abstract generating model, an encoder of the pre-training text abstract generating model is loaded, the whole short text dialogue generating model is updated by using the bidirectional dialogue corpus, and replies with diversity and strong context dependence are generated. The dialogue generating method utilizes a noise-added text abstract task to strengthen the comprehension capability of a model encoder to an input text, and utilizes bidirectional dialogue linguistic data to generate more semantically related and diversified replies.","['G06F40/289', 'G06F18/2415', 'G06F40/253', 'G06N3/044', 'G06N3/045', 'G06N3/048', 'G06N3/084']"
US11914954B2,Methods and systems for generating declarative statements given documents with questions and answers,"Described herein are systems and methods to enable generation of high-quality summaries of documents that have questions and answers. To help summarize such documents, parsing methods are disclosed that account for different document formats. Methods are disclosed to anonymize personal and identifying information present in the documents. An ontology is defined to categorize dialog acts for the questions and answers present. Classifiers are disclosed based on this ontology. Methods are also disclosed to transform a question-answer pair to a canonical form. Based on the dialog acts for the question and answer combination, transformation methods were developed that build upon each of traditional NLP, and deep learning, techniques.","['G06F40/30', 'G06F40/205', 'G06F40/279', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06Q30/018', 'G06Q50/265']"
CN113841195B,Joint endpoint determination and automatic speech recognition,"A method (800) comprising receiving audio data (120) of an utterance (120) and processing the audio data to obtain an output as a speech recognition model (140) from a speech decoding and endpoint determination configured to jointly perform the utterance: a partial speech recognition result of the utterance; and an end-point indication (175) indicating when the utterance has ended. While processing the audio data, the method further includes detecting an end of the utterance based on the end-point indication. In response to detecting the end of the utterance, the method further includes terminating processing of any subsequent audio data received after detecting the end of the utterance.","['G10L15/16', 'G10L15/04', 'G10L15/22', 'G10L15/08', 'G10L15/14', 'G10L15/28', 'G10L25/87', 'G10L15/02', 'G10L2015/025', 'G10L2015/088', 'G10L2015/221']"
US12045272B2,Auto-creation of custom models for text summarization,"A text summarization system auto-generates text summarization models using a combination of neural architecture search and knowledge distillation. Given an input dataset for generating/training a text summarization model, neural architecture search is used to sample a search space to select a network architecture for the text summarization model. Knowledge distillation includes fine-tuning a language model for a given text summarization task using the input dataset, and using the fine-tuned language model as a teacher model to inform the selection of the network architecture and the training of the text summarization model. Once a text summarization model has been generated, the text summarization model can be used to generate summaries for given text.","['G06F16/345', 'G06F16/3329', 'G06F40/30', 'G06N3/04', 'G06N3/044', 'G06N3/0455', 'G06N3/08', 'G06N3/096', 'G06N3/0985', 'G06N3/0464', 'G06N3/092']"
US11989507B2,"Computer implemented methods for the automated analysis or use of data, including use of a large language model","Methods are provided, such as a method of interacting with a large language model (LLM), including the step of a processing system using a structured, machine-readable representation of data that conforms to a machine-readable language, such as a universal language, to provide new context data for the LLM, in order to improve the output, such as continuation text output, generated by the LLM in response to a prompt; and such as a method of interacting with a LLM, including the step of providing continuation data generated by the LLM to a processing system that uses a structured, machine-readable representation of data that conforms to a machine-readable language, such as a universal language, in which the processing system is configured to analyse the continuation output generated by the LLM in response to a prompt to enable an improved version of that continuation output to be provided to a user. Related computer systems are provided.","['G06F40/20', 'G06F16/3344', 'G06F40/56']"
US11249734B2,"Tri-affinity model driven method and platform for authoring, realizing, and analyzing a cross-platform application","A tri-affinity model driven platform (TAMDP) employs a tri-affinity model driven method using a human affinity model (HAM), a machine affinity model (MAM), and an analysis affinity model (AAM), to generate an application specific instance of predefined meta-models for building a cross-platform application. A developer authors an application in the HAM which is compiled to the MAM by a compiler, which is transformed to the AAM by a model-to-model transformer. A translator optionally translates a HAM to another HAM. A generator generates source code from MAM. Build tooling builds application binaries for different rendering types from a source code generated for the application. A development time analyzer and visualizer (DTAV) enables development time analyses using the AAM. After prototyping and introspection, a TAMDP runtime subsystem executes the generated application and a machine learning based recommendation engine enhances the application using the AAM after analysis by the DTAV and a runtime analyzer and visualizer.","['G06F8/34', 'G06F8/30', 'G06F8/40', 'G06F8/41', 'G06F8/60', 'H04L51/02']"
US20230325154A1,Constrained decoding and ranking of language models for code generation,"In various embodiments, a process for constrained decoding and ranking of language models for code generation includes receiving a natural language input specifying a desired computer task. The process includes using a machine learning trained converter to convert the natural language input to an output in a computer language, including by, based on a specified grammar for the computer language, limiting eligible options for a token to include in the output in the computer language. The process includes providing the output in the computer language for computer execution.","['G06F8/33', 'G06F40/284', 'G06F40/40', 'G06N20/00', 'G06F40/216', 'G06F40/274', 'G06F8/427']"
CN111966826B,"A method, system, medium and electronic device for constructing a text classification system","The embodiment of the invention provides a method, a system, a medium and electronic equipment for constructing a text classification system, wherein the text classification system comprises a clause module, a sentence-level feature extraction module, a chapter-level feature extraction module and a classification module, and the method comprises the following steps: a1, obtaining a training set comprising a plurality of texts, wherein the texts in the training set are provided with labeled categories, and at least part of the texts are super-long texts; a2, a sentence dividing module is used for dividing each text in the training set according to a predefined sentence dividing rule to obtain a plurality of sentences; a3, performing multi-round training on a sentence-level feature extraction module, a chapter-level feature extraction module and a classification module of the text classification system by using the training set until convergence to obtain a text classification system; the method reduces the possibility of losing semantic information and structural information when extracting the characteristics, is finally used for subsequent classification prediction according to the chapter characteristics of the text, improves the accuracy of text classification, and is particularly suitable for accurately classifying the overlong text.","['G06F16/355', 'G06F18/2415', 'G06F40/216', 'G06F40/289', 'G06F40/30']"
CN112131366B,"Method, device and storage medium for training text classification model and text classification","The application provides a method, a device and a storage medium for training a text classification model and text classification, and relates to an artificial intelligence cloud technology for improving the accuracy of text classification. Inputting first sample data into a language model coding layer through an input layer to obtain a first feature vector of the first sample data, wherein the first sample data comprises at least one group of question-answer pairs and text information for determining answers of questions in the question-answer pairs; performing keyword highlighting operation on the feature vector used for representing the text information in the first feature vector through keyword highlighting operation introduced in the embedding layer, and performing keyword highlighting on the feature vector used for representing the question-answer pair in the first feature vector to obtain a second feature vector of the text information; inputting the second feature vector and the feature vector for representing the question-answer pair into the full-connection layer, and determining answer probability corresponding to the question in the question-answer pair; and reversely adjusting model parameters of the language model coding layer according to the answer probability output by the full-connection layer and the answer in the first sample data.","['G06F16/3329', 'G06F16/35', 'G06N3/045', 'Y02D10/00']"
US20250181328A1,Creating user interface using machine learning,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training and using machine learning models to generate graphical user interfaces from textual descriptions.","['G06F40/143', 'G06F3/0484', 'G06F40/279', 'G06F40/40', 'G06F8/33', 'G06F8/34', 'G06F8/38', 'G06N3/0455', 'G06N3/047', 'G06N3/0475', 'G06N3/088']"
WO2022020467A1,System and method for training multilingual machine translation evaluation models,"A system for training multilingual machine translation evaluation models and methods for making and using the same. The system can utilize a multilingual embedding space to leverage information from system inputs, including original source language, at least one machine translation of the original source language and at least one reference translation. The system can improve the accuracy of translation quality predictions by assigning a translation quality score based on the system inputs to be assigned to the machine translation. The translation quality score advantageously can demonstrate value added by using the original source language as an input to machine translation evaluation models.","['G06F40/51', 'G06F40/216', 'G06F40/284', 'G06F40/289', 'G06F40/30', 'G06F40/58', 'G06N3/045', 'G06N3/084']"
US20220383084A1,Layered Gradient Accumulation and Modular Pipeline Parallelism for Improved Training of Machine Learning Models,"A method is provided including: (i) assigning sequentially-ordered layers of a machine learning model to a plurality of compute nodes, each of the layers being assigned to exactly one of the nodes; (ii) dividing training data into micro-batches; (iii) forward-propagating the micro-batches through the model, each node operating in parallel to generate respective activation states for the micro-batches with their assigned layers, and with the activation states being communicated between the nodes according to the layers' sequential ordering; and (iv) backward-propagating the micro-batches through the model, each node operating in parallel to generate respective error states for the micro-batches with their assigned layers, with the error states being communicated between the nodes according to the layers' reverse sequential ordering, wherein each of the nodes completes the backward-propagation of all micro-batches through a given layer prior to performing backward-propagation through any layer that precedes the given layer in the sequential ordering.","['G06N3/063', 'G06F18/2155', 'G06K9/6259', 'G06N3/084']"
EP4566019A2,Social network with network-based rewards,"A user interface system is provided, comprising a content display output for presentation of content to a user; a communication network interface port; and at least one automated processor configured to: receive at least one hyperlink in a social network record of a social network; request content associated with the hyperlink; receive an advertisement associated with at least one of the user, the social network record, the hyperlink, and the content; verify presentation of the advertisement to the user; present the content to the user; and account for presentation of the advertisement to the user, by crediting at least one account distinct from an account associated with the user, an account associated with a content owner, and an account associated with a social network.","['G06Q30/0277', 'G06Q20/36', 'G06Q20/384', 'G06Q20/389', 'G06Q30/0214', 'G06Q30/0271', 'G06Q30/0273', 'G06Q30/0631', 'G06Q50/01', 'G06Q20/10']"
CN119847763B,A method for accelerating the inference operation of large language models for processing extremely long texts,"The invention discloses an inference operation acceleration method for processing extremely-long texts by a large language model, which belongs to the technical field of large-scale pre-training language model inference optimization, and particularly comprises the steps of selecting the large language model, starting a sparse attention mode, inputting different texts, recording execution time and attention mode types of attention heads of different layers, establishing a statistical database, solving a multi-head attention load balancing distribution strategy of the large language model under an actual text, establishing a weight index table by splitting a weight matrix, searching a weight sub-matrix corresponding to each attention head, loading the weight sub-matrix to corresponding GPU equipment, calculating weights and MLP (multi-level processing) by asynchronously preloading MHA (mobile high-definition) computing weights and MLP (multi-level processing) computing weights of adjacent layers, and combining with KV cache management to realize load balancing of an inference process. The invention effectively avoids the problems of unbalanced load and idle resource of multiple GPU by dynamically segmenting, reorganizing and scheduling the attention head in the reasoning stage, and remarkably improves the system throughput of long-sequence processing.","['G06F9/5083', 'G06F16/24552', 'G06F9/4843', 'G06F9/5005', 'G06N3/0455', 'G06N5/041']"
US20240249318A1,Determining user intent from chatbot interactions,"A system and method for determining user intent and providing targeted advertising using chatbot interactions is disclosed. The system receives user prompts during chat sessions with a chatbot and generates responses using a large language model. User intent is extracted by analyzing the chat conversations using natural language processing and machine learning techniques. The extracted user intent, comprising weighted keywords and concepts, is used to create a user intent profile. Targeted advertising content is generated based on the user intent profile and provided to the user during subsequent platform interactions. The large language model is continuously retrained using user engagement data to improve intent modeling accuracy. User privacy is maintained by limiting context extraction to chatbot conversations. The system enables personalized and relevant advertising by inferring user intent through conversational interactions.","['G06Q30/0269', 'G06Q30/0257', 'G06Q30/0277', 'H04L51/02', 'G06F40/20']"
US20250259022A1,Automated discovery of conversation flows using generative language models,"Techniques for analyzing conversation logs to identify system actions for an autonomous conversational AI system are disclosed. Historical conversation logs are analyzed using a generative language model to identify conversation topics and subtopics. Conversations within each topic and subtopic are ranked based on frequency of occurrence and representative conversations are selected using normalized mean conversation embeddings. The selected conversations are analyzed to identify opportunities for system actions, and user messages and human agent responses are converted into system actions using a transformer-based natural language processing model. An action configuration comprising the identified system actions and required parameters is generated and stored for training the autonomous conversational AI system. The system preprocesses conversation logs to normalize data from multiple communication channels, detects and anonymizes personally identifiable information, and automatically improves performance over time by analyzing conversations where poor performance was observed.","['G06F40/35', 'G06F40/49', 'G06F40/247', 'G06F40/289', 'H04M3/527', 'G06F40/216']"
US12236197B2,"Text extraction method and device, computer readable storage medium and electronic device","A text extraction method and device, computer-readable storage medium, and electronic device are described that relate to the technical field of machine learning. The method includes: acquiring to-be-extracted data and extracting a current trigger word in the to-be-extracted data using a target trigger word extraction model included in a target event extraction model; generating a current query sentence according to the current trigger word; and extracting a current event argument corresponding to the current trigger word according to the current query sentence and a target argument extraction model included in the target event extraction model, wherein the target trigger word extraction model and the target argument extraction model have a same model structure and parameter, and are connected in a cascading manner.","['G06F16/3329', 'G06F16/3347', 'G06F40/126', 'G06F40/216', 'G06F40/279', 'G06F40/284', 'G06F40/30']"
US11922550B1,Systems and methods for hierarchical text-conditional image generation,"Disclosed herein are methods, systems, and computer-readable media for generating an image corresponding to a text input. In an embodiment, operations may include accessing a text description and inputting the text description into a text encoder. The operations may include receiving, from the text encoder, a text embedding, and inputting at least one of the text description or the text embedding into a first sub-model configured to generate, based on at least one of the text description or the text embedding, a corresponding image embedding. The operations may include inputting at least one of the text description or the corresponding image embedding, generated by the first sub-model, into a second sub-model configured to generate, based on at least one of the text description or the corresponding image embedding, an output image. The operations may include making the output image, generated by the first second sub-model, accessible to a device.","['G06T11/60', 'G06F40/284', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06T11/001', 'G06T9/002']"
US11822424B2,Service request remediation with machine learning based identification of critical areas of log segments,"An apparatus comprises a processing device configured to receive a service request associated with a given asset, to obtain a log file associated with the given asset, to split the log file into log segments, to generate sets of log pattern identifiers for the log segments, and to determine risk scores for the log segments utilizing a machine learning model that takes as input the sets of log pattern identifiers and provides as output information characterizing risk of the log segments. The processing device is also configured to identify critical areas of the log file based at least in part on the determined risk scores, a given critical area comprising a sequence of log segments having determined risk scores above a designated risk score threshold. The processing device is further configured to analyze the identified critical areas to determine remedial actions to be applied for resolving the service request.","['G06F16/1815', 'G06F11/0793', 'G06F11/0709', 'G06F11/0751', 'G06F11/0775', 'G06F11/0778', 'G06F11/0787', 'G06F11/079', 'G06F18/214', 'G06F18/22', 'G06F40/205', 'G06F40/30', 'G06N20/00', 'G06N3/045', 'G06N5/022', 'G06Q10/0635', 'G06Q10/06393', 'G06N3/09', 'G06N3/096']"
US12034801B1,Cloud-based fleet and asset management for edge computing of machine learning and artificial intelligence workloads,"A process can include receiving monitoring information associated with a machine learning (ML) or artificial intelligence (AI) workload implemented by an edge compute unit of a plurality of edge compute units. Status information corresponding to a plurality of connected edge assets can be received, the plurality of edge compute units and connected edge assets included in a fleet of edge devices. A remote fleet management graphical user interface (GUI) can display a portion of the monitoring or status information for a subset of the fleet of edge devices, based on a user selection input, and can receive a user configuration input indicative of an updated configuration for at least one workload corresponding to a pre-trained ML or AI model deployed on the at least one edge compute unit. A cloud computing environment can transmit control information corresponding to the updated configuration to the at least one edge compute unit.","['H04B7/18513', 'G06F8/65', 'G06F9/5072', 'H04L41/0816', 'H04L41/16', 'H04L41/22', 'H04L43/0817', 'H04L67/1008', 'H04L67/101', 'G06F2209/508', 'H04L41/40']"
US11961010B2,Method and apparatus for performing entity linking,"Provided is a method for performing entity linking between a surface entity mention in a surface text and entities of a knowledge graph, including supplying the surface text to a contextual text representation model, pooling contextual representations of the tokens of a surface entity mention in the surface text with contextual representations of the other tokens within the surface text to provide a contextual entity representation vector representing the surface entity mention; supplying an identifier of a candidate knowledge graph entity to a knowledge graph embedding model, to provide an entity node embedding vector and combining the contextual entity representation vector with the entity node embedding vector to generate an input vector applied to a fully connected layer which provides an unnormalized output transformed by a softmax function into a normalized output processed to classify whether the surface entity mention corresponds to the candidate knowledge graph entity.","['G06N5/022', 'G06F40/284', 'G06F40/295', 'G06F40/49', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
CN107195296B,"Voice recognition method, device, terminal and system","The embodiment of the application provides a voice recognition method, a voice recognition device, a terminal and a voice recognition system, wherein the method comprises the following steps: receiving a voice to be recognized; extracting the characteristics of the voice to be recognized to obtain characteristic information; inputting the characteristic information into a weighted finite state converter WFST for recognition, wherein the WFST is obtained by combining a pre-created acoustic model, a pronunciation dictionary and a language model, each first language phoneme in the acoustic model has a corresponding relation with each second language phoneme, and each first language word in the pronunciation dictionary is annotated by the second language phoneme. By adopting the scheme in the application, the accuracy rate of voice recognition can be improved.","['G10L15/02', 'G10L15/08', 'G10L15/10', 'G10L15/14', 'G10L15/183']"
US20240378196A1,Prompt Tuning Using One or More Machine-Learned Models,"Systems and methods for prompt tuning can leverage semantic searching for determining similar prompts to use for retraining. A prompt can be generated then searched to find the similar prompts. Data related to the similar prompts can then be utilized for prompt tuning. Moreover, systems and methods for prompt tuning can generate and utilize a meta-prompt to reduce the computational cost of generating prompts. The prompt tuning techniques can be implemented as part of a prompt tuning application programming interface (API).","['G06N20/00', 'G06F16/243', 'G06N3/091', 'G06N3/096']"
CN110364171B,"Voice recognition method, voice recognition system and storage medium","The invention provides a voice recognition method, which is applied to a voice recognition system and comprises the following steps: collecting voice signals in an analog form, and processing the voice signals in the analog form to form voice signals in a digital form; preprocessing the voice signal in the digital form; extracting the voice characteristics of the voice signals in the digital form, and performing characteristic compensation processing and characteristic normalization processing on the voice characteristics to form dynamic characteristics corresponding to the voice signals in the digital form; and decoding the dynamic characteristics corresponding to the voice signals in the digital form by a decoder of the voice recognition system to form corresponding voice recognition results. The invention also provides a voice recognition system and a storage medium. The invention can realize the collection of the voice signals in the analog form and form the corresponding voice recognition result according to the processing of the voice recognition system, thereby realizing the accurate recognition of the voice signals in the analog form.","['G10L15/14', 'G10L15/18', 'G10L19/24', 'G10L25/69']"
CN110176230B,"Voice recognition method, device, equipment and storage medium","The embodiment of the invention discloses a voice recognition method, a voice recognition device, voice recognition equipment and a storage medium. The method comprises the steps that a preset first state diagram and a preset second state diagram are loaded; extracting a reference edge from the first state diagram, and searching an edge which is the same as a label of the reference edge in the second state diagram to be used as a keyword edge; acquiring the weight of a reference edge, and updating the weight of the keyword edge according to the weight of the reference edge; configuring the updated weight of the keyword edge in the second state diagram as the excitation weight of the corresponding edge in the language identification model; inputting a voice to be recognized into a preset voice recognition model to obtain a word sequence path output by the voice recognition model, wherein the voice recognition model comprises a language recognition model; and selecting a target path from the word sequence paths according to the excitation weight of the edges in the language recognition model to obtain a voice recognition result. According to the scheme, the probability of occurrence of the keywords in the voice recognition result is improved, and the accuracy of the voice recognition result is improved while the voice recognition speed is guaranteed.","['G10L15/183', 'G10L15/063', 'G10L15/02', 'G10L15/10', 'G10L15/22', 'G10L2015/0631', 'G10L2015/0635']"
US20240330927A1,Smart contract generation and validation,"A method for generating and validating a smart contract is disclosed. The method may include receiving text input describing a desired operation of a smart contract and processing the text input with a language model to output a smart contract logic description and synthetic data for validating the smart contract. The method may further include processing the smart contract logic description with a generative artificial intelligence model to generate smart contract code, validating the smart contract code using the synthetic data to simulate transactions with the smart contract, and deploying the smart contract code on a blockchain upon successful validation of the smart contract code.","['G06F8/35', 'G06F11/3457', 'G06F21/64', 'G06F8/31', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06Q20/065', 'G06Q20/382', 'G06Q20/389', 'G06Q20/401', 'G06Q2220/00']"
US11922121B2,"Method and apparatus for information extraction, electronic device, and storage medium","The present disclosure provides a method and an apparatus for information extraction, an electronic device, and a storage medium. The method for information extraction includes: first obtaining text data, and then inputting the text data into an information extraction model obtained through pre-training to obtain triple information contained in the text data, wherein the triple information includes a subject, a predicate and an object in the text data. The information extraction model includes a binary classification sub-model and a multi-label classification sub-model, wherein the binary classification sub-model is configured to extract the subject in the text data, and the multi-label classification sub-model is configured to extract the predicate and the object corresponding to the subject in the text data according to the subject and the text data.","['G06F16/35', 'G06F40/279', 'G06F16/353', 'G06F16/367', 'G06F40/169', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G06F40/30']"
CN112836064B,"Knowledge graph completion method and device, storage medium and electronic equipment","The application discloses a knowledge graph completion method, a knowledge graph completion device, a storage medium and electronic equipment, and belongs to the technical field of computers. The knowledge graph completion method comprises the following steps: obtaining a target knowledge text to be verified, generating a plurality of triples according to the target knowledge text and a preset knowledge graph, calculating each triplet to obtain corresponding confidence, verifying the target triples based on the corresponding confidence, and complementing the knowledge graph according to a verification result. Therefore, the method and the device have the advantages that by providing a mixed model combining a text coding technology and a graph embedding technology to learn context and structural knowledge simultaneously, reliable triplet confidence scores are obtained, advantage complementation of the two methods is realized, calculation cost is obviously reduced, and completion accuracy is improved. The self-adaptive integration scheme is also provided, the scores of the coding method and the graph embedding method are adaptively fused, and the accuracy of knowledge graph completion is further improved.","['G06F16/367', 'G06F40/295']"
US12118471B2,Mitigation for prompt injection in A.I. models capable of accepting text input,"A system for use with an artificial intelligence (AI) model configured to accept text input, such as generative pre-trained transformer (GPT), that detects and tags trusted instructions and nontrusted instructions of an input provided by a user responsive to an AI model prompt. The system uses reinforcement learning (RL) and a set of rules to remove the untrusted instructions from the input and provide only trusted instructions to the AI model. The input is represented as tokens, wherein the trusted instructions and the untrusted instructions are represented using incompatible token sets.","['G06F40/279', 'G06F40/30', 'G06F40/40', 'G06N3/0455', 'G06N3/0475', 'G06N3/091', 'G06N3/092']"
US12197486B2,Automatic labeling of text data,"The technology described herein determines whether a candidate text is in a requested class by using a generative model that may not be trained on the requested class. The present technology may use of a model trained primarily in an unsupervised mode, without requiring a large number of manual user-input examples of a label class. The may produce a semantically rich positive example of label text from a candidate text and label. Likewise, the technology may produce from the candidate text and the label a semantically rich negative example of label text. The labeling service makes use of a generative model to produce a generative result, which estimates the likelihood that the label properly applies to the candidate text. In another aspect, the technology is directed toward a method for obtaining a semantically rich example that is similar to a candidate text.","['G06F16/3346', 'G06F16/383', 'G06F16/313', 'G06F16/332', 'G06F16/3344', 'G06F16/35', 'G06F16/953', 'G06F40/30', 'G06N3/045', 'G06N3/0475', 'G06N3/096']"
US20210201044A1,Automatic digital content captioning using spatial relationships method and apparatus,"Disclosed are systems and methods for improving interactions with and between computers in content hosting and/or providing systems supported by or configured with personal computing devices, servers and/or platforms. The systems interact to identify and retrieve data within or across platforms, which can be used to improve the quality of data used in processing interactions between or among processors in such systems. The disclosed systems and methods provide systems and methods for automatically creating a caption comprising a sequence of words in connection with digital content.","['G06K9/00751', 'G06N3/08', 'G06N3/04', 'G06N3/045', 'G06T11/20', 'G06T7/73', 'G06T9/00', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/47', 'G10L13/00', 'G10L13/043', 'G06N3/048', 'G06T2207/20084', 'G06T2210/12']"
US20250094491A1,Structured Video Documents,"A method includes receiving a content feed that includes audio data corresponding to speech utterances and processing the content feed to generate a semantically-rich, structured document. The structured document includes a transcription of the speech utterances and includes a plurality of words each aligned with a corresponding audio segment of the audio data that indicates a time when the word was recognized in the audio data. During playback of the content feed, the method also includes receiving a query from a user requesting information contained in the content feed and processing, by a large language model, the query and the structured document to generate a response to the query. The response conveys the requested information contained in the content feed. The method also includes providing, for output from a user device associated with the user, the response to the query.","['G06F16/739', 'G06F16/74', 'G06F16/7834', 'G06F16/7844', 'G06F40/169', 'G06F40/30', 'G06N3/096', 'G10L15/04', 'G10L15/16', 'G10L15/22', 'G10L15/26']"
US20240311573A1,Content generation using target content derived modeling and unsupervised language modeling,"Content generation leverages an unsupervised, generative pre-trained language model. In this approach, a model derived by applying to given content relevant competitive content and one or more optimization targets is received. Based on optimization criteria encoded as embedding signals in the model, a determination is made regarding whether a template suitable for use as an input to the generative-AI exists in a set of templates. If so, the model embedding signals are merged into the template, or the template itself is transformed using the embedding signals, in either case creating a modified template. If, however, no template suitable as the input exists, the model and other information are input to a natural language processor to generate a generative-AI input. Either the modified template or the generative-AI input, as the case may be, is then applied through the generative-AI to generate an output competitively-optimized with respect to the optimization targets.","['G06F40/143', 'G06F16/907', 'G06F16/9538', 'G06F40/30', 'G06N5/02', 'G06N5/022', 'G06N5/04', 'G06F16/9038', 'G06F16/908', 'G06F40/134', 'G06F40/232', 'G06F40/253', 'G06N20/00']"
US11620814B2,Contextual grounding of natural language phrases in images,"Aspects of the present disclosure describe systems, methods and structures providing contextual groundingâ€”a higher-order interaction technique to capture corresponding context between text entities and visual objects.","['G06F40/205', 'G06F16/583', 'G06F18/21', 'G06F40/284', 'G06K9/6217', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N3/084', 'G06V10/255', 'G06V10/764', 'G06V10/768', 'G06V10/82', 'G06V20/00']"
WO2024182040A1,Text reduction and analysis interface to a text generation modeling system,A query request identifying a query and a plurality of text portions for determining an answer to the query may be received. Relevance scores corresponding with respective ones of the text portions may be determined based on application of one or more machine learning models to the respective text portion and the query. A subset of the text portions may be selected based on the relevance scores. A response message to the query request including an answer to the query in natural language text generated by a large language model based on the first subset of text portions may be determined.,"['G06F40/35', 'G06F40/40', 'G06F16/3329', 'G06F40/289', 'G06F40/205']"
US11537368B2,Integrating machine learning models into an interpreted software development environment,"The subject technology provides for parsing a line of code in a project of an integrated development environment (IDE). The subject technology executes indirectly, using the interpreter, the parsed line of code. The interpreter references a translated source code document generated by a source code translation component from a machine learning (ML) document written in a particular data format. The translated source code document includes code in a chosen programming language specific to the IDE, and the code of the translated source code document is executable by the interpreter. Further the subject technology provides, by the interpreter, an output of the executed parsed line of code.","['G06F8/427', 'G06F8/447', 'G06N20/00', 'G06N3/045', 'G06N3/0454', 'G06N3/105', 'G06F8/315']"
WO2021135548A1,"Voice intent recognition method and device, computer equipment and storage medium","A voice intent recognition method and device, computer equipment and a storage medium, relating to the artificial intelligence field of voice semantics. When receiving user initial voice data, performing recognition and obtaining initial voice text data; calling an NLU model to obtain a current reply text corresponding to same, and calling an NLG model to generate a current reply voice from said current reply text; if user reply voice data is received, identifying the voice data to obtain current reply voice text data; if the current reply voice text data comprises an affirmative reply keyword or a negative reply keyword, calling the corresponding target word slot; by means of target NLP model encoding, obtaining a target NLP model, and by performing recognition on first event-handling voice data by means of said target NLP model, obtaining a first recognition result. User intent recognition is achieved using multiple different modes, user-voice-based intent recognition accuracy is improved, and time spent on event handling through dialogue with an intelligent customer service robot is reduced.","['G10L15/22', 'G06F16/3329', 'G06F40/30', 'G06Q30/0281', 'G10L15/18', 'G10L15/1822', 'G10L15/30', 'G10L2015/223', 'Y02D10/00']"
US20250005020A1,Automated content creation and content services for collaboration platforms,"Embodiments described herein relate to systems and methods for automatically generating content, generating API requests and/or request bodies, structuring user-generated content, and/or generating structured content in collaboration platforms, such as documentation systems, issue tracking systems, project management platforms, and other platforms. The systems and methods described use a network architecture that includes a prompt generation service and a set of one or more purpose-configured large language model instances (LLMs) and/or other trained classifiers or natural language processors used to provide generative responses for content collaboration platforms.","['G06F16/2455', 'G06F16/243', 'G06F16/248', 'G06F16/3329', 'G06F16/3334', 'G06F16/345', 'G06F16/90332', 'G06F16/9038', 'G06F16/93', 'G06F16/954', 'G06F21/31', 'G06F3/04812', 'G06F3/0486', 'G06F40/117', 'G06F40/134', 'G06F40/143', 'G06F40/166', 'G06F40/169', 'G06F40/174', 'G06F40/186', 'G06F40/197', 'G06F40/20', 'G06F40/205', 'G06F40/30', 'G06F40/40', 'G06F9/451', 'G06F9/541', 'G06F9/547', 'G06N3/047', 'G06N3/0475', 'G06Q10/063114', 'G06Q10/06316', 'G06Q10/101', 'H04L51/02', 'H04L51/046', 'H04L51/21', 'G06F3/0484', 'G06Q10/103']"
WO2025110743A1,Dataset generation device utilized in image generation artificial intelligence model,"A dataset generation device utilized in an image generation artificial intelligence model according to one aspect of the present invention may comprise: a processor comprising a first caption generation module for generating first caption data describing a situation depicted by an image, a second caption generation module for generating second caption data describing features of the image, a prompt generation module for generating prompt data by merging the first caption data and the second caption data, and a dataset generation module for generating an image-text pair dataset by matching image data representing the image with the prompt data; and a dataset database for storing the image-text pair dataset.","['G06N3/0455', 'G06N3/0475', 'G06N3/096']"
US11482213B2,Automatic speech recognition correction,"Systems, methods, and computer-readable media for correcting transcriptions created through automatic speech recognition. A transcription of speech created using an automatic speech recognition system can be received. One or more domain-specific contexts associated with the speech can be identified and a text span that includes a mistranscribed entry can be recognized from the speech based on the one or more domain-specific contexts. Additionally, features can be extracted from the mistranscribed entry and the extracted features can be matched against an index of domain-specific entries to identify a correct entry of the mistranscribed entry. Subsequently, the transcription can be corrected by replacing with the mistranscribed entry with the correct entry.","['G10L15/183', 'G10L15/187', 'G10L15/02', 'G10L15/063', 'G10L15/22', 'G10L15/32', 'G10L2015/025']"
CN117217289A,Banking industry large language model training method,"The invention provides a banking large language model training method, which comprises the following steps: step S1: constructing a model training data set; step S2: training a model word segmentation device based on a bank word list; step S3: constructing a large model base based on the gain training of the llama pre-training model; step S4: performing instruction fine adjustment by using a prompt project; step S5: and (5) strengthening and learning to finely tune the large model. The large language model is trained based on the business, knowledge and data of the specific bank. The method can understand the professional vocabulary such as the amount of money, financial product names, handling processes and the like which are frequently used by banks; instruction training is carried out by using customer service dialogue data and a bank internal knowledge base, so that the model has the capabilities of customer service questioning and answering, search type questioning and answering and the like; the service facing the continuous development has the capability of rapid iteration.",[]
CN117709395A,Language model light weight method combining knowledge distillation and dynamic word pruning,"The invention discloses a language model light weight method combining knowledge distillation and dynamic word pruning, which belongs to the field of natural language processing.","['G06N3/045', 'G06F40/30', 'G06N3/082', 'G06N3/096', 'Y02D10/00']"
US12182678B1,Systems and methods for aligning large multimodal models (LMMs) or large language models (LLMs) with domain-specific principles,A system and method aligns generative artificial intelligence (a large language model (LLM) or a large multimodal model (LMM) with the principles of a specific domain so that the generative artificial intelligence is better able to respond to a user query in the specific domain. The system and method may post-train an already trained generative artificial intelligence system or fine tune the training of the generative artificial intelligence system to align that generative artificial intelligence system with the principles of the specific domain. The system and method may be used to align the generative artificial intelligence system to a plurality of different domains.,"['G06N3/045', 'G06N20/00', 'G06N3/08']"
CN118155182A,A smart driving target detection method based on fine-tuning of a large vision-language model,"The invention discloses an intelligent driving target detection method based on vision-language big model fine adjustment, and aims to solve the problem that the existing target detection method is poor in detection precision of common types and unknown types of intelligent driving scenes. The technical proposal is as follows: and constructing a target detection system consisting of a main feature extraction module, a double-branch feature fine adjustment module and a post-processing-free predicted value fusion module. The dual-branch feature fine tuning module consists of two dual first branch networks and two second branch networks, and each branch network consists of a feature enhancement module, a multi-mode decoder module and a header network. And training the second branch network in the double-branch characteristic fine-tuning module by adopting a training set, wherein the weight parameters of the second branch network are updated along with training iteration during training. The trained target detection system detects the input image to obtain a target detection result. The invention can obviously improve the detection precision of the known class, can prevent the problem of forgetting knowledge in the fine tuning process, and can maintain the detection precision of the vision-language large model on the unknown class.","['G06V20/58', 'G06N3/084', 'G06N3/09', 'G06V10/44', 'G06V10/806', 'Y02T10/40']"
CN115238893B,Quantification method and device for neural network model oriented to natural language processing,"The invention discloses a neural network model quantification method and device for natural language processing. The method comprises the following steps: carrying out scaling parameter transfer aiming at a LayerNorm structure in the full-precision pre-training language model to obtain an equivalent floating point pre-training language model; determining a clipping range based on the floating point pre-training language model using a word-based clipping step based on a small amount of data; and calculating a quantization step size according to the cutting range to obtain a quantized pre-training language model. By using the method and the device, the pre-training language model with better quantization degree can be obtained under the condition of no extra calculation overhead, so that the required calculation overhead is obviously reduced, and the method and the device are particularly suitable for the requirement of edge equipment on low power consumption.","['G06N3/082', 'G06F16/355', 'G06F40/205', 'G06N3/04']"
CN111581350A,"Multi-task learning, reading and understanding method based on pre-training language model","The invention discloses a multi-task learning, reading and understanding method based on a pre-training language model. The method comprises the following steps: training based on a corpus to establish a pre-training language model, and obtaining context perception representation of input documents and problems by using the pre-training language model; obtaining the vector representation of each word by setting semantic information between interaction layer fusion problems formed by attention networks and documents; and performing multi-task learning based on whether the question can be answered or not and the task of obtaining the answer to obtain the result whether the question can be answered or not and the answer to the question. According to the method, the inclusion relation between sentence pairs can be obtained by establishing the pre-training language model; semantic information between the problems and the documents can be fully fused by setting an interaction layer, so that the model has better expression capability; by performing multitask learning, whether a question is answered or not can be adaptively predicted, and an answer to the question can be acquired.","['G06F16/3329', 'G06F16/3344', 'G06F18/214', 'G06F40/211', 'G06F40/30']"
US11210523B2,Scene-aware video dialog,"A scene aware dialog system includes an input interface to receive a sequence of video frames, contextual information, and a query and a memory configured to store neural networks trained to generate a response to the input query by analyzing one or combination of input sequence of video frames and the input contextual information. The system further includes a processor configured to detect and classify objects in each video frame of the sequence of video frames; determine relationships among the classified objects in each of the video frame; extract features representing the classified objects and the determined relationships for each of the video frame to produce a sequence of feature vectors; and submit the sequence of feature vectors, the input query and the input contextual information to the neural network to generate a response to the input query.","['G06N5/022', 'G06K9/00718', 'G06F16/9035', 'G06F18/2113', 'G06F18/29', 'G06K9/0063', 'G06K9/623', 'G06N3/045', 'G06V10/764', 'G06V10/82', 'G06V10/84', 'G06V20/41', 'G06N3/048', 'G06N3/08']"
US20220310077A1,"Speech recognition method, apparatus, electronic device and computer readable storage medium","A speech recognition method, an apparatus, an electronic device, and a computer-readable storage medium are provided. The method includes acquiring a first speech recognition result of a speech; acquiring context information and pronunciation feature information about a target text unit in the first speech recognition result; and acquiring a second speech recognition result of the speech based on the context information and the pronunciation feature information.","['G10L15/187', 'G10L15/005', 'G10L15/02', 'G10L15/22', 'G10L15/32', 'G10L15/16', 'G10L15/18', 'G10L2015/025', 'G10L2015/081']"
WO2021135469A1,"Machine learning-based information extraction method, apparatus, computer device, and medium","Provided are a machine learning-based information extraction method, apparatus, computer device, and medium, relating to the field of artificial intelligence, said method comprising: extracting the title, abstract, and main text of an RCT article (S202); performing data pre-processing of the main text to obtain processed text information; taking the title, abstract, and text information as fusion features, and inputting the fusion features and RCT article into a preset BERT model for training; obtaining a candidate set of coarse-grained key information, and using the candidate set of coarse-grained key information as an initial candidate set (S204); according to preset filter conditions, screening the initial candidate set to obtain a target candidate set, the text information corresponding to the target candidate set, and taking it as the key information of the RCT article (S205); the method also relates to blockchain technology; the key information of the obtained RCT article is stored in a blockchain network; the method improves the accuracy of information extraction.","['G06F40/289', 'G06F16/345', 'G06F18/24', 'G06F40/253', 'G06F40/258', 'G06F40/30', 'G06N20/00', 'G06N3/045', 'G06N3/08']"
US11563852B1,System and method for identifying complaints in interactive communications and providing feedback in real-time,"Disclosed herein are system, method, and computer program product embodiments for machine learning systems to process incoming call-center calls based on inferred sentiments. An incoming call is routed to a call agent based on an inferred topic, classified based on one or more inferred sentiments of a current caller's speech, determining, based on the call classification, that a complaint has been articulated and initiating an automated assistance by searching for one or more similar callers to the current caller. Based on finding a successful call outcome associated with one or more similar callers, the system suggests one or more phrases to the call agent for use in a dialog with the current caller to improve the one or more inferred sentiments.","['H04M3/5175', 'G10L15/08', 'H04M3/523', 'H04M2203/357', 'H04M2203/401']"
CN111177366B,"Automatic generation method, device and system for extraction type document abstract based on query mechanism","According to the invention, training data and data preprocessing are constructed by a deep learning method; pre-training a language model based on the BERT model to encode documents and query contents; word coding is carried out based on query content of the BERT model; establishing a Hierarchical structure (Hierarchical structure) model based on sentence level, and realizing semantic modeling of query content and document relation; and after model training, packaging, outputting the extracted abstract through an interface, enabling the BERT model to learn word-level feature vector representation, extracting sentences representing the documents and inquired sentences, importing the features into a transducer model for sentence-level semantic relation feature learning, learning the relation between inquired contents and the documents by combining the thought of the inquiry model, and judging through a classification function to finally obtain the abstract of the text.","['G06F16/345', 'G06F16/35', 'Y02D10/00']"
US12019981B2,Method and system for converting literature into a directed graph,"A machine reading system is described herein that includes a framework in which grammar rules can be developed using a concise language that combines syntax and semantics. The resulting technology thus reduces the development time for new grammars in a new domain. An enormous amount of information appears in the form of natural language across millions of academic papers and other literature sources. For example, in the biological domain, there is a tremendous ongoing effort to extract individual chemical interactions from these texts, but these interactions are only isolated fragments of larger causal mechanisms such as protein signaling pathways. The proposed rule-based event extraction framework can model underlying syntactic representations of events in order to extract signaling pathway fragments. Though application to the biomedical domain is herein described, the framework is domain-independent and is expressive enough to capture most complex events annotated by domain experts.","['G06F40/211', 'G06F16/245', 'G06F16/9024', 'G06F3/0482', 'G06F40/169', 'G06F40/205', 'G06F40/253', 'G06F40/279', 'G06F40/284', 'G06F40/295', 'G06N5/02', 'G06F40/30']"
US20230145535A1,Neural network training technique,"Apparatuses, systems, and techniques to train a neural network to infer a condition based on an image. In at least one embodiment, a first portion of a neural network is trained to infer a condition from an image using a first dataset, and a second portion of the neural network is trained using a second dataset.","['G06N3/08', 'G06N20/00', 'G06N3/02', 'G06N3/04', 'G06N3/048', 'G06N3/063', 'G06N3/082', 'G06N3/084', 'G06N5/04', 'G06N5/046', 'G06T7/0012', 'G06T7/11', 'G16H30/40', 'G16H50/20', 'G06N3/045', 'G06T2207/20081', 'G06T2207/20084']"
US20240267344A1,Chatbot for interactive platforms,"A chatbot system for filtering conversation content. A chatbot system receives, from a client system, a prompt of a user during an interactive session. The chatbot system filters the prompt of the user based on a set of platform policies and generates a response based on the filtering of the prompt of the user, and communicates the response to the client system.","['H04L51/02', 'H04L51/04', 'H04L51/212', 'H04L51/214']"
CA3137096A1,Computer-implemented natural language understanding of medical reports,"A natural language understanding method begins with a radiological report text containing clinical findings. Errors in the text are corrected by analyzing character- level optical transformation costs weighted by a frequency analysis over a corpus corresponding to the report text. For each word within the report text, a word embedding is obtained, character-level embeddings are determined, and the word and character-level embeddings are concatenated to a neural network which generates a plurality of NER tagged spans for the report text. A set of linked relationships are calculated for the NER tagged spans by generating masked text sequences based on the report text and determined pairs of potentially linked NER spans. A dense adjacency matrix is calculated based on attention weights obtained from providing the one or more masked text sequences to a Transformer deep learning network, and graph convolutions are then performed over the calculated dense adjacency matrix.","['G16H15/00', 'G16H10/60', 'G06N3/044', 'G06N3/045', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06N7/01', 'G16H30/40', 'G16H40/20', 'G16H50/70']"
WO2024206231A1,Multi-modal neural networks with decoder-only language models,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a multi-modal neural network using contrastive and image captioning losses.","['G06N3/0455', 'G06N3/0475', 'G06N3/0895']"
US20240330409A1,Parameter utilization for language pre-training,"Embodiments are directed to pre-training a transformer model using more parameters for sophisticated patterns (PSP++). The transformer model is divided into a held-out model and a main model. A forward pass and a backward pass are performed on the held-out model, where the forward pass determines self-attention hidden states of the held-out model and the backward pass determines loss of the held-out model. A forward pass on the main model is performed to determine a self-attention hidden states of the main model. The self-attention hidden states of the main model are concatenated with the self-attention hidden states of the held-out model. A backward pass is performed on the main model to determine a loss of the main model. The parameters of the held-out model are updated to reflect the loss of the held-out model and parameters of the main model are updated to reflect the loss of the main model.","['G06F18/253', 'G06F18/2148', 'G06F18/2163', 'G06F40/00', 'G06N3/045', 'G06N3/0464', 'G06N3/048', 'G06N3/084', 'G06F40/20']"
US12112131B2,Systems and methods for factual extraction from language model,"Embodiments described herein provide a system and method for extracting factual information. The system transforms a query into a natural language prompt in a format of a query subject and a queried relation. The system encodes, via an embedding layer of a pre-trained language model, the natural language prompt into a first embedding. The system encodes, via the adapter model, the first embedding into a second embedding based on a probability that the second embedding returns the factual information when the second embedding is fed the first attention layer of the pre-trained language model. The system decodes, by the first attention layer of the pre-trained language mode, the second embedding into a response to the query. The system extracts the factual information from the decoded response to the query.","['G06F40/279', 'G06F40/126', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/096']"
US20220229985A1,Adversarial discriminative neural language model adaptation,"Systems and methods for updating a language model are provided. One example method includes, at an electronic device with one or more processors and memory, training a first language model using a training data set comprising user-generated and user-relevant data, and storing a reference version of the first language model including a first overall probability distribution. Based on the reference version of the first language model, a second language model including a second overall probability distribution is updated (i.e., adapted) using the first overall probability distribution as a constraint on the second overall probability distribution.","['G06F40/284', 'G06F40/274', 'G06N20/00', 'G06N3/045', 'G06N3/047', 'G06N3/088', 'G06N5/04', 'G06N7/005', 'G06N7/01']"
US11853817B2,Utilizing a natural language model to determine a predicted activity event based on a series of sequential tokens,"The present disclosure relates to systems, methods, and non-transitory computer-readable media that can leverage a natural language model to determine a most probable candidate sequence of tokens and thereby generate a predicted user activity. In particular, the disclosed systems can tokenize activity event vectors to generate a series of sequential tokens that correspond to recent user activity of one or more user accounts. In addition, the disclosed systems can, for each candidate (e.g., hypothetical) user activity, augment the series of sequential tokens to include a corresponding token. Based on respective probability scores for each of the augmented series of sequential tokens, the disclosed systems can identify as the predicted user activity, a candidate user activity corresponding to one of the augmented series of sequential tokens associated with a highest probability score. Based on the predicted user activity, the disclosed systems can surface one or more suggestions to a client device.","['G06F9/542', 'G06F40/284', 'G06N3/044', 'G06N3/08', 'G06N5/02', 'G06N3/045']"
US12148119B2,Utilizing a generative neural network to interactively create and modify digital images based on natural language feedback,"The present disclosure relates to systems, non-transitory computer-readable media, and methods that implement a neural network framework for interactive multi-round image generation from natural language inputs. Specifically, the disclosed systems provide an intelligent framework (i.e., a text-based interactive image generation model) that facilitates a multi-round image generation and editing workflow that comports with arbitrary input text and synchronous interaction. In particular embodiments, the disclosed systems utilize natural language feedback for conditioning a generative neural network that performs text-to-image generation and text-guided image modification. For example, the disclosed systems utilize a trained model to inject textual features from natural language feedback into a unified joint embedding space for generating text-informed style vectors. In turn, the disclosed systems can generate an image with semantically meaningful features that map to the natural language feedback. Moreover, the disclosed systems can persist these semantically meaningful features throughout a refinement process and across generated images.","['G06T3/10', 'G06N3/04', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N3/084', 'G06N3/088', 'G06T11/00', 'G06T11/60', 'G10L15/22', 'G10L15/26', 'G06F3/167', 'G10L2015/223']"
US11210306B2,"Dialogue system, a method of obtaining a response from a dialogue system, and a method of training a dialogue system","A method of obtaining a response to a query inputted by a user, the method comprising: receiving a user inputted query; representing the user inputted query as a sequence of embedding vectors using a first model; encoding the sequence of embedding vectors to produce a context vector using a second model; retrieving responses with associated response vectors; scoring response vectors against the context vector, wherein the scoring is a measure of the similarity between the context vector and a response vector; and outputting the responses with the closest response vectors, wherein the first model is configured to segment a user inputted query into a sequence of units from a vocabulary of units and represent each unit in the sequence as an embedding vector, wherein at least one of the units in the vocabulary is an incomplete word, and wherein the first model comprises parameters that are stored using eight bits per parameter; and wherein the second model has been trained using corresponding queries and responses such that an encoding is used that maximises the similarity between the response vector and context vector for a corresponding query and response.","['G06F40/284', 'G06F16/24578', 'G06F16/243', 'G06F16/3344', 'G06F40/216', 'G06F40/237', 'G06F40/35', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'G06N3/084']"
CN117194056B,"Large language model reasoning optimization method, device, computer equipment and storage medium","The application relates to a large language model reasoning optimization method, a device, computer equipment and a storage medium, and belongs to the technical field of deep learning, wherein the method comprises the following steps: constructing a large language model based on a deep learning mechanism; converting the user input request into a basic information unit and inputting the basic information unit into a large language model; pre-filling the basic information units to obtain matrix values corresponding to the basic information units; determining a caching result of a matrix value corresponding to the basic information unit according to a dynamic matrix value caching mechanism; and based on the cache result, carrying out reasoning decoding on at least one basic information unit to realize reasoning optimization of the large language model. The method and the device can efficiently utilize the memory of the device, avoid the memory waste of the device, determine whether to cache the matrix value by using a dynamic matrix value caching mechanism, reduce the total reasoning time, avoid the memory overload and further improve the utilization efficiency of the computing device.",['Y02D10/00']
US20210141798A1,"Dialogue system, a method of obtaining a response from a dialogue system, and a method of training a dialogue system","A method of obtaining a response to a query inputted by a user, the method comprising: receiving a user inputted query; representing the user inputted query as a sequence of embedding vectors using a first model; encoding the sequence of embedding vectors to produce a context vector using a second model; retrieving responses with associated response vectors; scoring response vectors against the context vector, wherein the scoring is a measure of the similarity between the context vector and a response vector; and outputting the responses with the closest response vectors, wherein the first model is configured to segment a user inputted query into a sequence of units from a vocabulary of units and represent each unit in the sequence as an embedding vector, wherein at least one of the units in the vocabulary is an incomplete word, and wherein the first model comprises parameters that are stored using eight bits per parameter; and wherein the second model has been trained using corresponding queries and responses such that an encoding is used that maximises the similarity between the response vector and context vector for a corresponding query and response. [FIG. 2]","['G06F16/24578', 'G06F16/3344', 'G06F16/243', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'G06N3/084']"
US11681911B2,Method and system for training neural sequence-to-sequence models by incorporating global features,"Methods for training a neural sequence-to-sequence (seq2seq) model. A processor receives the model and training data comprising a plurality of training source sequences and corresponding training target sequences, and generates corresponding predicted target sequences. Model parameters are updated based on a comparison of predicted target sequences to training target sequences to reduce or minimize both a local loss in the predicted target sequences and an expected loss of one or more global or semantic features or constraints between the predicted target sequences and the training target sequences given the training source sequences. Expected loss is based on global or semantic features or constraints of general target sequences given general source sequences.","['G06N3/08', 'G06F17/18', 'G06F40/30', 'G06F40/58', 'G06N3/006', 'G06N3/044', 'G06N3/045']"
US20240144651A1,Method and apparatus for vision-language understanding,"The disclosure provides computer-implemented method for training a vision-language machine learning, ML, model to classify images depicting novel or known classes. The method comprises obtaining a first training dataset comprising a plurality of class names and training the vision-language ML model. The training method comprises: generating at least one augmented textual prompt; inputting the at least one augmented textual prompt into a frozen text encoder; outputting a first text embedding for each augmented textual prompt; generating a plurality of first inputs by concatenating each learnable soft prompt from a plurality of learnable soft prompts; inputting the class names and the plurality of first inputs into the frozen text encoder; outputting a second text embedding for each first input; and minimizing a cross-entropy text-to-text loss between the first text embeddings and the second text embeddings.","['G06N3/0455', 'G06V10/764', 'G06N3/096', 'G06V10/761', 'G06V10/776', 'G06F40/56', 'G06N3/088']"
CN112417877B,Text inclusion relation recognition method based on improved BERT,"The invention discloses a text inclusion relation recognition method based on improved BERT (belief-based language) in the technical field of natural language processing, and provides a new training target TER for further learning the inclusion, conflict, neutrality and other relations in the text inclusion relation, so as to further enhance the task-related knowledge in a pretrained language BERT model and effectively relieve the problem that the BERT model is difficult to be fully trained due to insufficient training data in the fine tuning process, a three-stage method of pretrained and task-related pretrained and fine tuned is provided, and a construction method of corresponding task-related pretrained data is provided; the TER training target and the three-stage method provided by the TER-BERT not only enable the BERT model to better learn the text implication relationship, but also can fully utilize the labeled target task corpus to construct training data and enhance the task related knowledge of the BERT model, thereby improving the performance of the BERT model in application systems such as party building question and answer systems, party building text recognition, text summarization and the like.","['G06F40/295', 'G06F40/253', 'G06F40/284', 'G06F40/30']"
US20220129621A1,Bert-based machine-learning tool for predicting emotional response to text,"Certain embodiments involve using machine-learning tools that include Bidirectional Encoder Representations from Transformers (â€œBERTâ€) language models for predicting emotional responses to text by, for example, target readers having certain demographics. For instance, a machine-learning model includes, at least, a BERT encoder and a classification module that is trained to predict demographically specific emotional responses. The BERT encoder encodes the input text into an input text vector. The classification module generates, from the input text vector and an input demographics vector representing a demographic profile of the reader, an emotional response score.","['G06F40/166', 'G06F40/30', 'G06F18/24', 'G06K9/6267', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06V10/82', 'G06V40/174', 'G10L25/63', 'G06N3/044']"
US11960848B2,Machine-learned language models which generate intermediate textual analysis in service of contextual text generation,"The present disclosure is directed to systems and methods that include and/or leverage one or more machine-learned language models that generate intermediate textual analysis (e.g., including usage of structural tools such as APIs) in service of contextual text generation. For example, a computing system can obtain a contextual text string that includes one or more contextual text tokens. The computing system can process the contextual text string with the machine-learned language model to generate one or more intermediate text strings that include one or more intermediate text tokens. The computing system can process the one or more intermediate text strings with the machine-learned language model to generate an output text string comprising one or more output text tokens. The one or more intermediate text strings can include textual analysis of the contextual text string that supports the output text string.","['G06F40/284', 'G06F16/33295', 'G06F16/338', 'G06F16/90332', 'G06F16/90335', 'G06F16/9038', 'G06F40/20', 'G06F40/279', 'G06F40/35', 'G06F8/31', 'G06F8/38', 'G06N20/00', 'G06N3/045', 'G06N3/092', 'G10L13/02']"
CN117349427A,An artificial intelligence multi-modal content generation system for responding to public opinion events,"The invention discloses an artificial intelligence multi-mode content generating system facing public opinion event coping, which applies an artificial intelligence multi-mode content generating technology to the application scene of public opinion event coping, comprising: data processing, content generation and content quality credibility evaluation. The system abandons the traditional manual public opinion coping mode, automatically processes a large amount of content generating tasks through an intelligent algorithm and technology, can quickly and accurately generate public opinion content meeting the requirements of users, improves the efficiency and quality of content generation, provides more comprehensive and diversified public opinion event coping capability, saves the time and cost of manual processing, simultaneously has an intelligent algorithm and model, can generate data driving according to public opinion data and user feedback, and provides more accurate, comprehensive and diversified content, thereby effectively improving the effect and efficiency of public opinion event coping.","['G06F16/335', 'G06F16/3329', 'G06F16/3344', 'G06F16/35', 'G06F40/237', 'G06N3/04', 'G06V10/24', 'G06V10/764', 'G06V10/774', 'G06V10/82']"
US20230350929A1,Method and system for generating intent responses through virtual agents,"A method and system for generating a response through a virtual agent is provided herein. The method comprises receiving information associated with a plurality of themes and topics. The method further comprises creating a knowledgebase based on the information received. The method further comprises analyzing the knowledgebase based on an intent identified, using an Artificial Intelligence (AI) model. Further, the method comprises generating a response corresponding to the intent through the virtual agent based on analyzation.","['G06F16/3329', 'G06N5/022', 'G06N20/00', 'G06N3/04', 'G06N3/09', 'G06N7/01']"
US12387040B2,Model for textual and numerical information retrieval in documents,"The accuracy of existing machine learning models, software technologies, and computers are improved by using one or more machine learning models to predict a type of data that one or more numerical characters and/or one or more natural language word characters of a document correspond to. For instance, a Question Answering systems can be used to predict that a particular number value corresponds to a date, a billing amount, a page number, or the like.","['G06F40/295', 'G06F40/279', 'G06F16/93', 'G06F40/30', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/096', 'G06Q10/10', 'G06Q30/04', 'G06Q40/12', 'G06N3/0442']"
US20230177878A1,Systems and methods for learning videos and assessments in different languages,"Systems and methods of converting text into learning videos and assessments in different languages are described. A system receives content in a first language, then conducts a sentiment analysis in that first language. The system then translates the content into a second language, and generates (via a text-to-speech algorithm) second language speech based on the translated content. The system transliterates the second language speech into the first language, producing one or more visemes, and determines facial expressions of an animated avatar speaking the second language speech using the sentiment analysis results and the one or more visemes. The system then generates an animated presentation with the animated avatar having the facial expressions while speaking the second language speech.","['G06V10/82', 'G06V40/176', 'G06F40/30', 'G06F40/40', 'G06F40/58', 'G06T13/40', 'G06V10/7715', 'G06V40/165', 'G06V40/166', 'G10L13/027', 'G10L21/10']"
CN119090490B,Automatic operation and maintenance method and system for electric power distribution network based on artificial intelligence,"The invention provides an automatic operation and maintenance method and system of an electric power distribution network based on artificial intelligence, which relate to the technical field of electric power operation and maintenance and comprise the steps of collecting operation data of the electric power distribution network in real time through a distributed sensor network, transmitting the collected operation data to a cloud data center to design a multi-agent reinforcement learning framework, dividing the power distribution network into a plurality of subareas, and an intelligent operation and maintenance agent making an individualized fault treatment scheme by carrying out real-time monitoring and analysis on a digital twin model and making an optimal execution plan based on the fault treatment scheme and combining real-time road condition information and positions of maintenance personnel; and respectively issuing the optimal execution plan to mobile terminals of automatic equipment and maintenance personnel through a safe encryption channel, extracting preventive maintenance rules from the historical operation and maintenance records, and formulating a preventive maintenance plan based on the extracted preventive maintenance rules and combining a pre-trained equipment life prediction model.","['G06Q10/20', 'G06F18/2433', 'G06N3/044', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N5/02', 'G06N5/04', 'G06Q10/04', 'G06Q10/063', 'G06Q50/06', 'Y04S10/50']"
WO2023038654A1,Using large language model(s) in generating automated assistant response(s),"As part of a dialog session between a user and an automated assistant, implementations can receive a stream of audio data that captures a spoken utterance including an assistant query, determine, based on processing the stream of audio data, a set of assistant outputs that are each predicted to be responsive to the assistant query, process, using large language model (LLM) output(s), the assistant outputs and context of the dialog session to generate a set of modified assistant outputs, and cause given modified assistant output, from among the set of modified assistant outputs, to be provided for presentation to the user in response to the spoken utterance. In some implementations, the LLM output(s) can be generated in an offline manner for subsequent use in an online manner. In additional or alternative implementations, the LLM output(s) can be generated in an online manner when the spoken utterance is received.","['G10L15/183', 'G06F16/90332', 'G06F40/35', 'G10L15/22', 'G10L2015/228']"
US12210825B2,Image captioning,Systems and methods for image captioning are described. One or more aspects of the systems and methods include generating a training caption for a training image using an image captioning network; encoding the training caption using a multi-modal encoder to obtain an encoded training caption; encoding the training image using the multi-modal encoder to obtain an encoded training image; computing a reward function based on the encoded training caption and the encoded training image; and updating parameters of the image captioning network based on the reward function.,"['G06F16/583', 'G06F18/214', 'G06F18/217', 'G06F40/253', 'G06V10/774', 'G06V10/82']"
US12073824B2,Two-pass end to end speech recognition,"Two-pass automatic speech recognition (ASR) models can be used to perform streaming on-device ASR to generate a text representation of an utterance captured in audio data. Various implementations include a first-pass portion of the ASR model used to generate streaming candidate recognition(s) of an utterance captured in audio data. For example, the first-pass portion can include a recurrent neural network transformer (RNN-T) decoder. Various implementations include a second-pass portion of the ASR model used to revise the streaming candidate recognition(s) of the utterance and generate a text representation of the utterance. For example, the second-pass portion can include a listen attend spell (LAS) decoder. Various implementations include a shared encoder shared between the RNN-T decoder and the LAS decoder.","['G10L15/16', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G10L15/05', 'G10L15/063', 'G10L15/22', 'G10L2015/0635']"
WO2021072852A1,"Sequence labeling method and system, and computer device","A sequence labeling method, comprising: receiving a target text sequence, and converting the target text sequence into a corresponding sentence vector, a word vector of each word and a position vector of each word (S100); inputting the sentence vector of the target text sequence, the word vector of each word and the position vector of each word into a trained BERT model, and outputting, by means of the BERT model, a first labeling sequence corresponding to the target text sequence; inputting the first labeling sequence into a fully connected layer, and outputting a second labeling sequence by means of the fully connected layer; taking the second labeling sequence as an input sequence of a conditional random field (CRF) model so as to output a label sequence Y = (y1, y2,..., ym) by means of the CRF model (S106); and generating a named entity sequence according to the label sequence, and outputting the named entity sequence (S108). According to the method, the problem of existing models being unable to consider a long-term context information relationship is solved, and thus the technical effects of directly extracting a named entity in a text by means of the model, and improving the accuracy of identifying an entity are realized.",['G06N3/045']
US12148421B2,Using large language model(s) in generating automated assistant response(s,"As part of a dialog session between a user and an automated assistant, implementations can receive a stream of audio data that captures a spoken utterance including an assistant query, determine, based on processing the stream of audio data, a set of assistant outputs that are each predicted to be responsive to the assistant query, process, using large language model (LLM) output(s), the assistant outputs and context of the dialog session to generate a set of modified assistant outputs, and cause given modified assistant output, from among the set of modified assistant outputs, to be provided for presentation to the user in response to the spoken utterance. In some implementations, the LLM output(s) can be generated in an offline manner for subsequent use in an online manner. In additional or alternative implementations, the LLM output(s) can be generated in an online manner when the spoken utterance is received.","['G10L15/183', 'G06F16/90332', 'G10L15/22']"
US11977625B2,Using multimodal model consistency to detect adversarial attacks,"A method, apparatus and computer program product to defend learning models that are vulnerable to adversarial example attack. It is assumed that data (a â€œdatasetâ€) is available in multiple modalities (e.g., text and images, audio and images in video, etc.). The defense approach herein is premised on the recognition that the correlations between the different modalities for the same entity can be exploited to defend against such attacks, as it is not realistic for an adversary to attack multiple modalities. To this end, according to this technique, adversarial samples are identified and rejected if the features from one (the attacked) modality are determined to be sufficiently far away from those of another un-attacked modality for the same entity. In other words, the approach herein leverages the consistency between multiple modalities in the data to defend against adversarial attacks on one modality.","['G06F21/554', 'G06F21/52', 'G06F21/54', 'G06F21/64', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G06F2221/033', 'G06N3/048']"
US20220414320A1,Interactive content generation,"Aspects of the present disclosure relate to techniques for interactive content generation. In examples, processed content may be produced by a generative model based on a content seed, such as a sentence or paragraph. User input associated with the processed content may be received, for example to revise the processed content or provide additional input with respect to a subpart of the processed content that is associated with a low confidence score. A generative model may produce updated processed content based at least in part on the previously processed content, the user input, and/or, in some examples, additional content, as may be indicated by a user. Thus, a user may iterate on processed content that is produced by such generative models through successive interactions, thereby enabling the user to provide input to the generative model as part of the content generation process.","['G06F40/166', 'G06F3/0481', 'G06F3/0484', 'G06F40/169', 'G06F40/30', 'G06F40/40', 'G06F40/44', 'G06F40/56']"
US20220147715A1,"Text processing method, model training method, and apparatus","This application relates to the field of artificial intelligence, and provides a text processing method, a model training method, and an apparatus. The method includes: obtaining target knowledge data; processing the target knowledge data to obtain a target knowledge vector; processing to-be-processed text to obtain a target text vector; fusing the target text vector and the target knowledge vector based on a target fusion model, to obtain a fused target text vector and a fused target knowledge vector; and processing the fused target text vector and/or the fused target knowledge vector based on a target processing model, to obtain a processing result corresponding to a target task. The foregoing technical solution can improve accuracy of a result of processing a target task by the target processing model.","['G06N3/084', 'G06F18/214', 'G06F40/205', 'G06F40/211', 'G06F40/216', 'G06F40/295', 'G06F40/30', 'G06N3/042', 'G06N3/045', 'G06N3/063', 'G06F40/237', 'G06F40/279', 'G06F40/284']"
US11132988B1,"Dialogue system, a dialogue method, and a method of training","A computer implemented method comprising: receiving input data relating to a speech or text signal originating from a user; representing the input data as a first sequence of first representations, each representing a unit of the input data; representing the input data as a second sequence of second representations, each representing one of the units of the input data; using a model to determine a tag sequence from the first sequence of first representations, wherein the model comprises an attention layer using the second sequence of second representations, wherein the tag sequence comprises one or more tags from a set of tags comprising a first tag; if one or more units of the input data correspond to the first tag, determining a system dialogue act based on the part of the input data corresponding to the first tag; and outputting speech or text information specified by the determined dialogue act.","['G06F40/35', 'G06F18/2415', 'G06K9/6277', 'G10L15/063', 'G10L15/083', 'G10L15/22', 'G10L2015/0631', 'G10L2015/223']"
US11232358B1,Task specific processing of regulatory content,"A neural network system for performing a processing task on regulatory content and a method for training the system are disclosed. The method for training involves configuring a neural network language model capable of generating a language embedding output in response to receiving content. The method further involves fine-tuning the language model using regulatory content training data to generate a regulatory content language embedding output for regulatory content processed by the language model. The method also involves configuring at least one task specific output layer to generate task specific results in response to receiving the regulatory content language embedding output from the language model, and training the neural network system using task specific training data to output the task specific results, at least a portion of the task specific training data having been labeled prior to configuring the task specific neural network.","['G06N3/08', 'G06Q50/26', 'G06F40/216', 'G06F40/279', 'G06F40/284', 'G06F40/30', 'G06N3/045', 'G06N3/084', 'G06Q10/10']"
US20250200374A1,Transformer models with optimized first layer,"This specification discloses systems and methods for enhancing the efficiency of transformer models during inference and training by precomputing and storing in memory a significant portion of operations in the first transformer layer. The stored precomputed outputs are retrieved from memory during runtime, reducing computational complexity and memory bandwidth requirements. This approach results in decreased latency, increased throughput, and lower cost-per-token. The disclosed techniques are particularly advantageous for transformer models that incorporate positional encodings within the attention mechanism, such as Rotary Position Embedding (RoPE) and other relative position encoding schemes. The method of offline precomputing involves calculating the outputs of the eliminated operations and components for each of the original vocab_size embedding-vectors, where vocab_size is the size of the embedding vocabulary. One embodiment of the invention removes the feedforward network and the attention query, key, and value projections from the first transformer layer of the encoder and the decoder stacks.","['G06N3/045', 'G06N3/0455', 'G06N3/082']"
US20250117589A1,Large Language Models for Predictive Modeling and Inverse Design,"An inverse design system combines a large language model (LLM) with a task-specific optimizer, which includes a search function, a forward model, and a comparator. The LLM adjusts parameters of the optimizer's components in response to a design scenario. Then the optimizer processes the design scenario to produce design candidates. Optionally, the LLM learns from the design candidates in an iterative process. A stochastic predictive modeling system combines an LLM with input distributions and a forward model. The LLM adjusts one or more of the input distributions and/or the forward model in response to a forecast scenario. Then the forward model processes a sampling of the input distributions to produce a forward distribution. Optionally, the LLM informs the sampling process. Optionally, the LLM learns from the forward distribution.",['G06F40/30']
CN111241304B,"Answer generation method based on deep learning, electronic device and readable storage medium","The invention relates to the technical field of intelligent decision making, and discloses an answer generation method based on deep learning, which comprises the following steps: the method comprises the steps of inputting a sentence sample into a preset language model to conduct training of mask word prediction so as to determine structural parameters of the preset language model, inputting a positive text sample and a negative text sample into the preset language model with the determined structural parameters to conduct training of sentence position prediction so as to determine sentence position weight parameters of the preset language model, obtaining a target language model, and inputting a target text and target question word segmentation processing and vectorization processing into the target language model to obtain an answer corresponding to the target question. The invention also provides an electronic device and a computer readable storage medium. The invention solves the problem that the accuracy of the answer generated during text reading and understanding application is not high enough.","['G06F16/367', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
US12339918B2,"Combined wide and deep machine learning models for automated database element processing systems, methods and apparatuses","A method of automated database element processing includes training a wide machine learning model with historical feature vector inputs to generate a wide ranked element output. The method includes training a deep machine learning model with the historical feature vector inputs to generate a deep ranked element output. The method includes generating a set of inputs specific to an individual entity, obtaining a set of current article database elements, and creating a feature vector input according to the set of inputs and the set of current article database elements. The method includes processing the feature vector input with the wide machine learning model to generate a wide ranked element list, processing the feature vector input with the deep machine learning model to generate a deep ranked element list, and merging database elements of the wide and deep ranked element lists to generate a ranked element recommendation output.","['G06F16/9535', 'G06F16/9537', 'G06F40/284', 'G06F40/30', 'G06F40/40', 'G06N3/045', 'G06N3/088', 'G06N3/084']"
US11954435B2,"Text generation apparatus, text generation learning apparatus, text generation method, text generation learning method and program","A text generation apparatus includes a memory and a processor configured to execute acquiring a reference text based on an input text and information different from the input text; and generating a text based on the input text and the reference text, wherein the acquiring and the generating are implemented as neural networks based on learned parameters.","['G06F40/56', 'G06F40/279', 'G06F16/34', 'G06F40/216', 'G06F40/284']"
US12147771B2,Topical vector-quantized variational autoencoders for extractive summarization of video transcripts,"System and methods for a text summarization system are described. In one example, a text summarization system receives an input utterance and determines whether the utterance should be included in a summary of the text. The text summarization system includes an embedding network, a convolution network, an encoding component, and a summary component. The embedding network generates a semantic embedding of an utterance. The convolution network generates a plurality of feature vectors based on the semantic embedding. The encoding component identifies a plurality of latent codes respectively corresponding to the plurality of feature vectors. The summary component identifies a prominent code among the latent codes and to select the utterance as a summary utterance based on the prominent code.","['G06F40/216', 'G06F40/35', 'G06F40/279', 'G06F40/284', 'G06F40/30']"
CN111984784B,"Person post matching method, device, electronic equipment and storage medium","The embodiment of the disclosure relates to a person post matching method, a person post matching device, computer equipment and a storage medium. The method comprises the following steps: obtaining resume delivery data; extracting structural features and unstructured features in resume delivery data, wherein the structural features comprise resume information and numerical features or category features in post information, and the unstructured features comprise semantic features or interaction features in resume information and post information; and inputting the structured features and the unstructured features into a preset person post matching model to obtain a person post matching result. The method comprises the steps of extracting structural features and unstructured features in resume delivery data, taking the structural features and the unstructured features as input, matching between posts and resume, matching numerical features and category features of obtained post matching results, and meanwhile considering semantic information, interaction information and the like between posts and resume delivery, so that information in resume delivery data is effectively utilized, and the accuracy of post matching is improved.","['G06F16/35', 'G06F18/22', 'G06F40/30', 'G06Q10/1053']"
US12373473B2,Interactive conversation assistance using semantic search and generative AI,"A method of generating content in association with an information search and retrieval system. It begins by receiving a query from a user. The query is semantically-searched to identify a context. A conversation history between the user and the system is identified. An enriched query is then generated by associating to the query both the context and at least a portion of the conversation history. The enriched query is then evaluated/processed by a generative-AI. In response, information associated with the enriched query is received from the generative-AI. A response to the query is then generated using the information, e.g., by passing the information back to the user, by modifying (e.g., editing or supplementing) the information to generate modified information and passing the modified information back to the user, or by dismissing the information. If sensitive information is identified in the utterance, it is masked prior to generating the enriched query.","['G06F16/3329', 'G06F40/30', 'G06F40/40']"
US12223264B2,Multi-layer graph-based categorization,"A method may include a obtaining a first data model instance comprising an identifier string and. a set of attributes associated with a set of attribute name strings. The method may include obtaining an ontology graph that includes a first label, a second label, and an association between them. The method may include using a prediction model to select the first label based on the first data model instance and determining the second label based on the relationship. The method may include determining a selected set of labels that includes the first label and the second label to associate with the first data model instance. The method may include associating the selected set of labels with the first data model instance in a dataset that includes a plurality of records, where each record is associated with a different data model instance.","['G06F40/20', 'G06F40/247', 'G06F16/345', 'G06F16/367', 'G06F16/9024', 'G06F40/216', 'G06F40/30', 'G06F40/44', 'G06F40/56', 'G06N20/00', 'G06N3/048', 'G06N3/084', 'G06N5/02', 'G06F16/36', 'G06N3/044', 'G06N3/045']"
WO2021139108A1,"Intelligent emotion recognition method and apparatus, electronic device, and storage medium","An intelligent emotion recognition method, comprising: obtaining a voice data set of a user, and converting the voice data set into a text data set (S1); deleting, replacing and enhancing characters in the text data set according to a preset cleaning rule to obtain a standard text data set (S2); performing text information feature extraction on the standard text data set to obtain a text sequence vector set (S3); and inputting the text sequence vector set into a pre-constructed emotion recognition model to calculate a probability distribution set of emotional states corresponding to the text sequence vector set, calculating a maximum emotional state in the probability distribution set of the emotional states by utilizing a maximum score algorithm, and recognizing the emotion of the user according to the maximum emotional state (S4). Also provided are an intelligent emotion recognition apparatus (100), an electronic device (1), and a computer readable storage medium. The recognition of the emotion of a user is realized.","['G10L25/63', 'G06F18/2415', 'G10L15/26']"
US12141556B2,Transparent and controllable human-AI interaction via chaining of machine-learned language models,"The present disclosure provides to transparent and controllable human-AI interaction via chaining of machine-learned language models. In particular, although existing language models (e.g., so-called â€œlarge language modelsâ€ (LLMs)) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, the present disclosure introduces the concept of chaining instantiations of machine-learned language models (e.g., LLMs) together, where the output of one instantiation becomes the input for the next, and so on, thus aggregating the gains per step.","['G06F8/31', 'G06F8/34', 'G06F8/35']"
US20240256622A1,Generating a semantic search engine results page,"The present disclosure relates to generating semantic search engine results. Aspects of the present disclosure retrieve relevant information from a search engine based on user's search query. The query can be a classic search query (keyword or short phrase) or a conversational query (e.g., a chat messages between users and/or chatbots), a query based upon an email or other type of message, or a query generate based upon a content item (e.g., a webpage, image, video, document, etc.). Aspects of the disclosure leverage a large language model (LLM), such as, for example, a generative model, to summarizes the content according to the intent detected from the query. In some cases, aspects of the present disclosure may generate a direct answer to the query and provide relevant references to support the information.","['G06F40/30', 'G06F16/9538']"
US11775775B2,Systems and methods for reading comprehension for a question answering task,"Embodiments described herein provide a pipelined natural language question answering system that improves a BERT-based system. Specifically, the natural language question answering system uses a pipeline of neural networks each trained to perform a particular task. The context selection network identifies premium context from context for the question. The question type network identifies the natural language question as a yes, no, or span question and a yes or no answer to the natural language question when the question is a yes or no question. The span extraction model determines an answer span to the natural language question when the question is a span question.","['G06F40/40', 'G06N3/045', 'G06F40/30']"
CN113468708B,Power distribution network auxiliary planning method and system based on CIM (common information model),"The application discloses a power distribution network aided planning method and system based on a CIM (common information model), wherein the method stores equipment parameters and topology information of a power distribution network in an operating state and a planning state in a CIM/XML (common information model/extensible markup language) file form, so that interaction with other systems can be carried out through the CIM/XML file; modeling is carried out on the power distribution network topology based on a CIM model, load flow calculation, power distribution network reliability index calculation and space mapping of a geographical wiring diagram are achieved, feeder line typical daily load flow calculation results are rendered, and power distribution network reliability index values are displayed, so that data visualization display is achieved, working efficiency is improved, and power distribution network planning synchronism is improved.","['G06F30/18', 'G06Q10/06313', 'G06Q50/06', 'G06F2113/04', 'G06F2119/02']"
WO2023029351A1,"Self-supervised learning-based method, apparatus and device for predicting properties of drug small molecules","A self-supervised learning-based method, apparatus and device for predicting the properties of drug small molecules, relating to the technical field of artificial intelligence. The method comprises: generating a molecular graph structure according to a chemical molecular structure of a target drug small molecule and determining, by using a target graph neural network model, a first feature vector corresponding to the molecular graph structure (101); extracting first molecular linear input specification data corresponding to the target drug small molecule and second molecular linear input specification data corresponding to a drug small molecule the chemical molecular structure of which is different from that of the target drug small molecule, and determining, by using a preset language model, a second feature vector corresponding to the first molecular linear input specification data and a third feature vector corresponding to the second molecular linear input specification data (102); adjusting model parameters of the target graph neural network model by using the first feature vector, the second feature vector, and the third feature vector so as to output, on the basis of the adjusted target graph neural network model, a target feature vector that corresponds to the target drug small molecule and that meets a preset feature constraint condition (103); and inputting the target feature vector into a trained property prediction model and determining a property prediction result of the target drug small molecule (104).","['G16C20/30', 'G06N3/04', 'G06N3/08', 'G16C20/20', 'G16C20/70']"
CN113140220B,Lightweight end-to-end speech recognition method based on convolution self-attention transformation network,"The invention discloses a lightweight end-to-end voice recognition method based on a convolution self-attention transformation network, which comprises the steps of firstly constructing a lightweight end-to-end voice recognition model based on the convolution self-attention transformation network, and improving the convolution self-attention transformation network by the model to form a high-efficiency convolution self-attention transformation network; using the low-rank decomposition to a feedforward layer in a convolution self-attention transformation network to form a low-rank feedforward module; the multi-head high-efficiency self-attention MHESA is provided, and the MHESA is adopted to replace the dot product self-attention in a convolutional self-attention transform network encoder; finally, a speech recognition model is obtained through training to recognize the speech. The invention reduces the computational complexity of the encoder from the attention layer to linearity, and reduces the parameter quantity of the whole model by about 50 percent, and the performance is basically unchanged.","['G10L15/063', 'Y02T10/40']"
CN110069790B,Machine translation system and method for contrasting original text through translated text retranslation,"The invention provides a machine translation system and a machine translation method for contrasting original texts through translation retranslation, and relates to the technical field of natural language processing and machine translation. The invention comprises the following steps: step 1: acquiring a source language sentence data set to be translated, wherein the data in the training set is a source language sentence with a tag sequence artificially added; step 2: establishing a translation retranslation neural network model; adjusting the translated text retranslation neural network model parameters by minimizing the value of the loss function; and 3, step 3: and taking the source language sentence E in the verification set as the input of the translated text retranslation neural network model, and outputting a sentence D. The method can reduce loss, further improve the accuracy of machine translation, and further reduce the distortion or loss of information in the translation process of the language.","['G06F40/12', 'G06F40/58']"
CN111324717B,Open domain question-answering-oriented answer acquisition method and system based on machine reading understanding,"The invention discloses an answer acquisition method based on machine reading understanding for open domain questions and answers, which adopts a semantic coding module based on BERT and an information interaction attention network to deeply capture potential semantic representations of questions and documents, effectively extract and fuse information between the questions and the documents, and capture global features of the questions and the documents; and an answer acquisition module based on the Pointer Networks is adopted, attention weight is used as a Pointer, and the starting and ending positions of the predicted answers are more accurately positioned. The answer acquisition method based on reading understanding for open domain question and answer provided by the invention carries out evidence assessment on the CMRC 2018 data set. Experimental results show that the invention can reach the standard level of open-domain question-answering tasks and obtain excellent performance.","['G06F16/3329', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
US11947912B1,Natural language processing,"Devices and techniques are generally described for determining named entity recognition tags. In various examples, first input data representing a natural language input may be determined. In some examples, a first machine learned model may determine first data comprising a first encoded representation of the first input data. In various examples, second data representing a grouping of text of the first input data may be determined based at least in part on the first data. In some examples, first entity data may be determined by searching a memory layer using the second data. In at least some examples, the first entity data and the first data may be combined to generate third data. In various examples, output data comprising a predicted named entity recognition tag may be generated for the grouping of text based at least in part on the third data.","['G06F40/30', 'G06F40/295', 'G06F40/284']"
CN114818902A,Text classification method and system based on knowledge distillation,"The invention discloses a text classification method and a text classification system based on knowledge distillation, which belong to the technical field of natural language processing, and aim to solve the technical problem of how to utilize knowledge distillation and obtain a lightweight model with equivalent precision by means of the precision advantage of a complex model, and the technical scheme is as follows: the method comprises the following specific steps: acquiring unsupervised corpora and performing data preprocessing on the unsupervised corpora; obtaining a teacher language model based on large-scale unsupervised corpus training; carrying out classification task training on the teacher language model through fine-tuning by using supervised training corpora aiming at specific classification tasks to obtain a trained teacher language model; constructing a student model according to the specific classification task and the trained teacher language model; constructing a loss function according to the intermediate layer output and the final output of the teacher language model, and training the student model to obtain a final student model; prediction of text classification using the final student model: inputting new data to predict the classification structure.","['G06F18/2415', 'G06F40/284', 'G06N3/042', 'G06N3/088']"
CN110750616B,Retrieval type chatting method and device and computer equipment,"The invention provides a retrieval type chatting method, a retrieval type chatting device and computer equipment, relates to the technical field of natural language processing, and solves the technical problem that sentences with many co-occurring characters and large difference in actual meanings influence retrieval results to obtain replies far away from expectations. The method is applied to a chat robot, a text index library and a semantic index library are prestored in the chat robot, problem indexes in the text index library are text feature indexes, and problem indexes in the semantic index library are semantic vector indexes; the method comprises the following steps: determining a first alternative answer set through text characteristics of a target question to be answered based on a text index library; determining a second alternative answer set through a semantic vector of the target question based on a semantic index library; and determining at least one target answer with the highest matching degree with the target question as a reply of the target question according to the first candidate answer set and the second candidate answer set based on a pre-trained matching model.","['G06F16/316', 'G06F16/3329', 'G06F16/3344']"
CN112215004B,Application method of transfer learning-based text entity extraction in military equipment,"The invention provides an application method of transition learning-based text entity extraction of military equipment, which comprises the following steps: step 1, establishing a network model for boundary extraction and text segment classification as a skeleton model for text entity extraction, and effectively overcoming the difference of network structures caused by different types of entity extraction in different fields; step 2, analyzing source field data, constructing a source field problem set, and realizing task adaptation; step 3, realizing field adaptation by using a language model based on a mask; and step 4, applying the model for completing the field adaptation and the task adaptation to the target field, and completing the extraction of the military equipment text information. The invention effectively overcomes the difference of network structures caused by different types of extraction entities in different fields; the invention fully utilizes the existing open source sequence labeling data, trains a named entity recognition model on the basis, applies learned knowledge to the target field, and effectively reduces the data labeling work of the target field.","['G06F40/295', 'G06F18/214', 'G06F18/2415', 'G06F40/289']"
US11507756B2,System and method for estimation of interlocutor intents and goals in turn-based electronic conversational flow,"A system and method implemented on a computing device for analyzing a digital corpus of unstructured interlocutor conversations to discover intents, goals, or both intents and goals of one or more parties to the conversations, by grouping the conversation utterances according to semantic similarity clusters; selecting the best utterance(s) that mostly likely embody a party's stated goal or intent; creates a set of candidate intent names for each cluster based upon each intent utterance in each conversation in each cluster; rates each candidate intent (or goal) for each intent name; and selects the most likely candidate intent (or goal) name for the purposes of subsequent automation of future conversations such as, but not limited to, automated electronic responses using Artificial Intelligence and machine learning.","['G06F40/35', 'G06F40/216', 'G06F40/289', 'G06F40/58', 'H04L51/02']"
CN116861921A,Robot task analysis method and device based on large language model and readable medium,"The application discloses a robot task analysis method, a device and a readable medium based on a large language model, which are characterized in that data related to a robot task are collected and preprocessed to obtain training data, wherein the training data comprises task description of natural language and corresponding task analysis results; fine tuning the large language model in a P-tuning-v2 fine tuning mode to obtain a fine-tuned large language model; constructing a prompt corresponding to training data based on requirements and characteristics of a robot task, optimizing the prompt by adopting a thinking chain and small sample learning to obtain an optimized prompt, and training a trimmed large language model by adopting the optimized prompt to obtain a robot task analysis model; acquiring real-time task description of natural language, inputting the task description of natural language into a robot task analysis model, and outputting to obtain a task analysis result. The application can accurately analyze the task description of the natural language into the execution instruction which can be understood by the robot.","['G06F40/30', 'G06F18/15', 'G06F18/214', 'G06F40/205']"
US12093311B2,Generative AI systems and methods for economic analytics and forecasting,"Generative AI systems and methods are provided to produce leading indicators of economic activity based on, for example, agricultural, fishing, mining, lumber harvesting, environmental, or ecological attributes and other factors determined from a range of available data sources. A consistent, semantic metadata structure is described as well as a hypothesis generating and testing system capable of generating predictive analytics models in a non-supervised or partially supervised mode. Users may then subscribe to the date for the use in economic forecasting.","['G06F16/587', 'G06F16/907', 'G06F18/2133', 'G06F18/217', 'G06F18/24155', 'G06F18/29', 'G06N3/0675', 'G06N3/08', 'G06N3/092', 'G06N7/01', 'G06V10/70', 'G06V10/764', 'G06V10/84', 'G06V20/13', 'G06V20/188', 'G06V20/52', 'G08G1/0133', 'G08G3/00', 'G08G5/0004', 'G08G5/20', 'G06N3/044', 'G06N3/0464', 'G06V2201/10']"
CN117577305A,"Chinese multi-round dialogue medical system, device and storage medium based on Prompt language model","The invention discloses a Chinese multi-round dialogue medical system, a device and a storage medium based on a Prompt language model, which comprise the following modules: a patient information module, a dialog engine module, and a Prompt language model module; the patient information module is interactively connected with the dialogue engine module, and the dialogue engine module is interactively connected with the Prompt language model module; the dialogue engine module realizes the dynamic update of the patient information and the real-time call of the promtt language model through the cooperative work with the patient information module and the promtt language model module, and returns the reply of the promtt language model. The three modules are connected in an encryption way through an SSL/TLS protocol, privacy of patient information is guaranteed, and seamless integration with a Prompt language model is achieved through API calling. The invention realizes the diversity treatment of the problems raised by the patients in medical treatment based on artificial intelligence, including but not limited to common symptoms, disease diagnosis, treatment advice and the like, so as to provide comprehensive medical consultation services.","['G16H50/20', 'G06F16/3329', 'G06F16/3344', 'G06N3/0455', 'G06N3/08', 'Y02D10/00']"
CN118410457A,"Multi-modal identification method and device based on large language model, electronic equipment and storage medium","The embodiment of the application discloses a multi-mode identification method, a device, electronic equipment and a storage medium based on a large language model, which belong to the technical field of artificial intelligent information processing.","['G06F18/25', 'G06F18/213', 'G06F18/214', 'G06F40/30', 'G06N3/0442', 'G06N3/0464', 'G06V10/764', 'Y02T10/40']"
US20240330863A1,"System, method, and computer program for large language model processes for deep job profile customization and talent profile customization","A system and method perform obtaining a request for generating a job description directed towards job candidates, performing pre-processing operations on the request to generate a prompt to a large language model engine, submitting the prompt to the large language model engine and receive a job description generated by the large language model engine, performing post-processing operations on the generated job description to generate a customized job description, and providing the customized job description to a user interface for presentation.","['G06Q10/063112', 'G06Q10/1053']"
US20230161879A1,Malicious code detection method and apparatus based on assembly language model,"Disclosed herein a method and apparatus for detecting a malicious code based on an assembly language model. According to an embodiment of the present disclosure, there is provided a method for detecting a malicious code. The method comprising: generating an instruction code sequence by converting an input file, for which a malicious code is to be detected, into an assembly code; embedding the instruction code sequence by using a prelearned assembly language model for instruction code embedding and outputting an embedding result of the instruction code sequence; and detecting whether or not the input file is a malicious code, by using a prelearned malicious code classification model with the embedding result as an input.","['G06F21/554', 'G06F21/562', 'G06F21/563', 'G06F2221/034']"
CN113360610A,Dialog generation method and system based on Transformer model,"The scheme is based on a Transformer model, and multi-head attention coding and full-connection layer processing are carried out on role information and conversation historical information at an encoder end; the decoder end provides an attention routing mechanism to dynamically balance the relationship among the conversation historical information, the role information and the target reply in the decoder, so that the problem of insufficient reply personalized characteristics is solved, and the personalization degree of the reply is improved to a certain extent.","['G06F16/3329', 'G06F16/355', 'G06F40/126', 'G06F40/216', 'G06N3/044', 'G06N3/08']"
CN117131070B,Self-adaptive rule-guided large language model generation SQL system,"The invention discloses a large language model generating SQL system guided by self-adaptive rules, comprising: the table structure construction module is used for column name standardization and column name dictionary construction; the reference rule base construction module is used for constructing a reference rule base and comprises a table and column name screening sub-module, a column condition extraction sub-module, a merging table nested sub-module, an SQL fragment generation sub-module and a verification sub-module; the common rule base comprises various steps from Text to SQL and reasoning logic which are commonly used in an actual service scene; the self-adaptive rule construction module is used for constructing a self-adaptive rule matched with the Text query statement; the rule guiding SQL generating module guides the large language model to gradually generate SQL sentences according to the self-adaptive rule. The invention can help non-database technicians to convert natural language query into SQL sentences, and avoid the problems of error column data screening conditions and the like caused by implicit conditions and fuzzy semantics.","['G06F16/2433', 'G06F40/242', 'G06F40/30', 'Y02D10/00']"
US20240289733A1,Large language model interface for supply chain networks,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for using a large language model as a common interface between entities in a supply chain network. One of the methods includes receiving, by a supply chain analysis system, a plurality of messages from entities in a supply chain network having a plurality of entities. Each message is provided to a large language model that is configured to generate modified messages that are in a standardized format, wherein the standardized format includes one or more data elements representing a proposed exchange in the supply chain network. The standardized messages are provided to one or more of the entities in the supply chain network to effectuate a communications interface through the large language model for entities in the supply chain network.","['G06Q10/08345', 'G06Q10/067']"
US20240378391A1,Service Platform Integration with Generative Natural Language Models,"An example embodiment may include: receiving, from an application, a request, wherein the request includes textual content; in response to receiving the request, determining a context relating to the textual content; generating, from the textual content and the context, a prompt for a natural language model; transmitting, to the natural language model, the prompt; receiving, from the natural language model, a response to the prompt, wherein the response includes information relevant to the request or programmatic commands; and providing, to the application, further textual content that is based on the response.",['G06F40/35']
US20240362421A1,Systems and methods for language model-based content classification,"Disclosed herein are methods, systems, and computer-readable media for automatically classifying and moderating content. In an embodiment, a method may include receiving input data and one or more content policies, and generating a content taxonomy. The method may also include receiving multi-domain cold start data and generating training data. The method may also include accessing a language model based on the input data and the training data, and iteratively classifying the content of the input data using the language model and the content taxonomy, refining the training data based on the classified content of the input data, refining the language model based on the refined training data, probing the refined language model, and updating the threshold value based on the probing of the refined language model. The method may also include moderating the content of the input data based on the optimized language model and the content taxonomy.","['G06F40/216', 'G06F40/40', 'G06F40/16', 'G06F40/205', 'G06F40/284', 'G06F40/30']"
US12067039B1,Systems and methods for providing user interfaces for configuration of a flow for extracting information from documents via a large language model,"Systems and methods for providing user interfaces for configuration of a flow for extracting information from documents via a large language model are disclosed. Exemplary implementations may: present a user interface configured to obtain entry of user input from a user to select a set of exemplary documents; select one or more document classifications for the set of exemplary documents; select one or more extraction fields that correspond to individual queries; navigate between different portions of the user interface; present the set of document classifications; present a particular individual document in the user interface; present a set of extraction fields in the user interface, wherein the individual extraction fields present individual replies obtained from the large language model in reply to the individual queries; and/or perform other steps.","['G06F16/3328', 'G06F16/358', 'G06V30/412']"
CN118747904A,Intelligent review method and system for business contract risks based on large-scale language model,"The invention provides a business contract risk intelligent examination method and system based on a large-scale language model, comprising the following steps: defining a requirement boundary of risk examination, and determining a risk type to be identified; extracting text information from the contract file; setting up a BRAT text labeling platform, and labeling by taking a single contract document as a basic labeling unit and taking a paragraph object as a minimum labeling unit; training a semantic correlation model; designing an independent instruction template for each risk rule; training a language model base; after the training of the semantic correlation model and the language model is completed, the risk examination is carried out on the combination. The invention reduces human error: manual inspection is susceptible to factors such as fatigue, distraction, etc., resulting in errors and omissions. The method can reduce human errors and improve the consistency and accuracy of examination.","['G06V30/416', 'G06F18/10', 'G06F40/30', 'G06Q50/18', 'G06V30/19007', 'G06V30/19147']"
US20220327356A1,Transformer-Based Model Knowledge Graph Link Prediction,"A system, product, and method are provided for improving knowledge graph (KG) link prediction using transformer-based artificial neural networks. A first topic model is leveraged against a first dataset derived from a KG containing a plurality of first triples. The first triples include first entities and first edges connecting the first entities to represent relationships between the first connected entities. A first similarity function is applied to the first connected entities of the first triples to provide respective first similarity scores. A first subset of one of more first triples is selected from the plurality of first triples based upon the first similarity scores. An artificial neural network is trained using the selected first subset of one or more first triples.","['G06N3/0427', 'G06N5/025', 'G06N3/042', 'G06F16/284', 'G06F16/9024', 'G06N3/045', 'G06N3/08', 'G06N3/088']"
US11907672B2,Machine-learning natural language processing classifier for content classification,"Computer-readable media, systems and methods may improve classification of content based on a machine-learning natural language processing (ML-NLP) classifier. The system may train a general language model based on a general corpus, further train the general language model based on a domain-specific corpus to generate a domain-specific language model, and conduct supervised machine-learning based on the domain-specific language using topic-specific corpus labeled as relating to topics of interest to generate the ML-NLP classifier. Accordingly, the ML-NLP classifier may be trained on a general corpus, further trained on a domain-specific corpus, and fine-tuned on a topic-specific corpus. In this manner, domain-specific content may be classified into topics of interest. The ML-NLP classifier may classify content into the topics of interest.","['G06F16/35', 'G06F40/40', 'G06F40/30', 'G06N3/045', 'G06N3/088']"
CN111914067B,Chinese text matching method and system,"The embodiment of the invention provides a Chinese text matching method. The method comprises the following steps: carrying out character-level coding on the Chinese sentence pairs by using a plurality of word segmentation tools to obtain initial character vectors of the Chinese sentence pairs; inputting an initial word vector of a Chinese sentence pair into an input layer, and determining semantic representation of the word vector based on a knowledge network external knowledge base; respectively carrying out iterative updating on word lattices of the semantic representation and the word vectors through a multidimensional graph attention network, and outputting semantic word vectors with the semantic representation; inputting the semantic word vector into a sentence matching layer, and determining a final feature representation semantic word vector of the Chinese sentence pair; a match probability is determined for the feature representations of the chinese sentence pair based on the final feature representation semantic word vector of the chinese sentence pair and the plurality of word segmentation tools. The embodiment of the invention also provides a Chinese text matching system. According to the embodiment of the invention, the semantic information in the HowNet external knowledge base is integrated into the model, so that the semantic information in sentences is better utilized, and the matching effect is obviously improved.","['G06F16/3344', 'G06F40/289', 'G06F40/30', 'G06N3/08']"
US11637928B2,Method and apparatus for summarization of dialogs,"A method for summarizing dialogs may include obtaining an agent text stream and a customer text stream, segmenting the agent text stream and customer text stream into sentences, and labeling sentences associated with the segmented agent text stream and the segmented customer text stream. The method may further include extracting sentences from the agent text stream and the customer text stream based upon frequencies of appearance of words and terms of interest; generating an agent summary paragraph based on the extracted sentences from the agent text stream, and generating a customer summary paragraph based on the extracted sentences from the customer text stream. The method may identify keywords associated with each of the agent summary paragraph and the customer summary paragraph.","['H04M3/5183', 'G06Q30/0281', 'G06F40/20', 'G06F40/30', 'G06N3/049', 'H04M3/2218', 'H04M3/5175', 'G06N3/045', 'H04M2201/40', 'H04M2203/301', 'H04M2203/303', 'H04M2203/559']"
AU2022227854A1,Automated classification of emotio-cogniton,"A system and method for detecting a psychological affect in a natural language content with a rule-based engine includes receiving the natural language content as a textual input; searching for matches between linguistic rules for a given emotio-cognition and components of the natural language content, wherein instances of the linguistic rules have human dimensions; activating the matched linguistic rules, and evaluating the human dimensions of the matched rules; scoring each human dimension to obtain a profile of dimension scores for the given emotio-cognition; aggregating the dimensions in the obtained profile of dimension scores to obtain an intensity indication for the given emotio-cognition; and displaying the natural language content in a manner that relates the matched linguistic rules in conjunction with the given emotio-cognition and respective intensity indication of the given emotio-cognition.","['G06F40/30', 'G06F1/163', 'G06F1/1684', 'G06F18/24', 'G06F3/0488', 'G06F3/167', 'G06F40/169', 'G06F40/216', 'G06F40/268', 'G06F40/284', 'G06V20/40', 'G06V20/41', 'G06V30/10', 'G10L15/18', 'G10L25/57', 'G10L25/63', 'G16H50/20', 'G06F2203/011', 'G10L15/26']"
WO2021164310A1,"Text error correction method and apparatus, and terminal device and computer storage medium","A text error correction method and apparatus, and a terminal device and a computer storage medium, which are applicable to the technical field of artificial intelligence. In the method, before a decoder in an encoder-decoder model performs decoding, label classification needs to be performed first on each input word vector by using an error correction determination model so as to obtain an error correction label of each input word vector, wherein the error correction label is used for indicating whether a corresponding word needs to be subjected to error correction; and after a terminal device obtains the error correction label corresponding to each input word vector in input text, the error correction label corresponding to each input word vector is input into the decoder, such that the decoder can perform targeted decoding according to the error correction label corresponding to each input word vector so as to regulate and control a decoding process, thereby reducing misjudgements of a decoder, improving the accuracy of text error correction, and solving the problem of an existing decoding process of an encoder-decoder model being uncontrollable and being prone to producing misjudgements.","['G06F18/22', 'G06F40/126', 'G06F40/194', 'G06F40/289', 'G06F40/30', 'G06N3/04', 'G06N3/08']"
US20220138432A1,Relying on discourse analysis to answer complex questions by neural machine reading comprehension,"An autonomous agent receives a user query comprising the complex question. The agent can obtain, from a corpus of unstructured texts, an answer candidate text corresponding to the user query and comprising text from which the answer is subsequently identified. The agent may generate first linguistic data corresponding to the user query and second linguistic data corresponding to the answer candidate text. Each instance of linguistic data may comprise a combination of respective syntactic data, semantic data, and discourse data generated from the user query and/or answer candidate text. Both instances of linguistic data may be provided to a machine-learning model that has been previously trained to output an answer identified from an instance of unstructured text (e.g., the answer candidate text). The model may output the answer identified from the answer candidate text, which in turn may be provided in response to the user query.","['G06F40/35', 'G06F40/30', 'G06F16/3329', 'G06F16/3344', 'G06F40/211', 'G06F40/216', 'G06F40/284', 'G06F40/295', 'G06N20/00', 'G06N3/04', 'G06N3/045', 'G06N3/08', 'G06N5/04']"
US11604626B1,Analyzing code according to natural language descriptions of coding practices,Code may be analyzed according to natural language descriptions of coding practices. A practice for code written in a natural language description may be received. An embedding of the natural language description may be generated using a machine learning model trained to detect examples of practices. The embedding may be compared with embeddings of code portions stored in an index to detect one or more portions of code that satisfy a facet of the practice. The detected portions of code may be identified.,"['G06F8/33', 'G06F8/75', 'G06F8/36', 'G06F8/38', 'G06N20/00', 'G06N3/045', 'G06N3/09', 'G06N3/096']"
US10817530B2,"Systems, methods, and devices for an enterprise internet-of-things application development platform","Systems, methods, and devices for a cyberphysical (IoT) software application development platform based upon a model driven architecture and derivative IoT SaaS applications are disclosed herein. The system may include concentrators to receive and forward time-series data from sensors or smart devices. The system may include message decoders to receive messages comprising the time-series data and storing the messages on message queues. The system may include a persistence component to store the time-series data in a key-value store and store the relational data in a relational database. The system may include a data services component to implement a type layer over data stores. The system may also include a processing component to access and process data in the data stores via the type layer, the processing component comprising a batch processing component and an iterative processing component.","['G06F16/254', 'G06F16/25', 'G06F16/283', 'G06F16/288', 'G06F8/10', 'G06F8/24', 'G06F8/35', 'G06F8/77', 'G06F9/54', 'G06N20/00', 'G06N20/20', 'G06N5/01', 'G06Q10/06', 'H04L67/10', 'H04L67/12', 'H04L67/53', 'H04L67/566', 'H04L67/60', 'H04L67/61', 'H04L69/16', 'H04L69/40', 'H04L29/08783', 'H04L29/08792', 'H04L67/565', 'H04L67/5651', 'Y02P90/80', 'Y02P90/84', 'Y04S40/18']"
US11983806B1,Systems and methods for image generation with machine learning models,"Disclosed herein are methods, systems, and computer-readable media for regenerating a region of an image with a machine learning model based on a text input. Disclosed embodiments involve accessing a digital input image. Disclosed embodiments involve generating a masked image by removing a masked region from the input image. Disclosed embodiments involve accessing a text input corresponding to an image enhancement prompt. Disclosed embodiments include providing at least one of the input image, the masked region, or the text input to a machine learning model configured to generate an enhanced image. Disclosed embodiments involve generating, with the machine learning model, the enhanced image based on at least one of the input image, the masked region, or the text input.","['G06T5/60', 'G06T11/00', 'G06T11/60', 'G06T5/00', 'G06T5/77', 'G06V10/77', 'G06V10/945', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/20221']"
US12236200B2,Privacy-preserving text insight mining in a closed domain,An embodiment provides a method including obtaining language input data and providing the language input data to a first generative language model and a second generative language model. A first response from the first generative language model and a second response from a second generative language model are obtained. An indication is provided of a difference between the first response from the first generative language model and the second response from the second generative language model.,"['G06F40/40', 'G06F40/289', 'G06F40/56', 'G06N20/00', 'G06N3/045', 'G06N3/0475', 'G06N3/096', 'G06F40/30']"
US20240161017A1,Connectome Ensemble Transfer Learning,"The present disclosure describes a method of Connectome Ensemble Transfer Learning (CETL), which makes connectome-based predictive models useful for precision mental healthcare. CETL comprises a novel transfer learning process that incrementally trains Connectome Ensemble Predictive Models (CEPMs) by leveraging information from source domains to improve predictive performance in target domains. The disclosed methods broadly comprise selecting target and source domains, obtaining network connectivity data from individual persons, sampling source ensemble representations of connectome â€œviewsâ€ from the obtained network connectivity data of said persons in the source domain, reducing the dimensionality of the sampled connectome â€œviewsâ€, and transferring the distilled representations to the target domain to train more robust, generalizable, and clinically deployable CEPMs that predict diverse target mental health phenotypes. Implemented through massively parallel distributed computing, a system of synchronized computer hardware implementing this method is also disclosed.","['G06N20/20', 'G06N3/096', 'G06N3/042', 'G06N3/045', 'G06N3/09', 'G06N5/01', 'G06N5/02', 'G06N7/01', 'G16H15/00', 'G16H30/20', 'G16H30/40', 'G16H40/67', 'G16H50/20', 'G16H50/30', 'G16H50/70', 'G06N3/0895']"
WO2022037256A1,"Text sentence processing method and device, computer device and storage medium","A text sentence processing method, comprising: acquiring sample text sentences containing entity pairs and relationship labels of the entity pairs; extracting positive-example sentence pairs and negative-example sentence pairs from the sample text sentences according to the relationship labels, and performing positive-negative example sampling to obtain a training set; inputting the training set into a relationship extraction model to be trained, and generating loss values comprising a contrastive loss value, the contrastive loss value representing the difference between the similarity between sentences in the positive-example pairs and the difference between the similarity between sentences in the negative-example pairs; and adjusting the parameters in the relationship extraction model according to the loss values, returning the steps for extracting the positive-example sentence pairs and the negative-example sentence pairs from the sample text sentences according to the relationship labels, performing iterative training until a training stop condition is met, and obtaining a relationship extraction model for recognizing the entity relationship of the entity pairs in the text sentences.","['G06F40/279', 'G06F40/166', 'G06F40/194', 'G06F40/30']"
US20220391585A1,Multi-modal language interpretation using unified input model,"Systems and processes for multi-modal input interpretation are provided. For example, an input associated with a touch is received from a user. A first reconstruction based on the input is determined. A first simulated input is obtained based on a modification of the input. A second reconstruction is determined based on the first reconstruction and the first simulated input. Based on at least the first reconstruction and the second reconstruction, a probability representation is obtained. An output is determined, by a language model, based on the probability representation. The output is then provided to the user.","['G06F40/274', 'G06F17/18', 'G06F3/0237', 'G06F3/04842', 'G06F3/04883', 'G06F3/04886', 'G06F40/279', 'G06F40/30', 'H04L51/063', 'H04L51/04']"
CA3129720C,Method for serving parameter efficient nlp models through adaptive architectures,"A machine learning system executed by a processor may generate predictions for a variety of natural language processing (NLP) tasks. The machine learning system may include a single deployment implementing a parameter efficient transfer learning architecture. The machine learning system may use adapter layers to dynamically modify a base model to generate a plurality of fine-tuned models. Each fine-tuned model may generate predictions for a specific NLP task. By transferring knowledge from the base model to each fine-tuned model, the ML system achieves a significant reduction in the number of tunable parameters required to generate a fine- tuned NLP model and decreases the fine-tuned model artifact size. Additionally, the ML system reduces training times for fine-tuned NLP models, promotes transfer learning across NLP tasks with lower labeled data volumes, and enables easier and more computationally efficient deployments for multi-task NLP.","['G06N20/20', 'G06N20/00', 'G06F40/126', 'G06F40/284', 'G06N3/0495', 'G06N3/082', 'G06N3/0895', 'G06N3/09', 'G06N3/096', 'G06F40/279', 'G06F40/30', 'G06N3/08']"
CN118824432A,Method and system for quantitative prediction and design of new materials assisted by large language model,"The invention relates to the field of material performance prediction, and provides a method and a system for designing a new material by large language model assisted quantitative prediction, wherein the method comprises the following steps: s1, collecting documents, and preprocessing to obtain preprocessed corpus; s2, training a word segmentation device; s3, training a large language model; s4, obtaining a process feature code; s5, obtaining component feature codes; s6, constructing and training a neural network prediction model; s7, predicting the performance of the new material. The system comprises a corpus preprocessing module, a word segmentation training module, a large language model training module, a process feature coding module, a component feature coding module, a neural network prediction model training module and a material performance prediction module. The invention breaks through the problems of structural alignment, high-dimensional sparsity and the like faced by the traditional machine learning method in material preparation process route representation, ensures quantitative and accurate prediction of material performance while considering flexible representation of process route, and provides reliable tools and approaches for new material design and process optimization.","['G16C60/00', 'G06F40/242', 'G06F40/284', 'G06F40/30', 'G06N3/0464', 'G06N3/048', 'G06N3/082', 'G06N3/084', 'G16C20/70', 'G16C20/90', 'Y02P90/30']"
US20230306284A1,"Stateful, Real-Time, Interactive, and Predictive Knowledge Pattern Machine","This disclosure describes a knowledge pattern machine that is distinct from and goes beyond a traditional artificially intelligent predictive knowledge system employing simple domain-specific numerical regression models. Rather than generating purely quantitative projections within a static set of parameters and data, the disclosed pattern machine uses various layers of artificial intelligence to recognize and derive dynamically evolving predictive patterns and correlations among quantitative and/or qualitative information pertaining to one or multiple domains. The pattern machine extracts knowledge items, including various signals, events, properties, and correlations therebetween, and predicts future trends and evolvements of relating knowledge items to automatically and intelligently answer user queries. The generated predictive answers are rendered as reports updated in real-time without user interference as the underlying data sources evolve over time and are sharable among different users at various levels. The various knowledge items are timestamped and used to further yield a stateful pattern machine.","['G06F3/04847', 'G06N5/022', 'G06F16/24534', 'G06F16/24564', 'G06F16/248', 'G06F3/0482', 'G06F40/30', 'G06N20/00']"
US20250285015A1,SYSTEMS AND METHODS FOR ALIGNING LARGE MULTIMODAL MODELS (LMMs) OR LARGE LANGUAGE MODELS (LLMs) WITH DOMAIN-SPECIFIC PRINCIPLES,A system and method aligns generative artificial intelligence (a large language model (LLM) or a large multimodal model (LMM) with the principles of a specific domain so that the generative artificial intelligence is better able to respond to a user query in the specific domain. The system and method may post-train an already trained generative artificial intelligence system or fine tune the training of the generative artificial intelligence system to align that generative artificial intelligence system with the principles of the specific domain. The system and method may be used to align the generative artificial intelligence system to a plurality of different domains.,"['G06N20/00', 'G06N3/045', 'G06N3/08']"
CN116775922B,Cross-modal retrieval method of remote sensing images based on fusion of language and visual detail features,"The invention relates to a remote sensing image cross-modal retrieval method based on language and visual detail feature fusion, which comprises the following steps of 1, processing training data of a remote sensing image-text retrieval model, 2, constructing a multi-detail language and visual fusion model, 3, training a multi-target optimized detail language and visual fusion model, 4, constructing a remote sensing image-text description feature library, and 5, completing cross-modal retrieval of remote sensing image-text description. According to the method, the image and text features are respectively represented by utilizing a single-mode encoder, the features of two modes are fused by utilizing a multi-mode encoder, the expression of fine-granularity semantic features of the corresponding mode data by each encoder is improved through feature fusion and multi-task optimization training, cross-mode retrieval is completed through similarity calculation of the semantic features, and the model is enabled to have the feature expression capability of multiple details for remote sensing images and text description by designing a multi-objective optimization strategy for the model.","['G06F16/535', 'G06F16/5846', 'G06N20/00']"
WO2023129897A1,Mask patterns for protein language models for predicting pathogenicity,"The technology disclosed relates to accessing a multiple sequence alignment that aligns a query residue sequence to a plurality of non-query residue sequences, applying a set of periodically-spaced masks to a first set of residues at a first set of positions in the multiple sequence alignment, and cropping a portion of the multiple sequence alignment that includes the set of periodically-spaced masks at the first set of positions, and a second set of residues at a second set of positions in the multiple sequence alignment to which the set of periodically-spaced masks is not applied. The first set of residues includes a residue-of-interest at a position-of-interest in the query residue sequence.","['G16B20/20', 'G16B30/10', 'G16B40/20']"
CN113128199B,Word vector generation method based on pre-training language model and multiple word information embedding,"The invention provides a word vector generation method based on a pre-training language model and multi-word information embedding, which comprises the steps of crawling relevant unmarked data according to marked data topics; constructing a pre-training language model, pre-training labeled data and unlabeled data, and processing an input sentence based on the pre-training language model to obtain a word vector; extracting multiple word information characteristics from the labeled data; and fusing the obtained word vector and the multiple word information characteristics to obtain a final word vector. The word vectors are represented by the pre-training language model, so that the polysemous information can be better introduced; the multi-word information characteristics are constructed and added into the word vectors, word information and word segmentation information are brought to the word vectors, and the natural language processing effect is improved.","['G06F40/205', 'G06F16/951', 'G06F40/284']"
CN113435195B,Defect intelligent diagnosis model construction method based on main transformer load characteristics,"The invention discloses a defect intelligent diagnosis model construction method based on main transformer load characteristics, which comprises the following steps: (1) constructing a defect diagnosis system: summarizing the defect characteristics of the equipment to form a defect diagnosis system table; (2) defect diagnosis model: a) Establishing equipment defect diagnosis data indexes; b) Text preprocessing: according to the dictionary of the electric power field, word segmentation is adopted to obtain a word segmentation result of the electric power field; c) Text distributed representation: the text distributed expression method trains a language model of word vector expression of each word based on the principle that the semantics of the word is characterized by the adjacent word; d) Constructing a convolutional neural network algorithm; e) Model training: and learning by using a convolutional neural network algorithm to form a final equipment defect diagnosis model. The invention realizes the accurate identification and positioning of the defect reasons and defect parts of the equipment and assists the power enterprises to intelligently control the primary equipment of the power grid.","['G06F40/284', 'G06F16/35', 'G06N3/045', 'G06N3/08', 'G06Q50/06', 'Y04S10/50']"
CN115471851B,Burmese image text recognition method and device integrating dual attention mechanisms,"The invention relates to a method and a device for identifying a Myanmar language image text by fusing a dual-attention mechanism, and belongs to the field of natural language processing. The invention provides a Burmese image text recognition method integrating a dual attention mechanism, which mainly comprises Burmese image data set construction and preprocessing, burmese image feature enhancement integrating channel attention and space attention, burmese image text recognition model construction based on a multi-head attention decoder and Burmese image text recognition. Compared with the universal image text recognition method, the method and the device for recognizing the image text of the Myanmar, which are made by combining the four partial function modularization, effectively solve the problem of low recognition accuracy caused by the fact that the characters of the upper and lower marks of the characters in the Myanmar image are missing.","['G06V30/2445', 'G06N3/08', 'G06V30/18', 'G06V30/1801', 'G06V30/19147', 'G06V30/1918']"
CN110838288B,Voice interaction method and system and dialogue equipment,"The invention discloses a voice interaction method, which comprises compiling a dialogue story, simulating a natural dialogue process and compiling a human-computer dialogue template; preparing language understanding data, classifying possible input sentences of a user according to the represented intentions, and marking out entities contained in the sentences; preparing domain language model data for correcting the user sentence obtained by the recognition; training the voice recognition capability of the voice recognition module based on the end-to-end model; correcting the sentences identified by the voice identification module in a sentence correction module; classifying and identifying the corrected sentences in a language understanding module by using language understanding data; and training a dialogue management model of the dialogue management module by using the user story data and the domain knowledge. The invention also discloses a voice interaction system and a dialogue device, and the voice interaction method is adopted. The invention can effectively reduce the complexity of the system architecture model, does not need to predefine all possible conversations, and has easy enumeration and good functionality.","['G10L15/063', 'G10L15/16', 'G10L15/18', 'G10L15/22', 'G10L15/26']"
US11521200B1,Creating and managing artificially intelligent entities represented by non-fungible tokens on a blockchain,"Some examples of the present disclosure relate to generating artificially intelligent entities represented on a blockchain using a non-fungible token (NFT). In one such example, a system can generate an NFT on a blockchain. The NFT can represent an artificially intelligent entity. The system can also generate a personality dataset on the blockchain, the personality dataset describing personality characteristics of the artificially intelligent entity. The system can then correlate the NFT to the personality dataset, thereby assigning the personality characteristics to the artificially intelligent entity. Once generated, the artificially intelligent entity may reside in a virtual ecosystem in which it can perform tasks and learn over time.","['G06Q20/38215', 'G06Q30/0209', 'G06Q20/065', 'G06Q20/223', 'G06Q20/36', 'G06Q20/367', 'G06Q20/3672', 'H04L9/3213', 'G06Q2220/00', 'H04L2209/56', 'H04L2209/603', 'H04L9/50']"
CN112329467B,"Address recognition method and device, electronic equipment and storage medium","The embodiment of the application discloses an address identification method and device, electronic equipment and a storage medium, and the method and device can be applied to the fields of artificial intelligence, big data, maps and the like. The method comprises the following steps: acquiring an address text to be identified; acquiring identification guide information of an address text to be identified, wherein the identification guide information comprises at least one item of basic information of words contained in the address text to be identified, identification information of target address words or characteristic information of words, and the identification information of the target address words represents identification results of the target address words; and obtaining an address role recognition result of the address text to be recognized according to the address text to be recognized and the recognition guidance information. By adopting the method and the device, the address character recognition result of the address text to be recognized can be obtained through the recognition guidance information of the address text to be recognized and the recognition guidance information of the address text to be recognized, and the accuracy is high.","['G06F40/295', 'G06N3/045', 'G06N3/08']"
EP4433950A1,System(s) and method(s) to reduce a transferable size of language model(s) to enable decentralized learning thereof,"Implementations disclosed herein are directed to techniques for enabling decentralized learning of global language models (LMs). Remote processor(s) of a remote system can obtain a global LM that includes a global embedding matrix, generate a global embedding mask for the global embedding matrix using a masking technique, apply the global embedding mask to global embedding matrix to generate a sparsified global LM that includes a masked global embedding matrix that is a masked version of the global embedding matrix, transmit the sparsified global LM to computing device(s) that are participating in a given round of decentralized learning for the global language model, receive corresponding updates from the computing device(s), and cause the global LM to be updated based on the corresponding updates. By generating the global embedding mask and applying it to the global embedding matrix, the transferable size of the global LM is reduced thereby enabling decentralized learning thereof.","['G06F40/20', 'G06F40/216', 'G06F40/30', 'G06F40/40', 'G06N3/044', 'G06N3/045', 'G06N3/082', 'G06N3/0895', 'G06N3/09', 'G06N3/096', 'G06N3/098']"
US11893347B2,Contrastive meta-learning for zero-shot learning,"Disclosed herein are system, method, and computer program product embodiments for utilizing non-RAM memory to implement machine learning configured with a meta-learning training set (small dataset), to create a common-sense predictive language model, thus boosting the performance for downstream tasks. An embodiment operates by receiving a base sentence and perturbation sentences as an input and tokenizing the input to generate a sequence of tokens. Tokens of the semantic perturbation sentences are embedded with tokens of the base sentence as contextually similar tokens pairs to generate training data and classified to capture relationships of the base sentence and the perturbation sentences to generate a classification, which is used to train a language model.","['G06F40/284', 'G06F40/30', 'G06N20/00', 'G06N3/0455', 'G06N3/0985', 'G06F40/216']"
CN113010693B,Knowledge graph intelligent question-answering method integrating pointer generation network,"A knowledge graph intelligent question-answering method integrating a pointer generation network belongs to the field of artificial intelligent question-answering. The technical scheme is as follows: using a word segmentation tool to segment and check the original text and question in the WebQA data set; carrying out named entity recognition on the data after correct word segmentation by using a BiLSTM-CRF model; querying a triplet corresponding to the identified entity in a Neo4j database; counting the occurrence frequency of each word in the corresponding triplet, and storing the words in the queried triplet into a knowledge word list according to the word frequency order; obtaining word vectors of questions by using a deep learning method; and constructing a generative model and returning an answer. The method has the beneficial effects that the method uses the deep learning technology to identify the text, uses the knowledge graph technology to quickly inquire the knowledge, and combines the generated model to effectively solve the problems of hard answer, singleness and incomplete storage in the knowledge base; the time for obtaining the answer is saved, the intention of the user is more fully understood, and the answer which is more in line with the reading mode of the user is returned.","['G06F16/367', 'G06F16/3329', 'G06F18/2415', 'G06F40/216', 'G06F40/295', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'G06N3/048', 'G06N3/08']"
US20210124876A1,Evaluating the Factual Consistency of Abstractive Text Summarization,"A weakly-supervised, model-based approach is provided for verifying or checking factual consistency and identifying conflicts between source documents and a generated summary. In some embodiments, an artificially generated training dataset is created by applying rule-based transformations to sentences sampled from one or more unannotated source documents of a dataset. Each of the resulting transformed sentences can be either semantically variant or invariant from the respective original sampled sentence, and labeled accordingly. In some embodiments, the generated training dataset is used to train a factual consistency checking model. The factual consistency checking model can classify whether a corresponding text summary is factually consistent with a source text document, and if so, may identify a span in the source text document that supports the corresponding text summary.","['G06F16/345', 'G06F18/2155', 'G06F40/268', 'G06F40/30', 'G06K9/6259']"
CN111477216B,Training method and system for voice and meaning understanding model of conversation robot,"The invention relates to the field of meaning understanding, and discloses a training method and a training system for a meaning understanding model of a conversation robot, which solve the problems of insufficient voice marking data and poor generalization performance of the model in the current meaning understanding model training, and are characterized in that training words and sentences are written according to appointed intentions; performing data enhancement on the text of the training words and sentences to generate a plurality of synonymous word and sentence text data with the same intention as the training words and sentences; performing voice synthesis on the synonym text data to obtain corresponding training voice data; by training the voice meaning understanding model based on the deep neural network by using the training voice data, the effects of reducing the workload of manually marking the voice data and improving the accuracy of intention recognition and the generalization performance are achieved.","['G10L15/063', 'G06F16/3329', 'G06N3/045', 'G10L13/08', 'G10L15/22', 'G10L25/30', 'Y02D10/00']"
CN117370516A,Method for enhancing dialogue system training based on hierarchical comparison learning knowledge,"The invention relates to a method for enhancing dialogue system training based on hierarchical comparison learning knowledge, which comprises the following steps: firstly, selecting dialogue data and knowledge data, and processing the dialogue data and the knowledge data to obtain input data of a language model; secondly, generating a reply by the language model according to the input data, wherein the process of generating the reply comprises the steps of selecting proper language model processing data and selecting proper decoding algorithm to generate the reply; finally, using hierarchical contrast learning and its extended return gradient to update model parameters. On the basis of using a pre-trained language model, hierarchical contrast learning constraints and extended anchor loss constraints thereof are applied to the model in the training process. On the basis of guaranteeing the language grammar level quality, the text word representation is improved, the knowledge and dialogue text space are integrated, and the knowledge neuron distribution is adjusted, so that the accuracy of the dialogue system on the use of external knowledge is improved, and the dialogue reply generation quality is further improved.","['G06F16/3329', 'G06F40/284', 'G06N3/045', 'G06N3/0895', 'Y02D10/00']"
CN111125331B,"Semantic recognition method, device, electronic equipment and computer-readable storage medium","The embodiment of the application provides a semantic recognition method, a semantic recognition device, electronic equipment and a computer readable storage medium, wherein the semantic recognition method comprises the following steps: acquiring a target sentence to be identified and inputting a characterization model based on a bi-directional encoder; running a representation model based on a bidirectional encoder, and determining entity information and intention information of a target sentence; and determining the meaning of the target sentence according to the entity information and the intention information. According to the semantic recognition method provided by the application, the entity information of the target sentence is recognized and the intention information is recognized at the same time based on the bi-directional encoder characterization model, so that the processing structure of semantic recognition of natural language in the artificial intelligence field is changed, the original two-step processing mode is improved to a single-step processing mode, and the recognition efficiency of artificial intelligence on sentence meaning is improved to a great extent.","['G06F16/3329', 'G06F16/3344', 'G06F16/367', 'G06N3/044', 'G06N3/045', 'Y02D10/00']"
US20240203599A1,Method and system of for predicting disease risk based on multimodal fusion,"A method and system of predicting disease risk based on multimodal fusion, the method comprises: obtaining electronic health record (EHR) data of the patient, inputting the EHR data into the disease risk prediction model to obtain the disease risk prediction result; and outputting the disease risk prediction result; wherein, the disease risk prediction model performing steps of: identifying the EHR data as the structured data and the unstructured data; performing the data cleaning on the structure data and the unstructured data; extracting structured data features and unstructured data features; extracting fusion features, wherein the fusion features are features fusing the unstructured data feature and the structured data feature; and, performing the disease risk prediction on the fusion features.","['G16H50/30', 'G06F16/35', 'G16H10/60', 'G16H50/20', 'G16H50/70', 'Y02A90/10']"
AU2024220201A1,"Systems, methods, kits, and apparatuses for digital product networks in value chain networks","A system may include a product-to-product communication module configured to exchange inter-product communications for a plurality of digitally connected products. A system may include a product-to-user communication module configured to exchange product-to-user communications between the plurality of digitally connected products and their respective users. A system may include a product-to-business communication module configured to exchange product-to-user communications between the plurality of digitally connected products and their associated enterprises. A system may include a data processing module configured to process the inter-product communications, product-to-user communications, and the product-to-business communications to determine time-sensitive alerts related to corresponding one of the plurality of digitally connected products. A system may include a graphical user interface (GUI) module configured to generate one or more user interfaces for displaying a time-sensitive alerts. rrV)","['G06Q10/06', 'G06N20/00', 'G06N20/20', 'G06N3/08', 'G06Q10/04', 'G06Q10/08', 'G06Q30/01', 'G06Q30/0201', 'G06Q30/0241', 'G06Q30/06', 'G06Q50/04', 'G16Y30/00', 'G16Y40/10', 'G16Y40/20', 'G16Y40/40', 'G16Y40/60', 'H04L9/50', 'G06Q2220/00', 'H04L2209/56', 'H04L2209/805', 'H04L2209/84']"
US20220222441A1,Machine learning based named entity recognition for natural language processing,"A system performs named entity recognition for performing natural language processing, for example, for conversation engines. The system uses context information in named entity recognition. The system includes the context of a sentence during model training and execution. The system generates high quality contextual data for training NER models. The system utilizes labeled and unlabeled contextual data for training NER models. The system provides NER models for execution in production environments. The system uses heuristics to determine whether to use a context-based NER model or a simple NER model that does not use context information. This allows the system to use simple NER models when the likelihood of improving the accuracy of prediction based on context is low.","['H04L51/18', 'G06F18/214', 'G06F40/247', 'G06F40/284', 'G06F40/295', 'G06F40/35', 'G06N20/00', 'G06N3/08', 'G06F40/166', 'G06F40/253', 'G06F40/289', 'G06F40/56', 'H04L51/02']"
CN112256945B,Social network Cantonese rumor detection method based on deep neural network,"The invention discloses a social network Guangdong language rumor detection method based on a deep neural network, which comprises the steps of specifically collecting Guangdong language micro-blog data on a target social network platform, and strictly performing artificial labeling, so that a relatively complete Guangdong language rumor data set is constructed; then 27 statistical characteristics are extracted aiming at the microburst; finally, the invention provides a Guangdong language rumor detection model BLA, which combines BERT, Bi-LSTM and attention mechanism and integrates the extracted statistical characteristics to realize the classification detection of the Guangdong language rumor. The experimental evaluation result shows that the performance of the method on the problem of detecting the Cantonese rumor is superior to that of other detection methods, and meanwhile, the method and the thought are provided for future detection of the Cantonese rumor.","['G06F16/951', 'G06F16/9536', 'G06F16/955', 'G06F18/2411', 'G06F40/166', 'G06F40/216', 'G06F40/295', 'G06F40/30', 'G06N3/045', 'G06N3/049', 'G06N3/08', 'G06Q50/01']"
US11544461B2,Early exit for natural language processing models,"The disclosure provides a natural language processing (NLP) model arranged to operate on two lexicons, where one lexicon is a sub-set of the other lexicon. The NLP model can be arranged to generate output based on the sub-set lexicon and exit processing of the NLP model, to potentially save computation cycles.","['G06F40/284', 'G06F40/216', 'G06F40/30', 'G06N3/045', 'G06N3/10']"
US11417317B2,Determining input data for speech processing,"Aspects described herein may relate to the determination of data that is indicative of a greater range of speech properties than input text data. The determined data may be used as input to one or more speech processing tasks, such as model training, model validation, model testing, or classification. For example, after a model is trained based on the determined data, the model's performance may exhibit more resilience to a wider range of speech properties. The determined data may include one or more modified versions of the input text data. The one or more modified versions may be associated with the one or more speakers or accents and/or may be associated with one or more levels of semantic similarity in relation to the input text data. The one or more modified versions may be determined based on one or more machine learning algorithms.","['G10L15/063', 'G06F40/56', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G10L15/07', 'G10L15/183', 'G10L15/26', 'G06F40/205', 'G06F40/289', 'G06F40/30', 'G10L13/00']"
US12073182B2,Machine learning-based selection of metrics for anomaly detection,"A plurality of metrics records, including some records indicating metrics for which anomaly analysis has been performed, is obtained. Using a training data set which includes the metrics records, a machine learning model is trained to predict an anomaly analysis relevance score for an input record which indicates a metric name. Collection of a particular metric of an application is initiated based at least in part on an anomaly analysis relevance score obtained for the particular metric using a trained version of the model.","['H04L67/12', 'G06F40/284', 'H04L41/16', 'H04L43/16', 'G06F40/20', 'H04L43/024']"
CN118314353B,Remote sensing image segmentation method based on double-branch multi-scale feature fusion,"The invention belongs to the technical field of remote sensing image processing, in particular to a remote sensing image segmentation method based on double-branch multi-scale feature fusion, which comprises the following steps: step 1, constructing a network model: the whole double-branch multi-scale feature fusion network is composed of a double encoder-decoder, wherein the encoder part uses ResNet as a backbone for multi-scale feature extraction, and specifically comprises a stacked residual error module, a feature enhancement module, a detail reservation module, a pyramid pooling downsampling module and a feature fusion module. According to the invention, a double-branch multi-scale fusion network is adopted, an encoder module in an original encoding-decoding network is replaced by two parallel branches of CNN and a transducer, the CNN and the transducer are used for respectively extracting local features and global features under different resolutions and extracting multi-scale feature graphs, and the local information and the multi-scale global context information of a high-resolution remote sensing image are extracted and fused efficiently, so that the global-local information is fully integrated.","['G06V10/26', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06V10/764', 'G06V10/806', 'G06V10/82', 'G06V20/10', 'Y02T10/40']"
CN112084331B,"Text processing, model training method, device, computer equipment and storage medium","The application relates to a text processing and model training method, a device, computer equipment and a storage medium. The text processing method comprises the following steps: obtaining a target text to be processed, and coding the target text to obtain a target text coding vector; acquiring a target entity in a target text, and determining a first associated entity corresponding to the target entity; determining a target knowledge representation vector corresponding to the target entity according to the entity representation vector of the first associated entity and the corresponding attention weight; carrying out fusion processing on the target text coding vector and a target knowledge representation vector corresponding to the target entity to obtain a target fusion result; and determining a text processing result corresponding to the target text according to the target fusion result. The text processing result of the embodiment of the application can be processed by the text processing model based on artificial intelligence, and the accuracy of the obtained text processing result can be improved by adopting the method.","['G06F16/353', 'G06F16/345', 'G06F18/25', 'G06F40/211', 'G06F40/268', 'G06F40/295', 'G06F40/30', 'G06F40/58', 'G06N3/045', 'G06N3/08']"
CN112733550B,"Knowledge distillation-based language model training method, text classification method and device","The application discloses a language model training method based on knowledge distillation, a text classification method and a text classification device. The language model training method comprises the following steps: inputting the training corpus into the first model and the second model for processing so as to acquire corresponding middle layer data and output results; and calculating to obtain first hidden layer sentence content and second hidden layer sentence content by using corresponding middle layer data, constructing a comparison learning positive and negative example based on the first hidden layer sentence content and the second hidden layer sentence content, training a second model by using the comparison learning positive and negative example, the corresponding middle layer data and an output result, and determining the trained second model as a language model. Through the classification model, the sentence grammar and the semantic representation rich in the first model can be migrated to the second model, so that the second model obtained by distillation has better migration capability, and the application requirements of the cross-field are met.","['G06F40/30', 'G06F18/214', 'G06F40/211', 'Y02D10/00']"
US11727216B2,"Method, apparatus, device, and storage medium for linking entity","A method, apparatus, device, and storage medium for linking an entity, relates to the technical fields of knowledge graph and deep learning are provided. The method may include: acquiring a target text; determining at least one entity mention included in the target text and a candidate entity corresponding to each entity mention; determining an embedding vector of each candidate entity based on the each candidate entity and a preset entity embedding vector determination model; determining context semantic information of the target text based on the target text and each embedding vector; determining type information of the at least one entity mention; and determining an entity linking result of the at least one entity mention, based on the each embedding vector, the context semantic information, and each type information.","['G06F40/284', 'G06F16/9558', 'G06F40/30', 'G06F16/3347', 'G06F16/36', 'G06F17/16', 'G06F40/211', 'G06F40/295', 'G06N5/022', 'G06N3/044', 'G06N3/08']"
US20230394328A1,Prompting Machine-Learned Models Using Chains of Thought,"Example embodiments of aspects of the present disclosure provide an example computer-implemented method for improved prompting of a machine-learned model. The example method can include obtaining an instructive sequence descriptive of an instructive query, an instructive response, and an instructive trace of intermediate states from the instructive query to the instructive response. The example method can include inputting, to a machine-learned model, the instructive sequence and an operative query, wherein the machine-learned model is configured to process the operative query with attention over the instructive sequence. The example method can include generating, using the machine-learned model and responsive to the operative query, an operative response.","['G06N3/08', 'G06N5/022']"
US20240135106A1,Semantic map generation from natural-language-text documents,A computer-implemented process includes obtaining a natural-language-text document comprising a first and second clause and determining first and second embedding sequences based on n-grams of the first and second clauses. The process includes generating data model objects based on the embedding sequences and determining an association between the first data model object and the second data model object based on a shared parameter of the first and second clauses. The process includes receiving a query including the first category and the first n-gram and causing a presentation of a visualization of data model objects that includes shapes based on the data model objects and a third shape based on the association between the first data model object and the second data model object.,"['G06F40/30', 'G06F40/103', 'G06F40/284', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N5/02']"
CN111291166B,Method and device for training language model based on Bert,"An embodiment of the present specification provides a method for training a language model based on Bert, where the method includes: firstly, obtaining a historical conversation sample, wherein the historical conversation sample comprises a plurality of conversation sentences generated by a certain business conversation and corresponding class labels, and the class labels indicate whether the business purpose is achieved through the certain business conversation; secondly, determining a semantic symbol sequence based on the plurality of dialogue sentences, and inputting the semantic symbol sequence into the language model to obtain an integral semantic vector; then, inputting the whole semantic vector into a business conversation classification model to obtain a prediction classification result; finally, based on the prediction classification result and the class label, the model parameters of the business conversation classification model and the language model are adjusted. In this way, by judging whether the purpose of the multi-turn dialogue is achieved as a training task, the semantic understanding depth and the semantic understanding width of the trained language model to the multi-turn dialogue can be improved.","['G06F16/3329', 'G06F16/3344', 'G06F16/35', 'G06F18/241', 'G06F18/2415', 'G06N3/045', 'G06N3/047', 'G06N3/08']"
CN118608957A,Plant identification method and related device,"The present disclosure relates to plant identification methods and related devices. The plant identification method comprises the following steps: acquiring a plant image and a question text about identifying a plant in the plant image; inputting a plant image and a question text into a plant recognition model, the plant recognition model comprising a first visual model configured to receive the plant image to extract first image features of the plant image and a multimodal large language model configured to receive the first image features and the question text to identify plants in the plant image, the plant recognition model being trained with multimodal data comprising the plant image, questions about identifying the plants in the plant image, and answers to the questions; and outputting answer text provided by the plant recognition model regarding recognition of the plant in the plant image.","['G06V20/188', 'G06N3/0464', 'G06N3/08', 'G06V10/44', 'G06V10/764', 'G06V10/82', 'G06V20/62', 'G06V30/1801', 'G06V30/19173']"
US12020265B1,Systems and methods for discovering online influencers and generating market sentiment index,"Systems, apparatuses, and methods obtain and process data that may be used to identify or discover one or more â€œinfluencersâ€ in business, finance, fashion, sports, current events, and other areas, and also to generate an indication of each influencer's expertise and ability to influence others with their posts or comments. A measure of each influencer's accuracy with regards to the contents of their previous comments or posts may also be generated. Based on determining the sentiment associated with each influencer's current posts or comments, an index or measure of the accuracy weighted sentiment expressed by a set of influencers with regards to an event, stock, trend, or other aspect may be generated and presented to a user to assist them in making a decision.","['G06F40/30', 'G06Q30/0201', 'G06Q30/0243', 'G06Q50/01']"
US12067362B2,"Computer implemented methods for the automated analysis or use of data, including use of a large language model","Methods are provided, such as a method of interacting with a large language model (LLM), including the step of a processing system using a structured, machine-readable representation of data that conforms to a machine-readable language, such as a universal language, to provide new context data for the LLM, in order to improve the output, such as continuation text output, generated by the LLM in response to a prompt; and such as a method of interacting with a LLM, including the step of providing continuation data generated by the LLM to a processing system that uses a structured, machine-readable representation of data that conforms to a machine-readable language, such as a universal language, in which the processing system is configured to analyse the continuation output generated by the LLM in response to a prompt to enable an improved version of that continuation output to be provided to a user. Related computer systems are provided.","['G06N3/0475', 'G06F40/279', 'G06F40/40', 'G06N3/042', 'G06N3/0455', 'G06N5/04', 'G06N3/0442', 'G06N3/08']"
US11989527B2,"Computer implemented methods for the automated analysis or use of data, including use of a large language model","Methods are provided, such as a method of interacting with a large language model (LLM), including the step of a processing system using a structured, machine-readable representation of data that conforms to a machine-readable language, such as a universal language, to provide new context data for the LLM, in order to improve the output, such as continuation text output, generated by the LLM in response to a prompt; and such as a method of interacting with a LLM, including the step of providing continuation data generated by the LLM to a processing system that uses a structured, machine-readable representation of data that conforms to a machine-readable language, such as a universal language, in which the processing system is configured to analyse the continuation output generated by the LLM in response to a prompt to enable an improved version of that continuation output to be provided to a user. Related computer systems are provided.","['G06F40/35', 'G06F40/40', 'G06F40/30']"
US12073180B2,"Computer implemented methods for the automated analysis or use of data, including use of a large language model","Methods are provided, such as a method of interacting with a large language model (LLM), including the step of a processing system using a structured, machine-readable representation of data that conforms to a machine-readable language, such as a universal language, to provide new context data for the LLM, in order to improve the output, such as continuation text output, generated by the LLM in response to a prompt; and such as a method of interacting with a LLM, including the step of providing continuation data generated by the LLM to a processing system that uses a structured, machine-readable representation of data that conforms to a machine-readable language, such as a universal language, in which the processing system is configured to analyse the continuation output generated by the LLM in response to a prompt to enable an improved version of that continuation output to be provided to a user. Related computer systems are provided.","['G06N5/01', 'G06F40/279', 'G06N3/0442', 'G06N3/045', 'G06N3/0455', 'G06N3/0499', 'G06N3/09', 'G06N3/088', 'G06N5/045']"
CN111428054B,Construction and storage method of knowledge graph in network space security field,"The invention relates to a method for constructing and storing a knowledge graph in the field of network space security, which is characterized in that the knowledge graph is constructed and stored according to different data sources: for the structured data, defining a corresponding conversion rule to directly convert the structured data into multi-element knowledge and store the multi-element knowledge into a knowledge graph; and for semi-structured data and unstructured data, performing data preprocessing on the semi-structured data and/or the unstructured data to obtain effective text data, performing knowledge extraction on the effective text data, comparing and verifying a knowledge extraction result with multi-element knowledge in a constructed knowledge graph, performing entity alignment, realizing multi-element knowledge conversion, storing the multi-element knowledge conversion into the knowledge graph, and completing construction of the knowledge graph. According to the XLNET pre-training model based on the open domain large-scale labeled corpora and the auxiliary supervision training mode based on the established knowledge map corpora in the professional field, the safety knowledge is extracted efficiently, multi-source data information is fully utilized, and the efficiency and accuracy of the knowledge map construction in the network space safety field are improved.","['G06F16/367', 'G06F16/31']"
US20220172802A1,Retrosynthesis systems and methods,A synthesis protocol for a reaction pathway of a target molecule can be determined by: providing target compound data; performing a chemical synthesis search for at least one reaction pathway for the target compound; processing the target compound data through a single-step reaction enumeration algorithm to obtain at least one reaction step of the least one reaction pathway; processing at least one reaction step with the at least one reaction pathway scoring mechanism model to obtain a reaction step score; constructing reaction pathways based on at least one reaction step and at least one reaction step score; providing a selectivity filter having a selectivity criteria; filtering the reaction pathways so that reactions violating the selectivity criteria is filtered out; ranking the reaction pathways; and providing the reaction pathway ranking.,"['G16C20/10', 'G16C20/70']"
US11222167B2,Generating structured text summaries of digital documents using interactive collaboration,"The disclosure describes one or more embodiments of a structured text summary system that generates structured text summaries of digital documents based on an interactive graphical user interface. For example, the structured text summary system can collaborate with users to create structured text summaries of a digital document based on automatically generating document tags corresponding to the digital document, determining segments of the digital document that correspond to a selected document tag, and generating structured text summaries for those document segments.","['G06F40/56', 'G06F3/0481', 'G06F3/0482', 'G06F40/117', 'G06F40/169', 'G06F40/284', 'G06N20/00', 'G06F40/253', 'G06N3/045']"
US11528290B2,"Systems and methods for machine learning-based digital content clustering, digital content threat detection, and digital content threat remediation in machine learning-based digital threat mitigation platform","A machine learning-based system and method for content clustering and content threat assessment includes generating embedding values for each piece of content of corpora of content data; implementing unsupervised machine learning models that: receive model input comprising the embeddings values of each piece of content of the corpora of content data; and predict distinct clusters of content data based on the embeddings values of the corpora of content data; assessing the distinct clusters of content data; associating metadata with each piece of content defining a member in each of the distinct clusters of content data based on the assessment, wherein the associating the metadata includes attributing to each piece of content within the clusters of content data a classification label of one of digital abuse/digital fraud and not digital abuse/digital fraud; and identifying members or content clusters having digital fraud/digital abuse based on querying the distinct clusters of content data.","['H04L63/1416', 'H04L63/1425', 'G06F16/217', 'G06F16/2455', 'G06F16/285', 'G06F16/9535', 'G06N20/00', 'G06N3/045', 'G06N5/04']"
US11463772B1,Selecting advertisements for media programs by matching brands to creators,"Advertisements for brands that are to be aired during media programs are selected based on similarities between the brands and creators of the media programs. As a media program is being aired, data representing content of a media program is processed to identify words being spoken or sung during the media program and sentiments associated with the media program. The media program is classified based on the words and sentiments, and a classification of the media program is compared to attributes of advertisements to determine which of the advertisements is best suited for airing during the media program. Additionally, a set of words that, if spoken or sung during a media program would establish conditions favorable to a given advertisement may be identified and provided to a creator.","['H04N21/812', 'G10L15/02', 'G10L15/08', 'H04N21/4394', 'H04N21/458', 'G10L2015/088']"
US11537801B2,Structured text translation,"Approaches for the translation of structured text include an embedding module for encoding and embedding source text in a first language, an encoder for encoding output of the embedding module, a decoder for iteratively decoding output of the encoder based on generated tokens in translated text from previous iterations, a beam module for constraining output of the decoder with respect to possible embedded tags to include in the translated text for a current iteration using a beam search, and a layer for selecting a token to be included in the translated text for the current iteration. The translated text is in a second language different from the first language. In some embodiments, the approach further includes scoring and pointer modules for selecting the token based on the output of the beam module or copied from the source text or reference text from a training pair best matching the source text.","['G06F40/42', 'G06F40/58', 'G06F40/117', 'G06F40/154', 'G06N3/045', 'G06N3/048', 'G06N3/084', 'G06N3/088', 'G06F40/47', 'G06N3/08']"
US11842324B2,Method for extracting dam emergency event based on dual attention mechanism,"A method for extracting a dam emergency event based on a dual attention mechanism is provided. The method includes: performing data preprocessing, building a dependency graph, building a dual attention network, and filling a document-level argument. The performing data preprocessing includes labeling a dam emergency corpus and encoding sentences. Building a dependency graph includes assisting a model to mine a syntactic relation based on a dependency. Building a dual attention network includes weighing and fusing an attention network based on a graph transformer network (GTN) and capturing key semantic information in the sentence. Filling a document-level argument includes filling a document-level argument by detecting a key sentence and ordering a semantic similarity. The method introduces a dependency and overcomes the long-range dependency problem based on the dual attention mechanism, thus achieving high identification accuracy and reducing a lot of labor costs.","['G06Q10/20', 'G06F16/35', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N5/01', 'G06N5/022', 'G06Q50/06', 'G06Q50/08', 'G06Q50/26', 'Y02A10/40']"
CN110727806B,Text processing method and device based on natural language and knowledge graph,"The application discloses a text processing method and device based on natural language and knowledge graph, and relates to the technical field of artificial intelligence. The specific implementation scheme is as follows: the electronic equipment uses semantic representation obtained by a joint learning model, the joint learning model is obtained by combining knowledge graph representation learning and natural language representation learning, knowledge graph learning representation and natural language learning representation are combined, compared with the situation that only knowledge graph representation learning or natural language representation learning prediction objects are used for learning, factors considered by the joint learning model are more and more comprehensive, and therefore semantic representation accuracy can be improved, and text processing accuracy is further improved.","['G06F16/367', 'G06F16/9024', 'G06N3/04', 'G06N3/042', 'G06N3/0442', 'G06N3/08', 'G06N3/09', 'G06N5/02']"
CN111460833B,"Text generation method, device and equipment","The invention discloses a text generation method, a text generation device and text generation equipment. The core concept of the invention is to provide a text generation scheme based on attribute generation, attribute matching and attribute adjustment, in particular to obtain a target attribute according to a received attribute description statement and a preset attribute generation strategy; generating a text according to the words by using a language model; after each word is generated, extracting attributes of all generated texts, and matching with the target attributes; and adjusting the historical state information of the language model according to the matching result to generate the next word. The method has the advantages that the representation mode of the target attribute is improved by using the generated strategy, the attribute is expandable, the multi-attribute controllability is supported, the historical state information of the language model is modified through attribute matching and attribute control, the direction of the text attribute is finely controlled, the quality of the generated text is improved, and in addition, the smoothness of the generated text can be improved without excessive modification of the language model.","['G06F18/253', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
CN110399978B,Machine Learning Acceleration Architecture,"The application discloses a machine learning acceleration architecture. An apparatus for facilitating acceleration of machine learning operations is disclosed. The apparatus includes accelerator circuitry, the accelerator circuitry comprising: a first set of processing elements for performing a first calculation comprising a matrix multiplication operation; a second set of processing elements for performing a second calculation comprising a weight element summation and an offset multiplication operation; and a third set of processing elements to perform a third computation comprising input element summation and offset multiplication operations, wherein the second computation and the third computation are performed in parallel with the first computation.","['G06N3/08', 'G06N3/063', 'G06F17/16', 'G06F7/5443', 'G06N3/04', 'G06N3/044', 'G06N3/0442', 'G06N3/0464', 'G06N3/048', 'G06N3/0495', 'G06N3/084', 'G06N3/098', 'G06N3/0475', 'G06N3/088']"
WO2022135120A1,Adaptive knowledge graph representation learning method combining graph structure with text information,"An adaptive knowledge graph representation learning method combining a graph structure with text information. The method comprises: (1) sampling neighbor triples of head and tail entities of target triples; (2) calculating semantic representations of each target triple and the neighbor triples of head and tail entities of each target triple; (3) calculating structure representations of the head and tail entities of the target triples; (4) splicing the semantic representations of the target triples with the structure representations of the head and tail entities of the target triples, inputting the representations into an adaptive classification layer, and calculating a classification result and a classification loss; and (5) optimizing a module on the basis of a gradient descent optimization algorithm until a loss value converges, so as to obtain a final splicing result of the semantic representations of the target triples and the structure representations of the head and tail entities of the target triples. By means of the method, a semantic representation and a graph structure representation of a knowledge graph can be captured at the same time, and semantic information and structure information are adaptively combined and fully used, so that better robustness can be achieved for a knowledge graph lacking information.","['G06F16/367', 'G06F40/30', 'G06F16/355', 'G06F40/216', 'G06F40/279', 'G06F40/295', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N5/022']"
CN115035538B,"Training method of text recognition model, and text recognition method and device","The invention provides a training method of a text recognition model, a text recognition method and a text recognition device, relates to the technical field of artificial intelligence, in particular to the technical field of deep learning and computer vision, and can be applied to scenes such as optical character recognition. The scheme is as follows: the method comprises the steps of conducting mask prediction on partial images in an obtained first sample image to obtain a complete prediction image corresponding to the first sample image, conducting mask prediction on partial texts in an obtained second sample image to obtain predicted text contents corresponding to the partial texts, obtaining a pre-training model according to the complete prediction image and the predicted text contents, generating a text recognition model according to the pre-training model, wherein the text recognition model is used for conducting text recognition on images to be recognized, and enabling the pre-training model to learn strong image visual reasoning capability and text semantic reasoning capability, so that when the text recognition model generated based on the pre-training model conducts text recognition, accuracy and reliability of the text recognition are improved.","['G06V30/19147', 'G06F40/30', 'G06N3/08', 'G06T5/75', 'G06V10/82', 'G06V30/16', 'G06V30/1801', 'G06T2207/20081']"
US11971914B1,Artificial intelligence systems and methods,"An artificial intelligence (AI) method includes a processor receiving, from a human user, a query on an external database; performing transformations on the query to produce a refined query; generating an embedding of the refined query; and generating a system prompt and a user prompt based on the refined query. The processor then applies the embedding to a vector database by executing a similarity routine to identify discrete vectors in the vector database most similar to the embedding, and collects the most similar vectors for application to a large language model. Next, the processor applies the most similar vectors, the refined query, and the system and user prompts to a large language model to generate a comprehensive response to the query. The response includes a text document that is generated by execution of the large language model.","['G06F16/332', 'G06F16/31', 'G06F16/3347']"
US20220391587A1,"Method of training image-text retrieval model, method of multimodal image retrieval, electronic device and medium","A method of training an image-text retrieval model, a method of multimodal image retrieval, an electronic device and a storage medium, each relating to the technical field of artificial intelligence, and in particular, to fields of computer vision and deep learning technologies. Sample data including a sample text and a sample image is acquired. The sample text includes a sample text in a first language and a sample text in a second language. The sample text in the first language and the sample text in the second language are processed by using the text encoding sub-model to obtain a sample text feature of the sample data. The sample image is processed by using the image encoding sub-model to obtain a sample image feature of the sample data. The image-text retrieval model is trained according to the sample text feature and the sample image feature.","['G06F16/483', 'G06F40/58', 'G06F16/438', 'G06F16/532', 'G06F16/5846', 'G06F18/256', 'G06F40/279', 'G06V10/40', 'G06V10/467']"
CN112613273B,Compression method and system of multi-language BERT sequence labeling model,"The invention provides a compression method and a compression system of a multilingual BERT sequence labeling model, which relate to the technical field of knowledge distillation of BERT models, and the method comprises the following steps: step 1: extracting a word list from the multi-language corpus based on a Wordpieee algorithm; step 2: pre-training a multi-language/single-language BERT teacher model and a multi-language BERT student model; and step 3: fine-tuning a multi/single language BERT teacher model based on the manually labeled downstream task data; and 4, step 4: carrying out residual knowledge distillation on the pre-trained multilingual BERT student model by using a multilingual BERT teacher model; and 5: and fine-tuning the distilled multilingual BERT student model based on the manually labeled downstream task data. According to the invention, through residual learning and many-to-one knowledge distillation mode, the accuracy and the generalization degree of the student model are improved, and hardware resources required by deployment of the BERT sequence annotation model in a multi-language environment are reduced.","['G06F40/126', 'G06F40/242', 'G06F40/289', 'G06F40/295']"
US20230368284A1,System and method enabling application of autonomous agents,"Disclosed is system enabling application of autonomous agents (AAs) across problem domains. System comprises decentralised computing network to implement software framework (SF). SF comprises client-agent device (client-AA) to receive service request (SR), generate objective in vector(s) recorded in vector database (VD), send objective to agent-device (AA/micro-AA). Agent-device comprises context-builder software module (CBSM) to send query(-ies) to machine learning model agent (ML-Model AA) and/or access VD to retrieve task(s) associated with previous queries to obtain tasks associated with objective. CBSM to obtain order of task execution from ML-Model AA, to obtain list including AAs, to block communication signals between client-AA and AAs not associated with objective, to associate AA(s) with task(s); protocol generator software module comprising domain-independent protocol specification language (DIPSL) generate protocol specification(PS(s)) for task execution by AA(s); build executor software module compose task(s) in order, compose AA into further autonomous agent (further-AA), encrypt access to further-AA, further-AA implement PS(s) to execute each task and SR.","['G06Q30/08', 'G06Q50/10', 'G06Q20/02', 'G06Q20/223', 'G06Q30/0609', 'H04L63/107', 'H04L9/0618', 'H04L9/3236', 'H04L9/3239', 'H04L9/3297', 'G06N3/045', 'H04L2209/56', 'H04L9/50']"
US20210109958A1,"Conceptual, contextual, and semantic-based research system and method","Systems are described in the field of machine learning such as natural language processing for use in researching and searching a corpus of documents in various topical areas such as physical and social sciences. The systems may utilize training, testing, and deployment of models representing a defined space within the corpus. A network of computers and user input devices may be used for receiving research queries via human-computer interface devices and application programming interfaces. Queries may be processed and used as input to the machine learning models. Outputs from the models may include ranking of results reflecting the queries.","['G06F16/3335', 'G06F16/3347', 'G06F18/214', 'G06F40/20', 'G06F40/30', 'G06K9/6256', 'G06N3/045', 'G06N20/00', 'G06N3/044', 'G06N3/08', 'G06Q50/18']"
WO2021159751A1,"Semantic and image recognition-based electrocardiography information extraction method and apparatus, computer device, and storage medium","A semantic and image recognition-based electrocardiography information extraction method and apparatus, a computer device, and a storage medium, which relate to artificial intelligence technology, and may be applied to smart medical treatment scenarios. The method comprises: performing semantic recognition on current text information to be recognized, and obtaining a semantic vector corresponding to the current text information to be recognized; receiving an uploaded electrocardiogram image, calling an attention mechanism-based Res2Net classification network, classifying the electrocardiogram image according to the attention mechanism-based Res2Net classification network, and obtaining a corresponding output vector; calling a pre-trained Light GBM model, inputting the semantic vector and the output vector into the Light GBM model for classification, and obtaining a corresponding classification result. The method also relates to medical technology and blockchain technology, in that text information uploaded by a user end and image information corresponding to an electrocardiogram image are combined, and classification is then performed by means of a Light GBM algorithm, thereby improving the accuracy and credibility of a classification result.","['G06F2218/12', 'G06F40/30', 'G06N3/045', 'G06N3/08']"
WO2022222300A1,"Open relationship extraction method and apparatus, electronic device, and storage medium","The present application relates to artificial intelligence technology, and discloses an open relationship extraction method, comprising: obtaining an original training set by using remote supervision and entity linking techniques; performing policy annotation and entity reinforcement processing on the original training set to obtain a standard training set; using the standard training set to perform entity fine adjustment and relationship fine adjustment on a pre-trained language model to obtain an open entity extraction model and an open relationship extraction model; extracting, by using the open entity extraction model, entities in a text to be classified; and predicting an entity relationship between the entities by using the open relationship extraction model, and clustering the entities and the entity relationship to obtain a relationship extraction result. In addition, the present application further relates to blockchain technology, and the relationship extraction result may be stored in a node of a blockchain. The present application further provides an open relationship extraction apparatus, an electronic device, and a computer-readable storage medium. The present application can solve the problem of relatively low extraction efficiency of an open relationship.","['G06F16/285', 'G06F16/288', 'G06F16/35']"
CN113066494B,"Text error correction model generation method and system, text error correction method, system, equipment and medium","A text error correction model generation method and system, text error correction method, system, equipment and medium, through carrying on voice recognition to the audio data, obtain the correspondent voice recognition text; encoding the voice recognition text to obtain a feature vector of the voice recognition text; inputting the feature vector of the reference text and the feature vector of the voice recognition text to a decoder for decoding, calculating a loss function according to a decoding result, and generating a text error correction model through optimizing the loss function training; and performing text correction on one or more voice recognition texts to be processed by using the text correction model to obtain corrected correct texts. The invention provides a scheme for correcting errors of ASR recognition texts, which not only can correct the replacement type errors in the common ASR recognition texts, but also can correct the texts of text insertion type errors and text deletion type errors. In addition, the invention can lead the corrected sentences to be more smooth and more beneficial to reading and understanding of people.","['G06F18/2415', 'G06F40/126', 'G06F40/211', 'G06F40/216', 'G06F40/232', 'G06F40/289', 'G06N3/045', 'G06N3/08', 'G10L19/16']"
CN111552821B,"Legal intention searching method, legal intention searching device and electronic equipment","A legal intention search method, a legal intention search apparatus, and an electronic device are disclosed. The legal intention searching method comprises the following steps: performing knowledge injection and sentence tree conversion on the query request based on a legal knowledge graph through a knowledge layer of a legal intention classifier; transforming the sentence tree into an embedded representation through an embedding layer of the legal intent classifier; the visualization degree of the words in the query request and the words from the legal knowledge graph is controlled through a visual layer of the legal intention classifier, and the self-attention area in a conversion model is controlled through a mask converter layer of the legal intention classifier according to the visualization degree information to obtain a legal intention search result. Therefore, the legal intention classifier with a specific framework is constructed by fusing the legal knowledge map and the pre-training language model, so that the recognition accuracy of the legal intention is improved to optimize the search result.","['G06F16/367', 'G06F16/322', 'G06F16/334', 'G06F16/3344', 'G06F16/35', 'G06F18/214', 'G06Q50/18']"
US20240070434A1,Conversational knowledge base,"An information system provides a conversational knowledge base for responding to user queries. The information system incorporates contemporaneous advancements in NLP and deep learning to create the conversation knowledge base from its documents, which may be obtained from various sources. Domain-specific information is extracted and generated from the documents substantially without human intervention. From the domain-specific information, precise answers to synthesized questions are generated using transformer-based deep learning models.","['G06N3/006', 'G06F40/30', 'G06F40/35', 'G06F40/58', 'G06N20/00', 'G06N3/0455', 'G06N3/045']"
US11126446B2,Contextual extensible skills framework across surfaces,"In non-limiting examples of the present disclosure, systems, methods and devices for exposing computing service skills are presented. A task intent associated with a natural language input may be identified. A plurality of add-in matching language models may be applied to the natural language input, wherein each add-in matching language model corresponds to a computing service skill. A score for each of the computing service skills may be generated. Each of the computing service skills may be ranked. A selectable option to execute an add-in associated with a top ranked computing service skill may be caused to be displayed. An indication that a selection of the option has been made may be received. The add-in associated with the top ranked computing service skill may be executed.","['G06F9/453', 'G06F40/169', 'G06F40/20', 'G06F40/30', 'G06N20/00', 'G06F8/61']"
WO2023035015A1,Systems and methods for token management in social media environments,Systems and techniques to enable token-related functionality within social media platforms are illustrated. One embodiment include a method for accessing tokens. The method derives characteristics of one or more tokens owned by a first account on a platform. The method reviews one or more external accounts on the platform for compatible tokens that share at least one of the characteristics. The method confirms a compatible token is owned by a second account of the one or more external accounts. The method accesses the compatible token.,"['G06Q20/3678', 'H04L9/50', 'G06Q20/3823', 'H04L9/3213', 'H04L9/3231', 'H04L9/3247', 'G06Q2220/00', 'G06Q50/01']"
EP3955245A1,"Apparatus, method and program for parsing an utterance using a speaker feature vector","A vehicle-mounted apparatus comprises an audio interface configured to receive audio data from an audio capture device of the vehicle, the audio data featuring an utterance of a person within the vehicle; an image interface configured to receive image data from an image capture device to capture images from the vehicle, wherein the image data includes a facial area of the person within the vehicle; a speaker preprocessing module configured to receive the image data and obtain, based on the image data, a speaker feature vector; and a speech processing module configured to use the speaker feature vector to configure an acoustic model for use in parsing the utterance based on the audio data and the speaker feature vector, the speech processing module comprising: a database of acoustic model configurations; an acoustic model selector configured to select an acoustic model configuration from the database based on the speaker feature vector; and an acoustic model instance, instantiated based on the acoustic model configuration selected by the acoustic model selector, configured to generate phoneme data for use in parsing the utterance.","['G10L15/28', 'G10L15/22', 'B60R11/02', 'G06F3/167', 'G06V40/16', 'G10L15/02', 'G10L15/063', 'G10L15/14', 'G10L15/16', 'G10L15/183', 'G10L15/24', 'G10L15/25', 'G10L25/30', 'G10L25/57', 'G10L15/20', 'G10L2015/025', 'G10L2015/226']"
CN118468973A,A large language model training and decoding method,"The invention discloses a large language model training and decoding method, in the model training process, the continuous k token larger than the set probability threshold is regarded as a decoding unit capable of parallel decoding; generating a new piece of training data for each decoding unit; in the new training data, k token in the decoding unit is replaced by k-1 trainable [ PAD ] token, wherein the [ PAD ] token is a randomly initialized vector, and the [ PAD ] token can carry the current position information through training; training the model with new training data to obtain a trained model and a trained [ PAD ] token. In the decoding stage of the model, adding l-1 [ PAD ] token at the end of each input sequence; the last one logits is taken and then converted into a probability distribution, and for the predicted one token, the first one is accepted until the probability of the token is smaller than the threshold Î².","['G06N3/084', 'G06F18/214', 'G06F40/284', 'G06N3/045']"
CN117951242A,Basic medical proprietary database application algorithm based on large language model,"The invention relates to the technical field of large language models, in particular to a basic medical proprietary database application algorithm based on a large language model, which is characterized in that firstly, six existing medical question answer data sets covering professional medicine, research and consumer query and a new on-line search medical question data set are combined according to a collected medical knowledge data set, a basic medical proprietary database with clearer logic and clearer keywords is established through vectorization processing, secondly, a new prompt strategy is introduced to improve the reasoning capacity of LLM in the training process of the large language, instruction prompt is introduced, LLM is matched with the key medical field by using a simpler and parameter efficient technology, the limitation of the model in the aspects of scientific basis, hazard and prejudice is remarkably reduced, the model response is fast, the precision is high, the robustness is good, and the method can be widely applied to basic medical scenes.","['G06F16/31', 'G06F16/3344', 'G06F16/367', 'G06N3/045', 'G16H50/30']"
US20230196105A1,Generating labeled training data using a pre-trained language model neural network,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating labeled training data using a pre-trained language model neural network. In particular, the language model neural network can generate the text input in a new labeled training example from an input sequence that includes (i) one or more context inputs and (ii) a text label that identifies the ground truth category for the new labeled training example.","['G06N3/044', 'G06N3/08', 'G06N3/0455', 'G06N3/088', 'G06N3/09']"
US20210279576A1,Attention neural networks with talking heads attention,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for performing a machine learning task on a network input to generate a network output. In one aspect, one of the systems includes an attention neural network configured to perform the machine learning task, the attention neural network including one or more attention layers, each attention layer comprising an attention sub-layer and, optionally, a feed-forward sub-layer. At least one of the attention layers includes an attention sub-layer that applies talking heads attention instead of conventional multi-head attention.","['G06N3/08', 'G06N3/045', 'G06N3/04', 'G06N3/084']"
US11488579B2,Evaluating language models using negative data,"A method of evaluating a language model using negative data may include accessing a first language model that is trained using a first training corpus, and accessing a second language model. The second language model may be configured to generate outputs that are less grammatical than outputs generated by the first language model. The method may also include training the second language model using a second training corpus, and generating output text from the second language model. The method may further include testing the first language model using the output text from the second language model.","['G06F40/20', 'G10L15/01', 'G06N3/08', 'G10L15/063', 'G10L15/197', 'G06N3/044']"
US11620574B2,Holistic optimization for accelerating iterative machine learning,"A great deal of time and computational resources may be used when developing a machine learning or other data processing workflow. This can be related to the need to re-compute the workflow in response to adjustments to the workflow parameters, in order to assess the benefit of such adjustments so as to develop a workflow that satisfies accuracy or other constraints. Embodiments herein provide time and computational savings by selectively storing and re-loading intermediate results of steps of a data processing workflow. For each step of the workflow, during execution, a decision is made whether to store the intermediate results of the step. Thus, these embodiments can offer storage savings as well as processing speedups when repeatedly re-executing machine learning or other data processing workflows during workflow development.","['G06N20/00', 'G06F11/3466', 'G06F9/3838', 'G06N5/01', 'G06F11/3409', 'G06N5/02']"
US11586829B2,Natural language text generation from a set of keywords using machine learning and templates,"An embodiment of the present invention generates natural language content from a set of keywords in accordance with a template. Keyword vectors representing a context for the keywords are generated. The keywords are associated with language tags, while the template includes a series of language tags indicating an arrangement for the generated natural language content. Template vectors are generated from the series of language tags of the template and represent a context for the template. Contributions from the contexts for the keywords and the template are determined based on a comparison of the series of language tags of the template with the associated language tags of the keywords. One or more words for each language tag of the template are generated to produce the natural language content based on combined contributions from the contexts for the keywords and the template.","['G06F40/56', 'G06F40/40', 'G06F40/186', 'G06F40/30', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08']"
US20210065712A1,Automotive visual speech recognition,"Systems and methods for processing speech are described. Certain examples use visual information to improve speech processing. This visual information may be image data obtained from within a vehicle. In examples, the image data features a person within the vehicle. Certain examples use the image data to obtain a speaker feature vector for use by an adapted speech processing module. The speech processing module may be configured to use the speaker feature vector to process audio data featuring an utterance. The audio data may be audio data derived from an audio capture device within the vehicle. Certain examples use neural network architectures to provide acoustic models to process the audio data and the speaker feature vector.","['G10L15/16', 'G06F17/2705', 'G06F40/279', 'G06K9/00275', 'G06K9/00281', 'G06V10/764', 'G06V10/82', 'G06V20/59', 'G06V40/169', 'G06V40/171', 'G06V40/20', 'G10L15/02', 'G10L15/22', 'G10L15/25', 'G10L17/18', 'G06F40/205', 'G10L2015/025', 'G10L2015/223', 'G10L2015/227']"
CN114023316A,TCN-Transformer-CTC-based end-to-end Chinese voice recognition method,"The invention provides an end-to-end Chinese voice recognition method based on TCN-Transformer-CTC, belonging to the field of voice recognition. Aiming at the problems in the prior art, the invention firstly proposes to use a time sequence convolutional neural network (TCN) to strengthen the capture of the neural network model to the position information, secondly fuses and connects the time sequence classification (CTC) on the basis, and proposes a TCN-Transformer-CTC model with better recognition effect and stronger generalization. Without using any language model, the experimental results on Hill Shell Mandarin Chinese open source speech database AISHELL-1 showed a relative 10.91% reduction in TCN-Transformer-CTC and a model final word error rate of 5.31% compared to the Transformer word error rate.","['G10L15/16', 'G10L19/16']"
CN112418034B,"Multi-mode emotion recognition method and device, electronic equipment and storage medium","The invention relates to the technical field of voice recognition and image processing, and provides a multi-mode emotion recognition method, a device, electronic equipment and a storage medium, wherein the method comprises the following steps: performing de-duplication on video data of an object to be identified to obtain face time sequence image data of the object to be identified; acquiring text data of the object to be identified in real time when acquiring video data of the object to be identified; and inputting the aligned face time sequence image data and the text data into a multi-modal emotion recognition model so as to perform multi-modal emotion recognition of the object to be recognized. According to the invention, the emotion classification and detection accuracy and robustness are improved by acquiring the expression of the user and the text content of the conversation in the conversation process of the virtual person with the user in real time and acquiring the multidimensional rich features in a mode of joint input of the image and the text signal. Particularly in the forward and reverse words, irony and other scenes have higher accuracy.","['G06V40/174', 'G06F18/2411', 'G06F40/30', 'G10L15/26', 'G10L25/63']"
US11983210B2,Methods and systems for generating summaries given documents with questions and answers,"Described herein are systems and methods to enable generation of high-quality summaries of documents that have questions and answers. To help summarize such documents, parsing methods are disclosed that account for different document formats. Question-answer groups are transformed into declarative sentences. Sentence correction can be applied to the declarative sentences. Candidate summary sentences are identified from the declarative sentences, and a subset of the candidate summary sentences are selected for inclusion in a summary. Aspects, segmentation, and augmentation can help with generation and tailoring of summaries.","['G06F16/345', 'G06F16/335', 'G06F16/338', 'G06F16/35', 'G06F40/216', 'G06F40/289', 'G06F40/35', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06F40/42', 'G06F40/58']"
CN112100332B,"Word embedding representation learning method and device, text recall method and device","The disclosure provides a word embedding representation learning method and device and a text recall method and device, and relates to the field of artificial intelligence. The word embedding representation learning method comprises the steps of obtaining text corpus, carrying out word segmentation processing on the text corpus, constructing a graph structure based on the obtained word segmentation and pronunciation information corresponding to the word segmentation, taking each node in the graph structure as an initial node, randomly walking to obtain a node sequence corresponding to the initial node, training a word embedding representation model according to the node sequence to obtain a word embedding lookup table, and determining word embedding representation corresponding to the text corpus based on the word embedding lookup table. The method and the device can construct the graph according to the word segmentation and pronunciation information, and train word embedding based on the graph structure, so that words with morphology close to each other have similar distances in the word embedding space, the problem of recall text errors caused by input errors is avoided, recall efficiency and recall quality are improved, and user experience is further improved.","['G06F16/3344', 'G06F40/289']"
CN114169330B,Chinese named entity recognition method integrating time sequence convolution and transform encoder,"A Chinese named entity recognition method integrating time sequence convolution and a transducer coder belongs to the field of character recognition. Firstly, characters and word characteristics are modeled by adopting a flat lattice structure proposed by the former, and absolute position coding in a transducer coder is changed into relative position coding to avoid losing direction information. Secondly, the TCN is used for enhancing the capturing of the position information by the network model, and more local context semantic relations are acquired. And finally, carrying out regular constraint on the output distribution of the model by adopting an R-Drop strategy, preventing the model from being over-fitted, and improving the generalization capability of the model. Experimental results show that the F1 values of the model on the Weibo data set and the MSRA data set respectively reach 61.18% and 94.48%, and the model is superior to the traditional model and the reference model, and the superiority of the model in Chinese named entity recognition is verified.","['G06F40/295', 'G06N3/08']"
US11922124B2,"Method, apparatus and computer program product for generating multi-paradigm feature representations","Methods, apparatus, systems, computing devices, computing entities, and/or the like for programmatically generating multi-paradigm feature representations are provided. An example method may include generating a code dataset including a plurality of codes associated with a predictive entity; generating a plurality of semantic feature vectors based at least in part on code description metadata; generating a plurality of structural feature vectors based at least in part on code relation metadata; generating a plurality of multi-paradigm feature vectors based at least in part on the plurality of semantic feature vectors and the plurality of structural feature vectors; generating a prediction for the predictive entity by processing the plurality of multi-paradigm feature vectors using a prediction model; and performing one or more prediction-based actions based on the prediction.","['G06F40/30', 'G06F16/38', 'G06F16/9024', 'G06F17/16', 'G06F40/20', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G16H40/20', 'G06N5/022']"
CN111525694B,Intelligent switch cabinet marking device and method based on Internet of things,"The intelligent switch cabinet marking device and system based on the Internet of things specifically detect and predict faults of collected equipment operation parameters by constructing an SML model, further provides an information interaction technology platform integrating a data acquisition module, a data processing center module and a marking device, overcomes the defects of easy shift, untimely information update or disordered information and the like of the existing marking plate, effectively improves the efficiency of field operators for overhauling and troubleshooting faults and the fault detection and judgment capability, and then, the neural network algorithm is adopted to optimize the SML model, so that the accuracy and the authenticity of the SML model are improved, a Bayesian prediction model is further constructed to predict the fault risk, the fault is effectively restrained before the occurrence, scientific intelligent management is realized, and the safe and stable operation of the power system is ensured.","['H02J13/00001', 'G06F18/24', 'G06N20/00', 'G06N3/02', 'H02J2203/20', 'Y02E60/00', 'Y04S40/128']"
US11892746B1,Super system on chip,"A Super System on Chip (SSoC) is disclosed. The Super System on Chip (SSoC)'s input/outputs are coupled with a Mach-Zehnder interferometer (MZI), wherein the Mach-Zehnder interferometer (MZI) can generally include a phase transition material or a phase change material. The Mach-Zehnder interferometer (MZI) is coupled with a first optical waveguide in two-dimensions (2-D) or three-dimensions (3-D). The first optical waveguide is coupled with (i) a semiconductor optical amplifier (SOA) or (ii) a second optical waveguide that can include a nonlinear optical material in two-dimensions (2-D) or three-dimensions (3-D). Furthermore, the semiconductor optical amplifier (SOA) may be replaced by an optical resonator.","['G02F3/00', 'G01S17/34', 'G01S17/58', 'G01S17/89', 'G01S17/931', 'G01S7/4917', 'G02F1/212', 'G02F1/225', 'G06F15/7817']"
CN110288509B,Computational Optimization Mechanism,An apparatus for facilitating computational optimization is disclosed. The apparatus includes classification logic to classify the processing threads into thread groups based on bit depth of floating point thread operations.,"['G06T1/20', 'G06F3/14', 'G06F9/3001', 'G06F9/30014', 'G06F9/3017', 'G06F9/3851', 'G06F9/3887', 'G06F9/3888', 'G06F9/3895', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0464', 'G06N3/063', 'G06N3/084', 'G06N3/0895', 'G06N3/09', 'G06N3/098', 'G06T15/005', 'G09G5/363', 'G06T15/04', 'G09G2360/06', 'G09G2360/08', 'G09G2360/121']"
US11868714B2,Facilitating generation of fillable document templates,"Methods and systems are provided for facilitating generation of fillable document templates. In embodiments, a document having a plurality of tokens is obtained. Using a machine learned model, a token state is identified for each token of the plurality of tokens. Each token state indicates whether a corresponding token is a static token that is to be included in a fillable document template or a dynamic token that is to be excluded in the fillable document template. Thereafter, a fillable document template corresponding with the document is generated, wherein for each dynamic token of the document, the fillable document template includes a fillable field corresponding to the respective dynamic token.","['G06F40/174', 'G06F40/186', 'G06F40/103', 'G06F40/284']"
CN111539518B,Computation optimization mechanism for deep neural networks,"An apparatus for facilitating computational optimization is disclosed. The apparatus includes a plurality of processing units, each including a plurality of Execution Units (EUs), wherein the EUs include a first EU type and a second EU type.","['G06T1/20', 'G06F9/45533', 'G06F9/5061', 'G06F9/5094', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0464', 'G06N3/048', 'G06N3/063', 'G06N3/084', 'G06N3/0895', 'G06N3/09', 'G06N3/098', 'G06F2009/45583', 'G06F8/41', 'Y02D10/00']"
CN108734285B,Computational Optimization of Neural Networks,"One embodiment provides a computing device for performing a machine learning operation, the device comprising a decode unit to decode a single instruction into a decoded instruction specifying a plurality of operands including an input value and a quantization weight value associated with a neural network, an arithmetic logic unit comprising a barrel shifter, an adder, and an accumulator register, wherein to execute the decoded instruction, the barrel shifter shifts the input value by the quantization weight value to generate a shifted input value, and the adder adds the shifted input value to a value stored in the accumulator register and updates the value stored in the accumulator register.","['G06F5/015', 'G06F7/5443', 'G06F9/3001', 'G06F9/3851', 'G06F9/3887', 'G06F9/3888', 'G06F9/3893', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0464', 'G06N3/0495', 'G06N3/063', 'G06N3/084', 'G06N3/0895', 'G06N3/09', 'G06N3/098', 'G06T1/20', 'G06F2207/4824']"
US12314670B2,Semantic parsing of utterance using contractive paraphrasing,"Systems and methods are provided for automatically generating a program based on a natural language utterance using semantic parsing. The semantic parsing includes translating a natural language utterance into instructions in a logical form for execution. The methods use a pre-trained natural language model and generate a canonical utterance as an intermediate form before generating the logical form. The natural language model may be an auto-regressive natural language model with a transformer to paraphrase a sequence of words or tokens in the natural language utterance. The methods generate a prompt including exemplar input/output pairs as a few-shot learning technique for the natural language model to predict words or tokens. The methods further use constrained decoding to determine a canonical utterance, iteratively selecting sequence of words as predicted by the model against rules for canonical utterances. The methods generate a program based on the canonical utterance for execution in an application.","['G06F40/35', 'G06F40/30', 'G06F16/90332', 'G06F40/205', 'G06F40/216', 'G06F40/44', 'G06F40/55', 'G06F40/56', 'G06F40/58', 'G06N3/045', 'G06N3/08', 'G06N20/00']"
US20230376847A1,Conversation Based Diagnosis and Troubleshooting of Maintenance Requests Using a Large Language Model Driven Chatbot,"Aspects of the disclosure relate to diagnosing and troubleshooting maintenance repair requests using an artificial intelligence-driven chatbot. In some embodiments, a computing platform may receive a maintenance request from a user and may configure a chatbot to extract details that describe an item to be repaired. The computing platform may configure the chatbot to communicate with the user and to generate, based on the communication, an enriched work order. The computing platform may generate training data based on the maintenance request and the enriched work order, and may use the training data to train a plurality of regression models. The plurality of regression models to identify a plurality of technicians to handle the maintenance request and the computing platform may transmit the enriched work order to the plurality of technicians. The computing platform may continuously train the plurality of regression models based on feedback from the plurality of technicians.","['G06N20/00', 'G06N3/0455', 'G06N3/047', 'G06N3/0475', 'G06N3/088', 'G06Q10/20', 'G06N3/09']"
CN114168745B,Knowledge graph construction method for production process of ethylene oxide derivative,"The invention belongs to the technical field of knowledge maps, and provides a knowledge map construction method for an ethylene oxide derivative production process. And combing a data source in the production process of the ethylene oxide derivative into three types of data, namely structured data, unstructured data and other data according to the type and the characteristics of the data. And constructing a body layer and a data layer of the knowledge graph by adopting a method of combining top-down and bottom-up. An incremental ontology modeling method based on data driving is provided, and expandability of a knowledge graph is guaranteed. The method is oriented to structured knowledge extraction, adopts a virtual knowledge map method, ensures the safety of original data storage, and provides a new mapping mechanism to realize data materialization. The method is oriented to unstructured knowledge extraction, combines a pre-training language model BERT, and realizes an entity extraction task based on a BERT-BilSTM-CRF named entity recognition model.","['G06N3/042', 'G06F16/367', 'G06F16/3346', 'G06F16/335', 'G06F40/295', 'G06N3/044', 'G06N3/0442', 'G06N3/0455', 'G06N3/08', 'G06N5/025', 'G06N7/01', 'Y02P90/02']"
CN112084796B,Multi-language place name root Chinese translation method based on Transformer deep learning model,"The invention discloses a multilingual place name Chinese translation method based on a Transformer model, which covers English, French and German in language range: distinguishing the language of the place name to be translated based on the place name language knowledge base in combination with the language characteristics of the place name to be translated, and extracting the root of the place name to be translated according to the language by selecting corresponding place name root extraction rules in the place name root extraction rule base; converting the extracted place name root text into a character vector through a character embedding model; inputting character vectors of the place name roots to be translated in Chinese based on a Transformer model obtained by training and fine-tuning English, French and German place name roots and corresponding Chinese place name root translation linguistic data, and obtaining a final root Chinese translation result. The Chinese translation English, French and German place name root results provided by the invention have better readability, accord with Chinese reading habits, meet the Chinese translation requirements of multilingual place name root to a certain extent, and have good flexibility and universality.","['G06F40/58', 'G06F16/90344', 'G06F40/289', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/084']"
CN111581361B,Intention recognition method and device,"The application relates to the field of artificial intelligence, in particular to an intention recognition method and device, which are used for carrying out semantic analysis on sentences to be recognized, respectively obtaining semantic categories associated with a plurality of target classification tasks from the sentences to be recognized, wherein the semantic categories associated with the target classification tasks at least comprise entity categories, state categories and action categories; combining the obtained semantic categories into simplified sentences to be identified; according to the simplified sentences to be identified, the intention category of the sentences to be identified is determined, so that the sentence simplification accuracy is improved by combining semantic information, and the intention understanding accuracy is improved based on the simplified sentences.","['G06F16/3329', 'G06F40/211', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'Y02D10/00']"
US12373647B2,Semantic map generation from natural-language text documents,"Techniques include obtaining, with a computer system, a natural-language-text document comprising unstructured text; generating, with the computer system, based on a first set of machine learning model parameters, a neural representation of the unstructured text; identifying, with the computer system, based on the neural representation, a trigger word located within the unstructured text and associated with a first category; determining, with the computer system, based on the trigger word, a region within the unstructured text comprising descriptors associated with the first category; determining, with the computer system, from the region based on a second set of machine learning model parameters, a descriptor describing an action or condition of the first category; generating, with the computer system, a data model object comprising the descriptor defining an action or condition of the first category; and storing, with the computer system, the data model object in memory.","['G06F40/30', 'G06F16/313', 'G06F16/34', 'G06F16/35', 'G06F16/367', 'G06F16/9024', 'G06F16/9038', 'G06F16/93', 'G06F40/117', 'G06F40/137', 'G06F40/169', 'G06F40/20', 'G06F40/211', 'G06F40/258', 'G06F40/279', 'G06F40/284', 'G06F40/289', 'G06N5/022', 'G06N3/0442', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06Q50/18']"
US12229645B2,Target-based schema identification and semantic mapping for robotic process automation,"Target-based schema identification and semantic mapping for robotic process automation (RPA) are disclosed. When looking at a source, such as a document, a web form, a user interface of a software application, a data file, etc., it is often difficult for software to determine which fields are labels and which are values associated with those labels. Since values have not yet been entered for various labels (e.g., first name, company, customer number, etc.), these labels are easier to detect than when the target also includes various values associated with the labels. A selection of an empty target may be received and target-based schema identification may be performed on the empty target, determining labels and a type of the target. Semantic matching may then be performed between a source and the target. These features may be performed at design time or runtime.","['G06N3/006', 'B25J9/1653', 'G05B19/4155', 'G06F3/0482', 'G06F3/04847', 'G06F40/174', 'G06F40/18', 'G06F40/216', 'G06F40/30', 'G06F40/35', 'G05B2219/50391', 'G06N20/00', 'G06N3/045', 'G06N3/084', 'G06N7/01']"
US11568423B2,"Multi-dimensional product information analysis, management, and application systems and methods","Disclosed here are methods of analyzing product-related datasets using a computer system including a product data collection; receiving a product-related data submission; identifying system-identified terms in the submission; generating a query dataset comprising search elements, such as lexical vector(s), semantic vector(s), or both; querying the product data collection and identifying datasets that sufficiently match aspect(s) of the query dataset; comparing the content of the submission and matching datasets to provide an output such as a determination of an error or omission in the submission, identifying a relationship between the submission product and a product associated with an identified dataset; or assessing one or more product status characteristics; and optionally performing additional applications, such as generating a regulatory authority submission based on the determination that the submission product is subject a regulatory requirements based on the comparison of the submission with the identified datasets.","['G06Q50/26', 'G06Q30/018', 'G06F16/36', 'G06F40/279', 'G06F40/284', 'G06F40/30', 'G06Q10/0635', 'G06Q10/06395', 'G06Q10/0875', 'G06Q50/18']"
CN115564063A,"Pre-training language model construction method and device, electronic terminal and storage medium","The embodiment of the invention discloses a method and a device for constructing a pre-training language model, an electronic terminal and a storage medium, wherein the method comprises the following steps: acquiring dialogue data; processing the dialogue data according to the task requirements of each pre-training task to obtain training data and label data of the training data under each pre-training task; each pre-training task comprises: the method comprises the following steps of (1) a full word mask prediction task, a role prediction task, an intra-round sequence interchange prediction task and an inter-round sequence interchange prediction task; determining input data of a language model based on the training data, and executing each pre-training task on the input data through the language model to obtain each execution result; training the language model according to each execution result and label data of the training data under each pre-training task to obtain a pre-training language model; the pre-trained language model is used to encode dialogue data. The processing performance of the pre-training model for the dialogue data can be improved.","['G06N20/20', 'G06F40/35']"
US11893341B2,Domain-specific language interpreter and interactive visual interface for rapid screening,"A domain-specific language interpreter and live updating visual interface for rapid interactive exploration, filtering, and analysis of a dynamic data set. It includes a multi-line editor that allows a user to enter and edit input on any line at any time, and a grid view display. As the user enters an expression in the multi-line editor, it continually parses and executes the expression with respect to the domain-specific language, recognizing data tags and operations. Each data tag is associated with values for identifiers of the data set, and each operation can be applied to the values. The grid view display updates with a live display of identifiers and result values for the input expression according to the current contents of the multi-line editor user input interface.","['G06F40/166', 'G06F40/30', 'G06F16/2428', 'G06F16/248', 'G06F16/335', 'G06F16/38', 'G06F3/0481', 'G06F3/0482', 'G06F40/103', 'G06F40/177', 'G06F40/205', 'G06F40/216', 'G06F40/221', 'G06F40/274', 'G06N20/00']"
AU2022204702B2,Multimodal multitask machine learning system for document intelligence tasks,"Multimodal multitask machine learning system for document intelligence tasks includes a feature extractor processing token values obtained from a document to obtain features, and a token extraction head classifying, using the features, the token values to obtain classified tokens. The classified tokens are aggregated into entities. A document classification model is executed on the features to classify the document and obtain a document label prediction. Further a confidence head model applying the document label prediction processes the entities to obtain a result. 43 2/9 a) c C 0) 1-0 _0~ _0 -0 0 .2 U- U CnC. (DO - j (. 0 n 0 Ic N ) C )C4m 0 CU)O= k-.. 0)Cl)NC: 0 EE C H0 U0 0 CD H","['G06V30/413', 'G06V10/82', 'G06N3/084', 'G06V30/10', 'G06V30/224', 'G06V30/41', 'G06V30/412']"
CN116909991B,NLP-based scientific research archive management method and system,"The application discloses a scientific research archive management method and system based on NLP, and relates to the technical field of document management. According to the NLP-based scientific research archive management method, scientific research documents are classified and automatically classified into a set folder based on a transducer model; indexing and searching the documents according to the pre-training language model, topic modeling and metadata information of scientific research documents, and providing related documents; the method comprises the steps of extracting entity relations from scientific research documents, obtaining relevance among the scientific research documents, constructing a scientific research knowledge graph to show relevance among different scientific research documents and relations among domain knowledge, and constructing the knowledge graph through intelligent classification, document index search of pre-training language models and topic modeling and entity relation extraction, so that intelligent document management and search capability is provided for scientific research personnel, the scientific research personnel can position related documents more quickly, and document management and search efficiency is improved.","['G06F16/11', 'G06F16/14', 'G06F16/16', 'G06F16/335', 'G06F16/35', 'G06F16/367', 'G06F40/284', 'G06N3/0455', 'G06N3/092', 'Y02D10/00']"
US12265612B2,Method for identifying vulnerabilities in computer program code and a system thereof,"Open-source software is prevalent in the development of new technologies. Monitoring software updates for vulnerabilities is expensive and time consuming. Online discussions surrounding new software updates can often provide vital information regarding emerging risks. It is presented a novel approach for automating surveillance of software through the use of natural language processing methods on open-source issues. Further, the potential of virtual adversarial training, a popular semi-supervised learning technique, is used to leverage the vast amounts of unlabeled data available to achieve improved performance. On industry data, it is found that a hierarchical attention network with virtual adversarial training that utilizes the innate document structure to encapsulate the text can be used with good results.","['G06N3/088', 'G06F21/554', 'G06F18/214', 'G06F18/2178', 'G06F21/552', 'G06N3/044', 'G06N3/047', 'G06N3/082']"
US11861321B1,Systems and methods for structure discovery and structure-based analysis in natural language processing models,"A regular expression prompt may be determined by combining a regular expression prompt template with input text from an input document. The regular expression prompt template may include a natural language instruction to identify one or more regular expressions from the input text and one or more fillable portions designated for filling with the input text. The regular expression prompt may be sent to a large language model for evaluation, and one or more regular expressions may be identified based on a response received from the large language model. The regular expressions may be used to disaggregate the input text, and the disaggregated text portions may be used to determine a structured document based on the input document. The structured document may be used to determine a response to a query of the input document.","['G06F40/40', 'G06F16/33', 'G06F16/3344', 'G06F16/338', 'G06F40/174', 'G06F40/186', 'G06F40/205', 'G06F40/279', 'G06F40/289']"
US12361285B2,"Methods, systems and computer program products for media processing and display","The present disclosure relates generally to methods, systems and computer program products for classifying and identifying input data using neural networks and displaying results (e.g., images of vehicles, vehicle artifacts and geographical locations dating from the 1880s to present day and beyond). The results may be displayed on displays or in virtual environments such as on virtual reality, augmented reality and/or mixed-reality devices.","['G06N3/08', 'G06F16/433', 'G06F16/434', 'G06F16/438', 'G06F16/90332', 'G06F18/24143', 'G06N3/045', 'G06Q30/018', 'G06Q30/0241', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/20', 'G06V20/56', 'G06N3/042', 'G06N3/044', 'G06N3/048', 'G06N5/02', 'G06V2201/08', 'H04L63/02']"
US20200279105A1,Deep learning engine and methods for content and context aware data classification,"Methods, systems and deep learning engines for content and context aware data classification by business category and confidentiality level are provided. The deep learning engine includes a feature extraction module and a classification and labelling module. The feature extraction module extracts both context features and document features from documents and the classification and labelling module is configured for content and context aware data classification of the documents by business category and confidentiality level using neural networks.","['G06K9/00442', 'G06N3/08', 'G06F18/24', 'G06F18/24155', 'G06F18/2431', 'G06K9/6278', 'G06K9/628', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N20/20', 'G06N7/01']"
US11449556B2,Responding to user queries by context-based intelligent agents,"An automatic question answering system serves to respond to questions pertaining to a user-selected domain of a plurality of domains. A plurality of documents pertaining to the domain are received and processed to identify candidate answers. Questions corresponding to the candidate answers are automatically generated. The candidate answers and the questions are used to train a machine comprehension (MC) model. A user interface allows the selection of one of the plurality of domains for querying by a user. When a user query pertaining to the selected domain is received, the user query is analyzed to identify a relevant context from the plurality of documents. An answer span to the user query identified by the MC model from the relevant context is used to frame a complete response to the user query.","['G06F16/90332', 'G06F16/3329', 'G06F16/313', 'G06F16/90344', 'G06F40/284', 'G06N3/044', 'G06N3/045', 'G06N3/049', 'G06N3/08', 'G06N3/006']"
US9558734B2,Aging a text-to-speech voice,"A voice recipient may request a text-to-speech (TTS) voice that corresponds to an age or age range. An existing TTS voice or existing voice data may be used to create a TTS voice corresponding to the requested age by encoding the voice data to voice parameter values, transforming the voice parameter values using a voice-aging model, synthesizing voice data using the transformed parameter values, and then creating a TTS voice using the transformed voice data. The voice-aging model may model how one or more voice parameters of a voice change with age and may be created from voice data stored in a voice bank.","['G10L13/027', 'G10L13/033', 'G10L13/0335', 'G10L13/047', 'G10L13/06', 'G10L2021/0135', 'G10L25/48']"
US11775414B2,Automated bug fixing using deep learning,A device includes one or more processors configured to: receive source code including a section of source code associated with at least one bug or vulnerability; generate a formatted code section based at least partly on the section of source code associated with at least one bug or vulnerability; identify a matching patch model based on the formatted code section; provide the formatted code section to the matching patch model; receive a remedied code section from the matching patch model; and apply the remedied code section to the section of source code associated with at least one bug or vulnerability. Generating the formatted code section based at least partly on the section of source code associated with at least one bug or vulnerability includes: dividing the section of source code into sub-elements; associating type information to the sub-elements to generate tokens; and mapping each token to a unique identifier.,"['G06F11/3604', 'G06F11/3624', 'G06F11/3608', 'G06F21/57', 'G06F8/70', 'G06F8/71', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06F11/0793', 'G06F8/427']"
CN110431566B,Probability-based director,"The disclosed technology proposes to use a combination of computationally inexpensive, less accurate bag of words (BoW) models and computationally expensive, more accurate long-term memory (LSTM) models to perform natural processing tasks, such as emotion analysis. The use of inexpensive, less accurate BoW models is referred to herein as ""skimming"". The use of expensive, more accurate LSTM models is referred to herein as ""reading"". The disclosed technology proposes a probability-based director (PBG). The PBG combines the use of a BoW model and an LSTM model. The PBG uses a probability threshold strategy to determine whether to invoke the LSTM model to reliably classify the sentence as either positive or negative based on the results of the BoW model. The disclosed technology also proposes a deep neural network based decision network (DDN) trained to learn the relationship between the BoW model and the LSTM model and invoke only one of the two models.","['G06F15/76', 'G06F18/217', 'G06F18/24', 'G06F18/241', 'G06F18/24133', 'G06F40/169', 'G06F40/30', 'G06N20/00', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/047', 'G06N3/048', 'G06N3/0499', 'G06N3/084', 'G06N3/09', 'G06N5/04', 'G06V10/764', 'G06V10/776', 'G06V10/82']"
CN111191078B,Video information processing method and device based on video information processing model,"The present invention provides a video information processing method based on a video information processing model, comprising: acquiring a video to be processed, and processing the key frames through an image processing network to acquire image feature vectors matched with the key frames; processing the title text information to obtain a title text processing result corresponding to the title text information; determining a correlation parameter of the image feature vector and the title text processing result, and determining a corresponding cover image frame to be recommended according to the correlation parameter of the image feature vector and the title text processing result; the invention further provides an information processing device, electronic equipment and a storage medium. The method and the device can determine the cover image frame to be recommended, which is matched with the interest of the target user, through the correlation parameters of the interest tag vector of the target user and the cover image frame to be recommended.","['G06F16/7844', 'G06F16/7847', 'G06F16/7867', 'G06V20/40', 'G06V20/46', 'G06V20/48', 'G06V20/62', 'G06V30/153', 'G06V30/10']"
US11954243B2,Mapping and localization of a passable world,"An augmented reality (AR) device can be configured to generate a virtual representation of a user's physical environment. The AR device can capture images of the user's physical environment to generate or identify a user's location. The AR device can project graphics at designated locations within the user's environment to guide the user to capture images of the user's physical environment. The AR device can provide visual, audible, or haptic guidance to direct the user of the AR device to explore the user's environment.","['G06F3/011', 'G06F3/012', 'G06T13/20', 'G06T15/06', 'G06T19/003', 'G06T19/006', 'G06V10/82', 'G06V20/20', 'G06V40/18', 'G06F18/24137', 'G06T2210/21']"
CN107195306B,Recognizing credential-providing speech input,"Systems and processes are provided for recognizing a voice input providing one or more user credentials. In one exemplary process, a speech input may be received. Based on the speech input, a first character, a phrase for identifying a second character, and a word may be identified. In response to the recognition, the first character, the second character, and the word may be converted to text. The text may be caused to be displayed with the display in a sequence corresponding to an order of the first character, the second character, and the word in the speech input.","['G10L15/26', 'G06F21/00', 'G06F21/31', 'G06F40/166', 'G06F40/279', 'G10L15/08', 'G10L15/197', 'G10L15/22', 'G10L15/24', 'G10L15/30', 'G10L2015/086', 'G10L2015/223']"
US20230418848A1,Neural ranking model for generating sparse representations for information retrieval,A ranker for a neural information retrieval model comprises a document encoder having a pretrained language model layer and configured to receive one or more documents and generate a sparse representation for each of the documents predicting term importance of the document over a vocabulary. A separate query encoder is configured to receive a query and generate a representation of the query over the vocabulary. Generated representations are compared to generate a set of respective document scores and rank the one or more documents.,"['G06F40/30', 'G06F16/332', 'G06F16/3347', 'G06F40/284', 'G06F40/40']"
CN115712709A,Multi-modal dialog question-answer generation method based on multi-relationship graph model,"A multi-modal dialog question-answer generating method based on a multi-relationship graph model relates to a multi-modal dialog question-answer generating method. The invention aims to solve the problem that the existing multi-modal dialog system only considers scene serialization information to cause the effect of the existing model to be general. Firstly, video is serialized and segmented into a plurality of video segments, the color feature, the optical flow feature and the audio feature of each segment are obtained and spliced, and then position information and modal information are added to obtain the sequential representation of each video segment; taking each video clip as a vertex, constructing a video graph based on a full link relation and inputting the video graph into a graph convolution neural network to obtain a video hidden layer sequence and a fusion expression of the video hidden layer sequence and an original video sequence; then, processing word vectors corresponding to the audiovisual scene titles and the conversation histories in a similar mode to obtain corresponding text hidden layer sequences and fusion representations of the text hidden layer sequences and the original text sequences; and finally, generating an answer by using the neural network model.",['Y02D10/00']
EP4594939A1,Visual tokenization with language models,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for quantizing a visual input to generate a visual output. In one aspect, a method comprises receiving a visual input, processing the visual input using an encoder neural network to generate multiple feature vectors, generating a quantized representation that identifies a respective visual token from a vocabulary of visual tokens for each feature vector by mapping each dimension of the feature vector to a corresponding integer value from a set of integer values, and generating data identifying the visual token based on the integer values. In another aspect, the system receives and processes a conditioning input using a visual token generation neural network to generate a sequence of visual tokens each including an integer value for each of multiple dimensions, and processing the sequence of visual tokens using a decoder neural network to generate a visual output.","['G06N3/045', 'G06N3/0455', 'G06N3/0475']"
US20200050638A1,Systems and methods for analyzing the validity or infringment of patent claims,"The present invention relates to systems and methods for determining the validity or infringement of a patent claim using natural language processing and information retrieval techniques. Embodiments of the disclosed technology include methods for analyzing whether a plurality of references describe a limitation of a patent claim by a computing device by indexing the plurality of references to create a plurality of search documents, building a search index from the plurality of search documents, generating a query from the limitation, executing a search in the search index for search documents that match the query, and outputting a result, comprising a search document that matches the query and a score representing a relevancy of the search document to the query","['G06F16/90332', 'G06F16/901', 'G06F16/90344', 'G06F17/277', 'G06F17/2775', 'G06F17/2795', 'G06F40/247', 'G06F40/284', 'G06F40/289', 'G06F40/30', 'G06F16/951']"
US11573957B2,Natural language processing engine for translating questions into executable database queries,"A system and method for translating questions into database queries are provided. A text to database query system receives a natural language question and a structure in a database. Question tokens are generated from the question and query tokens are generated from the structure in the database. The question tokens and query tokens are concatenated into a sentence and a sentence token is added to the sentence. A BERT network generates question hidden states for the question tokens, query hidden states for the query tokens, and a classifier hidden state for the sentence token. A translatability predictor network determines if the question is translatable or untranslatable. A decoder converts a translatable question into an executable query. A confusion span predictor network identifies a confusion span in the untranslatable question that causes the question to be untranslatable. An auto-correction module to auto-correct the tokens in the confusion span.","['G06F16/243', 'G06F40/58', 'G06F16/24522', 'G06F16/24524', 'G06F40/284', 'G06F40/47']"
CN117236647B,Post recruitment analysis method and system based on artificial intelligence,"The invention provides a post recruitment analysis method and a system based on artificial intelligence, comprising the following steps: qualitative and quantitative analysis is carried out on different posts; establishing a talent assessment model, and assessing the first post matching degree of talents according to the talent assessment model; different weights and bias degrees are set for different posts; setting different evaluation values for different talent parameters; and optimizing the talent assessment model based on different weights, bias degrees and evaluation degrees, and performing second-post matching degree assessment by using the optimized talent assessment model.",[]
US11182432B2,Vertical processing of natural language searches,"The disclosed embodiments provide a system for performing a natural language search. During operation, the system applies a first machine learning model to a natural language query to predict one or more search intentions associated with the natural language query. Next, the system applies a second machine learning model to the natural language query to produce one or more search parameters associated with a first intention in the search intention(s), wherein the search parameter(s) include a field and a value of the field. The system then performs a first search of a first vertical associated with the first intention using the search parameter(s). Finally, the system generates a ranking containing a first set of search results from the first search of the first vertical and outputs at least a portion of the ranking in a response to the natural language query.","['G06F40/30', 'G06F16/24578', 'G06F16/90332', 'G06N20/00', 'G06N20/20', 'G06N20/10', 'G06N3/044', 'G06N3/045', 'G06N5/01', 'G06N7/01']"
CN112786006B,"Speech synthesis method, synthesis model training method, device, medium and equipment","The present disclosure relates to a speech synthesis method, a synthesis model training method, a device, a medium and equipment. The method comprises the following steps: acquiring voice characteristic information corresponding to a text to be synthesized; inputting the voice characteristic information into a voice synthesis model to obtain predicted waveform point information corresponding to a text to be synthesized, wherein the voice synthesis model comprises an acoustic submodel and a vocoder, and the voice synthesis model is obtained by directly carrying out joint training on the acoustic submodel and the vocoder; and performing mu-law expansion on the predicted waveform point information to obtain audio information. Therefore, the efficiency of voice synthesis can be improved, error accumulation generated by respectively training the acoustic submodel and the vocoder in the related technology can be effectively reduced, and the accuracy of voice synthesis can be improved. In addition, the problem that the generated audio information cannot adapt to special pronunciation demands due to the fact that the acoustic features do not have universality can be avoided, and the voice synthesis effect is improved. In addition, the training period of the model is short, and the rhythm fidelity of the model is better.","['G10L13/04', 'G10L25/03', 'G10L25/30']"
US11475067B2,"Systems, apparatuses, and methods to generate synthetic queries from customer data for training of document querying machine learning models","Techniques for generation of synthetic queries from customer data for training of document querying machine learning (ML) models as a service are described. A service may receive one or more documents from a user, generate a set of question and answer pairs from the one or more documents from the user using a machine learning model trained to predict a question from an answer, and store the set of question and answer pairs generated from the one or more documents from the user. The question and answer pairs may be used to train another machine learning model, for example, a document ranking model, a passage ranking model, a question/answer model, or a frequently asked question (FAQ) model.","['G06F16/335', 'G06F16/903', 'G06F16/90332', 'G06F16/9038', 'G06F16/93', 'G06F18/2148', 'G06F18/2178', 'G06F40/20', 'G06K9/6257', 'G06N20/00']"
US11972223B1,Query evaluation in natural language processing systems,"A system may determine relevance prompts based on input documents and a relevance prompt template and may transmit the plurality of relevance prompts to a large language model for completion. The system may receive response messages including chunk relevance scores. The system may select a subset of the input documents based on the chunk relevance scores. The system may determine query response prompts including text from the selected input documents the natural language query, and a second set of natural language instructions to address the natural language query. The system may determine a response to the natural language query based on answers determined in response to the query response prompts.","['G06F40/35', 'G06F40/40', 'G06F40/289', 'G06N20/00', 'G06N3/0455', 'G06N3/09', 'G06N3/0985', 'G06F40/131', 'G06F40/205']"
US20230395075A1,Human-machine dialogue system and method,"A human-machine dialogue system includes one or more processors configured to execute instructions to cause the human-machine dialogue system to perform operations. The operations include: performing intention clustering of a dialogue data sample based on a semantic representation of the dialogue data sample; constructing, based on a clustering result, a dialogue procedure corresponding to the dialogue data sample; obtaining a semantic representation corresponding to a voice dialogue of a user; performing intention analysis on the semantic representation to obtain an intention analysis result; determining, according to the intention analysis result and the dialogue procedure constructed in advance, a dialogue response; and performing voice interaction of the dialogue response with the user, wherein the dialogue response is an answer response to the voice dialogue, or a clarification response to clarify a dialogue intention of the voice dialogue.","['G06F16/3329', 'G10L15/22', 'G06F16/3343', 'G06F16/3344', 'G10L13/027', 'G10L15/1815', 'G10L15/1822', 'G10L25/63', 'G10L2015/223']"
WO2024155584A1,"Systems, methods, devices, and platforms for industrial internet of things","In example embodiments, a method of detecting an anomaly associated with a machine includes recording a data set associated with the machine; determining, by a first machine learning model, a label associated with the data set; determining whether the label is to be reviewed; and responsive to determining that the label is to be reviewed, subjecting the data set and the label to a review, and updating the label based on the review. Alternatively or additionally, in example embodiments, a method of presenting an analysis of a machine included in an industrial facility includes generating a digital twin of the machine; determining at least one property of the digital twin based on a simulation of an operation of the machine; and generating a presentation of the industrial facility that includes a visualization of the digital twin and a visual indicator of the at least one property of the digital twin.","['G05B23/024', 'G05B19/4184', 'G05B19/41885', 'G06N20/00', 'G06N7/01', 'G05B2219/31356', 'G05B2219/31467', 'G06N3/006']"
US12314017B2,Automatic data transfer between a source and a target using semantic artificial intelligence for robotic process automation,"Automatic data transfer between a source and a target using semantic artificial intelligence (AI) for robotic process automation (RPA) is disclosed. A user may be provided with the option of selecting a source and a target and indicating through an intuitive user interface that he or she would like to copy data from the source to the destination, regardless of format. This may be done at design time or at run time. For instance, the source and/or target may be a web page, a graphical user interface (GUI) of an application, an image, a file explorer, a spreadsheet, a relational database, a flat file source, any other suitable format, or any combination thereof. The source and the target may have different formats. The source, target, or both may not necessarily be visible to the user.","['G06F9/451', 'G05B13/04', 'G06F18/22', 'G06F40/30', 'G06F8/34', 'G06F8/38', 'G06F9/543', 'G06V30/10', 'G06N3/084', 'G06N3/09']"
CN108733051B,Autonomous Vehicle Advanced Sensing and Response,"The application discloses autonomous vehicle advanced sensing and response. One embodiment provides a computing device within an autonomous vehicle, the computing device comprising a wireless network device to enable a wireless data connection with an autonomous vehicle network, a set of multiple processors including a general purpose processor and a general purpose graphics processor to execute a computing manager to manage execution of computing workloads associated with the autonomous vehicle, the computing workloads being associated with autonomous operation of the autonomous vehicle, and offload logic configured to execute on the set of multiple processors to determine offloading of one or more of the computing workloads to one or more autonomous vehicles within range of the wireless network device.","['G07C5/008', 'G05D1/0214', 'B60W30/00', 'B60W50/0098', 'G01C21/34', 'G01C21/3415', 'G01C21/3492', 'G05D1/0088', 'G05D1/0276', 'G06F9/5027', 'G06N20/00', 'G06N20/10', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0464', 'G06N3/063', 'G06N3/084', 'G06N3/0895', 'G06N3/09', 'G06N3/098', 'G06T1/20', 'G06T1/60', 'G08G1/012', 'H04L67/12', 'H04W4/40', 'B60W2050/0006', 'B60W2556/65', 'G01S19/13', 'G05D1/0257', 'G06F2209/509', 'G08G1/0112', 'G08G1/052', 'H04L43/0852', 'H04L43/16', 'Y02D10/00']"
US11922303B2,Systems and methods for distilled BERT-based training model for text classification,"Embodiments described herein provides a training mechanism that transfers the knowledge from a trained BERT model into a much smaller model to approximate the behavior of BERT. Specifically, the BERT model may be treated as a teacher model, and a much smaller student model may be trained using the same inputs to the teacher model and the output from the teacher model. In this way, the student model can be trained within a much shorter time than the BERT teacher model, but with comparable performance with BERT.","['G06N3/08', 'G06F40/40', 'G06N3/045', 'G06N3/084', 'G06N3/048']"
US11741139B2,Systems and methods for determining a response to a user query,Systems and methods are presented for providing a response to a user query. Reception of a user query is detected. An augmentation machine learning model is utilized to determine one or more variations of the user query that correspond to a semantic meaning of the user query. A plurality of response candidates is determined that correspond to the user query by comparing the user query and the one or more variations of the user query to a plurality of documents. A final response candidate is determined from the plurality of response candidates based on utilizing a semantic machine learning model to perform a semantic comparison between the plurality of response candidates and at least the user query.,"['G06F40/247', 'G06F16/3322', 'G06F16/3329', 'G06F16/3334', 'G06F16/3338', 'G06F40/216', 'G06F40/30', 'G06N20/00', 'H04L51/02']"
WO2022252636A1,"Artificial intelligence-based answer generation method and apparatus, device, and storage medium","The present application relates to the field of artificial intelligence, and in particular, to an artificial intelligence-based answer generation method and apparatus, a device, and a storage medium, which enable a generation model to generate a more relevant and accurate answer when information of a sentence to be answered of a user is limited. The artificial intelligence-based answer generation method comprises: receiving a sentence to be answered, and inputting said sentence into a retrieval model; acquiring similar sentences screened out by the retrieval model from a preset dialogue corpus and matching said sentence, and acquiring answer sentences of the similar sentences in the dialogue corpus; on the basis of a preset semantic alignment keyword extraction model, extracting difference keywords between the similar sentences and said sentence to obtain a difference keyword set; extracting similar keywords of the answer sentences to obtain a similar keyword set; and inputting the difference keyword set and the similar keyword set into a trained generation model, and outputting an answer. By means of artificial intelligence-based natural language processing, a more relevant and accurate answer is given when the information of a question of a user is limited in an intelligent dialogue.","['G06F16/3329', 'G06F40/289', 'G06F40/35', 'Y02D10/00']"
US11074909B2,Device for recognizing speech input from user and operating method thereof,"Provided are a device for recognizing a speech input including a named entity from a user and an operating method thereof. The device is configured to: generate a weighted finite state transducer model by using a vocabulary list including a plurality of named entities; obtain a first string from a speech input received from a user, by using a first decoding model; obtain a second string by using a second decoding model that uses the weighted finite state transducer model, the second string including a word sequence, which corresponds to at least one named entity, and an unrecognized word sequence not identified as a named entity; and output a text corresponding to the speech input by substituting the unrecognized word sequence of the second string with a word sequence included in the first string.","['G10L15/197', 'G10L15/193', 'G10L15/04', 'G10L15/26', 'G10L2015/025', 'G10L2015/027']"
CN112559698B,Method and system for improving video question answering accuracy based on multimodal fusion model,"The invention provides a method and a system for improving video question-answer precision based on a multi-mode fusion model, which comprises the following steps: acquiring video data and question features to acquire questions asked and answered by videos; extracting visual features and subtitle features from video data; performing fusion processing on the visual features and the subtitle features to obtain fusion visual features and fusion subtitle features; inputting the fusion visual features, the fusion caption features and the problem features into a multi-modal fusion model for training to obtain a trained multi-modal fusion model; inputting the questions asked and answered by the video into a trained multi-mode fusion model to obtain answers to the questions; different target entity examples are focused aiming at different problems according to the characteristics of the problems, and the accuracy of model selection answers is improved.","['G06F16/3329', 'G06F18/253', 'G06F40/289', 'G06N3/045', 'G06V20/40', 'G06V30/10']"
US20230401386A1,Techniques for Pretraining Document Language Models for Example-Based Document Classification,"A data processing system implements a method for training machine learning modes, including receiving a set of one or more unlabeled documents associated one or more first categories of documents to be used to train machine learning models to analyze the one or more unlabeled documents, and fine-tuning a first machine learning model and a second machine learning model based on the one or more unlabeled document to enable the first machine learning model to determine a semantic representation of the one or more first categories of document, and to enable the second machine learning model to classify the semantic representations according to the one or more first categories of documents, the first machine learning model and the second machine learning model having been trained using first unlabeled training data including a second plurality of categories of documents that do not include the one or more first categories of documents.","['G06F40/30', 'G06F16/313', 'G06F16/35', 'G06F16/906', 'G06F16/93', 'G06F40/284', 'G06F40/289', 'G06N3/0455', 'G06N3/0895', 'G06N5/01', 'G06N5/022', 'G06N3/048']"
CN114372173A,Natural language target tracking method based on Transformer architecture,"The invention discloses a natural language target tracking method based on a Transformer architecture, which comprises the following steps: step S1: carrying out data loading and processing; step S2: extracting the characteristics of the input data in the step S1 by adopting an encoder and a decoder of a Transformer structure; step S3: carrying out feature fusion of different modes; step S4: and carrying out model training and reasoning to obtain a prediction result. The invention provides an end-to-end unified network based on Transformers, which adopts two pairs of encoder and decoder structures to cooperatively learn the reference and tracking. It can not only perform visual reference or tracking independently, but also perform language description based tracking by fusing low-level and high-level features for higher robustness and higher accuracy.","['G06F16/783', 'G06F16/75', 'G06F16/7867', 'G06F18/241', 'G06F18/2415', 'G06N3/045', 'G06N3/047', 'G06N3/048', 'G06N3/08']"
CN117571014B,A visual language navigation method combining image description and text generation,"The invention relates to a visual language navigation method for generating an image by combining image description and text, belonging to the technical field of visual language navigation. The method comprises the steps of obtaining a natural language target instruction of a visual language navigation task and a visual image of a current position, generating detailed natural language description based on a current scene through a scene description module, generating a similar scene with similar core object objects and core scene layout with the described scene through a text generation image model, encoding the current scene and the corresponding similar scene through a multi-layer Transformer structure and a fine-scale trans-modal encoder, and dynamically fusing the current scene encoding and the similar scene encoding to generate fusion action prediction comprehensively considering the current real scene and the corresponding similar scene. The method solves the problem of data scarcity of visual language navigation tasks by providing additional visual data input for task training, and improves the task performance of the intelligent agent and the generalization capability of the model.","['G06T11/00', 'G01C21/3626', 'G01C21/3629', 'G01C21/3638', 'G06N3/0455', 'G06N3/047', 'G06N3/048', 'G06N3/08', 'G06V10/454', 'G06V10/82', 'Y02D10/00']"
CN113656581B,"Text classification and model training method, device, equipment and storage medium","The disclosure provides a method, a device, equipment and a storage medium for text classification and model training, which relate to the technical field of computers, in particular to the fields of intelligent search, big data, deep learning and the like. The specific implementation scheme is as follows: acquiring a text to be classified; inputting a text to be classified into a pre-trained deep learning model, obtaining a classification result of the text to be classified through the deep learning model, wherein the deep learning model is trained based on weight information corresponding to a plurality of training samples respectively, and aiming at a training sample, the weight information comprises local attention weights among a plurality of sample tags of the training sample and mutual attention weights of the training sample and each sample tag. The deep learning model is trained by taking local attention weights among sample tags and mutual attention weights of training samples and each sample tag into consideration, so that classification accuracy can be improved by using the deep learning model.","['G06F16/35', 'G06F18/214']"
CN110795552B,"Training sample generation method and device, electronic equipment and storage medium","The invention provides a training sample generation method, which comprises the following steps: acquiring initial text data to be processed; word segmentation processing is carried out on the initial text data so as to form keywords matched with the initial text data; screening the initial text according to the keywords matched with the initial text data to form a target text aiming at the specified service; training a corresponding text processing model through the target text; and performing field data augmentation processing on the target text according to the training result of the text processing model to form a training sample aiming at the specified service. The invention also provides a training sample generation device, electronic equipment and a storage medium. The training sample recognition method and the training sample recognition device can improve pertinence of the training sample, are more suitable for machine reading and understanding tasks, improve recognition accuracy of the neural network model in the specific service field and enhance robustness.","['G06F16/335', 'G06N3/04', 'G06N3/08', 'Y02D10/00']"
US12277401B2,Method and apparatus for acquiring pre-trained model,"The present disclosure discloses a method and apparatus for acquiring a pre-trained model, and relates to natural language processing and deep learning technologies in the field of artificial intelligence technologies. An implementation includes: acquiring training data, the training data including a single-modal language material and a multi-modal language material, and the multi-modal language material including a language material pair formed by a first-modal language material and a second-modal language material; and performing a multi-task training operation on a pre-trained model using the training data, the multi-task including at least one cross-modal contrastive learning task and at least one single-modal learning task; the pre-trained language model obtained in the present disclosure may learn from different forms of language materials, i.e., the single-modal language material and the multi-modal language material, such that the pre-trained language model may effectively process information in various modals.","['G06N3/08', 'G06F16/9024', 'G06F18/214', 'G06F18/256', 'G06F40/205', 'G06F40/279', 'G06F40/47', 'G06F40/58', 'G06N3/045', 'G06V10/803', 'G06N3/02']"
WO2022198868A1,"Open entity relationship extraction method, apparatus and device, and storage medium","An open entity relationship extraction method, apparatus and device, and a storage medium, for solving the problem that existing open relationship extraction is difficult to process a variable type relationship. The open entity relationship extraction method comprises: preprocessing an entity relationship, a field length and a relationship triplet of a relationship classification data set to be processed to obtain a data set to be processed; building an initial unsupervised generative model by means of a pre-trained trunk model, and training and optimizing the initial unsupervised generative model by using the data set to obtain a target unsupervised generative model; performing word segmentation and word pairing processing on text to be processed to obtain preprocessed text; and performing hidden layer vector conversion, entity relationship prediction and text sequence generation on the preprocessed text by means of the target unsupervised generative model to obtain target entity relationship information. In addition, the relationship classification data set to be processed may be stored in a block chain.","['G06F40/295', 'G06F40/242', 'G06F40/247', 'G06N3/088']"
CN119272641A,A method for predicting the remaining life of industrial equipment by integrating pre-trained large language model,"The invention discloses a method for predicting the residual life of industrial equipment by fusing a pre-training large language model, which belongs to the technical fields of deep learning, large language model, time sequence data feature extraction and the like, and aims at the complexity and high-dimensional feature of the time sequence data of the industrial equipment, firstly, the original data is processed by a time sequence data preprocessing module to obtain independently segmented degradation data, then the coding of the degradation data is realized by using the mark coding, the position coding and the time coding of an input embedding module, and then realizing the characteristic alignment of the pre-trained large language model and the time sequence through a pre-trained large language model parameter fine adjustment module, transmitting pre-training weights to an additional network coding module of the pre-trained large model, reducing the number of parameters to be adjusted, improving the generalization capability and accuracy of the model, and finally obtaining the prediction result of the residual life of industrial equipment through an output layer.","['G06F30/27', 'G06N3/0455', 'G06N3/082', 'G06F2119/02', 'G06F2119/04', 'Y02P90/30']"
US11468098B2,Knowledge-intensive data processing system,"Embodiments of the invention provide systems and methods for managing and processing large amounts of complex and high-velocity data by capturing and extracting high-value data from low value data using big data and related technologies. Illustrative database systems described herein may collect and process data while extracting or generating high-value data. The high-value data may be handled by databases providing functions such as multi-temporality, provenance, flashback, and registered queries. In some examples, computing models and system may be implemented to combine knowledge and process management aspects with the near real-time data processing frameworks in a data-driven situation aware computing system.","['G06F16/285', 'G06F11/3006', 'G06F11/3442', 'G06F16/219', 'G06F16/273', 'G06F16/9535', 'G06F11/3452', 'G06F11/3466', 'G06F2201/84', 'G06F2201/865']"
CN113268995B,"Chinese academy keyword extraction method, device and storage medium","The invention provides a method, a device and a storage medium for extracting Chinese academic keywords, wherein the method comprises the following steps: acquiring academic predictions from academic text data sets, inputting the academic predictions into a pre-training model containing external semantic information, and generating dynamic word vectors of input academic corpora; obtaining a dynamic word vector input sequence based on the dynamic word vector, inputting the dynamic word vector input sequence into a BilSTM-CRF sequence labeling model, obtaining a score of a label sequence corresponding to the input sequence, obtaining an optimal label sequence based on the score, and obtaining a candidate keyword set based on the obtained optimal label sequence result; and ranking the candidate keywords based on the title similarity and the word frequency reverse document frequency TFIDF characteristics of the candidate keywords in the candidate keyword set, and acquiring the target keywords based on the ranking result.","['G06F40/30', 'G06F16/3344', 'G06N3/044']"
WO2022036616A1,Method and apparatus for generating inferential question on basis of low labeled resource,"Disclosed are a method and apparatus for generating an inferential question on the basis of a low labeled resource. The method comprises the following steps: S1, acquiring a labeled data set and an unlabeled data set, and establishing a question generation function; S2, building an entity diagram by taking entity words as nodes; S3, analyzing a relationship between the entity words of the entity diagram to connect the entity words to obtain a sub-diagram; S4, representing text and an inference chain as vectors, and then processing same by means of an attention mechanism to obtain a fusion vector of an input of step S5; S5, using the unlabeled data set to estimate a parameter for controlling an expression mode of a question, and using a probability distribution to perform calculation in order to generate the question; and S6, calculating a loss function index for the question, and if a preset condition is met, obtaining a final model, and ending calculation, otherwise, adjusting a model parameter, and then returning to step S4. The advantages of the present invention lie in that prior knowledge, such as an expression mode, is learned from unlabeled question data, and the syntax of a generated question is standardized, thereby effectively improving the readability of the generated question.","['G06F16/30', 'G06N3/04']"
CN104620314B,Constructed Embedded System for Small Speech Recognition with User-Definable Constraints,"The technology disclosed herein includes systems and methods that enable voice triggering that wakes up an electronic device or causes the device to activate additional voice commands without the need to manually initiate voice command functions. In addition, such voice triggers are dynamically programmable or customizable. The speaker may program or specify a particular phrase as a voice trigger. Generally, the techniques herein operate a voice-controlled wake-up system that operates on a Digital Signal Processor (DSP) or other low-power, auxiliary processing unit of an electronic device rather than a Central Processing Unit (CPU). The speech recognition manager runs two speech recognition systems on the electronic device. The CPU dynamically creates a compact speech system for the DSP. Such a compact system can continue to operate during standby mode without draining the battery supply too quickly.","['G10L15/22', 'G10L15/02', 'G10L15/32', 'H04M1/724', 'G10L2015/223', 'H04M2250/74', 'Y02D30/70']"
US20240289407A1,Search with stateful chat,"Implementations are described herein for augmenting a traditional search session with stateful chatâ€”via what will be referred to as a â€œgenerative companionâ€â€”to facilitate more interactive searching. In various implementations, a query may be received, e.g., from a client device operated by a user. Contextual information associated with the user or the client device may be retrieved. Generative model (GM) output may be generated based on processing, using a generative model, data indicative of the query and the contextual information. Synthetic queries may be generated using the GM output, and search result documents (SRDs) may be selected. State data indicative of: the query, contextual information, one or more of the synthetic queries, and the set of search result documents, may be processed to identify a classification of the query. Based on the classification downstream GM(s) may be selected and used to generate one or more additional GM outputs.","['G06F16/335', 'G06F16/9577', 'G06F16/3338', 'G06F16/9532', 'G06F16/9535', 'G06F16/9537', 'G06F40/40']"
CN112017645B,Voice recognition method and device,"The application discloses a voice recognition method and a device, wherein the method comprises the following steps: acquiring text data output after voice recognition of voice signals of all clients in a current call through a voice recognizer, wherein the voice recognizer comprises a target field language model; determining the target topic field of the current call according to the text data of each client; judging whether the target domain language model is matched with the target topic domain, if not, switching the target domain language model into a domain language model matched with the target topic domain, so as to dynamically select the target domain language model matched with the topic domain of the current call content, and selecting the optimal recognition result by utilizing the matched target domain language model, thereby improving the recognition rate of the voice recognizer and being suitable for the scene of the call content which dynamically changes in the terminal.","['G10L15/065', 'G06F16/3346', 'G06F16/35', 'G06F18/295', 'G06F40/30', 'G06N3/045', 'G10L15/26']"
US20220197923A1,Apparatus and method for building big data on unstructured cyber threat information and method for analyzing unstructured cyber threat information,"Disclosed herein are an apparatus and method for constructing big data on unstructured cyber threat information. The method may include collecting unstructured cyber threat information, structuring the collected unstructured cyber threat information based on a previously trained AI model, and constructing big data from the structured cyber threat information.","['G06F21/56', 'G06F16/258', 'G06F21/55', 'G06F16/316', 'G06F16/36', 'G06F16/38', 'G06F21/552', 'G06F21/554', 'G06F21/57', 'G06F40/205', 'G06F40/295', 'G06N20/00', 'G06N3/045', 'G06N3/08']"
US11080485B2,Systems and methods for generating and recognizing jokes,"Joke recognition methods include using server(s) coupled with data store(s) to communicatively couple with a first computing device through a telecommunications network. A first communication is provided to a user through a user interface of the first computing device or is received through the user interface. A second communication is provided to the user through the user interface or is received through the user interface. In response to providing or receiving the second communication, the server(s) determine whether the second communication, relative to the first communication, includes a joke and/or a punchline. Upon determining that the second communication includes a joke and/or a punchline, the server(s) initiate sending one or more responses to the first computing device. The response(s) initiate providing, through the user interface, an indication to the user that the second communication is recognized as a joke/punchline. Systems for joke recognition provide the disclosed joke recognition methods.","['G06F40/30', 'G06F40/289', 'G06F40/10', 'G06F40/211', 'G06F40/253', 'G06F40/56', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N5/046', 'G06N3/048']"
US11989518B2,"Normalized processing method and apparatus of named entity, and electronic device",A normalized processing method of a named entity includes: obtaining first text data; recognizing a named entity from the first text data; determining whether a first standard named entity exists in a standard named entity database according to the named entity; determining the first standard named entity as a normalized representation of the named entity in response to determining that the first standard named entity exists in the standard named entity database; and obtaining a second standard named entity from the standard named entity database and determining an obtained second standard named entity as the normalized representation of the named entity in response to determining that the first standard named entity does not exist in the standard named entity database.,"['G06F40/295', 'G06F16/3344', 'G06F40/247', 'G06F40/279', 'G06N3/044', 'G16H50/70', 'G16H70/60', 'G06N3/045', 'G06N3/048']"
CN115050064B,"Human face liveness detection method, device, equipment and medium","The application discloses a method, a device, equipment and a medium for detecting human face living body, and relates to the field of image processing. The method comprises the steps of generating an image sequence according to a face image, wherein the image sequence comprises the face image and n image local blocks, the image local blocks are obtained by dividing the face image, n is a positive integer, generating a target living body characteristic vector based on the image sequence, the target living body characteristic vector is generated based on living body characteristic information interaction between the face image and the image local blocks and living body characteristic information interaction between the image local blocks, classifying the target living body characteristic vector to obtain a prediction score, and determining that a face in the face image is a living body in response to the prediction score being higher than a judgment threshold. The application can extract the living body characteristic information between the whole and the part and the living body characteristic information between the part and the part in the face image, so that the living body detection result of the face is closer to the actual situation, and the detection precision and the accuracy are improved.","['G06V40/40', 'G06V10/82', 'G06Q20/40145', 'G06Q20/40', 'G06T3/40', 'G06T7/11', 'G06T7/50', 'G06V10/44', 'G06V10/764', 'G06V40/172', 'G06V40/45', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30201']"
WO2021143018A1,"Intention recognition method, apparatus, and device, and computer readable storage medium","An intention recognition method, apparatus, and device, and a computer readable storage medium, relating to the technical field of artificial intelligence. The method comprises: performing word vector feature extraction on an unlabeled text by means of a first language model to obtain an unlabeled feature, and according to the unlabeled feature, annotating the unlabeled text to obtain an annotated training text (S10); constructing an attention neural network model on the basis of a second language model and the annotated training text (S20); obtaining a text to be recognized, and by means of the attention neural network model, performing feature extraction on the text to be recognized, so as to obtain a candidate feature set (S30); and calculating, according to the candidate feature set, the similarity of the text to be recognized, and according to the similarity, determining whether the text to be recognized corresponds to the same expression intention, so as to obtain an intention recognition result (S40). Feature extraction is performed in a neural network mode, and thus, the present invention can more comprehensively consider text features, and improve the accuracy of intention recognition.",['G06N3/08']
US11615240B2,Systems and methods for a transformer network with tree-based attention for natural language processing,"Embodiments described herein provide an attention-based tree encoding mechanism. Specifically, the attention layer receives as input the pre-parsed constituency tree of a sentence and the lower-layer representations of all nodes. The attention layer then performs upward accumulation to encode the tree structure from leaves to the root in a bottom-up fashion. Afterwards, weighted aggregation is used to compute the final representations of non-terminal nodes.",['G06F40/205']
US11501187B2,Opinion snippet detection for aspect-based sentiment analysis,"A computer implemented method, computer system and computer program product are provided for aspect-based sentiment analysis. According to the method, at least one sentence and at least one aspect are received by one or more processing units. At least one opinion snippet of the at least one sentence is determined based on the at least one sentence and the at least one aspect, through a trained aspect-sentence fusion model, by one or more processing units. A sentiment prediction of the at least one opinion snippet is calculated by one or more processing units for the at least one aspect.","['G06N5/04', 'G06F40/30', 'G06N20/00', 'G06N3/006', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
US12288140B2,Classifier assistance using domain-trained embedding,"A classifier may be trained with less than all datasets manually annotated with labels. A small subset of verbatims may be manually labeled with topic labels as seeds. Data augmentations can be used to acquire seed verbatim sets for known topics and to assign temporary pseudo labels to the rest of the verbatims based on their vector space proximity to the labeled seed verbatims. The training may involve classification epochs during which embeddings are updated with the assumption that the pseudo labels are ground-truth labels. The training may also involve labeling epochs during which the updated embeddings are used to update the vectors corresponding to the verbatims, and pseudo labels are updated based on updated vector coordinates in the vector space. As the training process progresses through the epochs, the embeddings will converge.","['G06F40/30', 'G06F18/22', 'G06F18/23', 'G06F18/2411', 'G06F18/2431', 'G06N3/082', 'G06N3/084', 'G06N3/09', 'G06N20/00']"
US20220108086A1,Coarse-to-fine abstractive dialogue summarization with controllable granularity,"Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language style, and limited labelled data. The embodiments are directed to a coarse-to-fine dialogue summarization model that improves abstractive dialogue summarization quality and enables granular controllability. A summary draft that includes key words for turns in a dialogue conversation history is created. The summary draft includes pseudo-labelled interrogative pronoun categories and noisy key phrases. The dialogue conversation history is divided into segments. A generate language model is trained to generate a segment summary for each dialogue segment using a portion of the summary draft that corresponds to at least one dialogue turn in the dialogue segment. A dialogue summary is generated using the generative language model trained using the summary draft.","['G06F40/30', 'G06F40/205', 'G06F40/35', 'G06F40/56']"
CN113177124B,Method and system for constructing knowledge graph in vertical field,"The invention provides a method and a system for constructing a knowledge graph in the vertical field, wherein the method comprises the following steps: inputting the product document into a trained knowledge extraction model, wherein the trained knowledge extraction model comprises a form annotation model and a text knowledge extraction model; performing table labeling processing on the semi-structured data in the product document through a table labeling model to obtain a triplet of the product document, wherein the table labeling model is obtained by training a neural network through the semi-structured sample data after sequence labeling; carrying out knowledge extraction processing on unstructured data in a product document through a text knowledge extraction model to obtain a triplet of the product document, wherein the text knowledge extraction model is obtained by training an improved language model through unstructured sample data after plain text labeling; and constructing a knowledge graph of the product document according to the triples. The method improves the expertise, accuracy and high efficiency of the knowledge graph construction method in the vertical field.","['G06F16/367', 'G06F16/3346', 'G06F16/335', 'G06F16/35', 'G06F40/18', 'G06N3/044', 'G06N3/08', 'Y02P90/30']"
CN110765996B,Text information processing method and device,"The embodiment of the application discloses a text information processing method and device, and relates to the field of cloud computing. One embodiment of the method comprises: identifying a text to be processed from an image comprising the text to be processed; inputting a text to be processed into a pre-trained recurrent neural network language model, and identifying wrongly written characters in the text to be processed; inputting wrongly-written characters in a text to be processed into a pre-trained text error correction model to obtain similar characters corresponding to the wrongly-written characters; and determining correct characters corresponding to the wrongly written characters in the similar characters based on the consistency of the text to be processed by utilizing a text error correction model, and replacing the wrongly written characters with the correct characters to obtain an error correction text of the text to be processed. The wrongly-written characters are recognized through the pre-trained recurrent neural network language model, and the correct characters of the wrongly-written characters are obtained through the pre-trained text error correction model, so that the error correction text is obtained, and the accuracy of the recognition result is improved.",['G06V10/22']
CN108734286B,Coordination and increased utilization of GPUs during inference,"A mechanism for facilitating inferred coordination and processing utilization of machine learning at an autonomous machine is described. As described herein, a method of an embodiment includes detecting information related to one or more tasks to be performed during training from a training data set related to a processor including a graphics processor. The method may further include analyzing the information to determine one or more portions of hardware associated with the processor that are capable of supporting the one or more tasks, and configuring the hardware to pre-select the one or more portions to perform the one or more tasks while other portions of the hardware remain available for other tasks.","['G06F9/46', 'G06N3/045', 'G06N3/0464', 'G06N3/063', 'G06N3/08', 'G06N3/09', 'G06N3/098', 'G06T1/20', 'G06N3/044', 'G06N3/084', 'Y02D10/00']"
CN113094200B,Application program fault prediction method and device,"The application provides a method and a device for predicting faults of an application program, and relates to the field of artificial intelligence, wherein the method comprises the following steps: acquiring a target running log text corresponding to a preset time window of an application program to be detected; performing word segmentation processing on the target operation log text to obtain a log text word segmentation sequence corresponding to the target operation log text; calling a target fault prediction model to carry out fault prediction on the log text word segmentation sequence to obtain a fault prediction result of the application program to be detected; the target fault prediction model is obtained by performing constraint training of fault prediction on the initial prediction model based on a sample log text set corresponding to a preset time window and a matched fault label; the sample log text set and the fault label are generated based on-line problem feedback data corresponding to a preset time window and sample log text corresponding to the on-line problem feedback data. The method and the device can assist developers to solve online problems in advance under the condition that the users are not sensitive, and improve user experience.",['G06F11/0718']
EP4471614A1,Large data set management with large language models,"A system may receive a natural language query. A system may receive indications of one or more data object types, wherein each of the one or more data object types is associated with a respective one or more properties. A system may receive references to one or more data sets, wherein the one or more data sets are each associated with at least a respective data object type. A system may transmit a prompt to a large language model (""LLM""), the prompt comprising at least: the natural language query, the indications of the one or more data object types, and the references to the one or more data sets. A system may receive, from the LLM, a response to the prompt, wherein the response includes indications of: at least a first reference to a first data set and a query to be applied to the first data set.","['G06F16/24522', 'G06F16/243', 'G06F16/2455']"
US12131127B2,Computer implemented method for the automated analysis or use of data,"A computer implemented method for automated analysis or use of data, comprising: (a) storing in a non-transitory computer-readable medium a structured, machine-readable representation of data that conforms to a machine-readable language, wherein the data relates to social media postings; (b) automatically processing structured, machine-readable representation of data to determine if the social media postings are compliant with requirements preventing abusive or illegal social media postings.","['G06F40/35', 'G06F16/243', 'G06F16/322', 'G06F16/3329', 'G06F16/36', 'G06F16/951', 'G06F40/123', 'G06F40/126', 'G06F40/20', 'G06F40/205', 'G06F40/211', 'G06F40/226', 'G06F40/242', 'G06F40/279', 'G06F40/30', 'G06F40/45', 'G06F40/47', 'G06F40/58', 'G06N3/0442', 'G06N3/0455', 'G06N3/0499', 'G06N3/08', 'G06N5/02', 'G06Q10/1053', 'G06Q30/0255', 'G06Q30/0257', 'G06Q30/0631', 'G10L15/16', 'G10L15/1815', 'G10L15/22', 'G10L15/26', 'G10L25/63', 'G16H10/60', 'H04L51/02', 'G06N3/091', 'G10L2015/088']"
US20210133535A1,Parameter sharing decoder pair for auto composing,"Techniques for auto composing using a transformer-based language model having a parameter sharing decoder pair (PSDP) that reduces a number of parameters of the model and maintains the capability of generating understandable and reasonable compositions. In one particular aspect, a method is provided that includes obtaining a full encoder sequence, and inputting the full encoder sequence into a transformer model having a PSDP. The PSDP includes: a first decoder having parameters that are shared across all N layers of the first decoder; and a second decoder having parameters that are shared across all N layers of the first decoder. The parameters of the first decoder are different from the parameters of the second decoder. The method further includes using the transformer model to predict sequence elements based on the full encoder sequence, generate an output sequence comprising the sequence elements, and output an output sequence different from the full encoder sequence.","['G06N3/04', 'G06N3/084', 'G06N3/044', 'G06N3/045', 'G06N3/063', 'G06F40/56']"
CN115982350A,False news detection method based on multi-mode Transformer,"The invention discloses a false news detection method based on a multi-mode transducer, which comprises the steps of extracting image features in news through a visual transducer image feature extractor, extracting text features in the news through a Roberta text feature extractor, sending the extracted image features and the extracted text features into a common attention module for multi-mode feature fusion, inputting the fusion features into a false news detector to generate the probability that the predicted news is true and false news, comparing the results of the false news detection through a MEET and other baseline models, using the visual transducer as the image feature extractor to process the input of different modes in the same mode, introducing end-to-end pre-training in a false news detection task, carrying out comparative analysis on a TWITTER data set and a MEET model which is not pre-trained, verifying the superiority of an end-to-end pre-training method through experimental results, and enabling the MEET model to input supplementary information through images to be beneficial to the improvement of model detection performance.",['Y02D10/00']
US20230208869A1,Generative artificial intelligence method and system configured to provide outputs for company compliance,A system comprises a generative AI system including and engine for compliance applications.,"['H04L63/1425', 'G06F16/22', 'G06N3/006', 'G06N3/042', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N5/01', 'G06N5/022', 'G06N5/04', 'G06N5/041', 'G06N5/046', 'G06N7/01', 'H04L63/1433', 'G06N20/00', 'G06N20/10', 'G06N20/20']"
CN112256825B,Medical field multi-round dialogue intelligent question-answering method and device and computer equipment,"The application relates to the field of digital medical treatment, and can be applied to scenes such as remote medical consultation or self-service medical service, and discloses a multi-round dialogue intelligent question-answering method in the medical treatment field, which comprises the following steps: acquiring an Nth question description sentence; inputting the Nth question description sentence into a natural language understanding model, and extracting entity information and intention information; if the entity information or the intention information is not extracted, the entity information or the intention information in the N-1 question description sentence is used as the entity information or the intention information to be completed, so that a completed question description sentence is obtained, and a dialogue reply is output. According to the method and the device, the entity information and the intention information of the previous dialog are extracted, so that the communication of people can be better simulated, and even if a user uses a reference word or hides some entities or intentions, the dialog can be more natural and smoother by inheriting the context, and the user experience is better. The application also relates to a blockchain technology, and the medical field multi-round dialogue intelligent question-answering method is realized in the blockchain.","['G06F16/3329', 'G06F16/3344', 'G06F16/353', 'G06F40/295', 'G06F40/30', 'Y02D10/00']"
WO2022151930A1,"Speech synthesis method and apparatus, synthesis model training method and apparatus, and medium and device","A speech synthesis method and apparatus, a synthesis model training method and apparatus, and a medium and a device. The speech synthesis method comprises: acquiring speech feature information corresponding to text to be synthesized (S101); inputting the speech feature information into a speech synthesis model, so as to obtain predicted waveform point information (S102), wherein the speech synthesis model comprises an acoustic sub-model and a vocoder, and the speech synthesis model is obtained by means of pre-training the acoustic sub-model and the vocoder, and performing joint training on the acoustic sub-model obtained after pre-training and the vocoder obtained after pre-training; and performing Âµ-law expansion on the predicted waveform point information, so as to obtain audio information (S103). Therefore, the speech synthesis efficiency can be improved, the error accumulation generated by the training of an acoustic sub-model and a vocoder in the related art is effectively reduced, the speech synthesis accuracy is improved, and the problem of it being impossible for generated audio information to adapt to special pronunciation requirements due to acoustic features not having universality can also be avoided, thereby further improving the speech synthesis effect.","['G10L13/08', 'G10L13/10', 'G10L25/30']"
US11676067B2,System and method for creating data to train a conversational bot,"A system and method for creating input data to be used to train a conversational bot may include receiving a set of conversations, each conversation including sentences, classifying each sentence into a dialog act taken from a number of dialog acts, for each set of sentences classified into a dialog act, clustering the set of sentences into clusters based on the content (e.g. text) of the sentences, each cluster having a cluster name or label, and generating a language model based on the cluster labels. Slots may be identified in the sentences based in part on the dialog act classifications. A bot may be trained using data such as the slots, language model, and clusters.","['G06N20/00', 'G06F40/35', 'G06F40/30', 'G06N3/044', 'G06N3/088', 'G10L15/063', 'G10L15/16', 'G10L15/1815', 'G10L15/183', 'G10L15/22', 'H04L51/02', 'G10L13/00', 'G10L2015/0631', 'G10L2015/0633', 'G10L2015/223']"
CN111477221B,Speech recognition system using bidirectional time sequence convolution and self-attention mechanism network,"The invention discloses a voice recognition system adopting a bidirectional time sequence convolution and a self-attention mechanism network, which not only ensures the perception of voice time sequence but also improves the perception of the voice time sequence by adopting the bidirectional time sequence convolution and the self-attention mechanism network: through Bidirectional Temporal Convolution (BTCN) and a Self-Attention mechanism Network, the BTCN can capture past and future information at the same time, not only retains sequence information, but also better adapts to global modeling capability of Self-Attention. Therefore, the method not only ensures the description capability of the voice sequence signal, but also improves the operation speed. On the other hand, downsampling and upsampling are utilized to reduce resource requirements: a down-sampling module is designed to reduce the length of a voice sequence, so that the resource requirement is reduced, and the length of voice is recovered through an up-sampling module at an output layer due to the fact that a label corresponding to each frame of voice is needed.",['G10L15/16']
CN111159385B,Template-free general intelligent question-answering method based on dynamic knowledge graph,"The invention provides a template-free general intelligent question-answering method based on dynamic knowledge graph, comprising the following steps: natural language question understanding, automatically constructing a dynamic knowledge graph about entities and relations based on ontology and question-answering contexts, determining query intention by combining the contexts and the knowledge graph, and constructing a query-oriented dynamic knowledge graph. According to the invention, a question and answer template is not required to be constructed manually in advance, the natural language query question sentence can be effectively converted into the dynamic knowledge graph with equivalent semantics, the dynamic knowledge graph is used for understanding the natural language question sentence by a general question and answer system, the dynamic knowledge graph constructed under a general question and answer scene is ensured to fully express the semantics of the natural language question sentence, and good question and answer effect and performance are achieved.","['G06F16/3329', 'G06F16/367', 'Y02D10/00']"
US11743150B2,Automated root cause analysis of underperforming video streams by using language transformers on support ticket systems,"A method and system corrects a content delivery infrastructure. The method of the system includes receiving a request to resolve reported issues for the content delivery infrastructure, collecting content delivery metrics for the content delivery infrastructure, executes a language transformer model on the request and the content delivery metrics to generate a set of possible resolutions with confidence ratings, and implementing an automated solution based on a resolution from the set of possible resolutions, in response to the resolution having a confidence rating above a threshold.","['H04L41/0631', 'H04L41/509', 'G06N20/00', 'G06N3/0455', 'G06N3/0475', 'G06N3/0895', 'G06N3/096', 'H04L41/0695', 'H04L41/16', 'H04L41/5009', 'H04L41/5048', 'H04L41/5061', 'H04L41/5074', 'H04L43/0829', 'H04L43/0852', 'H04L43/087', 'H04L43/0882', 'H04L43/0894']"
CN114913729B,"Question selecting method, device, computer equipment and storage medium","The embodiment of the invention provides a method, a device, computer equipment and a storage medium for selecting questions, wherein the method comprises the following steps: determining a learning task of a user, determining a plurality of problem sets with contents related to the learning task, acquiring behavior data recorded when the user answers electronic problems with the contents related to the learning task, setting a first condition according to the behavior data under the dimension of difficulty, respectively selecting the electronic problems for the user from the plurality of problem sets as target problems, wherein the difficulty of the target problems meets the first condition, and the number of the target problems meets a preset second condition, so that the exposure rate of the electronic problems with different problem types can be improved, and the screened electronic problems are matched with the learning level of the user, so that the combination of the electronic problems is more reasonable, global optimum is realized, the repeated selection of the same or similar electronic problems is reduced, the user contacts more typical problems matched with the learning level of the user in a limited time, and the learning efficiency of the user is improved.","['G09B7/02', 'G06F16/906', 'G06Q50/20', 'G06Q50/205']"
US11901047B2,Medical visual question answering,"Aspects of the invention include a computer-implemented method including extracting a domain-specific object feature from a first image data, wherein the feature describes an object in the first image data. A domain-specific semantic meaning of text data is determined. The object feature is mapped to a portion of the text data, wherein the portion of the text data describes the object. A joint representation of the object and the portion of the text data is created. A second image data and a query directed towards an object in the second image data is received. An answer to the query is generated based on the joint representation.","['G16H10/20', 'A61B6/5217', 'G06F16/24522', 'G06F16/53', 'G06F16/532', 'G06F16/538', 'G06F16/583', 'G06F16/5854', 'G06F16/90332', 'G06F18/213', 'G06F18/214', 'G06F40/205', 'G06F40/284', 'G06F40/30', 'G06N3/04', 'G06N3/045', 'G06N3/08', 'G06T11/00', 'G06T11/60', 'G06V10/25', 'G06V10/40', 'G06V10/454', 'G06V10/82', 'G06V30/41', 'G06V30/413', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'G16H50/70', 'A61B5/0077', 'A61B6/032', 'G06F40/216', 'G06N3/063', 'G06T2200/24', 'G06T2210/12', 'G06V2201/03']"
US11610271B1,Transaction data processing systems and methods,"Described embodiments relate to determining a candidate financial record associated with a transaction between a first accounting entity and a second entity, and determining, using a numerical representation generation model, a numerical representation of the candidate financial record, the numerical representation generation model having been trained on a corpus generated from historical transaction records. The method further comprises providing, to a transaction attribute prediction model, the numerical representation of the candidate financial record, the transaction attribute prediction model having been trained using a dataset of previously reconciled financial records, each associated with a respective first transaction attribute; and determining, by the transaction attribute prediction model, at least one first transaction attribute associated with the candidate financial record.","['G06Q40/12', 'G06F40/279', 'G06F40/284', 'G06F40/30', 'G06Q30/06', 'G06Q40/02', 'G06N20/00', 'G06N3/045', 'G06N3/084']"
US12217033B2,Systems and methods for code understanding and generation,"Embodiments described herein a code generation and understanding model that builds on a Transformer-based encoder-decoder framework. The code generation and understanding model is configured to derive generic representations for programming language (PL) and natural language (NL) in code domain via pre-training on unlabeled code corpus, and then to benefit many code-related downstream tasks with fine-tuning. Apart from the denoising sequence-to-sequence objectives widely adopted for pre-training on natural language, identifier tagging and prediction pre-training objective is adopted to enable the model to better leverage the crucial token type information from PL, which specifically are the identifiers assigned by developers.","['G06F8/30', 'G06F8/427', 'G06F18/214', 'G06F40/20', 'G06N3/0455', 'G06N3/047', 'G06N3/084', 'G06F40/216', 'G06F40/30']"
WO2020253060A1,"Speech recognition method, model training method, apparatus and device, and storage medium","A speech recognition method, a model training method, apparatus and device, and a storage medium. The training method comprises: acquiring a pinyin training corpus and a data label (S101); performing word segmentation processing on the pinyin training corpus (S102); performing word vector transformation on training word segmentation data (S103); acquiring position data information, and performing vector transformation on the position data information (S104); splicing a word embedding vector and a position vector (S105); and performing model training according to the spliced word vector and the data label so as to obtain a language transformation model (S106).","['G10L15/063', 'G10L15/16', 'G10L15/22']"
CN116226334B,Generative large language model training method and model-based search method,"The disclosure provides a large language model training method and a model-based searching method, and relates to the technical fields of artificial intelligence such as generating models and intelligent searching. The method comprises the following steps: constructing a first training set based on the user query text and the matched output result containing the interface call instruction; performing supervised fine tuning training on the pre-trained first generated large language model by using the first training set to obtain a second generated large language model; constructing a second training set based on user preference ordering between the same user query text and different candidate outputs and a preset template set; performing supervised training on the pre-trained third generation type large language model by using the second training set to obtain a reward model; and training the second generated large language model in a reinforcement learning mode based on the scores returned by the rewarding model. The generated large language model obtained through training can remarkably improve the accuracy rate of search results and user experience in a search scene.","['G06F16/3344', 'G06F16/953']"
CN117218498B,Multi-modal large language model training method and system based on multi-modal encoder,"The invention discloses a multi-modal large language model training method and a system based on a multi-modal encoder, which relate to the technical field of electronic information and comprise the steps of constructing a multi-modal large language model training frame which consists of a parameter frozen multi-modal encoder, a parameter frozen large language model and a projection layer; the training process comprises the steps of extracting images Queries known by text instructions based on a multi-mode encoder, generating text replies and calculating a loss function; the reasoning process includes extracting text instruction-aware image query based on the multimodal encoder and generating text replies to the text instruction and the image query based on the large language model. According to the method, the extracted image features interact with the input text instructions in the early stage by means of the multi-mode encoder, so that the extracted image features can be aligned with the input text instructions more accurately, the training cost is reduced by applying the existing knowledge of the existing model, and the method has better effects in the aspects of accuracy, training cost and expansibility.",['Y02D10/00']
CN118606348B,"Intelligent question answering method, system and device based on large language model and database","The application discloses an intelligent question-answering method, system and equipment based on a large language model and a database, and relates to the field of natural language processing, wherein the method comprises the following steps: converting information stored in a database into text character strings and converting the text character strings into vector representations to obtain a plurality of knowledge vectors; converting the user problem into a vector representation to obtain a problem vector; calculating the similarity between the problem vector and each knowledge vector, and determining a plurality of related knowledge vectors; constructing a first prompt word according to text character strings corresponding to the relevant knowledge vectors and user questions, and determining a first answer based on a first large language model; constructing a second prompt word according to the user problem and the text string set, and converting the user problem into a structured query language sentence based on a second large language model; executing the structured query language statement in the database to obtain a second answer; the final answer is determined according to the first answer and the second answer, and the application improves the precision of answer generation in the intelligent question-answering process.","['G06F16/2433', 'G06F16/24522', 'G06N5/04', 'G06N5/045']"
CN116127045B,Training method for generating large language model and man-machine voice interaction method based on model,"The disclosure provides a large language model training method and a man-machine voice interaction method based on a model, and relates to the technical field of artificial intelligence such as a large language model, intelligent voice and man-machine interaction. The method comprises the following steps: constructing a first training set based on the user input text and the matched service interface call sequence; performing supervised fine tuning training on the pre-trained first generated large language model by using the first training set to obtain a second generated large language model; constructing a second training set based on user preference ordering between the same user input text and different candidate outputs and a preset template set; performing supervised training on the pre-trained third generation type large language model by using the second training set to obtain a reward model; and training the second generated large language model in a reinforcement learning mode based on the scores returned by the rewarding model. The generated large language model obtained through training can be used for remarkably improving the recovery accuracy and the user experience under the human-computer voice interaction scene.","['G06F16/3329', 'G06F16/3343', 'G06N3/08']"
US11526667B2,Language-model-based data augmentation method for textual classification tasks with little data,"Embodiments of the present systems and methods may provide techniques for augmenting textual data that may be used for textual classification tasks. Embodiments of such techniques may provide the capability to synthesize labeled data to improve text classification tasks. Embodiments may be specifically useful when only a small amount of data is available, and provide improved performance in such cases. For example, in an embodiment, a method implemented in a computer system may comprise a processor, memory accessible by the processor, and computer program instructions stored in the memory and executable by the processor, and the method may comprise fine-tuning a language model using a training dataset, synthesizing a plurality of samples using the fine-tuned language model, filtering the plurality of synthesized samples, and generating an augmented training dataset comprising the training dataset and the filtered plurality of synthesized sentences.","['G06F40/279', 'G06F40/284', 'G06F40/30', 'G06N20/00', 'G06N5/04', 'G06N3/08']"
US11250842B2,Multi-dimensional parsing method and system for natural language processing,"A method for translating a text written or otherwise communicated in a source natural language into a text written or otherwise communicable in target natural language, in reliance upon a multidimensional model, relies on determining the core concept in the sentences of the source text, and leverages the determined core concepts to create the target language translation. The method includes processing the source natural language text into sentences, then parsing the sentences, including assigning codes and/or directional operators to realize parsed sentences according to the model. The sentence models are then processed effect the actual translation to the target natural language text, and communicated.","['G10L15/19', 'G06F40/211', 'G06F16/3344', 'G06F40/268', 'G06F40/279', 'G06F40/284', 'G06F40/30', 'G06F40/40', 'G06F40/45', 'G06F40/58', 'G10L13/00', 'G10L15/1822', 'G10L15/22', 'G06F40/242', 'G10L15/26']"
US11763129B2,System and method for machine learning with long-range dependency,"A system, electronic device and method for improved neural network training are provided. The improved system is adapted for tracking long range dependence in sequential data during training, and includes bootstrapping a lower bound on the mutual information (MI) over groups of variables (segments or sentences) and subsequently applying the bound to encourage high MI.","['G06N3/006', 'G06F18/217', 'G06F18/241', 'G06F18/24133', 'G06N20/20', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N3/088', 'G06N3/044', 'G06N3/084', 'G06N7/01']"
CN112784578B,Legal element extraction method and device and electronic equipment,"The application relates to a legal element extraction method and device and electronic equipment. The legal element extraction method comprises the following steps: acquiring a legal referee document; preprocessing the legal referee document based on the case routing of the legal referee document; using a text classification model to perform paragraph identification on the natural paragraph of the legal referee document so as to obtain a candidate paragraph of legal elements to be extracted; performing initial extraction of legal elements on the candidate paragraphs in combination with a neural network model and a regular expression for extracting the legal elements to obtain an initial extraction result of the legal elements; and adjusting the preliminary extraction result based on an element extraction strategy configured according to the legal knowledge graph and the element extraction requirement to obtain a final extraction result of the legal elements. Therefore, the natural language understanding technology can be combined with the legal logic system by fusing model extraction and rule extraction, and the specific legal concept is extracted from the natural language description of the document on the basis of performing model training by using the labeling data as less as possible.","['G06F40/211', 'G06F16/35', 'G06N3/047', 'G06N3/049', 'G06N3/08', 'G06Q50/18']"
CN117806980B,Automatic test case generating device based on large language model,"The invention discloses a test case automatic generation device based on a large language model, and relates to the technical field of artificial intelligence. The device comprises: the system comprises a data source module, a test demand module, a generation algorithm module, a test arrangement module, a test execution module and a feedback optimization module; the data source module is used for providing test data for generating test cases; the test requirement module is used for extracting the test requirement input by the user; the generating algorithm module is used for generating test cases meeting specifications; the test arrangement module is used for forming test cases to be executed and a test execution plan; the test execution module is used for calling test data from the data source module according to a test execution plan so as to automatically execute test cases to be executed and collect test results; and the feedback optimization module is used for optimizing according to the test result. The automatic test case generating device based on the large language model can effectively improve the test efficiency and quality of an electronic system.","['G06F11/3684', 'G06F11/3688', 'G06F11/3692', 'G06F40/211', 'G06F40/295', 'G06F40/30', 'G06N3/0442', 'G06N3/0455', 'G06N3/0475', 'G06N3/094']"
US11514894B2,Adaptively modifying dialog output by an artificial intelligence engine during a conversation with a customer based on changing the customer's negative emotional state to a positive one,"In some examples, a server may receive an utterance from a customer. The utterance may be included in a conversation between the artificial intelligence engine and the customer. The server may convert the utterance to text and determine a customer intent based on the text and a user history. The server may determine a user model of the customer based on the text and the customer intent. The server may update a conversation state associated with the conversation based on the customer intent and the user model. The server may determine a user state based on the user model and the conversation state. The server may select, using a reinforcement learning based module, a particular action from a set of actions, the particular action including a response and provide the response to the customer.","['G10L15/1815', 'G06F40/30', 'G06N3/045', 'G06N3/092', 'G06Q30/0631', 'G06Q30/0633', 'G10L15/26', 'G10L15/16', 'G10L15/32', 'G10L2015/081']"
US20230223016A1,User interface linking analyzed segments of transcripts with extracted key points,"A user interface (UI) linking analyzed segments of transcripts with extracted key points may be provided by capturing audio of a conversation including first and second pluralities of utterances respectively spoken by first and second parties; transmitting the audio to a Natural Language Processing (NLP) system; receiving a transcript of the conversation and analysis outputs from the transcript including a key point and hyperlink to a most-semantically-relevant segment of a plurality of segments included in the transcript for the key point according to a semantic context for the key point within the conversation; displaying, in a UI, the transcript and a selectable representation of the key point; and in response to receiving a selection of the selectable representation via the UI, adjusting display of the transcript in the UI to highlight the most-semantically-relevant segment.","['G10L15/1815', 'G06F16/35', 'G06F16/94', 'G06F40/134', 'G06F40/169', 'G06F40/279', 'G06F40/284', 'G06F40/30', 'G06F40/35', 'G10L15/26']"
US20250200401A1,Artificial intelligence engine for directed hypothesis generation and ranking,"An artificial intelligence engine for directed hypothesis generation and ranking uses multiple heterogeneous knowledge graphs integrating disease-specific multi-omic data specific to a patient or cohort of patients. The engine also uses a knowledge graph representation of â€˜what the world knowsâ€™ in the relevant bio-medical subspace. The engine applies a hypothesis generation module, a semantic search analysis component to allow fast acquiring and construction of cohorts, as well as aggregating, summarizing, visualizing and returning ranked multi-omic alterations in terms of clinical actionability and degree of surprise for individual samples and cohorts. The engine also applies a moderator module that ranks and filters hypotheses, where the most promising hypothesis can be presented to domain experts (e.g., physicians, oncologists, pathologists, radiologists and researchers) for feedback. The engine also uses a continuous integration module that iteratively refines and updates entities and relationships and their representations to yield higher quality of hypothesis generation over time.","['G06N5/041', 'G06F16/24578', 'G06F16/284', 'G06N3/045', 'G06N5/02']"
US9652723B2,Electrical transformer failure prediction,"A computing device predicts a probability of a transformer failure. An analysis type indicator defined by a user is received. A worth value for each of a plurality of variables is computed. Highest worth variables from the plurality of variables are selected based on the computed worth values. A number of variables of the highest worth variables is limited to a predetermined number based on the received analysis type indicator. A first model and a second model are also selected based on the received analysis type indicator. Historical electrical system data is partitioned into a training dataset and a validation dataset that are used to train and validate, respectively, the first model and the second model. A probability of failure model is selected as the first model or the second model based on a comparison between a fit of each model.","['G06N99/005', 'G05B23/0254', 'G05B23/0283', 'G06F11/00', 'G06N20/00', 'G06N3/08', 'G06N7/005', 'G06N7/01']"
US11657233B2,Systems and methods for unifying question answering and text classification via span extraction,"Systems and methods for unifying question answering and text classification via span extraction include a preprocessor for preparing a source text and an auxiliary text based on a task type of a natural language processing task, an encoder for receiving the source text and the auxiliary text from the preprocessor and generating an encoded representation of a combination of the source text and the auxiliary text, and a span-extractive decoder for receiving the encoded representation and identifying a span of text within the source text that is a result of the NLP task. The task type is one of entailment, classification, or regression. In some embodiments, the source text includes one or more of text received as input when the task type is entailment, a list of classifications when the task type is entailment or classification, or a list of similarity options when the task type is regression.","['G06F40/30', 'G06F40/284', 'G06N3/045', 'G06N3/084', 'G06F16/3329', 'G06N3/08']"
US11962546B1,Leveraging inferred context to improve suggested messages,"Systems and methods for using a generative artificial intelligence (AI) model to generate a suggested draft reply to a selected message. A message generation system and method are described that use inferred context to improve the suggested draft reply message for the user. Various message data and additional context are obtained and included in a prompt provided to the AI model to improve suggested content. In some examples, the message data and additional context include a message thread history and previously sent messages, profile information of the sender and recipient(s) of the selected message, known relationship information between the sender and the user, etc. For instance, the user's preferred communication style and talking points can be inferred based on the profile data, relationship data, and the user's past communications with similar participants and used to tailor the suggested draft reply to the user.","['G06Q10/107', 'H04L51/02', 'G06F3/0481', 'G06F3/04842', 'G06F40/166', 'G06F40/216', 'G06F40/284', 'G06F40/30', 'G06F40/40', 'G06F40/44', 'G06F40/56', 'G06N20/00', 'G06N3/0475', 'G06Q10/10', 'H04L51/216', 'G06F40/205', 'G06N3/09']"
US11775843B2,Directed trajectories through communication decision tree using iterative artificial intelligence,Embodiments relate to configuring artificial-intelligence (AI) decision nodes throughout a communication decision tree. The decision nodes can support successive iteration of AI models to dynamically define iteration data that corresponds to a trajectory through the tree.,"['G06N20/20', 'G06N5/02', 'G06F9/5011', 'G06N20/00', 'G06N5/01', 'G06N20/10', 'G06N3/045', 'G06N3/126', 'G06N5/025', 'G06N7/01']"
WO2024182041A1,Large language model artificial intelligence text evaluation system,"Relevance scores may be determined based on text included in a document. The text may be divided into a text portions, with the relevance scores being determined based on a comparison of a text portion of the plurality of text portions with a criterion specified in natural language. A subset of the plurality of text portions may be selected based on the plurality of relevance scores, with each of the subset of the plurality of text portions having a relevance score surpassing a threshold. A criteria evaluation prompt may be sent to a remote text generation modeling system via a communication interface. The criteria evaluation prompts may include an instruction to evaluate one or more of the subset of text portions against the criterion.","['G06F40/226', 'G06F40/289', 'G06F40/40', 'G06N3/045', 'G06N3/0985']"
CN104978965B,Electronic device and voice recognition execution method using electronic device and server,"The invention discloses an electronic device and a voice recognition execution method using the electronic device and a server. The electronic device includes: a processor for performing Automatic Speech Recognition (ASR) for the speech input using the speech recognition model stored in the memory; and a communication module that provides the voice input to a server, receives a voice command corresponding to the voice input from the server, and may perform different operations according to a reliability of an execution result for automatic voice recognition. Further, various embodiments that can be grasped by the specification can also be realized.","['G10L15/30', 'G10L15/08', 'G10L15/22', 'G10L15/32', 'G10L17/22', 'G10L15/01', 'G10L2015/223', 'G10L2015/225']"
US11797780B1,Context-biased artificial intelligence video generation,A method includes receiving a set of text documents. The method also includes generating a summary of the set of text documents by a set of large language machine learning models. The method further includes generating a set of keywords from the summary by the set of large language machine learning models. The method additionally includes generating an image prompt from the set of keywords by the set of large language machine learning models. The method also includes generating a set of images from the image prompt by a text-to-image machine learning model. The method further includes generating a video clip from the set of images. The method additionally includes presenting the video clip.,"['G06F40/40', 'G06F40/216', 'G06F40/30', 'G06F40/56', 'G06T11/60']"
US12361215B2,Performing machine learning tasks using instruction-tuned neural networks,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for performing a machine learning task on an input to generate an output. In one aspect, one of the method includes receiving input data that describes an input of a machine learning task; receiving candidate output data that describes a set of candidate classification outputs of the machine learning task for the input; generating an input sequence that includes the input and the set of candidate classification outputs; processing the input sequence using a neural network to generate a network output that specifies a respective score for each candidate classification output in the set of candidate classification outputs; and generating an output of the machine learning task for the input, comprising selecting, as the output, a selected candidate classification output from the set of candidate classification outputs using the respective scores.","['G06F40/284', 'G06F40/30', 'G06N3/096', 'G06N3/0455', 'G06N3/084', 'G06N3/0895']"
CN112199956B,Entity emotion analysis method based on deep representation learning,"The invention discloses an entity emotion analysis method based on deep representation learning, which sequentially adopts an ELMo model, a BERT model and an ALBERT model for pre-training to obtain pre-training word vectors based on the three models; the pre-training word vector generated in the step 2 is used as the input of a BilSTM layer, and a hidden layer after the final iteration is finished is used as the output of the layer; calculating attention scores of each word and all other words in the sentence s by utilizing an attention layer to judge the relation weight of the word and the other words; inputting the corresponding hidden layer into a classifier according to the result of obtaining the attention layer; and calculating the probability of the emotion types, and outputting the classification result to a final result through an output layer. Compared with the prior art, the method improves the accuracy of emotion recognition based on entity attributes; the expression of the model on the Chinese data set is improved; the generalization capability of the model is strengthened.","['G06F40/295', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'G06N3/084']"
US12039286B2,Automatic post-editing model for generated natural language text,Techniques are disclosed for training and/or utilizing an automatic post-editing model in correcting translation error(s) introduced by a neural machine translation model. The automatic post-editing model can be trained using automatically generated training instances. A training instance is automatically generated by processing text in a first language using a neural machine translation model to generate text in a second language. The text in the second language is processed using a neural machine translation model to generate training text in the first language. A training instance can include the text in the first language as well as the training text in the first language.,"['G06F40/51', 'G06F40/166', 'G06F40/253', 'G06F40/58', 'G10L13/00']"
US11893363B2,Unit test case generation with transformers,"A unit test generation system employs a neural transformer model with attention to generate candidate unit test sequences given a focal method of a programming language. The neural transformer model is pre-trained with source code programs and natural language text and fine-tuned with mapped test case pairs. A mapped test case pair includes a focal method and a unit test case for the focal method. In this manner, the neural transformer model is trained to learn the semantics and statistical properties of a natural language, the syntax of a programming language and the relationships between the code elements of the programming language and the syntax of a unit test case.","['G06F11/3684', 'G06F8/35', 'G06F17/18', 'G06F40/30', 'G06N3/045', 'G06N3/084', 'G06N3/088', 'G06N5/046', 'G06N7/01']"
CN111897964B,"Text classification model training method, device, equipment and storage medium","The application discloses a text classification model training method, device, equipment and storage medium, and belongs to the field of artificial intelligence. According to the embodiment of the application, on one hand, the countermeasure sample is introduced, and the text classification model is trained by using the text sample and the countermeasure sample, so that the text classification model learns a classification method for the disturbed text, the robustness of the text classification model is improved, and the accuracy of text classification is improved. On the other hand, the text classification model can reconstruct the text characteristics of the countermeasure sample extracted during classification, restore the text characteristics into text contents, and improve the interpretability of the countermeasure training method. Model parameters are trained by combining errors between the reconstructed text content and the text content of the text sample, so that the text classification model can extract more accurate text features, namely more accurate feature expression of the text content is obtained, and the robustness and accuracy of feature extraction of the text classification model are improved.","['G06F16/35', 'G06F18/214', 'G06F18/22', 'G06F40/30', 'G06N3/045', 'G06N3/047', 'G06N3/08']"
CN112509564B,End-to-end voice recognition method based on connection time sequence classification and self-attention mechanism,"The invention discloses an end-to-end voice recognition method based on connection time sequence classification and a self-attention mechanism, which uses a mixed mechanism of the connection time sequence classification CTC and the self-attention mechanism SA to directly model English words or Chinese characters without preprocessing or post-processing, and the output result directly corresponds to a correct English sequence or Chinese character sequence. The method shares the same encoder network, the output of the encoder uses a CTC training criterion, the output of the encoder is also used as the input of the decoder, the attention relationship between the encoder and the decoder is realized, the decoder uses a cross entropy training criterion for training, and finally, the two training criteria are given different weights in a weighted mode. The invention not only can accelerate the convergence rate of the model and obtain more accurate alignment attribute, but also can acquire internal relation between inputs, thereby improving the accuracy and the robustness of the voice recognition system.","['G10L15/08', 'G10L15/02', 'G10L15/063', 'G10L15/183']"
US11006077B1,Systems and methods for dynamically concealing sensitive information,"Systems and methods for dynamically concealing sensitive information in a shared screen session of a video conference are disclosed. The system may establish communication with one or more computing devices active in a video conference in which each computing device may switch between a screen share mode and a video mode. The system may determine that one or more articles of sensitive information are visible in a graphical user interface associated with a first computing device of the plurality of computing devices. The system may receive a first signal from the first computing device that indicates a first intent of a host associated with the first computing device to switch the screen share mode which includes sharing the first graphical user interface with the one or more computing devices during the video conference. In response to the first signal, the system may execute one or more privacy actions.","['H04N7/152', 'H04N7/15', 'G06F3/1454', 'G09G5/14', 'H04L63/04', 'H04L63/168', 'H04W12/02', 'G09G2358/00', 'H04L2209/16']"
US12299404B2,"Computer-generated content based on text classification, semantic relevance, and activation of deep learning large language models","The disclosure relates to systems and methods of automatically generating unique content including natural language text based on a corpus of previously generated response documents and discrete requirements defined in a requirements specification. The system may use generative stitching that includes multi-layer processes that execute to influence the generation of unique content including natural language text through an artificial intelligence (AI) language transformer model trained to output the content based on previously written material that is semantically relevant to the discrete requirements and is weighted against labeled attributes. The labeled attributes may determine the influence asserted against the language transformer, thereby generating unique on-target content that may be combined to create a computer-generated response document.","['G06F40/131', 'G06F40/40', 'G06F16/3329', 'G06F16/3344', 'G06F40/169', 'G06F40/30', 'G06F40/56', 'G06N20/00', 'G06N3/0475', 'G06N3/09', 'G06F40/284', 'G06N3/0455']"
US20230289528A1,Method for constructing sentiment classification model based on metaphor identification,"A method for constructing a sentiment classification model based on metaphor identification is disclosed, including: constructing a metaphor training corpus, and training a first pre-trained language model using the metaphor training corpus to obtain a metaphor recognition model; constructing and inputting a sentiment classification corpus into a second pre-trained language model and the metaphor recognition model to extract an explicit characteristic value and a metaphor information characteristic value of a text in the sentiment classification corpus; combining the explicit characteristic value and the metaphor information characteristic value to obtain a comprehensive characteristic value which is input to a feedforward neural network and a binary classification softmax layer to obtain a sentiment classification result; and performing optimization training based on the sentiment classification result using BP algorithm to obtain an optimal sentiment classification model. Explicit and implicit sentiment expressions are comprehensively considered, and more accurate and credible evaluation results are provided.","['G06F40/30', 'G06F40/289', 'G06F16/35', 'G06F18/2431', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06F40/279', 'Y02T10/40']"
US12361217B2,System and method to extract customized information in natural language text,"The present disclosure relates to systems and methods to extract customized keywords and their corresponding values occurring in a given natural language text. The desired keyword or keywords may occur in different forms, synonyms, abbreviations, and spellings. The disclosed automatic extraction method captures the meaning and context of the desired keywords by transforming the extraction problem into a question answering problem together with capturing the context to narrow down the answer to a unique value for a given keyword. A trained model on an existing corpus of text is used to get a value as an answer to the question phrased using the keyword. When the answer is ambiguous, a context model that uses conditional random field (CRF) is used to provide a most likely value.","['G06F40/30', 'G06F40/289', 'G06F16/3329', 'G06F40/284', 'G06F40/295', 'G06F40/47', 'G06N3/045', 'G06N3/09', 'G06Q10/063', 'G06N20/00', 'G06N7/01']"
AU2024203846B2,Confidence generation for managing a generative artificial intelligence model,"#$%^&*AU2024203846B220250821.pdf##### 2312758AU ABSTRACT Systems and methods are disclosed for managing a generative artificial intelligence (AI) model. Managing the generative AI model may include training or tuning the generative AI model before use or managing the operation of the generative AI model during use. Training or tuning a generative AI model typically requires manual review of outputs from the model based on the queries provided to the model to reduce hallucinations generated by the generative AI model. Once the model is in use, though, hallucinations still occur. Use of a confidence (whose generation is described herein) to train or tune the generative AI model and/or manage operation of the model reduces hallucinations, and thus improves performance, of the generative AI model. 29 2312758AU ABSTRACT Systems and methods are disclosed for managing a generative artificial intelligence (AI) model. Managing the generative AI model may include training or tuning the generative AI model before use or managing the operation of the generative AI model during use. Training or tuning a generative AI model typically requires manual review of outputs from the model based on the queries provided to the model to reduce hallucinations generated by the generative AI model. Once the model is in use, though, hallucinations still occur. Use of a confidence (whose generation is described herein) to train or tune the generative AI model and/or manage operation of the model reduces hallucinations, and thus improves performance, of the generative AI model. 29 20 24 20 38 46 06 J un 2 02 4 2 3 1 2 7 5 8 A U A B S T R A C T S y s t e m s a n d m e t h o d s a r e d i s c l o s e d f o r m a n a g i n g a g e n e r a t i v e a r t i f i c i a l i n t e l l i g e n c e 2 0 2 4 2 0 3 8 4 6 0 6 J u n 2 0 2 4 ( A I ) m o d e l . M a n a g i n g t h e g e n e r a t i v e A I m o d e l m a y i n c l u d e t r a i n i n g o r t u n i n g t h e g e n e r a t i v e A I m o d e l b e f o r e u s e o r m a n a g i n g t h e o p e r a t i o n o f t h e g e n e r a t i v e A I m o d e l d u r i n g u s e . T r a i n i n g o r t u n i n g a g e n e r a t i v e A I m o d e l t y p i c a l l y r e q u i r e s m a n u a l r e v i e w o f o u t p u t s f r o m t h e m o d e l b a s e d o n t h e q u e r i e s p r o v i d e d t o t h e m o d e l t o r e d u c e h a l l u c i n a t i o n s g e n e r a t e d b y t h e g e n e r a t i v e A I m o d e l . O n c e t h e m o d e l i s i n u s e , t h o u g h , h a l l u c i n a t i o n s s t i l l o c c u r . U s e o f a c o n f i d e n c e ( w h o s e g e n e r a t i o n i s d e s c r i b e d h e r e i n ) t o t r a i n o r t u n e t h e g e n e r a t i v e A I m o d e l a n d / o r m a n a g e o p e r a t i o n o f t h e m o d e l r e d u c e s h a l l u c i n a t i o n s , a n d t h u s i m p r o v e s p e r f o r m a n c e , o f t h e g e n e r a t i v e A I m o d e l . 2 9 Figure 3 300 Receive a question to be provided to a generative AI model. 302 Receive an answer generated by the generative AI model based on the question provided to the generative AI model. 306 Provide the question, the answer, and the content to an NLP model. 308 Generate, by the NLP model, a first similarity metric between the question and the content. 310 Generate, by the NLP model, a second similarity metric between the answer and the content. 312 Generate a confidence metric based on the first similarity metric and the second similarity metric. 314 Manage the generative AI model based on the confidence metric. 316 Prevent outputting the answer to the user based on the confidence metric indicating that the answer is not a desirable response to the answer. 318 Train the generative AI model based on the confidence metric. 320 Retrieve, by a content retrieval model, a content from a knowledge base based on the question. 304 3/43/4 300 Receive a question to be provided to a generative AI model. 302 Retrieve, by a content retrieval model, a content from a knowledge base based on the question. 304 Receive an answer generated by the generative AI model based on the question provided to the generative AI model. 306 Provide the question, the answer, and the content to an NLP model. 308 Generate, by the NLP model, a first similarity metric between the question and the content. 310 Generate, by the NLP model, a second similarity metric between the answer and the content. 312 Generate a confidence metric based on the first similarity metric and the second similarity metric. 314 Manage the generative AI model based on the confidence metric. 316 Prevent outputting the answer to the user based on the confidence metric indicating that the answer is not a desirable response to the answer. 318 1 Train the generative AI model based on the confidence metric. 320 Figure 3 20 24 20 38 46 06 J un 2 02 4 3 / 4 2 0 2 4 2 0 3 8 4 6 2 0 2 4 0 6 J u n 3 0 0 R e c e i v e a q u e s t i o n t o b e p r o v i d e d t o a g e n e r a t i v e A I m o d e l . 3 0 2 R e t r i e v e , b y a c o n t e n t r e t r i e v a l m o d e l , a c o n t e n t f r o m a k n o w l e d g e b a s e b a s e d o n t h e q u e s t i o n . 3 0 4 R e c e i v e a n a n s w e r g e n e r a t e d b y t h e g e n e r a t i v e A I m o d e l b a s e d o n t h e q u e s t i o n p r o v i d e d t o t h e g e n e r a t i v e A I m o d e l . 3 0 6 P r o v i d e t h e q u e s t i o n , t h e a n s w e r , a n d t h e c o n t e n t t o a n N L P m o d e l . 3 0 8 G e n e r a t e , b y t h e N L P m o d e l , a f i r s t s i m i l a r i t y m e t r i c b e t w e e n t h e q u e s t i o n a n d t h e c o n t e n t . 3 1 0 G e n e r a t e , b y t h e N L P m o d e l , a s e c o n d s i m i l a r i t y m e t r i c b e t w e e n t h e a n s w e r a n d t h e c o n t e n t . 3 1 2 G e n e r a t e a c o n f i d e n c e m e t r i c b a s e d o n t h e f i r s t s i m i l a r i t y m e t r i c a n d t h e s e c o n d s i m i l a r i t y m e t r i c . 3 1 4 M a n a g e t h e g e n e r a t i v e A I m o d e l b a s e d o n t h e c o n f i d e n c e m e t r i c . 3 1 6 P r e v e n t o u t p u t t i n g t h e a n s w e r t o t h e u s e r b a s e d o n t h e c o n f i d e n c e m e t r i c i n d i c a t i n g t h a t t h e a n s w e r i s n o t a d e s i r a b l e r e s p o n s e t o t h e a n s w e r . 3 1 8 T r a i n t h e g e n e r a t i v e A I m o d e l b a s e d o n t h e c o n f i d e n c e m e t r i c . 3 2 0 F i g u r e 3 Figure 4 400 Generate, by the NLP model, a first similarity metric between the question and the content. 402 Generate, by the NLP model, a second similarity metric between the answer and the content. 406 Compare the second similarity metric to a second similarity threshold to generate a second similarity indication. 408 Generate, by a sensitivity model, a first sensitivity indication based on the answer. 410 Generate, by the sensitivity model, a second sensitivity indication based on the question. 412 Compare the first similarity metric to a first similarity threshold to generate a first similarity indication. 404 Combine the first similarity indication, the second similarity indication, the first sensitivity indication, and the second sensitivity indication to generate the confidence metric. 414 Logically AND the first similarity indication, the second similarity indication, the first sensitivity indication, and the second sensitivity indication. 416 4/44/4 400 Generate, by the NLP model, a first similarity metric between the question and the content. 402 Compare the first similarity metric to a first similarity threshold to generate a first similarity indication. 404 Generate, by the NLP model, a second similarity metric between the answer and the content. 406 Compare the second similarity metric to a second similarity threshold to generate a second similarity indication. 408 Generate, by a sensitivity model, a first sensitivity indication based on the answer. 410 Generate, by the sensitivity model, a second sensitivity indication based on the question. 412 Combine the first similarity indication, the second similarity indication, the first sensitivity indication, and the second sensitivity indication to generate the confidence metric. 414 Logically AND the first similarity indication, the second similarity indication, the first sensitivity indication, and the second sensitivity indication. 416 Figure 4 20 24 20 38 46 06 J un 2 02 4 4 / 4 2 0 2 4 2 0 3 8 4 6 0 6 J u n 2 0 2 4 4 0 0 G e n e r a t e , b y t h e N L P m o d e l , a f i r s t s i m i l a r i t y m e t r i c b e t w e e n t h e q u e s t i o n a n d t h e c o n t e n t . 4 0 2 C o m p a r e t h e f i r s t s i m i l a r i t y m e t r i c t o a f i r s t s i m i l a r i t y t h r e s h o l d t o g e n e r a t e a f i r s t s i m i l a r i t y i n d i c a t i o n . 4 0 4 G e n e r a t e , b y t h e N L P m o d e l , a s e c o n d s i m i l a r i t y m e t r i c b e t w e e n t h e a n s w e r a n d t h e c o n t e n t . 4 0 6 C o m p a r e t h e s e c o n d s i m i l a r i t y m e t r i c t o a s e c o n d s i m i l a r i t y t h r e s h o l d t o g e n e r a t e a s e c o n d s i m i l a r i t y i n d i c a t i o n . 4 0 8 G e n e r a t e , b y a s e n s i t i v i t y m o d e l , a f i r s t s e n s i t i v i t y i n d i c a t i o n b a s e d o n t h e a n s w e r . 4 1 0 G e n e r a t e , b y t h e s e n s i t i v i t y m o d e l , a s e c o n d s e n s i t i v i t y i n d i c a t i o n b a s e d o n t h e q u e s t i o n . 4 1 2 C o m b i n e t h e f i r s t s i m i l a r i t y i n d i c a t i o n , t h e s e c o n d s i m i l a r i t y i n d i c a t i o n , t h e f i r s t s e n s i t i v i t y i n d i c a t i o n , a n d t h e s e c o n d s e n s i t i v i t y i n d i c a t i o n t o g e n e r a t e t h e c o n f i d e n c e m e t r i c . 4 1 4 L o g i c a l l y A N D t h e f i r s t s i m i l a r i t y i n d i c a t i o n , t h e s e c o n d s i m i l a r i t y i n d i c a t i o n , t h e f i r s t s e n s i t i v i t y i n d i c a t i o n , a n d t h e s e c o n d s e n s i t i v i t y i n d i c a t i o n . 4 1 6 F i g u r e 4","['G06F40/35', 'G06F40/40', 'G06F16/3329', 'G06N3/042', 'G06N3/0475', 'G06N3/09', 'G06N5/022', 'G06N3/0455']"
CN112926344B,"Word vector replacement data enhancement-based machine translation model training method and device, electronic equipment and storage medium","The invention discloses a word vector substitution data enhancement-based machine translation model training method, a word vector substitution data enhancement-based machine translation model training device, electronic equipment and a storage medium, wherein the specific implementation scheme is as follows: acquiring a training sample data set; and preprocessing the sample data set. Respectively training a forward language model and a reverse language model based on a Transformer structure aiming at the existing source language or target language corpus; obtaining probability distribution of words at any position in sentences on the whole word list through a forward language model and a reverse language model; determining a final word vector according to the probability distribution and the word vector of the whole word list, and replacing the word at the position by the final word vector; training a neural machine translation model by using the replaced bilingual parallel corpus to obtain a translation result; meanwhile, the monolingual data can be merged into the method to obtain a better translation effect. Experimental results show that the method can remarkably improve the translation quality of the machine translation model.","['G06F40/58', 'G06F40/211', 'G06F40/216', 'G06F40/284', 'G06N3/04', 'G06N3/08', 'Y02D10/00']"
CN114048464B,Method and system for security vulnerability detection of Ethereum smart contract based on deep learning,"The invention discloses an Ethernet intelligent contract security vulnerability detection method and system based on deep learning, wherein the Ethernet intelligent contract vulnerability detection problem is modeled into an end-to-end classification detection model, and whether a vulnerability is included is judged aiming at an intelligent contract source code, so that the detection of the intelligent contract security vulnerability is realized; the method comprises the following steps: preprocessing the source code data of the intelligent contracts of the Etheng; constructing an intelligent contract source code semantic representation learning module which comprises a coding layer/coder, a detection layer/classifier and a model fusion output module; training a model; in the testing stage, the trained intelligent contract source code semantic representation learning module is used for realizing the block chain intelligent contract security vulnerability detection based on machine learning, and the detection performance of the Ethernet intelligent contract security vulnerability is effectively improved.","['G06F21/52', 'G06N3/044', 'G06N3/084']"
CN116932728B,"Language interaction method, device, communication equipment and storage medium","The embodiment of the application provides a language interaction method, a device, communication equipment and a storage medium, comprising the following steps: acquiring a training input text and a training output text; determining a training target aiming at a language model to be trained; masking the characters in the training input text according to the training target to obtain a masking input text; inputting the mask input text into the language model to be trained to obtain a target output text; calculating a loss value according to the training target, the target output text and the training output text; and when the loss value reaches a preset convergence condition, taking the language model to be trained as a language model after training. According to the embodiment of the application, the language model to be trained is trained simultaneously by setting different training targets, so that the language model with excellent semantic understanding capability and text generation capability is trained simultaneously.","['G06F16/3329', 'G06F16/3344', 'G06F17/16', 'G06F40/30', 'G06N3/04', 'G06N3/08']"
US12367375B2,System and method for structure learning for graph neural networks,"A graph structure having nodes and edges is represented as an adjacency matrix, and nodes of the graph structure have node features. A computer-implemented method and system for generating a graph structure are provided, the method comprising: generating an adjacency matrix based on a plurality of node features; generating a plurality of noisy node features based on the plurality of node features; generating a plurality of denoised node features using a neural network based on the plurality of noisy node features and the adjacency matrix; and updating the adjacency matrix based on the plurality of denoised node features.","['G06N3/088', 'G06N3/045', 'G06N5/022', 'G06N3/042', 'G06N3/082']"
US11481448B2,Semantic matching and retrieval of standardized entities,"During operation, the system obtains a first embedding produced by an embedding model from an input string representing an entity and a hierarchy of clusters of embeddings generated by the embedding model from a set of standardized entities. Next, the system searches the hierarchy of clusters for a subset of the embeddings that are within a threshold proximity to the first embedding in a vector space. The system then calculates embedding match scores between the input string and a first subset of the standardized entities represented by the subset of the embeddings based on distances between the subset of the embeddings and the first embedding in the vector space. Finally, the system modifies, based on the embedding match scores, content outputted in response to the input string within a user interface of an online system.","['G06F40/295', 'G06F16/355', 'G06F16/90344', 'G06F16/9535', 'G06F18/2113', 'G06F18/2148', 'G06F18/22', 'G06F18/231', 'G06F40/169', 'G06F40/284', 'G06F40/30', 'G06K9/6219', 'G06K9/623', 'G06K9/6257', 'G06N20/00', 'G06N3/084', 'G06N3/044', 'G06N3/045', 'G06N7/01']"
US12394191B2,Neural networks based multimodal transformer for multi-task user interface modeling,"A method includes receiving, via a computing device, a screenshot of a display provided by a graphical user interface of the computing device. The method also includes generating, by an image-structure transformer of a neural network, a representation by fusing a first embedding based on the screenshot and a second embedding based on a layout of virtual objects in the screenshot. The method additionally includes predicting, by the neural network and based on the generated representation, a modeling task output associated with the graphical user interface. The method further includes providing, by the computing device, the predicted modeling task output.","['G06N20/00', 'G06F3/167', 'G06F40/284', 'G06F9/451', 'G06N3/04', 'G06N3/0455', 'G06N3/0464', 'G06N3/082', 'G06V10/82', 'G06V20/62', 'G06N5/041', 'G06V2201/02']"
US11481552B2,Generative-discriminative language modeling for controllable text generation,The embodiments describe a generative-discriminative (GeDi) language modeling for determining a next token in a text sequence. A class conditional language model and a positive control code determine a first class conditional probability for each token candidate. The class conditional language model and a negative control code determine a second class conditional probability for the each token candidate. A logarithmic probability difference between the first class conditional probability and the second class conditional probability is determined for each token candidate. An unconditional language model determines an unconditional probability for each token candidate. A combined probability is determined by combining the unconditional probability and the logarithmic probability difference for each token candidate. The next token is selected from the token candidates based on the combined probabilities of the token candidates.,"['G06F40/274', 'G06F40/216', 'G06F40/284', 'G06N3/045', 'G06N3/047']"
US20250111192A1,Generating knowledge graphs using large language models,"Techniques for a knowledge-graph system to use large language models (LLMs) to build knowledge graphs to answer queries submitted to a chatbot by users. The knowledge-graph system builds the knowledge graph using answers produced by an LLM for novel queries. The chatbot will continue to use the LLM to answer novel queries, but the chatbot may harness the knowledge graph to answer repeat questions to gain various efficiencies over LLM-backed chatbots. For example, the knowledge-graph system may easily debug or otherwise improve the answers in knowledge graphs, store provenance information in knowledge graphs, and augment the knowledge graphs using other data sources. Thus, the reliability and correctness of chatbots will be improved as the bugs and inaccuracies in answers provided by the LLM will be corrected in the knowledge graphs, but the chatbots can still harness the abilities of LLMs to provide answers across various subject-matter domains.","['G06F16/3329', 'G06N3/042', 'G06N3/006', 'G06N3/0455']"
CN113722493B,"Data processing methods, equipment, and storage media for text classification","The present disclosure provides a data processing method, apparatus, storage medium, and program product for text classification, and relates to the field of data processing, in particular to the fields of natural language processing, big data, intelligent search, deep learning, and the like. The specific implementation scheme is as follows: the method comprises the steps of pre-constructing a plurality of initial tag mining functions, and testing target parameters of the initial tag mining functions on a test set to obtain tag mining functions with the target parameters meeting training requirements of a text classification model, wherein the tag mining functions are used as final target mining functions; by using the target label mining function, the classification labels of the initial text are generated, a large number of training data with labeled classification labels in specific application scenes can be obtained, a large number of training data with accurate classification labels are provided for training of the text classification model, the cost of manually labeling the data is greatly saved, and the efficiency of obtaining the training data is improved.","['G06F16/35', 'G06F16/332', 'G06F18/214', 'Y02D10/00']"
US12307204B2,Systems and methods for contextualized and quantized soft prompts for natural language understanding,"Embodiments described herein provide a soft prompt tuning technique referred to as the Vector quantized Input-contextualized Prompt (VIP). The VIP techniques has two integral properties i) instead of learning a fixed set of prompt tokens irrespective of the input, it generates a contextualized version of the soft prompts, conditional on the input text ii) it further passes the input-contextualized prompt tokens through a quantization network, inspired by Vector Quantized Transformers. The quantization network uses nearest neighbor search over a learnable codebook to train a discrete latent variable model over the prompt-space, thus generating quantized version of contextual prompt tokens. These quantized contextual prompt tokens are finally fed into the frozen language model along with the original input text.","['G06F40/284', 'G06F40/12', 'G06F40/216', 'G06F40/289', 'G06F40/30', 'G06F40/40', 'G06N20/00', 'G06N3/045', 'G06N3/084', 'G06N3/09', 'G06N3/096']"
US11972232B2,Neural method completion based on natural language and source code,"A code completion tool uses a neural transformer model with attention to generate candidate sequences to complete a method body of a method signature. The neural transformer model is trained with source code programs and natural language text. The neural transformer model learns the meaning of a method name, its corresponding method parameters and types from a large corpus of unsupervised dataset of source code methods and a supervised dataset of tasks including source code constructs in combination with natural language docstrings to infer a candidate sequence of subtokens that represent a method body for a particular method signature.","['G06F8/33', 'G06F40/274', 'G06F8/35', 'G06F8/36', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G06N7/01']"
US11886907B2,Analytic model execution engine with instrumentation for granular performance analysis for metrics and diagnostics for troubleshooting,"At an interface an analytic model for processing data is received. The analytic model is inspected to determine a language, an action, an input type, and an output type. A virtualized execution environment is generated for an analytic engine that includes executable code to implement the analytic model for processing an input data stream.","['G06F9/45558', 'G06F8/31', 'G06F8/51', 'G06F8/60', 'G06F9/455', 'G06F9/45504', 'G06F9/5077', 'G06F8/30', 'G06N20/00', 'G06N20/20', 'G06N5/01', 'G06Q10/067']"
CA3161393C,Initialization of parameters for machine-learned transformer neural network architectures,"An online system trains a transformer architecture by an initialization method which allows the transformer architecture to be trained without normalization layers of learning rate warmup, resulting in significant improvements in computational efficiency for transformer architectures. Specifically, an attention block included in an encoder or a decoder of the transformer architecture generates the set of attention representations by applying a key matrix to the input key, a query matrix to the input query, a value matrix to the input value to generate an output, and applying an output matrix to the output to generate the set of attention representations. The initialization method may be performed by scaling the parameters of the value matrix and the output matrix with a factor that is inverse to a number of the set of encoders or a number of the set of decoders.","['G06N3/084', 'G06F18/214', 'G06F18/2413', 'G06F9/30036', 'G06F9/3555', 'G06F9/463', 'G06N20/00', 'G06N3/045', 'G06N3/0455', 'G06N3/0499', 'G06N3/09']"
CN111177326B,Key information extraction method and device based on fine labeling text and storage medium,"The invention relates to the technical field of key information extraction, and provides a method, a device and a storage medium for extracting key information based on a fine labeling text, wherein the method comprises the following steps: s110, pre-training text data through a BERT pre-training model to obtain word vectors, and combining the obtained word vectors into matrix text data; s120, inputting the matrix text data into a key information extraction model, wherein the key information extraction model is trained by utilizing a CMRC (China Mobile radio control) data set and acquires key information according to the matrix text data; s130, sequencing the obtained key information according to a preset sequencing rule, and outputting the key information which accords with the set selection rule. The invention solves the problem of automatic marking of the text segment fragments, greatly reduces the marking cost and achieves the technical effect of providing powerful support for downstream tasks.","['G06F16/334', 'G06F16/338', 'G06N3/04', 'G06N3/048', 'G06N3/088']"
CN111177324B,Method and device for carrying out intention classification based on voice recognition result,"The embodiment of the specification provides a method and a device for performing intention classification based on a voice recognition result, wherein the method comprises the following steps: acquiring a target text obtained after automatic voice recognition is performed on target voice; searching pinyin corresponding to each Chinese character contained in the target text from a pre-established mapping table from Chinese character to pinyin; forming a target pinyin sequence based on the pinyin corresponding to each Chinese character; taking the target text and the target pinyin sequence as a target text voice pair, inputting the target text voice pair into a pre-trained text coding model, and outputting a target coding vector corresponding to the target text voice pair through the text coding model; and carrying out intention classification on the target voice according to the target coding vector. The error rate of the intention classification can be reduced.","['G06F16/3329', 'G06F16/3343', 'G06F16/35', 'G10L15/22', 'G06F18/241', 'G06F18/2415', 'Y02P90/30']"
US20220139096A1,"Character recognition method, model training method, related apparatus and electronic device","A character recognition method, a model training method, a related apparatus and an electronic device are provided. The specific solution is: obtaining a target picture; performing feature encoding on the target picture to obtain a visual feature of the target picture; performing feature mapping on the visual feature to obtain a first target feature of the target picture, where the first target feature is a feature that has a matching space with a feature of character semantic information of the target picture; inputting the first target feature into a character recognition model for character recognition to obtain a first character recognition result of the target picture.","['G06V30/19013', 'G06V20/62', 'G06V30/10', 'G06F18/214', 'G06F18/2415', 'G06T9/00', 'G06V10/48', 'G06V10/757', 'G06V20/70', 'G06V30/18', 'G06V30/18057', 'G06V30/19127', 'G06V30/19147']"
CN113297975B,"Method, device, storage medium and electronic equipment for table structure recognition","The present disclosure relates to a method, an apparatus, a storage medium, and an electronic device for identifying a table structure, where a table image corresponding to a table to be identified may be used as an input of a target detection model to obtain a position feature and an element category of each table element in the table to be identified, where the element category includes a text line; carrying out character recognition on the characters by using a character recognition model to obtain text semantic features of each character row; extracting features of the table images through an image feature extraction model to obtain image feature images, and sampling the image feature images to obtain target image features corresponding to each table element respectively; according to the position features, the text semantic features and the target image features, obtaining target relation features for representing the topological relation between every two table elements in the table to be identified through a preset relation extraction model, and determining a graph adjacency matrix for representing the table structure of the table to be identified through a preset classification model according to the target relation features.","['G06V30/414', 'G06F18/24147', 'G06N3/045', 'G06N3/08', 'G06V10/22']"
US9558454B2,System and method for model-based inventory management of a communications system,"A method for management entity operations includes receiving a request to collect data for an entity in a communications system, collecting the data for the entity utilizing a set of protocols selected using knowledge defined by a first data model of a data model list derived from an information model of the communications system, and saving the data collected.","['H04L41/16', 'G06N99/005', 'G06F8/35', 'G06N20/00', 'G06N5/02', 'H04L41/085']"
US12399890B2,Scene graph modification based on natural language commands,"Systems and methods for natural language processing are described. Embodiments are configured to receive a structured representation of a search query, wherein the structured representation comprises a plurality of nodes and at least one edge connecting two of the nodes, receive a modification expression for the search query, wherein the modification expression comprises a natural language expression, generate a modified structured representation based on the structured representation and the modification expression using a neural network configured to combine structured representation features and natural language expression features, and perform a search based on the modified structured representation.","['G06F18/214', 'G06F16/243', 'G06F18/253', 'G06F40/166', 'G06F40/20', 'G06F40/30', 'G06F40/56', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N5/01']"
CN112529150B,"A model structure, model training method, image enhancement method and device","The embodiment of the application discloses a model structure, a model training method, an image enhancement method and equipment, which can be applied to the field of computer vision in the field of artificial intelligence, wherein the model structure comprises the following components: the model breaks through the limitation that the transducer module can only be used for processing natural language tasks and can be applied to the bottom visual tasks, the model is provided with a plurality of first/second neural network layers, different first/second neural network layers correspond to different image enhancement tasks, and therefore the model can be used for processing different image enhancement tasks after training.","['G06V10/771', 'G06V10/82', 'G06N3/045', 'G06F18/241', 'G06F18/2415', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/08', 'G06N3/088', 'G06N3/09', 'G06T5/50', 'G06T7/11', 'G06V10/44', 'G06V10/454', 'G06V10/7715', 'G06V10/774', 'G06V10/778', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221']"
US12242433B2,Automatic database enrichment and curation using large language models,"A method to be executed on a computing device comprising (i) accessing and/or modifying a database to be automatically curated, (ii) optionally accessing additional data or information sources for further useful data or information, (iii) using one or more pre-trained large language models (LLMs) accessed via API or other connections, issuing prompts and retrieving prompt-answers and executing database curation requests that specify database curation tasks to be performed on at least one sub-structure of the database, the tasks comprising (a) a database enrichment task to compute new data records to be inserted into the database sub-structure, (b) a database verification task to verify, using the one or more LLMs, data contained in the sub-structure, and identify incorrect data, (c) a database update, and (d) a null-value or a missing value replacement task. The requested tasks are automatically performed via a computation comprising an adaptively generated prompt sequence.","['G06F16/211', 'G06F16/215', 'G06F16/2365', 'G06F16/24564']"
US12361093B2,Embedded predictive machine learning models,Data associated with one or more data sources is transformed into a format associated with a common ontology using one or more transformers. One or more machine learning models are generated based at least in part on the transformed data. The one or more machine learning models and the one or more transformers are provided to a remote device.,"['G06F16/86', 'G06F16/907', 'G06F18/213', 'G06N20/00', 'G06N5/022', 'G06F16/258', 'G06N3/08', 'G06N5/01', 'G06N7/01']"
CN114841122A,"Text extraction method combining entity identification and relationship extraction, storage medium and terminal","The invention discloses a text extraction method, a storage medium and a terminal combining entity identification and relationship extraction, belonging to the technical field of natural language processing, and a serialized label data set for model training is obtained in a sequence label mode; an encoder-decoder model is adopted to completely fuse the processes of entity identification and relationship extraction, so that the interaction between the entity and the relationship is studied to the maximum extent; in the relation classification stage, clause representation information is obtained through an attention mechanism and is combined with entity pair coding information, and the model precision is further improved; the language training model is used as a shared coding layer of the entity recognition and relationship extraction tasks, the dependence of the entity recognition and relationship extraction tasks is further established, and the problem of error accumulation is solved.","['G06F40/117', 'G06F40/295', 'G06N3/044', 'G06N3/047', 'G06N3/08']"
US20180150758A1,Method and apparatus for predictive classification of actionable network alerts,"An approach is provided for providing predictive classification of actionable network alerts. The approach includes receiving the plurality of alerts. Each alert of the plurality of alerts indicates an alarm condition occurring at a monitored network system, and is a data record comprising one or more data fields describing the alarm condition. The approach also includes classifying said each alert using a predictive machine learning model. The predictive machine learning model is trained to classify said each alert as actionable or non-actionable using the one or more data fields of said each alert as one or more respective classification features, and to calculate a respective probability that said each alert is actionable or non-actionable. The approach further includes presenting the plurality of alerts in a network monitoring user interface based on the respective probability of said each alert.","['G06N20/00', 'G06N7/005', 'G06N5/025', 'G06N99/005', 'H04L41/147', 'H04L41/16', 'H04L43/10', 'H04L41/22']"
CN114357694B,Large-scale point cloud-based digital twin method and device for transformer substation,"The invention discloses a digital twin method and device for a transformer substation based on a large-scale point cloud, which effectively extracts key equipment data in the point cloud through technologies such as point cloud preprocessing, cluster segmentation, linear fitting and the like, and facilitates subsequent classified storage. The method comprises the steps of storing point cloud data with different importance degrees according to different requirements through a tile type layered storage method, reducing the size of the point cloud data quantity, facilitating subsequent rendering, searching nodes which need to be loaded and rendered in corresponding layers for loading through a large-scale point cloud rendering technology in combination with actual conditions, improving the rendering efficiency of the large-scale point cloud, realizing more visual monitoring and analysis of substation equipment, comprehensively managing the whole life cycle through digital twin virtual-real mapping, data simulation, real-time synchronization and the like, and synchronizing virtual reality of a robot inspection process into a digital twin virtual model of a substation through a robot simulation technology, so that inspection tasks are more convenient to monitor and manage.",[]
CN113935330B,"Disease early warning method, device, equipment and storage medium based on voice","The invention relates to the field of artificial intelligence and digital medical treatment, and discloses a disease early warning method, device, equipment and storage medium based on voice. The method comprises the following steps: slicing the collected target voice data to obtain voice fragments; performing code conversion on the voice fragment to obtain semantic feature coding information of the voice fragment; inputting the semantic feature coding information into a preset intention recognition model for feature extraction to obtain target semantic features and context semantic features corresponding to the semantic feature coding information; inputting the target semantic features and the context semantic features into a feature fusion layer of the intention recognition model to perform feature fusion, obtaining features to be classified, inputting the features to be classified into a classifier to perform intention prediction, and performing Alzheimer's disease early warning on a user according to a prediction result. According to the method, the potential patients with early Alzheimer's disease can be screened according to the prediction result, and early warning of Alzheimer's disease can be carried out.","['G06F40/30', 'G06F18/214', 'G06F18/241', 'G06F18/253', 'G06F40/211', 'G08B31/00', 'G10L25/51', 'G16H50/30', 'Y02A90/10']"
WO2023065545A1,"Risk prediction method and apparatus, and device and storage medium","The present application relates to artificial intelligence technology. Disclosed is a risk prediction method, comprising: constructing a temporal knowledge graph on the basis of a risk perception factor set that is extracted from a multi-source information set, and performing implicit relationship supplementation and causal relationship supplementation on the temporal knowledge graph, so as to obtain a standard knowledge graph and an event evolutionary graph; performing prediction by using a risk prediction model that is constructed on the basis of a reinforcement learning algorithm, so as to obtain a target risk entity; and performing relationship quantification and degree quantification on the standard knowledge graph, so as to obtain a dependency closeness and an event hazard degree, performing training on the basis of the event evolutionary graph, the dependency closeness and the event hazard degree and in view of a graph neural network and a semi-supervised method, so as to obtain a macro prediction model, and performing prediction by using the macro prediction model, so as to obtain a risk industry corresponding to the target risk entity. In addition, the present application further relates to blockchain technology. The event evolutionary graph can be stored in a node of a blockchain. Also provided in the present application are a risk prediction apparatus, an electronic device and a storage medium. By means of the present application, the accuracy of performing risk prediction on an industry can be improved.","['G06Q10/04', 'G06F16/367', 'G06Q10/0635']"
US11520997B2,Computing device and method for generating machine translation model and machine-translation device,"A device and a method for generating a machine translation model and a machine translation device are disclosed. The device inputs a source training sentence of a source language and a dictionary data to a generator network so that the generator network outputs a target training sentence of a target language according to the source training sentence and the dictionary data. Then, the device inputs the target training sentence and a correct translation of the source training sentence to a discriminator network so as to calculate an error between the target training sentence and the correct translation according to the output of the discriminator network, and trains the generator network and the discriminator network respectively. The trained generator network is the machine translation model.","['G06F40/58', 'G06F40/44', 'G06F40/242', 'G06N20/00', 'G06N3/045', 'G06N3/047', 'G06N3/08']"
US20220398462A1,Automated fine-tuning and deployment of pre-trained deep learning models,A cloud platform includes several web services that facilitate the automated tuning and deployment of pre-trained deep learning models configured for software engineering tasks. The automated tuning and deployment allow a developer to fine-tune a pre-existing model without having access to the parameters of the pre-existing and the fine-tuned model in a manner that does not require user management input. The cloud platform provides a set of files for each pre-trained models used to automatically build a fine-tuning infrastructure to fine-tune a model and a deployment infrastructure that deploys the fine-tuned model without requiring user input.,"['G06N3/088', 'G06N3/08', 'G06F8/36', 'G06N3/045', 'G06N3/0454', 'G06F8/35', 'G06F8/60']"
WO2020117504A1,Training of speech recognition systems,"A method may include obtaining first audio data of a first communication session between a first and second device and during the first communication session, obtaining a first text string that is a transcription of the first audio data and training a model of an automatic speech recognition system using the first text string and the first audio data. The method may further include in response to completion of the training, deleting the first audio data and the first text string and after deleting the first audio data and the first text string, obtaining second audio data of a second communication session between a third and fourth device and during the second communication session obtaining a second text string that is a transcription of the second audio data and further training the model of the automatic speech recognition system using the second text string and the second audio data.","['G10L15/063', 'G10L15/06', 'G06F21/6245', 'G10L15/22', 'G10L15/26', 'G10L15/28', 'G10L2015/0631']"
EP3891732A1,Transcription generation from multiple speech recognition systems,"A method may include obtaining first audio data originating at a first device during a communication session between the first device and a second device. The method may also include obtaining a first text string that is a transcription of the first audio data, where the first text string may be generated using automatic speech recognition technology using the first audio data. The method may also include obtaining a second text string that is a transcription of second audio data, where the second audio data may include a revoicing of the first audio data by a captioning assistant and the second text string may be generated by the automatic speech recognition technology using the second audio data. The method may further include generating an output text string from the first text string and the second text string and using the output text string as a transcription of the speech.","['G10L15/22', 'G10L15/187', 'G10L15/26', 'G10L15/30', 'G10L15/32', 'H04M1/2475', 'H04M2201/40', 'H04M3/42391']"
WO2020117505A1,Switching between speech recognition systems,"A method may include obtaining first audio data originating at a first device during a communication session between the first device and a second device. The method may also include obtaining an availability of revoiced transcription units in a transcription system and in response to establishment of the communication session, selecting, based on the availability of revoiced transcription units, a revoiced transcription unit instead of a non-revoiced transcription unit to generate a transcript of the first audio data. The method may also include obtaining revoiced audio generated by a revoicing of the first audio data by a captioning assistant and generating a transcription of the revoiced audio using an automatic speech recognition system. The method may further include in response to selecting the re voiced transcription unit, directing the transcription of the revoiced audio to the second device as the transcript of the first audio data.","['G10L15/28', 'G10L15/32', 'G10L15/22', 'G10L15/26', 'H04M3/42382', 'H04M2201/39', 'H04M2201/40', 'H04M2203/552', 'H04M3/42391']"
WO2020117507A1,Training speech recognition systems using word sequences,"A method may include obtaining first audio data of a communication session between a first device and a second device, obtaining a text string that is a transcription of the first audio data, and selecting a contiguous sequence of words from the text string as a first word sequence. The method may further include comparing the first word sequence to multiple word sequences obtained before the communication session and in response to the first word sequence corresponding to one of the multiple word sequences, incrementing a counter of multiple counters associated with the one of the multiple word sequences. The method may also include deleting the text string and the first word sequence and training and after deleting the text string and the first word sequence, training a language model of an automatic transcription system using the multiple word sequences and the multiple counters. The method is for estimating n-gram statistics based on the communication and provides data protection mechanisms for private data.","['G06N20/00', 'G06F21/6254', 'G06F40/279', 'G06F40/30', 'G06F40/44', 'G10L15/183', 'G10L15/197', 'G10L15/22', 'G10L15/30', 'G10L15/063', 'G10L15/26']"
EP4118610A1,Systems and methods of product recommendation and integrated language modelling,"Systems and method are provided for tracking actions from one or more users of an electronic product catalog, assigning a first token to each of the tracked actions that are unrelated to product item information, assigning second tokens to the one or more tracked actions which have catalog information or product identity, assigning third tokens to the one or more tracked actions based on at least one category of a page of the electronic product catalog, and assigning fourth tokens to at least one search queries and/or search refinement of the one or more tracked actions. The server may generate a sequence of tokens in one or more vectors for each of the one or more users, may encode profile information for the one or more users as a matrix of vectors, and may determine at least one product recommendation for the one or more users from the electronic catalog.","['G06Q30/0631', 'G06Q30/0603']"
US11886815B2,Self-supervised document representation learning,"One example method involves operations for a processing device that include receiving, by a machine learning model trained to generate a search result, a search query for a text input. The machine learning model is trained by receiving pre-training data that includes multiple documents. Pre-training the machine learning model by generating, using an encoder, feature embeddings for each of the documents included in the pre-training data. The feature embeddings are generated by applying a masking function to visual and textual features in the documents. Training the machine learning model also includes generating, using the feature embeddings, output features for the documents by concatenating the feature embeddings and applying a non-linear mapping to the feature embeddings. Training the machine learning model further includes applying a linear classifier to the output features. Additionally, operations include generating, for display, a search result using the machine learning model based on the input.","['G06F40/279', 'G06F16/334', 'G06F16/93', 'G06F40/103', 'G06F40/131', 'G06F40/205', 'G06F40/216', 'G06F40/289', 'G06F40/30', 'G06N3/045', 'G06N3/0455', 'G06N3/088']"
US12259889B2,Query generation based on a logical data model with one-to-one joins,"Systems and methods for query generation based on a logical data model with one-to-one joins are described. For example, methods may include accessing a join graph representing tables in a database; receiving a first query; selecting a connected subgraph of the join graph that includes the two or more tables referenced in the first query; accessing an indication that a directed edge of the connected subgraph corresponds to a one-to-one join; modifying the connected subgraph based on the indication to obtain a modified subgraph; generating one or more leaf queries based on the modified subgraph; generating a query graph that specifies joining of results from queries based on the one or more leaf queries; invoking a transformed query on the database that is based on the query graph and the queries based on the one or more leaf queries.","['G06F16/211', 'G06F16/2246', 'G06F16/2282', 'G06F16/24534', 'G06F16/2456']"
US12335313B2,Automated and adaptive model-driven security system and method for operating the same,"A system and method for managing implementation of policies in an information technologies system receives at least one policy function, at least one refinement template and at least one available policy function from the at least one memory, receives a policy input indicating a high-level policy for the IT system where the policy input is compliant with the at least one policy function and is received in a format that is not machine-enforceable at an enforcement entity of the IT system, based on the received policy input, automatically or semi-automatically generates a machine-enforceable rule and/or configuration by filling the at least one refinement template, where the machine-enforceable rule and/or configuration includes the at least one available policy function and being compliant with the received policy input, and distributes the machine-enforceable rule and/or configuration to the at least one memory of the IT system or another at least one memory to thereby enable implementation of the policies.","['H04L63/20', 'G06F21/57', 'G06F21/604', 'G06F2221/034', 'G06F2221/2141']"
US11886542B2,Model compression using cycle generative adversarial network knowledge distillation,"Systems and processes for prediction using generative adversarial network and distillation technology are provided. For example, an input is received at a first portion of a language model. A first output distribution is obtained, based on the input, from the first portion of the language model. Using a first training model, the language model is adjusted based on the first output distribution. The first output distribution is received at a second portion of the language model. A first representation of the input is obtained, based on the first output distribution, from the second portion of the language model. The language model is adjusted, using a second training model, based on the first representation of the input. Using the adjusted language model, an output is provided based on a received user input.","['G06F18/2148', 'G06F18/22', 'G06F18/2413', 'G06N3/045', 'G06N3/0475', 'G06N3/08', 'G06V30/194']"
US20210174023A1,Systems and Methods for Explicit Memory Tracker with Coarse-To-Fine Reasoning in Conversational Machine Reading,"Embodiments described herein provide systems and methods for an Explicit Memory Tracker (EMT) that tracks each rule sentence to perform decision making and to generate follow-up clarifying questions. Specifically, the EMT first segments the regulation text into several rule sentences and allocates the segmented rule sentences into memory modules, and then feeds information regarding the user scenario and dialogue history into the EMT sequentially to update each memory module separately. At each dialogue turn, the EMT makes a decision among based on current memory status of the memory modules whether further clarification is needed to come up with an answer to a user question. The EMT determines that further clarification is needed by identifying an underspecified rule sentence span by modulating token-level span distributions with sentence-level selection scores. The EMT extracts the underspecified rule sentence span and rephrases the underspecified rule sentence span to generate a follow-up question.","['G06F40/289', 'G06F40/35', 'G06F16/3329', 'G06N3/0445', 'G06N3/045', 'G06N3/08', 'G06N3/044', 'G06N3/084']"
CN116468725B,"Industrial defect detection method, device and storage medium based on pre-training model","The application relates to an industrial defect detection method, a device and a storage medium based on a pre-training model, which are applied to the technical field of industrial defect detection and comprise the following steps: training a visual language model by acquiring an industrial defect detection image data set, setting language texts aiming at each industrial defect in each scene, describing characteristics of target defects through the language texts, inputting an image to be detected and the language texts corresponding to the target defects into a pre-trained visual language model, combining the image to be detected and the language texts of the target defects by the pre-trained visual language model, and searching object areas related to the language texts on the image to be detected so as to realize detection of the target defects; in the application, different language texts are set for each defect, instead of independently training a model for each defect, so that the time spent in the model training process is saved, the cost is saved, and the identification and detection of multiple defects in industrial detection can be realized.","['G06T7/0004', 'G06F18/253', 'G06N3/0455', 'G06N3/0499', 'G06N3/084', 'G06V10/764', 'G06V10/806', 'G06V10/82', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30108', 'Y02P90/30']"
CN112329964B,"Method, device, equipment and storage medium for pushing information","The application discloses a method, a device, equipment and a storage medium for pushing information, relates to the fields of data processing, knowledge maps and intelligent recommendation, and can be applied to cloud services. The specific implementation scheme is as follows: acquiring an uploading document and a historical knowledge graph of a target user; analyzing the uploaded document to obtain text information; constructing a target knowledge graph based on the text information; and determining target recommendation information based on the target knowledge graph and the historical knowledge graph, and pushing the target recommendation information to a target user. According to the implementation mode, the artificial intelligence technology is applied to life, the documents uploaded by the user can be intelligently analyzed to obtain the target knowledge graph, and based on the obtained target knowledge graph and the obtained historical knowledge graph, the user is assisted to quickly find out proper matching resource information and push the matching resource information to the user, so that the time for browsing web pages and inquiring by the user is saved, and the rationality and accuracy of information recommendation can be improved.","['G06Q10/02', 'G06F16/367', 'G06F16/9535', 'H04L67/55']"
CN116821308B,"Generation method, training method and device of model and storage medium","The embodiment of the specification provides a dialogue content generation method, a model training method, a prompt text generation method, a device and a storage medium. The dialogue content generation method comprises the following steps: a question of receiving an input; judging whether the questions need to be replied by combining with external knowledge; if yes, acquiring target knowledge from a preset knowledge base according to the problem, and generating a prompt text corresponding to the target knowledge; inputting the questions, the target knowledge and the prompt text corresponding to the target knowledge into a dialogue content generation model, and generating dialogue content by the dialogue content generation model; the prompt text is used for prompting the dialogue content generation model to refer to target knowledge to answer the questions; if not, inputting the problems into a dialogue content generation model, and generating dialogue content by the dialogue content generation model; the dialogue content generation model is obtained by fine tuning a pre-training language model. The method is beneficial to accurately solving the universal intention problem of the user and improving the accuracy of the dialogue content and the satisfaction degree of the user.","['G06F16/3329', 'G06F18/214', 'G06F40/205']"
CN110795945B,"Semantic understanding model training method, semantic understanding device and storage medium","The invention provides a semantic understanding model training method, which comprises the following steps: carrying out recall processing on training samples matched with the vehicle-mounted environment in the data source; carrying out boundary corpus expansion processing on sentence samples with noise, which are matched with the vehicle-mounted environment; labeling the sentence samples with noise, which are matched with the vehicle-mounted environment and are subjected to boundary corpus expansion processing, so as to form a first training sample set; processing the second training sample set through a semantic understanding model; and according to the updated parameters of the semantic understanding model, iteratively updating the semantic representation layer network parameters and the task related output layer network parameters of the semantic understanding model through a second training sample set. The invention also provides a semantic understanding method, a semantic understanding device and a storage medium. The method and the device can improve the training precision and the training speed of the semantic understanding model, enable the semantic understanding model to adapt to the full duplex use scene of the vehicle-mounted environment, and avoid the influence of environmental noise on the semantic understanding model.","['G06F16/35', 'G06N20/00']"
CN112733533B,Multi-modal named entity recognition method based on BERT model and text-image relation propagation,"The invention relates to a multi-mode named entity recognition method based on BERT model and text-image relation propagation, which comprises the following steps: step 1, designing RpBERT, which is a BERT model for identifying multi-modal named entity based on text-image relation propagation; step 2, carrying out relation propagation through different probability gates G; the relationship propagation includes two gating propagation of soft and hard relationship propagation; step 2.1, soft relation propagation: the output of the probability gate G is regarded as continuous distribution, and the visual characteristics are filtered according to the strength of the text-image relationship; step 2.2, hard relation propagation; and 3, training RpBERT for multi-modal named entity recognition by adopting a multi-task learning mode. The beneficial effects of the invention are as follows: the invention further analyzes the change of visual attention before and after using relation transmission in experiments. And achieves the most advanced performance achievable on the dataset identified by the multimodal named entity.","['G06F40/279', 'G06F18/2415', 'G06N3/045', 'G06N3/049', 'G06N3/08', 'G06N5/04', 'G06V10/44']"
US20220004935A1,Ensemble learning for deep feature defect detection,"An apparatus to facilitate ensemble learning for deep feature defect detection is disclosed. The apparatus includes one or more processors to receive a deep feature vector from a feature extractor of an ensemble learning system, the deep feature vector extracted from input data; cluster the deep feature vector into a plurality of clusters based on a distance into the plurality of clusters; execute a probabilistic machine learning model corresponding to a cluster of the plurality of clusters to which the deep feature vector is clustered; and detect whether the deep feature vector comprises a defect based on an output of execution of the probabilistic machine learning model.","['G06N20/20', 'G06N7/01', 'G06N3/042', 'G06N3/0427', 'G06N3/044', 'G06N3/045', 'G06N3/0464', 'G06N7/005', 'G06N10/00']"
CN113792159B,Knowledge graph data fusion method and system,"The embodiment of the specification provides a knowledge graph data fusion method and system, wherein the method comprises the following steps: acquiring a target entity field and a target relation description; the target entity field and the target relationship describe ontology-defining data selected from two or more knowledge maps; and acquiring related data instances of each platform or each service field, and processing the acquired data instances according to a map operator used for carrying out fusion processing on entity fields and relationship descriptions of different platforms or service fields in the fusion knowledge map body definition data to generate a fusion knowledge map.","['G06F16/367', 'G06N5/022', 'G06F16/35', 'G06F40/30', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N5/02']"
CN111523326B,"Entity chain finger method, device, equipment and storage medium","The application discloses a method, a device, equipment and a storage medium for entity chain finger, and relates to the field of knowledge graphs. The specific implementation scheme is as follows: acquiring a target text; determining at least one entity mention included in the target text; determining candidate entities corresponding to the entities according to a preset knowledge base; determining a reference text of each candidate entity and determining additional characteristic information of each candidate entity; and determining an entity chain finger result based on the target text, the reference texts and the additional characteristic information. The realization mode can provide external knowledge when the entity chain indicates, and actively play the role of knowledge driving, thereby improving the accuracy of the entity chain indication and meeting the application requirement under the complex scene.","['G06F40/295', 'G06F16/35', 'G06F16/3344', 'G06F16/36', 'G06F16/367', 'G06F16/9024', 'G06F16/9558', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/048', 'G06N3/082', 'G06N3/09', 'G06N5/022', 'G06F40/30', 'G06N3/042']"
CN118093621B,"Structured query language generation method and device, electronic equipment and storage medium","The invention provides a method, a device, electronic equipment and a storage medium for generating a structured query language, wherein the method comprises the following steps: acquiring an input data query demand text, and searching in a vector database to obtain a plurality of candidate database tables; inputting the data query requirement text and a plurality of candidate database tables into a fine-ranking model, and determining a target database table from the candidate database tables; decomposing the data query demand text into a plurality of reasoning tasks, and splicing each reasoning task with a corresponding candidate answer to obtain each question answer pair; and splicing the data query requirement text, the target database table and the answers of all the questions to obtain SQL code generation prompts, and inputting the SQL code generation prompts into a large language model to obtain the output SQL code. The method introduces RAG technology and thinking chain technology, improves each node of the SQL code generation path from the angles of database table selection and table field use, and improves the accuracy of SQL code generation of a large language model.","['G06F16/2433', 'G06F16/243', 'Y02D10/00']"
US20230229852A1,Machine-learning-based natural language processing techniques for low-latency document summarization,"Various embodiments of the present invention provide methods, apparatuses, systems, computing devices, and/or the like that are configured to effectively and efficiently generate one or more abstractive summaries of one or more multi-section documents. For example, certain embodiments of the present invention provide methods, apparatuses, systems, computing devices, and/or the like that are configured to generate an abstractive summary of a multi-section document comprising one or more sections, by generating one or more section summaries, section input batches for each selected section, model outputs created by one or more text summarization machine learning models through the performance of a batch processing operation sequence, abstractive summaries, and then storing the abstractive summaries.","['G06F16/345', 'G06F40/166', 'G06F40/284', 'G06F40/40', 'G06F40/216', 'G06F40/30', 'G06F40/56']"
US12124924B2,Machine-learning model to predict probability of success of an operator in a PaaS cloud environment,"Systems and methods are provided that integrate a machine-learning model, and more specifically, utilizing a platform as a service (PaaS) cloud to predict probability of success for an operator in an environment. An embodiment comprises a system having: a processor that executes computer executable components stored in memory, trained machine-learning model that predicts probability of success for deployment of an operator in an environment with a namespace of a platform as a service (PaaS) cloud, and a deployment component that receives a first operator and a first namespace and employs the trained machine-learning model to predict success of deployment of the first operator in a first environment.","['G06F8/60', 'G06F8/61', 'G06N20/00', 'G06N7/01', 'H04L41/50', 'G06N3/02']"
US11693630B2,Multi-lingual code generation with zero-shot inference,"A neural transformer model with attention is trained to predict candidates to complete a line of source code with a zero-inference capability. The model is trained on an unsupervised training dataset that includes features from source code written in multiple programming languages. The features include a file-level context and a local context, where the file-level context includes a global context, a class context, a function context, and/or a method context for each class, function and/or method of the source code programs used in the training dataset. The local context includes method bodies, function bodies, and/or stand-alone code of main method routines. From these features, the model is able to learn to predict an ordered sequence of code elements that complete a line of source code in a programming language seen and not seen during training.","['G06F8/33', 'G06F18/211', 'G06F18/213', 'G06F8/44', 'G06N3/02', 'G06N3/044', 'G06N3/045', 'G06N3/088']"
US12020706B2,Generating automated assistant responses and/or actions directly from dialog history and resources,"Training and/or utilizing a single neural network model to generate, at each of a plurality of assistant turns of a dialog session between a user and an automated assistant, a corresponding automated assistant natural language response and/or a corresponding automated assistant action. For example, at a given assistant turn of a dialog session, both a corresponding natural language response and a corresponding action can be generated jointly and based directly on output generated using the single neural network model. The corresponding response and/or corresponding action can be generated based on processing, using the neural network model, dialog history and a plurality of discrete resources. For example, the neural network model can be used to generate a response and/or action on a token-by-token basis.","['G06N5/041', 'G10L15/1822', 'G06N3/006', 'G06N3/045', 'G06N3/0455', 'G06N3/047', 'G06N3/0475', 'G06N3/08', 'G06N5/04', 'G10L15/063', 'G10L15/16', 'G10L15/22', 'G10L15/26', 'G10L2015/0638', 'G10L2015/223', 'G10L2015/225']"
US12379908B2,Systems and methods for code clustering analysis and transformation,"The present application is directed towards systems and methods for cluster-based code analysis and transformation. Cluster-based analysis may group code objects based on their similarity across functional areas, such as where a code object is cloned in multiple areas (e.g. sort functions that are duplicated across areas, or reports or tables that are identical). In some implementations, objects may be grouped into clusters by type, or based on reading from or writing to a common table. In some implementations, clustering at different layers may be possible.","['G06F8/35', 'G06F8/425', 'G06F8/436', 'G06F8/4436', 'G06F8/65', 'G06F8/751', 'G06F8/427', 'G06F9/44505']"
WO2024042508A1,Geosynchronization of an aerial image using localizing multiple features,"A georegistration (a.k.a. georectification) of an image captured by a camera in an aerial vehicle, such as a satellite, is based on identifying multiple features using descriptor sets, and sending to a ground station only the descriptors of the identified features and the associated locations in the captured image, without sending of the captured image itself, thus requiring a low communication bandwidth. Using a database of geosynchronized reference images, the ground station uses the received descriptors sets and the associated image locations to localize the features on a selected geosynchronized reference image from the database, and forms a mapping function that map any locations in the captured image to geographical coordinates on Earth. The mapping may be used to geosynchronize an additional feature identified in the aerial vehicle, or to geo synchronize a region that may be cropped from the captured image and sent to the ground station.","['G06V20/10', 'G06F16/29', 'G06V10/16', 'G06V10/46', 'G06V10/82', 'G06V20/13', 'G06V20/17', 'G06V20/52', 'G06V2201/10']"
US12099816B2,Multi-factor modelling for natural language processing,"Techniques are disclosed for systems including techniques for multi-factor modelling for training and utilizing chatbot systems for natural language processing. In an embodiment, a method includes receiving a set of utterance data corresponding to a natural language-based query, determining one or more intents for the chatbot corresponds to a possible context for the natural language-based query and associated with a skill for the chatbot, generating one or more intent classification datasets, each intent classification dataset associated with a probability that the natural language query corresponds to an intent of the one or more intents, generating one or more transformed datasets each corresponding to a skill of one or more skills, determining a first skill of the one or more skills based on the one or more transformed datasets and processing, based on the determined first skill, the set of utterance data to resolve the natural language-based query.","['G06F40/56', 'G06F40/295', 'G06F40/30', 'G06F40/35', 'H04L51/02']"
US11436006B2,Systems and methods for code analysis heat map interfaces,"The present application is directed towards systems and methods for providing a heat map interface for analyzing and reporting transformation capabilities of a source installation to a target installation of an application. Characteristics of the source installation are displayed in an easy, intuitive interface, providing improved efficiency in analysis and planning. Furthermore, the interface is interactive, allowing an administrator or user to select and apply transformation dispositions to code objects grouped into regions and sub-regions, providing versatility and accuracy of configuration.","['G06F8/65', 'G06F8/71', 'G06F3/04842', 'G06F8/425', 'G06F8/427', 'G06F8/436', 'G06F8/4436', 'G06F8/751']"
US11726760B2,Systems and methods for entry point-based code analysis and transformation,"The present application is directed towards systems and methods for identifying and grouping code objects into functional areas with boundaries crossed by entry points. An analysis agent may select a first functional area of a source installation of an application to be transformed to a target installation of the application from a plurality of functional areas of the source installation, each functional area comprising a plurality of associated code objects; and identify a first subset of the plurality of associated code objects of the first functional area having associations only to other code objects of the first functional area, and a second subset of the plurality of associated code objects of the first functional area having associations to code objects in additional functional areas, the second subset comprising entry points of the first functional area.","['G06F8/65', 'G06F8/64', 'G06F8/40']"
US11936667B2,Cyber security system applying network sequence prediction using transformers,"A cyber threat defense system and a method for detecting a cyber threat may use a predictor, e.g. a Transformer deep learning model, which is configured to predict a next item in the sequence of events and to detect one or more anomalies in the sequence of events. This provides a notification comprising (i) information about the one or more anomalies; and (ii) a prediction of what would have been expected.","['H04L63/1416', 'G06F21/554', 'G06N20/00', 'G06N3/04', 'G06N3/08', 'G06N7/01', 'H04L43/028', 'H04L63/105', 'H04L63/1425', 'H04L63/1433', 'H04L63/1441', 'H04L67/141', 'G06F2221/2113', 'G06N20/20', 'H04L63/145']"
WO2022237911A1,Method and apparatus for evaluating effectiveness of transformer fire extinguishing system,"A method for evaluating the effectiveness of a transformer fire extinguishing system on the basis of natural language fuzzy analysis. An evaluation method and apparatus for a fire extinguishing system are established. By means of a natural language fuzzification and defuzzification method for the effectiveness of the fire extinguishing system, an expert fuzzy evaluation matrix is established. For the relative influence extent of each index in an effectiveness evaluation index system of the fire extinguishing system, a weight contrast of each index is determined, and on this basis, a subjective weight of the index is constructed. According to the expert fuzzy evaluation matrix, defuzzification is performed to obtain a defuzzification matrix, and on the basis of the defuzzification matrix, an objective weight is obtained by using an entropy weight method. A comprehensive weight is obtained by combining the subjective weight and the objective weight, and an index score matrix is combined with the comprehensive weight to complete the comprehensive evaluation of a transformer fire extinguishing system.","['G06Q10/06393', 'G06Q50/06', 'G06Q50/265']"
US11429365B2,Systems and methods for automated retrofitting of customized code objects,"The present application is directed towards systems and methods for automatic retrofitting of customized code objects during transformation of a system from a source installation to a target installation. In many instances, new objects may be created or objects modified on an online or production system while a development system is being upgraded. Simply copying the upgraded development system to the production system when complete would delete these new objects or modifications. Accordingly, the modifications or new objects may need to be retrofitted, or propagated to the development system and upgraded or transformed for compatibility with the new software, prior to placing the system online.","['G06F8/61', 'G06F8/65', 'G06F9/44526']"
US8458467B2,Method and apparatus for adaptive application message payload content transformation in a network infrastructure element,"Application message payload data elements are transformed within a network infrastructure element such as a packet data router or switch. The network element has application message transformation logic for receiving one or more packets representing an input application message logically associated with OSI network model Layer 5 or above; extracting an application message payload from the input application message; identifying one or more first content elements in the application message payload; transforming the first content elements into one or more second content elements of an output application message; and forwarding the output application message to a destination that is identified in the input application message. Transformations performed in the network element can include field reordering, field enrichment, field filtering, and presentation transformation.","['H04L41/026', 'G06F8/656', 'H04L41/5003', 'H04L41/5009', 'H04L41/5012', 'H04L41/5096', 'H04L43/0811', 'H04L45/00', 'H04L45/56', 'H04L45/563', 'H04L63/0245', 'H04L63/0428', 'H04L63/08', 'H04L63/102', 'H04L63/12', 'H04L67/02', 'H04L67/34', 'H04L69/22', 'G06F2221/2141', 'H04L41/06', 'H04L41/0894', 'H04L41/22']"
US20210357210A1,Automatic generation of code documentation,"A code completion tool uses a neural transformer model with attention to generate code documentation for a method in a particular code documentation style. The neural transformer model is trained with source code programs and natural language text. The neural transformer model is pre-trained to learn the meaning of a method name, its corresponding method parameters and types from a large corpus of unsupervised dataset of source code methods. The neural transformer model is then fine-tuned on translation tasks where the model leans to translate a method signature/method body into a docstring of particular code documentation style.","['G06F8/73', 'G06F8/60', 'G06N3/045', 'G06N3/0454', 'G06N3/084', 'G06N3/088', 'G06N3/082', 'G06N7/01']"
US11462304B2,Artificial intelligence engine architecture for generating candidate drugs,"An artificial intelligence engine architecture for generating candidate drugs is disclosed. In one embodiment, a method includes generating, via a creator module, a candidate drug compound including a sequence of a candidate drug compound, including the candidate drug compound as a node in a knowledge graph; generating, via a descriptor module, a description of the candidate drug compound at the node in the knowledge graph, wherein the description comprises drug compound structural information, drug compound activity information, and drug compound semantic information; based on the description, performing, via a scientist module, a benchmark analysis of a parameter of the creator module; and modifying, based on the benchmark analysis, the creator module to change the parameter in a desired way during a subsequent benchmark analysis.","['G16C20/50', 'G06N20/00', 'G06N3/042', 'G06N3/0427', 'G06N3/0445', 'G06N3/045', 'G06N3/0454', 'G06N3/047', 'G06N3/08', 'G06N3/084', 'G06N3/088', 'G16C20/70', 'G16C60/00', 'G06N3/044']"
US20240256254A1,Systems and methods for transformation of reporting schema,"The present application is directed towards systems and methods for automatically transforming reporting and view database schema during upgrading of a system from a source installation to a target installation. An analyzer executed by a processor of a client device may identify an application of a source installation configured to process a first one or more objects of the source installation. The analyzer may determine that the first one or more objects are modified during upgrading or transformation of the source installation to a target installation. The analyzer may generate a mapping between the first one or more objects of the source installation to a second one or more objects of the target installation, responsive to the determination. A transformer executed by the processor of the client device may modify a schema of the application, according to the generated mapping.","['G06F8/65', 'G06F8/427', 'G06F8/443']"
US9411559B2,Resolution of textual code in a graphical hierarchical model of a technical computing environment,"A device may receive a chart generated via a technical computing environment, where the chart includes a textual portion and a graphical portion, and the graphical portion includes state information. The device may parse the chart into the textual portion and the graphical portion, and may process the textual portion with a textual engine of the technical computing environment to generate textual results. The device may process the graphical portion with a graphical engine of the technical computing environment to generate graphical results, and may combine the textual results with the graphical results to generate chart results. The device may output or store the chart results.","['G06F8/35', 'G06F8/34', 'G06F8/10', 'G06F8/51', 'G06F9/4425', 'G06F9/4484', 'G06F9/4498']"
US20240003950A1,Detection of electric discharges that precede fires in electrical wiring,"Described herein are methods and systems for detecting electrical discharges that precede electrical fires in electrical wiring. One or more sensor devices coupled to a circuit detect one or more signal waveforms generated by electrical activity on the circuit. The sensor devices identify one or more transient signals within the one or more signal waveforms, and generate one or more transient characteristics based upon the identified transient signals. A server communicably coupled to the sensor devices receives the one or more transient characteristics. The server analyzes the one or more transient characteristics to identify one or more electrical discharge indications. The server generates one or more alert signals when one or more electrical discharge indications are identified.","['G01R27/28', 'G01R31/14', 'G01R19/10', 'G01R19/16576', 'G01R19/175', 'G01R19/30', 'G01R23/16', 'G01R31/001', 'G01R31/085', 'G01R31/086', 'G01R31/58', 'G08B21/18', 'H02H1/0015']"
US12361036B2,"Visual dialog method and apparatus, method and apparatus for training visual dialog model, electronic device, and computer-readable storage medium","Disclosed in this application are a visual content dialog method performed by an electronic device. The method includes: acquiring an image feature of an input image and state vectors corresponding to first n rounds of historical question answering dialog, n being a positive integer; acquiring a question feature of a current round of questioning related to the input image; performing multimodal encoding on the image feature of the input image, the state vectors corresponding to the first n rounds of historical question answering dialog, and the question feature of the current round of questioning, to obtain a state vector corresponding to the current round of questioning; and performing multimodal decoding on the state vector corresponding to the current round of questioning and the image feature of the input image, to obtain an actual output answer corresponding to the current round of questioning.","['G06F16/3329', 'G06F16/3347', 'G06F16/583', 'G06F16/5846', 'G06F40/126', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
US12236220B2,Flow control for reconfigurable processors,"The technology disclosed relates to storing a dataflow graph with a plurality of compute nodes that transmit data along data connections, and controlling data transmission between compute nodes in the plurality of compute nodes along the data connections by using control connections to control writing of data.","['G06F8/452', 'G06F15/7867', 'G06F8/41', 'G06F15/825']"
US11921824B1,Sensor data fusion using cross-modal transformer,"Techniques are generally described for fusing sensor data of different modalities using a transformer. In various examples, first sensor data may be received from a first sensor and second sensor data may be received from a second sensor. A first feature representation of the first sensor data may be generated using a first machine learning model and a second feature representation of the second sensor data may be generated using a second machine learning model. In some examples, a modified first feature representation of the first sensor data may be generated based at least in part on a self-attention mechanism of a transformer encoder. The modified first feature representation may be generated based at least in part on the first feature representation and the second feature representation. A computer vision task may be performed using the modified first feature representation.","['G06V10/803', 'G06F18/25', 'B25J9/1697', 'G06F18/2163', 'G06N3/045', 'G06N3/08', 'G06V10/82', 'G06V20/10']"
CN112418011B,"Video content integrity identification method, device, equipment and storage medium","The application discloses a method, a device, equipment and a storage medium for identifying the integrity of video content, and relates to the field of deep learning. A video integrity recognition model is constructed through an artificial intelligence technology, and a function of recognizing video integrity is realized by using computer equipment. The method comprises the steps of obtaining video files and video release information of the video files, wherein the video release information represents information provided when video contents corresponding to the video files are released, separating audio data from the video files, extracting audio features from the audio data, extracting text features from the video release information, splicing the audio features and the text features to obtain spliced features, and identifying the spliced features to obtain the integrity of the video contents corresponding to the video files. The method comprises the steps of identifying the vector after the audio features and text features corresponding to the video file are spliced, and determining the integrity of video content by integrating the features of multiple dimensions, so that the accuracy of video integrity auditing is improved.","['G06V20/40', 'G06V20/46', 'G10L25/03']"
US12246241B2,Method and system of capturing and coordinating physical activities of multiple users,The present disclosure relates to system and method for coordinating and providing overall feedback for one or more users performing one or more physical activities at one or more locations. The feedback may be generated as AI feedback or human feedback. The method involves data capturing and coordinating the physical activities of the multiple users. The information to be captured is regarding performance activity of the multiple users and processing the same information in real time using AI assisted model. The method includes comparing each user's activity performance data including various performance parameters having a set of target activity performance parameters. The method includes generating feedbacks based on the comparison of the performance parameters. The feedbacks generated are shared with the users and rendered on the multimedia output device available to the users. The method includes sending the feedback to external portals via corresponding Application Programming Interfaces (APIs).,"['A63B71/0622', 'G06F3/167', 'A63B24/0062', 'A63B71/0616', 'G06F3/16', 'G06T13/40', 'G06T13/80', 'G06V40/23', 'A63B2024/0068', 'A63B2071/0625', 'A63B2071/0655', 'A63B2071/0694', 'G06T2200/24']"
US11775839B2,Frequently asked questions and document retrieval using bidirectional encoder representations from transformers (BERT) model trained on generated paraphrases,"An example system includes a processor to receive a query. The processor can retrieve ranked candidates from an index based on the query. The processor can re-rank the ranked candidates using a Bidirectional Encoder Representations from Transformers (BERT) query-question (Q-q) model trained to match queries to questions of a frequently asked question (FAQ) dataset, wherein the BERT Q-q model is fine-tuned using paraphrases generated for the questions in the FAQ dataset. The processor can return the re-ranked candidates in response to the query.","['G06N3/088', 'G06F16/24578', 'G06N3/045', 'G06N3/08']"
US11972754B2,Multi-task training architecture and strategy for attention-based speech recognition system,"Methods and apparatuses are provided for performing sequence to sequence (Seq2Seq) speech recognition training performed by at least one processor. The method includes acquiring a training set comprising a plurality of pairs of input data and target data corresponding to the input data, encoding the input data into a sequence of hidden states, performing a connectionist temporal classification (CTC) model training based on the sequence of hidden states, performing an attention model training based on the sequence of hidden states, and decoding the sequence of hidden states to generate target labels by independently performing the CTC model training and the attention model training.","['G10L15/063', 'G10L15/10', 'G10L15/16', 'G10L25/03', 'G10L25/54']"
US12361732B2,System and method for efficient visual navigation,"A method, apparatus and system for efficient navigation in a navigation space includes determining semantic features and respective 3D positional information of the semantic features for scenes of captured image content and depth-related content in the navigation space, combining information of the determined semantic features of the scene with respective 3D positional information using neural networks to determine an intermediate representation of the scene which provides information regarding positions of the semantic features in the scene and spatial relationships among the semantic features, and using the information regarding the positions of the semantic features and the spatial relationships among the semantic features in a machine learning process to provide at least one of a navigation path in the navigation space, a model of the navigation space, and an explanation of a navigation action by the single, mobile agent in the navigation space.","['G06V20/70', 'G06V10/774', 'G01C21/206', 'G01C21/3635', 'G06N3/02', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G06V10/25', 'G06V10/806', 'G06V10/82', 'G06V20/56', 'G06V20/58', 'G06V20/64', 'G06N3/008', 'G06N3/042']"
CN114333982B,Protein representation model pre-training and protein interaction prediction method and device,"The invention provides a method and a device for pretraining a protein expression model and predicting protein interaction, which relate to the technical field of artificial intelligence, in particular to the technical field of natural language processing and deep learning. The specific implementation scheme is as follows: and acquiring the amino acid sequence, the functional information and the structural information of the protein, and pre-training the protein representation model according to the amino acid sequence, the functional information and the structural information. Thus, a pre-training approach based on a multimodal protein representation model is provided.","['G16B5/00', 'G16B15/20', 'G16B20/00', 'G16B30/00', 'G16B40/00', 'G16B40/20']"
US9672821B2,Robust speech recognition in the presence of echo and noise using multiple signals for discrimination,"Systems and methods for speech recognition system having a speech processor that is trained to recognize speech by considering (1) a raw microphone signal that includes an echo signal and (2) different types of echo information signals from an echo cancellation system (and optionally different types of ambient noise suppression signals from a noise suppressor). The different types of echo information signals may include those used for echo cancelation and those having echo information. The speech recognition system may convert the raw microphone signal and different types of echo information signals (and optional noise suppression signals) into spectral features in the form of a vector, and a concatenator to combine the feature vectors into a total vector (for a period of time) that is used to train the speech processor, and during use of the speech processor to recognize speech.","['G10L15/20', 'G10L15/16', 'G10L2021/02082']"
US11966395B2,Query generation based on merger of subqueries,"Systems and methods for query generation based on merger of subqueries are described. For example, methods may include accessing a first join graph representing tables in a database, wherein the first join graph has vertices corresponding to respective tables in the database and directed edges corresponding to join relationships; receiving a first query specification that references data in two or more of the tables of the database to specify multiple subqueries in a set of subqueries; checking that two or more subqueries from the set of subqueries have the same join graph; checking that the two or more subqueries have the same set of grouping columns; responsive, at least in part, to the two or more subqueries having the same join graph and the same set of grouping columns, merging the two or more subqueries to obtain a consolidated query.","['G06F16/24542', 'G06F16/24535', 'G06F16/2456', 'G06F16/248', 'G06F16/9024']"
CN111931513B,Text intention recognition method and device,"The invention provides a text intention recognition method, a text intention recognition device, computer equipment and a computer readable storage medium, wherein the method comprises the following steps: acquiring a target text and a plurality of preset text intention categories; inputting the target text into a pre-training language model, and determining a semantic vector corresponding to the target text; and determining the probability value of the target text belonging to each text intention category according to the semantic vector, and determining the target text intention category corresponding to the target text. The invention is based on the idea of transfer learning, uses a pre-trained language model which is trained in advance by massive data in the NLP field and has good semantic expression effect, can generate the semantic vector representing the semantic feature of the target text more accurately, and further can further determine the target text intention category corresponding to the target text according to the semantic vector even if a large number of sample texts are not available during the cold start of the system, thereby improving the accuracy of the intention recognition system.","['G06F40/30', 'G06F16/35', 'G06F40/216', 'G06N3/045']"
US11676001B2,Learning graph representations using hierarchical transformers for content recommendation,"Knowledge graphs can greatly improve the quality of content recommendation systems. There is a broad variety of knowledge graphs in the domain including clicked user-ad graphs, clicked query-ad graphs, keyword-display URL graphs etc. A hierarchical Transformer model learns entity embeddings in knowledge graphs. The model consists of two different Transformer blocks where the bottom block generates relation-dependent embeddings for the source entity and its neighbors, and the top block aggregates the outputs from the bottom block to produce the target entity embedding. To balance the information from contextual entities and the source entity itself, a masked entity model (MEM) task is combined with a link prediction task in model training.","['G06N3/0455', 'G06N5/02', 'G06N3/045', 'G06N3/084', 'G06N3/048']"
CN111080032B,A Load Forecasting Method Based on Transformer Structure,"The invention discloses a load prediction method based on a transducer structure, which comprises the following steps: collecting and storing historical data; preprocessing historical data, dividing the preprocessed historical data into a training set and a testing set; establishing a load prediction model; training and testing the load prediction model by using the training set and the testing set to obtain a trained load prediction model; and inputting the data to be predicted, which is the same as the historical data, into a training post-load prediction model, and outputting a prediction result. The invention solves the technical problems that in natural language processing, the traditional time sequence model data preprocessing is complex, the model parallelization degree is low, the training speed is low, and the effect is not greatly improved on various latest data sets. The invention is obviously superior to the deep cyclic neural network series model with similar parameter numbers in training speed, is superior to the traditional model and algorithm in testing precision, and can predict any time sequence length in a prediction stage.","['G06Q10/04', 'G06N20/00', 'G06Q50/06', 'Y04S10/50']"
CN116340584B,Implementation method for automatically generating complex graph database query statement service,"The application relates to the technical field of query of graph databases, solves the problem that a graph database cannot be searched and queried by using natural language, and discloses a realization method for automatically generating a query statement service of a complex graph database, which comprises the following steps: pre-training and fine-tuning the large-scale language model; extracting primitive information from a map where a user is located; the large-scale language model automatically generates prompt results and/or graph database query sentences according to the input of a user in a query section; executing the query statement of the graph database to obtain a real-time query result on the graph, and checking, executing and verifying the query statement of the graph database according to the real-time query result.","['G06F16/9024', 'G06F16/9032', 'Y02D10/00']"
CN116541911B,Packaging design system based on artificial intelligence,"The invention relates to a packaging design system based on artificial intelligence, and belongs to the technical field of packaging design. The technical scheme of the invention mainly comprises the following steps: the demand analysis module is used for carrying out multi-round query interaction with a user based on the packaging language model and obtaining design demands according to user input analysis; the scheme generating module comprises a packaging model acquiring unit, a design image generating unit and a design template acquiring unit; the packaging model obtaining unit calls a corresponding packaging model from a box-type library according to the packaging type; the design image generating unit generates a package design pattern according to the type of the packaged product and the design style based on an image generating model; the design template obtaining unit calls a corresponding design template from a template library according to the design style; the scheme generating module generates a design scheme according to the packaging model, the packaging design pattern and the design template; and the editing module is used for editing the design elements in the design scheme.","['G06F30/10', 'G06F30/27', 'G06F40/166', 'G06F40/284', 'G06N3/0464', 'G06N3/08', 'G06T15/00', 'Y02P90/30']"
US12393504B2,Automated program repair tool,"An automated program repair tool utilizes a neural transformer model with attention to predict the contents of a bug repair in the context of source code having a bug of an identified bug type. The neural transformer model is trained on a large unsupervised corpus of source code using a span-masking denoising optimization objective, and fine-tuned on a large supervised dataset of triplets containing a bug-type annotation, software bug, and repair. The bug-type annotation is derived from an interprocedural static code analyzer. A bug type edit centroid is computed for each bug type and used in the inference decoding phase to generate the bug repair.","['G06N3/088', 'G06F11/362', 'G06F11/3604', 'G06F21/577', 'G06N20/00', 'G06N3/045', 'G06N3/084', 'G06F2221/033', 'G06N3/042']"
US12374109B2,Method and apparatus for video recognition,"Broadly speaking, the present techniques generally relate to a method and apparatus for video recognition, and in particular relate to a computer-implemented method for performing video recognition using a transformer-based machine learning, ML, model. Put another way, the present techniques provide new methods of image processing in order to automatically extract feature information from a video.","['G06V20/41', 'G06N3/04', 'G06N3/045', 'G06V10/454', 'G06V10/62', 'G06V10/7715', 'G06V10/82', 'G06V20/40', 'G06V20/46', 'G06V20/49', 'G06V40/20', 'H04N23/61', 'H04N23/667']"
CN113268609B,"Knowledge graph-based dialogue content recommendation method, device, equipment and medium","The invention relates to the field of artificial intelligence and discloses a dialogue content recommendation method, device and equipment based on a knowledge graph and a storage medium. The dialogue content recommendation method based on the knowledge graph comprises the following steps: constructing an atlas knowledge base based on a preset event atlas and a target knowledge atlas, wherein the target knowledge atlas has a corresponding relation with the preset event atlas; acquiring a history dialogue sentence and a sentence to be replied currently input by a target user, wherein the history dialogue sentence is a multi-round dialogue sentence between a system and the target user; based on a sentence to be replied currently input by a target user, matching knowledge associated with the sentence to be replied in a map knowledge base to obtain a target knowledge set; training a preset language model according to the target knowledge set and the historical dialogue sentences to obtain a dialogue recommendation model; and calling a dialogue recommendation model to process the sentence to be replied to generate dialogue content corresponding to the sentence to be replied, so that the accuracy of the generated recommendation dialogue is improved.","['G06F16/367', 'G06F16/3329', 'G06F16/3344', 'G06F16/3346', 'G06F16/335', 'G06F18/213', 'G06F40/30']"
CN109992782B,Legal document named entity identification method and device and computer equipment,"The invention relates to a legal document named entity identification method, a legal document named entity identification device and computer equipment, wherein the method comprises the steps of obtaining legal documents to be identified; inputting legal documents to be identified into a deep neural network model for identification to obtain an identification result; the deep neural network model is obtained by training a language model, a bidirectional cyclic neural network and a conditional random field through a plurality of legal document data with labels; the language model is obtained by training a google Bert model through a plurality of corpus. According to the invention, the deep neural network model is adopted to carry out entity recognition, the language model obtained by training the Google Bert model is adopted to extract the character vector from the Chinese character sequence of the legal document to be recognized, the character vector is input into the bidirectional cyclic neural network, the output code of the bidirectional cyclic neural network is input into the linear chain random field to obtain the recognition result, so that the network structure for realizing named entity recognition is simple, the training cost is low and the prediction capability is strong.","['G06F18/241', 'G06F40/295', 'Y02D10/00']"
US11934801B2,Multi-modal program inference,"Embodiments use a multi-modal approach to generate software programs that match a solution program description. The solution program description may include natural language, input-output examples, partial source code, desired operators, or other hints. Some embodiments use optimized prompts to a pre-trained language model to obtain initial candidate programs. Maximal program components are extracted and then recombined variously using component-based synthesis. Beam search reduces a solution program search space by discarding some candidates from a given synthesis iteration. Relevance metrics, string similarity metrics, operator frequency distributions, token rareness scores, and other optimizations may be employed. By virtue of optimizations and the multi-modal approach, a solution program may be obtained after fewer iterations than by use of a language model alone. The multi-modal approach is domain agnostic, as illustrated by examples using regular expression and cascading style sheet selector domain specific languages.","['G06F8/36', 'G06F8/33', 'G06F40/30', 'G06F40/40', 'G06F8/10', 'G06F8/30', 'G06F8/38']"
US12074940B2,Utility network project modeling and management,"Provided herein are embodiments of systems, devices, and methods for a digital collaborative system for electric, water and gas engineering and operations. The systems, devices, and methods include an automatic and intelligent synchronization of simultaneous and parallel project or digital twin modifications between multiple users in a collaborative environment that integrates utility engineering and operation teams.","['H04L67/1095', 'G06F16/27', 'G06Q10/101', 'G06Q10/20', 'G06Q50/06', 'G06F3/04817']"
US11604956B2,Sequence-to-sequence prediction using a neural network model,"A method for sequence-to-sequence prediction using a neural network model includes A method for sequence-to-sequence prediction using a neural network model, generating an encoded representation based on an input sequence using an encoder of the neural network model, predicting a fertility sequence based on the input sequence, generating an output template based on the input sequence and the fertility sequence, and predicting an output sequence based on the encoded representation and the output template using a decoder of the neural network model. The neural network model includes a plurality of model parameters learned according to a machine learning process. Each item of the fertility sequence includes a fertility count associated with a corresponding item of the input sequence.","['G06N3/04', 'G06N3/084', 'G06F40/216', 'G06F40/44', 'G06F40/51', 'G06F40/56', 'G06N3/045', 'G06N3/0454', 'G06N3/047', 'G06N3/0472', 'G06N3/08']"
CN114064855B,An information retrieval method and system based on transformer knowledge base,"The invention discloses an information retrieval method and system based on a transformer knowledge base, comprising the following steps: 38 standards/specifications commonly used in the field of transformer operation, maintenance and overhaul are selected to construct a sample library; the standard/normative documents are analyzed in a structured mode to form a corpus; iterating a transformer knowledge base dictionary and segmenting Chinese words in the electric power field, extracting keywords of sentences in a corpus, and expanding the iterated transformer knowledge base dictionary; establishing a word frequency feature library and a semantic feature library, and constructing a feature vector library of terms according to word frequency correlation and semantic similarity; the integrated learning coarse ordering is carried out, the query quantity typed by the user is matched with the feature vector library, the normalized measurement value is obtained, and a coarse ordering list is formed; and (5) post-processing the fine ordering, screening and adjusting the coarse ordering list through a logic strategy. The invention has the advantages of high accuracy of sorting the search results, good expandability, easy expansion operation of the dictionary and the corpus, and the like, can be conveniently incorporated into the intelligent operation and maintenance service flow, and promotes standard digital construction.","['G06F16/3335', 'G06F16/3344', 'G06F16/3346', 'G06F16/36', 'G06F40/216', 'G06F40/237', 'G06F40/289']"
US20240303487A1,Multimodal machine learning model for data including examples with missing modalities,"Multimodal training data comprising samples of a prediction target is received. Each sample includes at least a subset of the full set of a plurality of modalities, and the samples collectively include instances of each modality. An attention-based encoder receives sets of training vectors for the samples in fixed-dimensional input vector format, and generates a fixed-dimensional vector representation template for the prediction target. The number of dimensions in the template is constant and is independent of the number of modalities represented by the training vectors. The attention-based encoder uses the samples and the fixed-dimensional vector representation template to generate, from the training vectors for the samples, a latent distribution. The samples in fixed-dimensional input vector format and the latent distribution are used as input to a second attention-based neural network to generate an attention-based decoder that can predict from samples with missing modalities.","['G06N3/0455', 'G06N3/045', 'G06N3/08']"
CN109783827B,Deep neural machine translation method based on dynamic linear polymerization,"The invention discloses a deep neural machine translation method based on dynamic linear aggregation, which is characterized in that a memory network is added at an encoding end and a decoding end of a Transformer model simultaneously to store the output of a previous middle lamination, the memory network is accessed before the calculation of a next lamination network is carried out, dense vectors stored in the memory network and semantic vectors of all previous laminations are aggregated by a linear multi-step method based on a normal differential equation to obtain a hidden layer representation of the characteristics of each aggregation layer, and the hidden layer representation obtains a semantic vector of which the parameter obeys standard positive and negative through layer regularization operation and is used as the input of the next lamination. By the method, the feature vectors extracted by all the previous overlay networks are fully considered when the current overlay network is calculated, and further a deeper model is trained to improve the representation capability of the model and improve the performance of machine translation.",[]
US20230394247A1,Human-machine collaborative conversation interaction system and method,"A system for human-machine collaborative conversation interaction includes one or more processors configured to execute instructions to cause the system to perform operations including: outputting, according to conversation data to be processed, structural information of the conversation data, wherein the conversation data comprises multiple turns of conversation; obtaining, according to the structural information, a semantic representation vector carrying phrase-dimensional semantic information, sentence-dimensional semantic information and topic-dimensional semantic information corresponding to the conversation data; obtaining semantic transfer relationships between each turn of conversation according to the semantic representation vector; and determining, according to the semantic representation vector and the semantic transfer relationships, conversation data matching service requirements so as to perform preset service processing through the determined conversation data.","['G06F40/35', 'G06F40/211', 'G06F40/284', 'G06F40/289', 'G06F40/40', 'G06N20/00']"
US20220067626A1,Enterprise spend optimization and mapping model architecture,"Various embodiments described herein relate to providing optimization related to enterprise performance management. In this regard, a request to obtain one or more insights with respect to a formatted version of disparate data associated with one or more data sources is received. The request includes an insight descriptor that describes a goal for the one or more insights. In response to the request, aspects of the formatted version of the disparate data is associated to provide the one or more insights. The associated aspects are determined by the goal and relationships between the aspects of the formatted version of the disparate data. Furthermore, one or more actions are performed based on the one or more insights.","['G06Q10/06375', 'G06F16/2465', 'G06F3/14', 'G06N3/044', 'G06N3/045', 'G06N3/088', 'G06Q10/0639', 'G06Q10/067', 'G06Q10/087', 'G06Q30/0283', 'G06Q30/0201']"
CN113627447B,"Label identification method, label identification device, computer equipment, storage medium and program product","The application provides a tag identification method, a tag identification device, computer equipment, a storage medium and a program product, and relates to the technical fields of artificial intelligence, cloud technology, intelligent traffic, driving assistance and the like. Performing multi-type feature extraction on information to be identified through a feature extraction network to obtain multi-type features; respectively determining the matching degree between the information to be identified and each label based on the global features of each label in the multi-type features and the global label features, so as to determine the label of the information to be identified based on the matching degree; the global characteristics of at least two labels included in the global label characteristics are determined based on the initial characteristics of each label and the incidence relation between the at least two labels, the incidence relation existing between the labels in the range of the plurality of labels can be represented, the label identification is carried out by combining the global correlation among the plurality of labels, the problem of identification errors caused by the independent processing of a single label is avoided, and the accuracy of the label identification can be improved.","['G06F18/22', 'G06N3/045', 'G06N3/08']"
US12259930B2,System and method for automated file reporting,"A document index generating system and method are provided. The system comprises at least one processor and a memory storing a sequence of instructions which when executed by the at least one processor configure the at least one processor to perform the method. The method comprises preprocessing a plurality of pages into a collection of data structures, classifying each preprocessed page into at least one document type, segmenting groups of classified pages into documents, and generating a page and document index for the plurality of pages based on the classified pages and documents. Each data structure comprises a representation of data for a page of the plurality of pages. The representation comprises at least one region on the page.","['G06N5/022', 'G06F16/13', 'G06F16/901', 'G06F16/906', 'G06F16/93', 'G06N20/00', 'G06N3/045', 'G06N3/08']"
CN113850052B,"Training method, device, equipment and storage medium of autoregressive language model","The application relates to the technical field of artificial intelligence and discloses a training method, a device, a medium and equipment of an autoregressive language model, wherein the method comprises the steps of obtaining training sentences, extracting training text vectors of the training sentences, obtaining configuration instructions and expected training results of the training sentences, configuring sentence templates according to identifier information and configuration rules in the configuration instructions, inputting the training text vectors and the sentence templates into a word vector model, carrying out mapping calculation on the training text vectors and the sentence templates through the word vector model to obtain content mapping information of the training text vectors under the sentence templates, and carrying out iterative training on the word vector model based on an attention model according to the content mapping information and the expected training results to obtain an autoregressive language model based on the sentence templates, so that the generation efficiency is ensured while generating texts according to formats.","['G06F40/111', 'G06F40/186', 'G06F40/289', 'G06N3/04', 'G06N3/08']"
US11342055B2,Method and system for automatically generating a section in a radiology report,A system 100 for automatically generating a field of a radiology report includes a set of one or more models. A method for automatically generating a field of a radiology report includes: receiving a radiologist identifier (radiologist ID); receiving a set of finding inputs; determining a context of each of the set of finding inputs; determining text associated with a portion or all of the radiology report based on the context and the radiologist style; and inserting the text into the report.,"['G16H15/00', 'G16H10/60', 'G16H20/40', 'G16H40/63', 'G16H40/67', 'G16H50/20']"
US11886955B2,Self-supervised data obfuscation in foundation models,"Provided are methods and system for obtaining, by a computer system, a machine learning/machine learning model; obtaining, by the computer system, a training data set; training, with the computer system, an obfuscation transform based on the machine learning/machine learning model and the training data set; and storing, with the computer system, the obfuscation transform in memory.","['G06N3/0895', 'G06N3/0455', 'G06N3/0475', 'G06N3/0495', 'G06N3/084', 'G06N3/088', 'G06N3/09', 'G06N3/094', 'G06N3/098']"
US20230080671A1,User intention recognition method and apparatus based on statement context relationship prediction,"A user intention recognition method and apparatus based on statement context relationship prediction, and a computer device and a storage medium. The method comprises: setting a plurality of sample data, the sample data comprising a first statement, a second statement, and the statement attribute features and positional relationship of the first statement and the second statement (S10); inputting each piece of sample data into a pre-training language model for pre-training, and when the recognition accuracy of the pre-training language model for the sample data reaches a first set accuracy, determining an initial model according to the current operating parameters of the pre-training language model (S20); inputting a test statement into the initial model to predict the next statement of the test statement as a unique target to finely adjust the initial model, and when the prediction accuracy of the initial model reaches a second set accuracy, determining an intention recognition model according to the current operating parameters of the initial model (S30); and determining, by using the intention recognition model, the next statement of a statement input by a user, and determining a user intention according to the determined next statement (S40). Therefore, the determined user intention has relatively high accuracy.","['G06F16/3329', 'G06F40/35', 'G06F16/3343', 'G06F40/279', 'Y02D10/00']"
CN110928994B,"Similar case retrieval method, similar case retrieval device and electronic equipment","The application discloses a similar case retrieval method, a similar case retrieval device and electronic equipment. The similar case retrieval method comprises the following steps: receiving a case to be retrieved, which comprises at least one of a text description and a multimedia file related to the case; performing dispute focus analysis, legal element analysis, keyword extraction, multi-model semantic processing and multi-granularity semantic processing on the text description to generate a document analysis result; performing semantic processing on the multimedia file to generate a semantic analysis result; and matching the document analysis result and the semantic analysis result of the case to be retrieved with the document analysis result and the semantic analysis result of the case in the case library to obtain a retrieval result. Therefore, similar case retrieval is carried out based on multiple models, multiple particle sizes and multi-mode semantics and by combining dispute focus analysis, search results are increased, and matching accuracy is improved.","['G06F16/3334', 'G06F16/338', 'G06N3/045', 'G06N3/08', 'G06Q50/18']"
CN110413746B,Method and device for identifying intention of user problem,"According to one embodiment of the method, the user problem is acquired, meanwhile, associated information of the user problem is acquired, the associated information comprises information related to a scene where the user is posed, then the user problem and the associated information are spliced to obtain a spliced text, the spliced text is input into a pre-trained prediction model to determine an intention category corresponding to the user problem according to an output result of the prediction model, at least one feature extraction layer in the prediction model sequentially passes through a general corpus and a customer service corpus in a first model in advance, pre-training is conducted according to a preset prediction task, and the first model comprises at least one feature extraction layer and at least one prediction layer. As more user information is utilized and the accuracy of the model is optimized, the accuracy of identifying the intention contained in the user problem can be improved.","['G06F16/3329', 'G06F16/3344', 'G06F16/355']"
US12002450B2,Speech recognition systems and methods,"A computer-implemented method for speech recognition, comprising receiving a frame of speech audio; encoding the frame of speech audio; calculating a halting probability based on the frame of speech audio; adding the halting probability to a first accumulator variable; in response to the first accumulator variable exceeding or reaching a first threshold, calculating a context vector based on the halting probability and the encoding of the frame of speech audio; performing a decoding step using the context vector to derive a token; and executing a function based on the derived token, wherein the executed function comprises at least one of text output or command performance.","['G10L15/26', 'G10L25/87', 'G10L15/00', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G10L15/183', 'G10L15/22', 'G10L19/02', 'G10L25/78', 'G06N3/044', 'G10L15/04', 'G10L15/16', 'G10L2025/783']"
US11875233B2,Automatic recognition of entities related to cloud incidents,"Systems and methods for automatic recognition of entities related to cloud incidents are described. A method, implemented by at least one processor, for processing cloud incidents related information, including entity names and entity values associated with incidents having a potential to adversely impact products or services offered by a cloud service provider is provided. The method may include using at least one processor, processing the cloud incidents related information to convert at least words and symbols corresponding to a cloud incident into machine learning formatted data. The method may further include using a machine learning pipeline, processing at least a subset of the machine learning formatted data to recognize entity names and entity values associated with the cloud incident.","['G06Q10/103', 'G06N20/00', 'G06F18/2155', 'G06F40/284', 'G06F40/295', 'H04L41/50']"
CN114254655B,Network security tracing semantic identification method based on prompt self-supervision learning,"The invention discloses a network security traceability semantic identification method based on prompt self-supervision learning, which comprises the following steps: constructing a network security professional corpus; multi-dimension is rich, and a data set of an attack source is captured in a reconstruction conversation; the transformer coding part identifies semantic features and expresses the semantic features vectorially; decoding and selecting key semantics by using a transformer; training cross entropy loss with a real label, and training model parameters; outputting a corresponding label by the multiple iteration optimization model, and identifying a corresponding IP or domain name; the method takes the transformer model as a basic structure of the mask language model, can carry out semantic recognition on the text according to external information and context content, and can carry out prompt learning of the mask language model aiming at network security professional vocabularies with scarce data set resources to fully mine the information of the existing data, thereby realizing semantic extraction with high efficiency and low cost and leading a machine to understand the intention of a real person.","['G06F40/30', 'G06F16/353', 'G06N3/045', 'G06N3/047', 'G06N3/048', 'G06N3/084', 'G06N3/088', 'H04L63/1408', 'H04L63/1425', 'H04L63/1483', 'G06F2216/03', 'H04L2463/146']"
US9524275B2,Selectively translating specified document portions,"Methods, systems and computer program products are provided for selectively translating documents. For example, a computer system may be provided that selectively loads a specified portion of a document in a meta-markup language into system memory. Portions of the document that are not specified remain unloaded into the system memory. Once the specified portions of the document have been loaded into memory, the computer system translates the selectively loaded portion of the document from one format to another format. Translated portions of the document may then be provided, in some cases, to entities such as users, software applications or data stores. At least some portions of the document that were not specified or selected for loading into memory remain untranslated and unloaded into system memory. As such, unnecessary loading and translating of unspecified portions of the document is avoided.","['G06F40/143', 'G06F17/218', 'G06F17/2247', 'G06F17/227', 'G06F40/117', 'G06F40/154', 'G06F16/322', 'G06F17/30625', 'G06F8/4434']"
US12087446B2,Multimodal dynamic attention fusion,"Methods and systems are provided for diagnosing mental health conditions using multiple data modalities. In particular, a trained machine learning model is used for mental health diagnosis, wherein the trained model utilizes a dynamic fusion approach for capturing and preserving interactions as well as timing information between the multiple data modalities.","['G16H50/20', 'G16H50/50', 'A61B5/0205', 'A61B5/165', 'A61B5/7264', 'G06N20/00', 'G06N3/0455', 'G06N3/0464', 'G06N3/0475', 'G06N5/045', 'G16H10/20', 'G16H20/10', 'G16H20/70', 'G16H50/30', 'G06N3/0442', 'G06N3/048', 'G06N3/082', 'G06N3/084', 'G06N3/0985']"
CN111241279B,Natural language relation extraction method based on multi-task learning mechanism,"The invention discloses a natural language relation extraction method based on a multitask learning mechanism, which comprises the following steps: a plurality of auxiliary tasks are utilized to introduce mutually implicit information among different tasks to improve the effect of relation extraction. Knowledge distillation is introduced to enhance the effect of assisting tasks to guide and train the multitask model, and a teacher annealing algorithm is introduced for relation and extraction based on multitask learning, so that the effect of the multitask model can be used as a single task model for guiding tasks, and finally the accuracy of relation extraction is improved. The method includes the steps that firstly, a multi-task model for guiding training is obtained through training on different auxiliary tasks, then the model learned by the auxiliary tasks and a real label are used as supervision information to simultaneously guide the learning of the multi-task model, and finally evaluation is conducted on a SemEval2010 task-8 data set.","['G06F16/355', 'G06N3/045', 'G06N3/08']"
US11126800B2,Systems and methods for simultaneous translation with integrated anticipation and controllable latency (STACL),Presented herein are embodiments of a prefix-to-prefix framework for simultaneous translation that implicitly learns to anticipates in a single translation. Within these frameworks are effective â€œwait-kâ€ policy model embodiments that may be trained to generate a target sentence concurrently with a source sentence but lag behind by a predefined number of words. Embodiments of the prefix-to-prefix framework achieve low latency and better quality when compared to full-sentence translation in four directions: Chineseâ†”English and Germanâ†”English. Also presented herein is a novel latency metric that addresses deficiencies of previous latency metrics.,"['G06F40/58', 'G06F40/42', 'G06F40/274', 'G06F40/47', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06F40/284', 'G06N3/006']"
US12367862B2,Method of generating response using utterance and apparatus therefor,"Systems and techniques to generate imitative responses are illustrated. response generation method performed in an electronic apparatus of the present disclosure includes acquiring at least one piece of utterance data, acquiring a first context corresponding to the utterance data from a context candidate set, generating one or more dialogue sets including the first context and the utterance data, receiving a second context from a user, and acquiring a response corresponding to the second context using a language model based on the one or more dialogue sets.","['G06F40/35', 'G10L15/02', 'G06F40/20', 'G06N3/006', 'G06N5/041', 'H04L51/02', 'G06N3/045', 'G06N3/0475', 'G06N3/096']"
CN119942803B,Multi-mode traffic flow prediction method based on dynamic space-time hypergraph and large language model,"The invention discloses a multi-mode traffic flow prediction method based on a dynamic space-time hypergraph and a large language model, which relates to the field of computer machine learning and comprises the steps of acquiring a traffic data set, aligning text description and a time stamp of sensor data, and dividing a training set, a verification set and a test set; and constructing a traffic flow prediction model, training the model by adopting a training set, monitoring the training process by adopting a verification set, adjusting super parameters and finally evaluating the prediction performance of the model by adopting a test set. The invention is superior to the existing method, and proves the effectiveness of the method in complex dynamic traffic environment.",[]
KR102203355B1,System and method extracting experience information according to experience of product,"According to one embodiment of the present invention, a system for extracting experience information according to experience of a product comprises: a storage unit for storing product group experience data corresponding to consumer experience of an experience product group; a text extraction unit for extracting text information of an experiencer from a voice of the experiencer generated according to experience of an experience target product; a feature information extraction unit for performing pretreatment for the text information extracted in the text extraction unit, and extracting feature information that is an input value for machine learning from the pretreated text information; a reaction information extraction unit for extracting experience reaction information that is classified for each category according to experience of the experience target product by applying the extracted feature information to a first model trained based on the product group experience data; and a control unit for matching experiencer identification information of the experiencer and product identification information for the experience target product with the experience reaction information, and controlling the matched experience reaction information to be stored in the storage unit. According to the present invention, it is possible to accurately realize actual perception of consumers about an experience target product.","['G06Q30/0631', 'A61B5/441', 'G10L15/26', 'G11B20/10', 'A45D2044/007']"
CN116244418B,"Question answering method, device, electronic equipment and computer readable storage medium","The embodiment of the application provides a problem solving method, a device, electronic equipment and a computer readable storage medium, which are at least applied to the field of artificial intelligence and the field of information searching, wherein the method comprises the following steps: acquiring a to-be-solved problem; performing problem disassembly on the problem to be solved through a pre-trained problem disassembly model to obtain a plurality of sub-problems corresponding to the problem to be solved; the problem dismantling model is a model obtained by training a first quantity of supervision data and a second quantity of weak supervision data serving as sample data; inquiring each sub-problem through a search engine to obtain a reference information set corresponding to a plurality of sub-problems; and solving the problem to be solved based on the reference information in the reference information set to obtain a solution result of the problem to be solved. The application can greatly reduce the resource consumption in the problem solving process.","['G06F16/3329', 'Y02D10/00']"
CN113642330A,Entity recognition method of rail transit specification based on catalog topic classification,"The invention mainly relates to a track traffic standard entity recognition method based on catalog subject classification, which adopts a RoBERTA pre-training language model and a Whole Word Masking (Whole Word Masking) mechanism, realizes field self-adaptive pre-training by collecting larger-scale building standard texts, and adds subject classification information to improve the performance of a named entity recognition task. In addition, a pre-training language model obtained by training is applied to a named entity recognition task, so that important support is provided for constructing a domain knowledge graph; many benefits are brought to the user: the named entity recognition model can better represent the field text, and the recognition performance of the building entity is improved. Gradually increasing a text corpus and expanding the completed pre-training language model, so that the pre-training language model is suitable for more various and complicated text contents; the language model after the field self-adaptive pre-training can be directly applied to other natural language processing tasks after one-time training and multiple-time use.","['G06F40/295', 'G06F16/345', 'G06F16/35', 'G06F40/242', 'G06F40/30', 'G06N3/04', 'G06N3/08']"
CN110852087B,"Chinese error correction method and device, storage medium and electronic device","The invention discloses a Chinese error correction method and device, a storage medium and an electronic device. Wherein, the method comprises the following steps: acquiring candidate words from the candidate word list according to the target words in the statement to be corrected, and replacing the target words in the statement to be corrected with the candidate words; calculating a first confusion index PPL value of a sentence to be corrected before replacement and a second confusion index PPL value of characters in a short sentence to be corrected after replacement; under the condition that the difference value between the first PPL value and the second PPL value is smaller than a first threshold value, acquiring the position of a suspected wrongly-written word in a statement to be corrected; replacing the characters marked with the suspected wrongly written character positions with the predicted characters, and calculating the probability of the predicted characters in the target sentence; and replacing the suspected wrongly-written words with the predicted words when the probability is larger than a second threshold value. The invention solves the technical problems that the error correction mode for the sentences is single, and the error correction cannot be realized quickly and effectively in the prior art.",[]
CN116719520B,Code generation method and device,"The embodiment of the specification provides a code generation method and a device, wherein in the code generation method, query text of a user is obtained, and at least the function realized by the code is indicated. And according to the query text, querying the target code library to obtain a plurality of sections of reference codes matched with the query text. Wherein, the object code library records a plurality of sections of program codes corresponding to different code functions. And constructing a first prompt text based on the query text and the reference codes, wherein the first prompt text indicates that program codes corresponding to the query text are generated based on the reference codes. And inputting the first prompt text into a pre-trained large generation model to obtain an object code generated for the query text.","['G06F8/36', 'G06F8/33']"
US11928600B2,Sequence-to-sequence prediction using a neural network model,A method for sequence-to-sequence prediction using a neural network model includes generating an encoded representation based on an input sequence using an encoder of the neural network model and predicting an output sequence based on the encoded representation using a decoder of the neural network model. The neural network model includes a plurality of model parameters learned according to a machine learning process. At least one of the encoder or the decoder includes a branched attention layer. Each branch of the branched attention layer includes an interdependent scaling node configured to scale an intermediate representation of the branch by a learned scaling parameter. The learned scaling parameter depends on one or more other learned scaling parameters of one or more other interdependent scaling nodes of one or more other branches of the branched attention layer.,"['G06N3/084', 'G06F40/47', 'G06F40/58', 'G06N3/044', 'G06N3/045', 'G06N3/048', 'G06N3/006', 'G06N3/082', 'G06N3/088']"
CN111967268B,"Event extraction methods, devices, electronic devices and storage media from text","The application discloses an event extraction method, an event extraction device, electronic equipment and a storage medium in a text, and relates to the technical fields of knowledge maps, deep learning and natural language processing. The specific implementation scheme is as follows: when an event is extracted from an input text, trigger word extraction is carried out on the input text through a trigger word extraction model, so that a trigger word extraction result of the input text is obtained, the input text and the trigger word extraction result are input into an argument extraction model, so that an argument extraction result of the input text is obtained, and the event extraction result of the input text is determined by combining the trigger word extraction result and the argument extraction result. Therefore, the trigger word extraction model and the argument extraction model are combined to realize event extraction on the input text, so that the construction cost of event extraction can be reduced while the accuracy of event extraction in the input text is improved, and no manual feature is needed.","['G06F40/279', 'G06F40/30', 'G06F40/117', 'G06F40/237', 'G06F40/284', 'G06F40/289', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N7/01', 'Y02D10/00']"
US11436065B2,System for efficient large-scale data distribution in distributed and parallel processing environment,"The present invention relates to a system for efficient large-scale data distribution in a distributed and parallel processing environment. In particular, the present invention relates to global Top-k sparsification for low bandwidth networks. The present invention verifies that gTop-k S-SGD has nearly consistent convergence performance with S-SGD and evaluates the training efficiency of gTop-k on a cluster with 32 GPU machines which are inter-connected with 1 Gbps Ethernet. The experimental results show that the present invention achieves up to 2.7-12Ã— higher scaling efficiency than S-SGD with dense gradients, and 1.1-1.7Ã— improvement than the existing Top-k S-SGD.","['G06F9/546', 'G06N3/084', 'G06N3/04', 'G06N3/045', 'G06N3/0454', 'H04L69/04', 'H04L9/40']"
CN111813802B,Method for generating structured query statement based on natural language,"The invention discloses a method for generating a structured query statement based on natural language, which comprises the steps that a user inputs a natural language question and selects a database which is expected to be queried; mapping natural language question sentences and database information to vector space, inputting the three groups of obtained input vectors into a pre-trained first model to obtain a structured query language grammar block sequence, generating structured query sentences represented by each grammar block according to a set grammar block generating formula, filling a default occupation value in numerical comparison, and combining into a complete result through main and foreign key relation; judging whether a place value exists in the generated structured query statement, and if the place value does not exist, directly outputting the combined complete result as a final generated result; if the occupation value exists, enumerating different condition values to be filled in the structured query statement, inputting the condition values to the second model to calculate the similarity with the natural language question, and outputting the structured query statement with the highest similarity as a final result.","['G06F16/24522', 'G06F16/243', 'G06F16/284']"
CN113485156B,A transformer digital twin cloud platform and its implementation method,"The invention discloses a digital twin cloud platform of a transformer and an implementation method thereof, wherein the platform is in a C/S mode, adopts a multi-module architecture, is provided with a state monitoring device, and has the communication protocol conversion and edge computing capabilities of TCP/IP, MQTT and the like. The data access module performs structured and unstructured data access in part in the mainstream IoT protocol. And the modeling simulation module completes the three-dimensional geometric modeling of the transformer. And calling a simulation result by using a visual simulation module to perform visual display and experience. The state monitoring device records the running condition of the equipment in real time, sends real-time data to the cloud end to be stored in a file form, feeds back the current transformer state through comparison analysis with historical data in a database, guides the update and prediction of the local model, and realizes the management of the whole life cycle. The edge computing device may significantly improve the twin volume data computation and update speed. The method effectively combines a plurality of functions required by digital twin, and is beneficial to realizing the digital twin of the complex equipment with multiple working conditions.",['G05B17/02']
US10980483B2,Remote physiologic parameter determination methods and platform apparatuses,"Certain aspects of the disclosure are directed to an apparatus including a scale and external circuitry. The scale includes a platform for a user to stand on, and data-procurement circuitry for collecting signals indicative of the user's identity and cardio-physiological measurements while the user is standing on the platform. The scale includes processing circuitry to process data obtained by the data-procurement circuitry and therefrom generate cardio-related physiologic data, and an output circuit to send user data from the scale for reception at a remote location. The external circuitry receives and validates the user data as concerning a specific user associated with a user ID and determine at least one physiologic parameter of the user using the user data. Further, the external circuitry derives additional health information corresponding to the user data based on categories of interest and outputs the additional health information to the scale for display.","['A61B5/6887', 'A61B5/0015', 'A61B5/0205', 'A61B5/0402', 'A61B5/1102', 'A61B5/117', 'A61B5/318', 'A61B5/7435', 'G01G19/00', 'G01G19/44', 'G01G19/50', 'A61B5/02125', 'A61B5/024', 'A61B5/0535', 'A61B5/7475', 'G01G23/3735']"
US10553306B2,Scaled-based methods and apparatuses for automatically updating patient profiles,"Aspects of the present disclosure are directed to an apparatus comprising a scale and external circuitry. The scale includes a platform, data-procurement circuitry to engage the user with electrical signals and collect signals indicative of the user's identity and cardio-physiological measurements while the user is standing on the platform, processing circuitry to process data obtained by the data-procurement circuitry while the user is standing on the platform and therefrom generate cardio-related physiologic data corresponding to the collected signals, and an output circuit. The output circuit sends user data, including data indicative of the user's identity and the generated cardio-related physiologic data, for reception at external circuitry. The external circuitry receives the user data, validates the cardio-related physiologic data as concerning the user associated with a patient profile, and updates the patient profile using the generated cardio-related physiologic data.","['G16H10/60', 'A61B5/0022', 'A61B5/0205', 'A61B5/0535', 'A61B5/318', 'A61B5/6887', 'A61B5/7225', 'A61B90/96', 'A61B90/98', 'G01G19/50', 'G01G23/18', 'A61B2090/064', 'A61B2562/046']"
CN110362847B,"System, method and apparatus for generating integrated modular architecture model","Systems, methods, apparatus, and articles of manufacture for generating an Integrated Modular Architecture (IMA) model are disclosed. An example apparatus includes an interface importer to import a first IMA protocol definition into a computer modeling environment to generate libraries and generate IMA model objects, a model object handler to import some of the IMA model objects into the computer modeling environment to generate a first IMA model corresponding to an aircraft system of an aircraft, and generate an IMA system model corresponding to the aircraft by generating a plurality of IMA models including the first IMA model. The example apparatus also includes an interface validator to validate the IMA system model by comparing an output of the IMA system model to a validation output, an interface exporter to generate manufacturing build instructions of the aircraft system when the IMA system model is validated, and a report generator to initiate production of the aircraft system.","['G05B19/4155', 'B64F5/00', 'G05B19/41865', 'G06F30/15', 'G06F30/20', 'G06F2111/20', 'G06F2119/18', 'Y02P90/02']"
CN113435179B,"Composition review method, device, equipment and storage medium","The application provides a composition review method, a device, equipment and a storage medium, wherein the method comprises the following steps: detecting whether a target composition to be reviewed is an abnormal composition; if not, correcting the target composition from the word level, the sentence level and the chapter level respectively to obtain correction results corresponding to the target composition at the word level, the sentence level and the chapter level respectively; determining a scoring grade of the target composition from the plurality of review dimensions to obtain a scoring grade of the target composition in the plurality of review dimensions; and generating comments of the target composition according to the grading of the target composition in a plurality of comment dimensions. The composition review method provided by the application can automatically review the composition to be reviewed, and avoids the problems caused by manual participation because manual participation is not needed, and the composition review method provided by the application can obtain the review results with rich contents, and the review results with rich contents can play a good guiding role for writers, so that the user experience is good.","['G06F40/211', 'G06F40/205', 'G06F40/216', 'G06F40/232', 'G06F40/253', 'G06F40/289']"
US11947914B2,Fact checking based on semantic graphs,"In embodiments of the present disclosure, there is provided an approach for fact checking based on semantic graphs. According to embodiments of the present disclosure, after obtaining a text to be fact checked, a plurality of evidence sentences related to the text are retrieved from an evidence database. Then, semantic graphs of the text and the evidence sentences are constructed based on the semantic analysis, and a veracity of a statement in the text can be determined based on the semantic graphs. Embodiments of the present disclosure propose a graph-based reasoning approach for fact checking, and use the constructed semantic graphs to facilitate verification of the truthfulness of the text, thereby improving the accuracy for fact checking.","['G06F16/36', 'G06F40/30', 'G06F16/212', 'G06N3/042', 'G06N3/045', 'G06N3/08', 'G06N5/02', 'G06N3/048']"
US11900068B1,Generative summaries for search results,"At least selectively utilizing a large language model (LLM) in generating a natural language (NL) based summary to be rendered in response to a query. In some implementations, in generating the NL based summary additional content is processed using the LLM. The additional content is in addition to query content of the query itself and, in generating the NL based summary, can be processed using the LLM and along with the query contentâ€”or even independent of the query content. Processing the additional content can, for example, mitigate occurrences of the NL based summary including inaccuracies and/or can mitigate occurrences of the NL based summary being over-specified and/or under-specified.","['G06F40/56', 'G06F40/40', 'G06F16/3328', 'G06F16/3344', 'G06F16/345', 'G06N20/00', 'G06N3/0455', 'G06N3/0475']"
US11922541B1,Enhancement of machine-generated product image,"Methods, systems, and computer programs are presented for enhancing a machine-generated product image. One method includes an operation for receiving a request on a user interface (UI) to generate an image, where the request comprises a description of the image to be generated and identification of a product for inclusion in the image. The method further includes operations for generating, by a generative artificial intelligence (GAI) model, a first image based on the request, analyzing the first image to identify a presentation of the product in the first image, and selecting a product image from a database of product images based on the identification of the product. The method further includes replacing the presentation of the product in the first image with the selected product image to obtain a second image, and causing presentation in the UI of the second image.","['G06T11/00', 'G06F3/0482', 'G06F3/04812', 'G06F3/0484', 'G06F3/04845', 'G06F40/103', 'G06F40/186', 'G06F40/216', 'G06F40/30', 'G06F40/44', 'G06F40/56', 'G06N3/0475', 'G06N5/022', 'G06T5/005', 'G06T5/50', 'G06T5/77', 'G06T7/11', 'G06T7/194', 'G06V20/62', 'G06T2200/24', 'G06T2207/20084']"
US20230245654A1,Systems and Methods for Implementing Smart Assistant Systems,"In one embodiment, a system includes an automatic speech recognition (ASR) module, a natural-language understanding (NLU) module, a dialog manager, one or more agents, an arbitrator, a delivery system, one or more processors, and a non-transitory memory coupled to the processors comprising instructions executable by the processors, the processors operable when executing the instructions to receive a user input, process the user input using the ASR module, the NLU module, the dialog manager, one or more of the agents, the arbitrator, and the delivery system, and provide a response to the user input.","['G06N5/027', 'G10L15/063', 'G10L15/1815', 'G10L15/1822', 'G10L15/197', 'G10L15/22', 'H04L63/0428', 'G06N3/0442', 'G06N3/0455', 'G06N3/0895', 'G06N3/096', 'G06N3/098', 'G06N5/04', 'G10L15/183', 'G10L15/30', 'G10L2015/086', 'G10L2015/223', 'G10L25/87']"
US12230049B2,Multi-segment text search using machine learning model for text similarity,Systems and methods may be provided for performing a search on an input text block. The input text block may be split into a plurality of input text segments. A text similarity algorithm may be used to find similar stored text segments to each of the plurality of input text segments.,"['G06V30/414', 'G06F40/226', 'G06V30/413', 'G06V30/418', 'G06F16/93', 'G06F40/194', 'G06F40/30']"
CN113240056B,Multi-mode data joint learning model training method and device,"The disclosure discloses a multi-modal data joint learning model training method and device, and relates to the technical field of computers. The specific implementation scheme is as follows: by acquiring multimodal data; wherein the multi-modal data comprises at least one single-modal data and at least one Pair Pair multi-modal data; the method comprises the steps of inputting single-mode data and Pair multi-mode data into a decoupling attention transformation Transformer network model, respectively generating semantic element Token semantic representation characteristics and cross-mode semantic representation characteristics, and training the decoupling attention transformation Transformer network model, so that the decoupling attention transformation Transformer network model effectively utilizes multiple different-mode data to mutually enhance, more robust and stronger general semantic representation capability is obtained from the multi-mode data, the unified network structure model can be used for processing the data of different modes, understanding and generating tasks of the data of different modes are executed simultaneously, and the method has more accurate scene cognition and more comprehensive logical reasoning capability.","['G06V10/778', 'G06F40/30', 'G06F18/214', 'G06F16/45', 'G06F18/217', 'G06F18/24143', 'G06F18/256', 'G06F40/284', 'G06N3/045', 'G06V10/26', 'G06V10/774']"
US12412637B2,Embedding-based generative model for protein design,"A system and method for designing protein sequences conditioned on a specific target fold. The system is a transformer-based generative framework for modeling a complex sequence-structure relationship. To mitigate the heterogeneity between the sequence domain and the fold domain, a Fold-to-Sequence model jointly learns a sequence embedding using a transformer and a fold embedding from the density of secondary structural elements in 3D voxels. The joint sequence-fold representation through novel intra-domain and cross-domain losses with an intra-domain loss forces two semantically similar (where the proteins should have the same fold(s)) samples from the same domain to be close to each other in a latent space, while a cross-domain loss forces two semantically similar samples in different domains to be closer. In an embodiment, the Fold-to-Sequence model performs design tasks that include low resolution structures, structures with a region of missing residues, and NMR structural ensembles.","['G16B15/20', 'G06F30/10', 'G06F30/27', 'G06T19/20', 'G16B15/10', 'G16B40/00', 'G06T2219/2016', 'G16B35/00', 'G16H50/50', 'G16H50/70', 'G16H70/60']"
US20240428017A1,"Modular reasoning, knowledge, and language systems","The presently disclosed embodiments may include a computer readable medium including instructions that when executed by one or more processing devices cause the one or more processing devices to perform a method. The method may include receiving a user input, wherein the user input includes a natural language question, using a trained language model to decompose the natural language question into two or more information requests, routing each of the two or more information requests to at least one information resource, receiving two or more information responses from the at least one information resource, wherein the two or more information responses correspond to the two or more information requests, generating a natural language response to the user input based on the two or more information responses and providing the natural language response to the user.","['G06F16/243', 'G06F40/40', 'G06F16/24522', 'G06F16/248', 'G06F40/103']"
US11836438B2,ML using n-gram induced input representation,"Generally discussed herein are devices, systems, and methods for generating an embedding that is both local string dependent and global string dependent. The generated embedding can improve machine learning (ML) model performance. A method can include converting a string of words to a series of tokens, generating a local string-dependent embedding of each token of the series of tokens, generating a global string-dependent embedding of each token of the series of tokens, combining the local string dependent embedding the global string dependent embedding to generate an n-gram induced embedding of each token of the series of tokens, obtaining a masked language model (MLM) previously trained to generate a masked word prediction, and executing the MLM based on the n-based induced embedding of each token to generate the masked word prediction.","['G06F40/216', 'G06F40/126', 'G06F40/151', 'G06F40/284', 'G06F40/289', 'G06F40/30', 'G06N3/045', 'G06N3/08', 'G06N3/084']"
CN112686058B,"BERT embedded speech translation model training method and system, and speech translation method and equipment","The invention belongs to the technical field of voice translation, and relates to a training method and a system of a BERT embedded voice translation model, and a voice translation method and a voice translation device, wherein the training method comprises the following steps: collecting model training data; pre-training a BERT model by using a source language in training data, taking the pre-trained BERT model as a machine translation model coding layer, training the machine translation model by using paired source language and target language texts, and acquiring a plurality of machine translation models by setting the number of decoding layer layers in the machine translation model; training a speech recognition model by using paired speech translation data of a source language; and taking the trained speech recognition model coding layer as a speech translation model coding layer initialization parameter, weighting the outputs of a plurality of machine translation models in an entropy weighting mode to train the speech translation model, and finishing the training of the speech translation model by combining a model loss function. The invention improves the recognition performance of the voice translation model, and further improves the voice translation efficiency and quality.",[]
WO2022116716A1,"Cloud robot system, cloud server, robot control module, and robot","A cloud robot system, a cloud server, a robot control module, and a robot. The cloud robot system comprises the cloud server and the robot control module; the cloud server comprises a robot access and data exchange module, a knowledge and data smart module, an artificial enhancement machine smart module, a digital twin running core module, and a robot big data module; the robot control module is located in an entity robot; the robot control module communicates with the cloud server by means of a private network.",['G06N3/00']
US10698900B2,Generating a distributed execution model with untrusted commands,"Systems and methods are disclosed for generating a distributed execution model with untrusted commands. The system can receive a query, and process the query to identify the untrusted commands. The system can use data associated with the untrusted command to identify one or more files associated with the untrusted command. Based on the files, the system can generate a data structure and include one or more identifiers associated with the data structure in the distributed execution model. The system can distribute the distributed execution model to one or more nodes in a distributed computing environment for execution.","['G06F16/24553', 'G06F16/90335', 'G06F16/13', 'G06F16/2379', 'G06F16/2433', 'G06F16/901', 'H04L63/10', 'H04W12/10']"
US20220383126A1,Low-Rank Adaptation of Neural Network Models,A computer implemented method obtains neural network-based model base model weight matrices for each of multiple neural network layers. First low-rank factorization matrices are added to corresponding base model weight matrices to form a first domain model. The low-rank factorization matrices are treated as trainable parameters. The first domain model is trained with first domain specific training data without modifying base model weight matrices.,"['G06N3/084', 'G06N3/082', 'G06N3/04', 'G06N3/10', 'G06N5/04']"
US20240013504A1,Techniques for weakly supervised referring image segmentation,"One embodiment of a method for training a machine learning model includes receiving a training data set that includes at least one image, text referring to at least one object included in the at least one image, and at least one bounding box annotation associated with the at least one object, and performing, based on the training data set, one or more operations to generate a trained machine learning model to segment images based on text, where the one or more operations to generate the trained machine learning model include minimizing a loss function that comprises at least one of a multiple instance learning loss term or an energy loss term","['G06F40/30', 'G06V10/26', 'G06F40/284', 'G06V10/7715', 'G06V10/774', 'G06V10/80', 'G06V10/87', 'G06V20/70', 'G06F40/169', 'G06F40/216', 'G06V10/25']"
EP3916579A1,"Method for resource sorting, method for training sorting model and corresponding apparatuses","A method for resource sorting, a method for training a sorting model and corresponding apparatuses which relate to the technical field of natural language processing under artificial intelligence are disclosed. The method according to some embodiments includes: forming an input sequence in order with an item to be matched and information of candidate resources; performing Embedding processing on each Token in the input sequence, the Embedding processing including: word Embedding, position Embedding and statement Embedding; and inputting result of the Embedding processing in a sorting model to obtain sorting scores of the sorting model for the candidate resources, the sorting model is obtained by pre-training of a Transformer model.","['G06F16/3331', 'G06F16/9532', 'G06F16/2379', 'G06F16/38', 'G06F16/9538', 'G06F18/214', 'G06F18/22', 'G06F40/284', 'G06F40/30', 'G06F7/08', 'G06N20/00']"
US20240346295A1,Multi-task machine learning architectures and training procedures,"This document relates to architectures and training procedures for multi-task machine learning models, such as neural networks. One example method involves providing a multi-task machine learning model having one or more shared layers and two or more task-specific layers. The method can also involve performing a pretraining stage on the one or more shared layers using one or more unsupervised prediction tasks. The method can also involve performing a tuning stage on the one or more shared layers and the two or more task-specific layers using respective task-specific objectives","['G06F40/20', 'G06F40/216', 'G06N3/045', 'G06N3/047', 'G06N3/084', 'G06N3/088', 'G06N3/048', 'G06N3/082']"
EP3611619A1,Multi-cloud virtual computing environment provisioning using a high-level topology description,"In one example, a method may include obtaining, by a computing device, a high-level topology description for a virtual computing environment to be provisioned in a plurality of computing infrastructures. Each of the computing infrastructures may be implemented using a different computing architecture and deployed by a different provider. The example method may further include transforming, by a rules engine executing on the computing device, the high-level topology description to respective templates for the computing infrastructures that each describes a topology for a virtual computing environment in a format that conforms to a schema that can be processed by a corresponding one of the computing infrastructures to implement the virtual computing environment in the corresponding one of the computing infrastructures, and outputting the templates for configuring the computing infrastructures.","['G06F9/5077', 'H04L12/4641', 'G06F8/60', 'G06F9/45558', 'G06F9/5072', 'H04L41/0843', 'H04L41/12', 'H04L41/5054', 'G06F2009/45562', 'G06F2009/45595']"
US11631338B2,Deep knowledge tracing with transformers,"Digital learning or tutoring systems as described herein embed, by a trained machine learning knowledge tracing engine, an array for learner interactions X into a static representation ej corresponding to a prior learner interaction xj and determine a contextualized interaction representation hj based on this. Digital tutoring systems described herein calculate, by a masked attention layer of the trained machine learning knowledge tracing engine, an attention weight Aij based on a time gap between two learner interactions with the system, and can calculate a contextualized interaction representation hj, wherein the contextualized interaction representation hj is proportional to the attention weight Aij. The systems can provide for display at the GUI a second question item based on the contextualized interaction representation hj, the second question item corresponding to a recommended learner recommendation.","['G09B7/02', 'G06F16/3329', 'G06F17/16', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G09B7/04', 'G06N3/048', 'G06N3/084']"
US12013850B2,Method and system for advanced data conversations,"A method for querying and analyzing datasets via natural language processing (NLP) that can maintain context is disclosed. According to one embodiment, a computer-implemented method includes receiving, by a user interface, at least one of an utterance or a structured query language statement. The method includes identifying zero or more previous data conversation steps indicated by the utterance. The method includes determining, based on the utterance and the zero or more previous data conversation steps, an effective schema targeted by the utterance. The method includes generating, based on the utterance and the effective schema, an intermediate structured query language statement that is representative of the utterance. The method includes generating an executable structured query language statement based on the intermediate structured query language statement. The method includes executing the executable structured query language statement for the data query engine schema. The method includes communicating a result set and metadata.","['G06F16/213', 'G06F16/243', 'G06F16/24522', 'G06F16/26', 'G06F16/288', 'G06F40/284', 'G06F40/30']"
US11663444B2,Pipelined neural network processing with continuous and asynchronous updates,"Systems and methods for pipelined neural network processing with continuous and asynchronous updates are described. A method for processing a neural network comprising L layers, where L is an integer greater than two, includes partitioning the L layers among a set of computing resources configured to process forward passes and backward passes associated with each of the L layers. The method further includes initiating processing of the forward passes and the backward passes using the set of computing resources. The method further includes upon completion of a first set of forward passes and a first set of backward passes associated with a first layer of the L layers, initiating update of parameters associated with the first layer when gradients are available for updating the parameters associated with the first layer without waiting to calculate gradients associated with any of remaining L layers.","['G06N3/04', 'G06F40/20', 'G06F9/5038', 'G06N3/045', 'G06N3/084', 'G06F9/5061', 'G06N3/063']"
US10120919B2,System and method for multi-modal audio mining of telephone conversations,"A system and method for the automated monitoring of inmate telephone calls as well as multi-modal search, retrieval and playback capabilities for said calls. A general term for such capabilities is multi-modal audio mining. The invention is designed to provide an efficient means for organizations such as correctional facilities to identify and monitor the contents of telephone conversations and to provide evidence of possible inappropriate conduct and/or criminal activity of inmates by analyzing monitored telephone conversations for events, including, but not limited to, the addition of third parties, the discussion of particular topics, and the mention of certain entities.","['G06F16/254', 'G06F17/30563', 'G06F16/283', 'G06F16/685', 'G06F16/686', 'G06F17/30592', 'G06F17/30746', 'G06F17/30752', 'G10L15/26', 'H04M3/2281', 'H04M3/42221', 'H04M2201/40']"
US11580957B1,"Method for training speech recognition model, method and system for speech recognition","Disclosed are a method for training speech recognition model, a method and a system for speech recognition. The disclosure relates to field of speech recognition and includes: inputting an audio training sample into the acoustic encoder to represent acoustic features of the audio training sample in an encoded way and determine an acoustic encoded state vector; inputting a preset vocabulary into the language predictor to determine text prediction vector; inputting the text prediction vector into the text mapping layer to obtain a text output probability distribution; calculating a first loss function according to a target text sequence corresponding to the audio training sample and the text output probability distribution; inputting the text prediction vector and the acoustic encoded state vector into the joint network to calculate a second loss function, and performing iterative optimization according to the first loss function and the second loss function.","['G10L15/063', 'G06F40/284', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G10L15/02', 'G10L15/197', 'G10L15/22', 'G10L15/26', 'G10L19/16', 'G10L25/03', 'G10L25/24', 'G10L15/16']"
CN114464247B,Binding affinity prediction method and device based on antigen and antibody sequences,"The application discloses a binding affinity prediction method and device based on antigen and antibody sequences. Wherein the method comprises the following steps: acquiring a first amino acid sequence of a target antigen and a second amino acid sequence of a target antibody, determining initial characteristics of M dimensions according to the first amino acid sequence of the target antigen and the second amino acid sequence of the target antibody, inputting the initial characteristics of M dimensions into a target characteristic fusion model to obtain M fusion characteristics, and inputting the M fusion characteristics into a target prediction neural network model to obtain predicted binding affinity parameters. The application can be at least applied to the field of intelligent medical treatment, and solves the technical problems of complex prediction mode and low prediction efficiency of the binding affinity of antigen and antibody in the related technology.","['G16B5/00', 'G06F18/213', 'G06F18/24', 'G06F18/253', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G16B30/00', 'G16B40/00', 'Y02A90/10']"
CN111563166B,Pre-training model method for classifying mathematical problems,"The invention discloses a pre-training model method aiming at mathematical problem classification, which comprises the following steps: constructing a knowledge graph of the mathematical knowledge point relationship, and generating a knowledge point vector for each knowledge point in the knowledge graph according to the relationship between each knowledge point; generating text vectors according to mathematical problems in a training set and a verification set respectively, importing the text vectors and knowledge point vectors and constructing a text pre-training model, wherein the text pre-training model comprises semantic mask language model training, related problem prediction model training and problem correlation sequencing training; and importing the test set into a pre-training model, and predicting the processed mathematical questions and outputting the result. The knowledge graph is integrated, and a novel masking and predicting strategy is provided to enhance knowledge point representation, so that the predicting effect is more accurate; the model uses a knowledge embedding algorithm to encode the graph structure of the knowledge graph, and embeds multiple information as the input of the model, so that the accuracy of pre-training is greatly improved.","['G06F16/355', 'G06F16/367', 'G06Q10/04', 'G06Q50/205']"
US20220012296A1,Systems and methods to automatically categorize social media posts and recommend social media posts,"Systems and methods are described to generate for presentation recommended social media posts for a user. The recommended social media posts may be generated based on one or more identified content categories of a first social media post by a first user and one or more parsed social media posts by one or more other users, where the one or more social media posts are associated with the first social media post. In response to determining that selection of the recommended social media post has been received, a second social media post associated with the first social media post may be generated, where the second social media post corresponds to the recommended social media.","['G06F40/58', 'G06F16/90332', 'G06F16/9536', 'G06F16/9538', 'G06F40/30', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'H04L51/52', 'G06N3/088', 'G06N5/02', 'G06N7/01']"
US11600067B2,Action recognition with high-order interaction through spatial-temporal object tracking,"Aspects of the present disclosure describe systems, methods, and structures that provide action recognition with high-order interaction with spatio-temporal object tracking. Image and object features are organized into into tracks, which advantageously facilitates many possible learnable embeddings and intra/inter-track interaction(s). Operationally, our systems, method, and structures according to the present disclosure employ an efficient high-order interaction model to learn embeddings and intra/inter object track interaction across the space and time for AR. Each frame is detected by an object detector to locate visual objects. Those objects are linked through time to form object tracks. The object tracks are then organized and combined with the embeddings as the input to our model. The model is trained to generate representative embeddings and discriminative video features through high-order interaction which is formulated as an efficient matrix operation without iterative processing delay.","['G06V40/20', 'G06V10/454', 'G06V10/82', 'G06V20/41', 'G06V20/46', 'G06V10/62']"
CN111339305B,"Text classification method and device, electronic equipment and storage medium","The embodiment of the invention discloses a text classification method, a text classification device, electronic equipment and a storage medium, wherein the text classification method comprises the following steps: performing data preprocessing on a first text to be processed to obtain a second text for text emotion classification; performing text coding on the second text according to a set coding form to obtain a corresponding vector code of the second text; inputting the vector code of the second text into a pre-trained text sentiment classification model to obtain a corresponding sentiment category and a probability value of the sentiment category; the text emotion classification model is obtained by training a pre-training model by using a set loss function FL. Therefore, the embodiment of the invention improves the judgment capability of the emotional tendency fuzzy text and improves the accuracy of text classification.","['G06F16/35', 'Y02D10/00']"
US11487797B2,Iterative application of a machine learning-based information extraction model to documents having unstructured text data,"An apparatus comprises a processing device configured to receive a query to extract information from a document, and to perform two or more iterations of utilizing a machine learning-based information extraction model to extract portions of unstructured text data from the document. In each iteration, a portion of the unstructured text data extracted from the document and an associated relevance score are output. In a first iteration, the query and document are input while in subsequent iterations the query and modified versions of the document are input, the modified versions having previously-extracted portions of the unstructured text data removed therefrom. The processing device is also configured to generate a response to the query comprising a subset of the portions of the unstructured text data extracted from the document determined to have associated relevance scores exceeding a threshold relevance score and at least a threshold level of similarity to the query.","['G06F16/3347', 'G06F16/334', 'G06F16/3349', 'G06F16/34', 'G06N20/00']"
CN119067171A,"A method, system and medium for fine-tuning training of large language model parameters","The invention discloses a fine tuning training method, a system and a medium for large language model parameters, which relate to the technical field of natural language processing and comprise the steps of setting adjustment data, dividing the adjustment data, calculating the accuracy of a model, outputting initial accuracy, freezing partial parameters of the initial language model, performing model training, outputting a fine tuning language model, calculating the accuracy of the model, outputting the adjustment accuracy, analyzing the adjustment accuracy and the initial accuracy, and re-training or outputting adjustment completion information.","['G06N3/0455', 'G06N3/045']"
WO2024164616A1,"Visual question answering method and apparatus, electronic device and storage medium","The present application relates to the technical field of visual question answering. Disclosed are a visual question answering method and apparatus, an electronic device and a storage medium. The method comprises: rewriting description texts in training samples to generate positive samples and negative samples; using an object attribute detection model and a BERT model to extract multi-modal image-text modal information of the positive samples and the negative samples, the object attribute detection model being a two-stage Deformable DETR model constructed on the basis of a multi-scale deformable attention module; retrieving similar images of images in the training samples, and extracting multi-modal image feature information of the similar images as image modal information; retrieving similar texts of the description texts in the training samples, and extracting text feature information of the similar texts as text modal information; and on the basis of the image-text modal information of the positive samples and the negative samples, the image modal information and the text modal information, training a visual question answering model so as to execute a visual question answering task. The performance of the visual question answering model is improved.","['G06N3/045', 'G06N3/08', 'G06V10/82', 'G06V20/62', 'G06V30/186', 'G06V30/19', 'G06V30/41', 'Y02P90/30']"
US10223337B2,"Markup language system, method, and computer program product","A system, method, and computer program product are provided for use in connection with at least one computer-readable Extensible Markup Language (XML)-compliant data document capable of including: a plurality offline items with a plurality of data values, and a plurality of computer-readable semantic tags that describe a semantic meaning of the data values.","['G06F40/143', 'G06F17/2247', 'G06F16/93', 'G06F16/94', 'G06F16/9558', 'G06F17/218', 'G06F17/30011', 'G06F17/30014', 'G06F17/30882', 'G06F3/0482', 'G06F3/04842', 'G06F40/117']"
US9268748B2,"System, method, and computer program product for outputting markup language documents","A system, method, and computer program product are provided for use in connection with at least one computer-readable Extensible Markup Language (XML)-compliant data document capable of including: a plurality of line items with a plurality of data values, and a plurality of computer-readable semantic tags that describe a semantic meaning of the data values.","['G06F16/986', 'G06F17/218', 'G06F16/9038', 'G06F17/212', 'G06F17/215', 'G06F17/2235', 'G06F17/2247', 'G06F17/2252', 'G06F17/2264', 'G06F17/243', 'G06F17/246', 'G06F17/30896', 'G06F17/30991', 'G06F3/0482', 'G06F3/04842', 'G06F40/106', 'G06F40/111', 'G06F40/117', 'G06F40/134', 'G06F40/143', 'G06F40/146', 'G06F40/151', 'G06F40/174', 'G06F40/18', 'Y10S707/99953', 'Y10S707/99956']"
US11587551B2,Leveraging unpaired text data for training end-to-end spoken language understanding systems,"An illustrative embodiment includes a method for training an end-to-end (E2E) spoken language understanding (SLU) system. The method includes receiving a training corpus comprising a set of text classified using one or more sets of semantic labels but unpaired with speech and using the set of unpaired text to train the E2E SLU system to classify speech using at least one of the one or more sets of semantic labels. The method may include training a text-to-intent model using the set of unpaired text; and training a speech-to-intent model using the text-to-intent model. Alternatively or additionally, the method may include using a text-to-speech (TTS) system to generate synthetic speech from the unpaired text; and training the E2E SLU system using the synthetic speech.","['G10L15/063', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G10L15/005', 'G10L15/16', 'G10L15/1822', 'G10L15/26', 'G10L13/00']"
CN110750959B,"Text information processing method, model training method and related device","The embodiment of the application provides a text information processing method, a model training method and a related device. The text sequence corresponding to the text information to be labeled is calculated through the sequence labeling model, if a target element exists in the text sequence, it is stated that a wrong word exists in the text information to be labeled, the wrong word can also be called as a first word, then a second word corresponding to the target element is determined, and finally the first word in the text information to be labeled is replaced by the second word, so that the replacement of the wrong word can be realized. Moreover, the second word needs to be determined according to the associated word bank and the word to be replaced, so that the replacement effect of the second word can be ensured, the first word cannot be replaced by other words which are not associated, and the situation that the sentence is not communicated is avoided.",['G06F18/214']
US10810996B2,System and method for performing automatic speech recognition system parameter adjustment via machine learning,"A system, method and computer-readable storage device provides an improved speech processing approach in which hyper parameters used for speech recognition are modified dynamically or in batch mode rather than fixed statically. The method includes estimating, via a model trained on audio data and/or metadata, a set of parameters useful for performing automatic speech recognition, receiving speech at an automatic speech recognition system, applying, by the automatic speech recognition system, the set of parameters to processing the speech to yield text and outputting the text from the automatic speech recognition system.","['G10L15/32', 'G10L15/063', 'G06F16/68', 'G06N20/00', 'G06N3/082', 'G06N5/01', 'G10L15/16', 'G10L15/26', 'G10L25/03', 'G06N20/10', 'G06N3/044', 'G06N3/045', 'G10L15/20', 'G10L25/30']"
US20240211311A1,System and method for a decentralized financial simulation and decision platform,"A system and method for a decentralized financial simulation and decision platform has a model definition language service configured to create a first dataset comprising at least a user-defined set of computing instructions comprising at least instructions regarding data flow locality, a parametric evaluator configured to retrieve the first dataset, and process the first dataset by performing at least a plurality of transformations and predictive analysis on the first dataset and specifying at least an intended focus on financial trading, and an optimizer configured to retrieve the processed first dataset from the parametric evaluator and determine an optimal locality for executing a trade.","['G06F9/5011', 'G06F16/9024', 'G06F18/29', 'G06N5/022', 'G06N5/025', 'G06Q10/067', 'G06Q30/0201', 'G06Q30/0205', 'G06Q40/04']"
CN117522372B,Deep learning-based maintenance suggestion generation method and system for automobile fault model,"The invention provides a maintenance suggestion generation method and a system of an automobile fault model based on deep learning, wherein the method comprises the following steps: acquiring automobile fault description data, a maintenance manual and an expert database, preprocessing to obtain dialogue data for asking and answering automobile faults, and formatting the dialogue data to obtain formatted dialogue data; the automobile fault description, the maintenance manual and the expert database are used as nodes, and the nodes are connected through relation mapping to form a knowledge graph; performing multitasking fine tuning training in the vertical field; acquiring an automobile fault description input by a user, inquiring a knowledge graph to acquire expert knowledge as a prompt of the automobile fault description, positioning the automobile fault, generating a maintenance suggestion, and presenting a fault positioning result and the maintenance suggestion; the system comprises: the system comprises a data acquisition processing module, a knowledge graph forming module, a model fine adjustment training module and a fault result output module; and automobile fault location and maintenance suggestion generation are realized.","['G06Q10/20', 'G06F16/33', 'G06F16/3329', 'G06F16/367', 'G06N20/00', 'G06N5/02']"
US11763100B2,System and method for controllable machine text generation architecture,"A system is provided comprising a processor and a memory storing instructions which configure the processor to process an original sentence structure through an encoder neural network to decompose the original sentence structure into an original semantics component and an original syntax component, process the original syntax component through a syntax variation autoencoder (VAE) to receive a syntax mean vector and a syntax covariance matrix, obtain a sampled syntax value from a syntax Gaussian posterior parameterized by the syntax mean vector and the syntax covariance matrix, process the original semantics component through a semantics VAE to receive a semantics mean vector and a semantics covariance matrix, obtain a sampled semantics vector from the Gaussian semantics posterior parameterized by the semantics mean vector and the semantics covariance matrix, and process the sampled syntax vector and the sampled semantics vector through a decoder neural network to compose a new sentence.","['G06F40/56', 'G06F40/30', 'G06F40/211', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/088', 'G06N3/08']"
US12354024B2,Intelligent question answering on tabular content,"An intelligent table-based question answering system receives an input user query requesting information from an input table to generate a response to the input query. Two types of user queries including textual user queries and scalar user queries that require at least one mathematical operation to be executed can be processed for answer generation. The input table is processed to generate a paragraph of table text which includes row and column information extracted from the input table. The input query is provided along with the paragraph of table text to a deep Quans model which outputs a candidate answer that forms a textual portion of the response. Also, the candidate answer is reverse mapped to the input table and a portion of the input table including the candidate answer is provided as a tabular portion of the response.","['G06N5/04', 'G06F16/334', 'G06N20/00', 'G06N3/045', 'G06N3/08']"
US11468317B2,Learning radio signals using radio signal transformers,"Methods, systems, and apparatus, including computer programs encoded on a storage medium, for processing radio signals. In once aspect, a system is disclosed that includes a processor and a storage device storing computer code that includes operations. The operations may include obtaining first output data generated by a first neural network based on the first neural network processing a received radio signal, receiving, by a signal transformer, a second set of input data that includes (i) the received radio signal and (ii) the first output data, generating, by the signal transformer, data representing a transformed radio signal by applying one or more transforms to the received radio signal, providing the data representing the transformed radio signal to a second neural network, obtaining second output data generated by the second neural network, and determining based on the second output data a set of information describing the received radio signal.","['H04B1/0003', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'H04B17/309']"
US20190251431A1,Multitask Learning As Question Answering,"Approaches for multitask learning as question answering include a method for training that includes receiving a plurality of training samples including training samples from a plurality of task types, presenting the training samples to a neural model to generate an answer, determining an error between the generated answer and the natural language ground truth answer for each training sample presented, and adjusting parameters of the neural model based on the error. Each of the training samples includes a natural language context, question, and ground truth answer. An order in which the training samples are presented to the neural model includes initially selecting the training samples according to a first training strategy and switching to selecting the training samples according to a second training strategy. In some embodiments the first training strategy is a sequential training strategy and the second training strategy is a joint training strategy.","['G06F40/30', 'G06F16/243', 'G06F16/3329', 'G06F16/3334', 'G06F16/3344', 'G06F17/2881', 'G06F40/56', 'G06N20/20', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N5/04', 'G06N20/00', 'G06N20/10', 'G10L15/16', 'G10L15/1822']"
US11954461B2,Autonomously delivering software features,"A system for autonomously delivering software features is disclosed. The system parses data obtained from a variety of sources, and extracts source concepts from the parsed data to generate models for inclusion in a set of agglomerated models. Over time, additional data from the variety of sources may be utilized to update the set of agglomerated models. The updated agglomerated models may be analyzed by the system to determine whether new features and/or functionality may be added to an application under evaluation by the system. In the event that new features and/or functionality may be added to the application under evaluation, the system may automatically generate code corresponding to the new features and/or functionality and incorporate that features and/or functionality into the application under evaluation.","['G06F8/35', 'G06F11/3447', 'G06F11/3428', 'G06F11/3608', 'G06F11/3688', 'G06F11/3692', 'G06F8/10', 'G06F8/41', 'G06F8/60', 'G06N20/00', 'G06N5/04', 'G06N3/006', 'G06N3/08', 'G06N7/01']"
CN118116353A,Music and dance MV generation system and method based on Transformer autoregressive model,"The invention relates to the field of art, and particularly discloses a music dance MV generation system based on a transducer autoregressive model, which comprises a song processor, a data set, an autoregressive model network based on MTTIM, an autoregressive model network based on a transducer, a multi-mode transducer network and a network identifier; the song processor separates the song into lyrics and song melodies; the data sets include training data sets, validation data sets, and test data sets; generating a scene image matched with lyrics based on MTTIM autoregressive model network; generating dance motion characteristics corresponding to the song melody based on the auto-regression model network of the transducer; the multimode transducer network is used for fusing the generated scene image and dance motion characteristics; the network discriminator judges whether the synthesized image is consistent with the real image or not by utilizing multi-classification cross loss entropy under the same resolution scale.","['G10H7/002', 'G10L25/03', 'G10L25/27', 'G10L25/48', 'Y02T10/40']"
CN113220836B,"Training method and device for sequence annotation model, electronic equipment and storage medium","The application discloses a training method, a training device, electronic equipment and a storage medium of a sequence annotation model, relates to the technical field of computers, and in particular relates to the technical field of artificial intelligence such as knowledge graph, natural language processing and deep learning. The specific implementation scheme is as follows: acquiring a sample text and a sequence labeling model to be trained; segmenting the sample text to obtain a plurality of text words, wherein the text words comprise entity words and non-entity words; labeling a plurality of text words respectively to generate training samples; and training the sequence annotation model according to the training sample. Therefore, the accuracy and the diversity of the sequence labeling model can be improved.","['G06F16/3344', 'G06F16/35', 'G06F16/367', 'G06F40/284', 'G06F40/295']"
US11646017B1,Efficient memory transformer based acoustic model for low latency streaming speech recognition,"In one embodiment, a method includes accessing a machine-learning model configured to generate an encoding for an utterance by using a module to process data associated with each segment of the utterance in a series of iterations, performing operations associated with an i-th segment during an n-th iteration by the module, which include receiving an input comprising input contextual embeddings generated for the i-th segment in a preceding iteration and a memory bank storing memory vectors generated in the preceding iteration for segments preceding the i-th segment, generating attention outputs and a memory vector based on keys, values, and queries generated using the input, and generating output contextual embeddings for the i-th segment based on the attention outputs, providing the memory vector to the module for performing operations associated with the i-th segment in a next iteration, and performing speech recognition by decoding the encoding of the utterance.","['G10L15/183', 'G06N3/044', 'G06N3/0445', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G10L15/16', 'G10L15/22', 'G10L15/28']"
CN114064967B,Cross-modal timing behavior localization method and device for multi-granularity cascade interaction network,"The invention discloses a cross-modal time sequence behavior positioning method and device of a multi-granularity cascade interactive network, which are used for solving the problem of time sequence behavior positioning based on given text query in an untrimmed video. The invention implements a novel multi-granularity cascade cross-modal interaction network, and cascade cross-modal interaction is carried out in a coarse-to-fine mode so as to improve the cross-modal alignment capability of the model. In addition, the invention introduces a local-global context-aware video encoder (local-global context-aware video encoder) for improving the context timing dependency modeling capability of the video encoder. The method is simple, flexible in means and superior in the aspect of improving the vision-language cross-modal alignment precision, and the timing sequence positioning accuracy of the trained model can be remarkably improved on paired video-query test data.","['G06F16/735', 'G06F16/7844', 'G06F16/7867', 'G06N3/044', 'G06N3/08', 'H04N19/149', 'H04N19/21']"
CN117493504A,A medical event extraction method based on generative pre-trained language model,"The invention discloses a medical event extraction method and a device based on a generated pre-training language model, wherein the method comprises the following steps: acquiring a target medical text; giving all medical event types of medical events to be extracted; defining corresponding elements to be extracted for each type of medical event; judging event types involved in the target medical text by using an event type judging model, wherein the event type judging model is a generated pre-training language model subjected to field adaptation and event type judging task fine tuning; and according to the event type, extracting an element corresponding to the event of the target medical text by using an event extraction model, wherein the event extraction model is a generated pre-training language model subjected to field adaptation and event element extraction task fine tuning.","['G06F16/3329', 'G06F16/35', 'G06F40/186', 'G06F40/289', 'G06N3/045', 'G06N3/0499', 'G06N3/082', 'G16H10/60', 'G16H50/70', 'Y02A90/10']"
US11477151B2,Purpose detection in communications using machine learning,"Generally discussed herein are devices, systems, and methods for identifying a purpose of a communication. A method can include receiving a communication including communication content and communication context, the communication content a first portion of the communication and the communication context a second, different portion of the communication. The method can include identifying, by a machine learning (ML) model, based on the communication content and the communication context, one or more purposes associated with the communication, the one or more purposes indicating respective actions to be performed by a user that generated or received the communication. The method can include providing data indicating the purpose of the first portion of the content.","['H04L51/18', 'G06Q10/10', 'G06F40/30', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G06Q10/107', 'G06Q10/109']"
US11971955B1,Example-based image annotation,"Techniques are generally described for machine learning exampled-based annotation of image data. In some examples, a first machine learning model may receive a query image comprising a first depiction of an object-of-interest. In some examples, the first machine learning model may receive a target image representing a scene in which a second depiction of the object-of-interest is visually represented. In various examples, the first machine learning model may generate annotated output image data that identifies a location of the second depiction of the object-of-interest within the target image. In some examples, an object detection model may be trained based at least in part on the annotated output image data.","['G06F18/2148', 'G06F18/2155', 'G06F18/2163', 'G06F18/2178', 'G06F18/22', 'G06F18/2413', 'G06F18/40', 'G06F3/04842', 'G06F3/04845', 'G06N3/04', 'G06N3/045', 'G06N3/08', 'G06T7/73', 'G06V10/25', 'G06V10/751', 'G06V10/7788', 'G06V10/82', 'G06F3/0482', 'G06T2207/20081', 'G06T2207/20084', 'G06V2201/09']"
WO2023030520A1,"Training method and apparatus of endoscope image classification model, and image classification method","A training method and apparatus of an endoscope image classification model, and an image classification method. The endoscope image classification model comprises: a plurality of expert sub-networks. The method comprises: obtaining a training data set, wherein the training data set comprises a plurality of endoscopic images and labeling tags of the plurality of endoscopic images, and the training data set presents a long-tail distribution; and training the endoscope image classification model on the basis of the training data set till a target loss function of the endoscope image classification model converges to obtain a trained endoscope image classification model, wherein the target loss function is determined at least on the basis of a corresponding plurality of output results of the plurality of expert sub-networks.","['G06F18/214', 'G06F18/2415', 'G06N3/045', 'G06T7/0012', 'G06T2207/10068', 'G06T2207/30101']"
US20230177384A1,Attention Bottlenecks for Multimodal Fusion,Example embodiments according to aspects of the present disclosure provide an example computer-implemented method for multimodal data processing with improved cross-modal attention. The example method includes inputting a multimodal sequence to an example machine-learned model. The example model includes a first modal processing stream receiving a first modal portion of the multimodal sequence and a second modal processing stream receiving a second modal portion of the multimodal sequence. The example model includes fusing the first modal processing stream and the second modal processing stream across one or more fusion layers of the machine-learned model through a plurality of cross-modal context encodings. The example method includes outputting an inference based at least in part on the plurality of cross-modal context encodings.,"['G06N3/045', 'G06N20/00', 'G06N3/084', 'G06N5/04']"
US11010284B1,System for understanding navigational semantics via hypothesis generation and contextual analysis,"A system for understanding navigational semantics via hypothesis generation and contextual analysis is disclosed. The system may, such as when examining and testing a software application, address the handling and resolution of constraint hypotheses in an uncertain environment, where potentially overlapping or conflicting suggestions are generated with various confidences. The system may utilize algorithmic and/or machine learning tools to identify consistent constraints for the software application with the highest levels of confidence. During operation, the system may continuously perform hypothesis testing on constraints generated by the system, which may result in the creation of new hypotheses yielding improved confidences. Feedback from the hypothesis testing may be provided to knowledge sources to improve the processing of information subsequently processed by the system. The system may construct complex constraints on multiple fields or functional transitions with associated confidences. A constraint optimizer of the system may simplify constraints or reduce their quantities.","['G06F11/3684', 'G06F11/3688', 'G06F11/3692', 'G06N5/041', 'G06N20/00', 'G06N5/02']"
CN110222188B,Company notice processing method for multi-task learning and server,"The invention discloses a company bulletin processing method and a server for multi-task learning, wherein historical bulletin data are input into a sharing layer of a multi-task learning model, and are pre-trained through Bert; inputting a data set corresponding to a processing task into a task layer of the multi-task learning model to train the multi-task learning model; acquiring current announcement data, and inputting the current announcement data into a trained multi-task learning model to obtain a task processing result; the invention constructs a multi-task learning model by adopting a mode of transfer learning and multi-task learning, and has the advantages of higher learning efficiency, stronger generalization, low manual maintenance cost, higher accuracy of a plurality of tasks, higher recall rate and convenience for engineering deployment and maintenance.","['G06F16/3344', 'G06F16/35', 'G06F40/30', 'Y02D10/00']"
US10769056B2,System for autonomously testing a computer system,"A system for autonomously testing a computing system is disclosed. The system parses data obtained from a variety of sources, and extracts source concepts from the parsed data to generate models for inclusion in a set of agglomerated models. The system interacts with and explores features of a software application being tested by the system. Outputs and information obtained from the interaction are utilized by the system to update the models or generate new models for the set of agglomerated models. The agglomerated models are utilized by the system to execute tests on the application to detect potential defects and conflicts. Detected defects and conflicts may be included in a report for review. Feedback on the defects and conflicts may be utilized to further update the agglomerated models. The agglomerated models are updated recursively as additional data is obtained, further interactions are performed, and further outputs are generated over time.","['G06F11/3688', 'G06F11/3684', 'G06F11/3692']"
EP3851999A1,Method and apparatus for processing information,"The present disclosure discloses a method and an apparatus for processing information. A specific embodiment of the method includes: acquiring a word sequence obtained by performing word segmentation on two paragraphs in a text; inputting the word sequence into a to-be-trained natural language processing model to generate a word vector corresponding to a word in the word sequence; inputting the word vector into a preset processing layer of the to-be-trained natural language processing model; predicting whether the two paragraphs are adjacent, and a replaced word in the two paragraphs; and acquiring reference information of the two paragraphs, and training the to-be-trained natural language processing model to obtain a trained natural language processing model, based on the prediction result and the reference information. The present disclosure may use a large number of samples without manual labeling to train a natural language processing model, so that the problem of insufficient samples may be solved and the prediction accuracy of the model may be improved, while the manpower consumption is reduced.","['G06F40/30', 'G06F40/40', 'G06F18/22', 'G06F40/284', 'G06F40/289', 'G06N20/00', 'G06N20/20', 'G06N3/045', 'G06N3/048', 'G06N3/08', 'Y02D10/00']"
US12026759B2,Artificial intelligence based service recommendation,"In one aspect, a method includes receiving merchant profile data describing attributes of merchants; generating, using one or more machine-trained models and the merchant profile data, a respective first profile for each merchant of the merchants; generating, using the one or more machine-trained models, a respective second profile for each of the plurality of merchant services using at least one type of description for each of the plurality of merchant services, wherein the respective first profile for each merchant and the respective second profile for each of the plurality of merchant services are numerically comparable; identifying a list of merchant service recommendations for at least one of the merchants using generated first profiles and second profiles; and configuring a merchant computing device of the at least one merchant with at least one merchant service selected by the merchant from the list of merchant service recommendations.","['G06N20/00', 'G06N3/045', 'G06N3/047', 'G06N3/084', 'G06Q10/0637', 'G06Q20/12', 'G06Q30/0201', 'G06Q30/0202', 'G06Q30/0282', 'G06N3/044', 'G06N3/048']"
US20240289824A1,Machine learning based user targeting,"Apparatuses, methods, program products, and systems are disclosed for machine learning based user targeting. An apparatus includes one or more memory devices in communication with one or more processing devices. An apparatus includes a subject module that receives subject data associated with a subject. Subject data includes data that describes a subject. An apparatus includes an affinity module that determines, using a machine learning model, an affinity level of each of one or more entities in relation to a subject. Subject data is provided as input to a machine learning model. An apparatus includes a campaign module that generates one or more promotional campaign strategies for a subject based at least in part on an affinity level of each of one or more entities in relation to a subject. One or more promotional campaign strategies target a subject to one or more entities.","['G06N20/00', 'G06N3/045', 'G06N3/047', 'G06N3/088', 'G06Q30/0201', 'G06Q30/0204', 'G06Q30/0276', 'G06N3/044']"
US20240078337A1,Systems and Methods for Managing Data Security,"Exemplary embodiments for data security include a data access proxy coupled with a database, further coupled with a server configured to operate the data access proxy to: identify a user and request to access a data item; validate the user and request, including inspecting the user's identity, evaluating the user's history, and evaluating permissions and restrictions associated with the user and the data item; access the database to retrieve the data item; inspect security attributes related to the data item; and transform the data item based on one or more privacy rules, including redacting the at least one data item, deleting information from the at least one data item, substituting information from the at least one private data item with other information, adding information to the at least one data item, providing synthetic data as a private data item, or providing proxy data for the data item.","['G06F21/6245', 'G06F16/211']"
CN115222019B,Reservoir parameter prediction method based on deep Transformer transfer learning based on well logging data,"The invention discloses a depth transducer transfer learning reservoir parameter prediction method based on logging data, which comprises the following steps: and carrying out outlier processing on the source domain logging data, and screening out abnormal logging data. And the logging data is subjected to standardized preprocessing, so that the magnitude order and dimension influence among parameters is effectively eliminated, the network error is reduced, the convergence is accelerated, and the model prediction precision is improved. The standardized data is input to a transform migration learning network, and the correlation between the source domain and the target domain is found in the feature space. And finally, designing a loss function of the whole network. And outputting the error of the pre-position value and the core data to the source domain logging data by calculating the distribution difference of the logging characteristic data of the source domain and the target domain and the model. Based on the back propagation of the loss values, the network parameters are derived through a chain rule, and the network parameters are updated by using a random gradient descent algorithm. The method can find the similarity of the source domain and the target domain, and transmit the knowledge of the source domain to the target domain, thereby effectively solving the problem of large reservoir parameter prediction error.","['G01V11/00', 'G06N20/00', 'G06N3/084', 'Y04S10/50']"
US20210397738A1,Filtered data lake for enterprise security,A data lake for enterprise security is created from an asynchronous stream of security events by deduplicating objects and creating metadata related to downstream security functions. Deduplication of objects may be efficiently performed with a bloom filter as objects are ingested into the data lake. The objects may also be augmented with metadata arranged in schemas to facilitate monitoring and use within the data lake.,"['G06F21/6245', 'G06Q10/067', 'G06F16/211', 'G06F16/215', 'G06F16/24568', 'G06F21/567', 'G06F21/577', 'H04L63/1416', 'H04L63/1425', 'H04L63/1433', 'H04L63/1441', 'H04L63/20']"
US12056455B2,Written word refinement system and method for truthful transformation of spoken and written communications,"A method for processing a narrative generated by an artificial Intelligence based natural language generator, to assess relationships between words and phrases in the generated narrative, where necessary, to replace particular words and phrases and more clearly convey a desired intended semantic content of the generated narrative, and/or generate learning data for use by the natural language generator to improve its text generating operation. Based on an input received by the AI based natural language generator, the method generates a narrative, processing the narrative by implementing one or more mechanisms to provide at least one cue in the narrative, in accordance with a plurality of rules to identify semantic content and based on a cue, and the identified semantic content of the narrative, determining how relationships between the words and phrases comprising the narrative could be altered to more clearly convey the semantic content, to realize a directive and communicating the directive.","['G06F40/166', 'G06F40/226', 'G06F40/253', 'G06F40/268', 'G06F40/284', 'G06F40/30', 'G06F40/56', 'G09B19/00', 'G10L13/08', 'G09B5/06']"
CN112041924B,Visual Speech Recognition via Phoneme Prediction,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for performing visual speech recognition. In one aspect, a method includes: receiving a video comprising a plurality of video frames, wherein each video frame depicts a double lip; processing the video using a visual speech recognition neural network to generate a respective output score for each marker in a vocabulary of possible markers for each output position in an output sequence, wherein the visual speech recognition neural network comprises one or more volumetric convolutional neural network layers and one or more time-aggregated neural network layers; wherein the vocabulary of possible tags comprises a plurality of phonemes; and determining a sequence of words expressed by the two lips depicted in the video using the output score.","['G10L15/25', 'G06F18/21', 'G06N3/044', 'G06N3/0442', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/0002', 'G06V20/40', 'G06V40/171', 'G06V40/20', 'G10L15/02', 'G10L15/16', 'G10L15/197', 'G06N3/045', 'G06N7/01', 'G06T2207/10016', 'G06T2207/30168', 'G06T2207/30201', 'G10L2015/025']"
US8739150B2,Systems and methods for dynamically replacing code objects via conditional pattern templates,"The present application is directed towards systems and methods for dynamically creating a substitution template from a conditional pattern template to transform code objects from a source installation to a target installation. Variable tokens in a pattern template may be dynamically replaced with corresponding values from a source object, and conditional statements of the pattern template analyzed and dynamically applied to build a substitution template for creating a replacement object for the source object.","['G06F8/72', 'G06F8/36']"
US12068602B2,Advanced power distribution platform,"A control system for a power distribution grid including an electrical distribution circuit includes a processor configured to perform operations including constructing a grid model comprising edges and nodes representing assets and transmission paths of the power distribution grid, generating an analysis of an operation of the power distribution grid over a predetermined time duration, determining a plurality of constraint violations based on the analysis of the operation of the power distribution grid within the predetermined time duration, generating a plurality of alterations to the power distribution grid, respective ones of the plurality of alterations resolving at least one of the constraint violations, selecting a first alteration of the plurality of alterations to the power distribution grid responsive to determining that the selected first alteration resolves at least two of the plurality of constraint violations, and autonomously implementing the first alteration to the power distribution grid.","['H02J3/0012', 'G05B19/042', 'H02J3/06', 'G05B2219/2639', 'H02J2203/10', 'H02J2203/20', 'H02J2300/22', 'Y02B90/20', 'Y02E60/00', 'Y04S20/00', 'Y04S40/20']"
CN112749752B,A Hyperspectral Image Classification Method Based on Deep Transformer,"The invention belongs to the technical field of remote sensing images, and particularly relates to a hyperspectral image classification method based on depth transform. The method is characterized in that hyperspectral images to be classified are respectively input into a trained depth space-spectrum Transformer model, and then the hyperspectral images can be classified. The depth space-spectrum Transformer model comprises a spectrum dimension attention model, a space dimension attention model and a classifier, wherein the spectrum dimension attention model and the space dimension attention model respectively comprise a corresponding position coding layer and a corresponding Transformer layer, a traditional convolution layer is replaced by a Transformer structure, and attention mechanisms are respectively applied to spectrum and space dimensions to extract space-spectrum characteristics of hyperspectral images so as to better utilize abundant space and spectrum information of the hyperspectral images, meanwhile, the classification model is more flexible and efficient, and classification results are more accurate.","['G06F18/24', 'G06N3/044', 'G06N3/08']"
US20230419074A1,Methods and systems for neural and cognitive processing,"Provided herein is a system for creating, modifying, deploying and running intelligent systems by combining and customizing the function and operation of reusable component modules arranged into neural processing graphs which direct the flow of signals among the modules, analogous in part to biological brain structure and operation as compositions of variations on functional components and subassemblies.","['G06N3/04', 'G06N3/10']"
US11748232B2,System for discovering semantic relationships in computer programs,"A system for discovering semantic relationships in computer programs is disclosed. In particular, the system may synergistically identify and validate semantic relationships, concepts, and groupings associated with data elements within a static or dynamic, time varying, source input. The system may utilize feature extractors to extract features from the input and reasoners to develop associations using data from multiple feature set types, and, can thus generate reliable, robust, and complete sets of semantic relationships from the input. The system may generate hypotheses associated with the relationships, concepts, and groupings, and validate the hypotheses by testing an application under evaluation by the system and observing the outputs generated from the testing. Information pertaining to validated or invalidated hypotheses may be provided to a learning engine to maximize reasoning and performance in subsequent discovery processes by adjusting models, vocabularies, dictionaries, parameters utilized by the system in identifying the relationships, concepts, and groupings.","['G06F40/216', 'G06F11/3608', 'G06F16/288', 'G06F40/30']"
US10747651B1,System for optimizing system resources and runtime during a testing procedure,"A system for optimizing system resources and runtime during a testing procedure is disclosed. During operation, the system may perform hypothesis testing on an application under evaluation, such as performing hypothesis testing on suggested constraints for the application under evaluation, which may result in the creation of new hypotheses yielding improved confidences. This testing may be optimized by simplifying and/or reducing the number of hypothesis tests used during the testing process. To do so, the hypothesis tests may be sorted into sets, and a subset of these sorted hypothesis tests may be executed. Based on the results of this testing, a remaining portion of the hypothesis tests may be skipped, allowing for an optimization of the testing process without reducing the quality of the testing procedure.","['G06F11/3684', 'G06F11/3688', 'G06F11/3692']"
CN114187331B,Unsupervised optical flow estimation method based on Transformer feature pyramid network,"The invention belongs to the technical field of computer vision, and particularly relates to an unsupervised optical flow estimation method based on a Transformer feature pyramid network, which comprises the steps of constructing the Transformer feature pyramid network; enhancing the feature extraction capability of the feature pyramid network on the image through a re-attention mechanism operation by utilizing a transducer model; constructing an optical flow estimation network, so that the network can conduct optical flow prediction; and carrying out shielding compensation processing on pixels of a shielding region, designing a loss function of overall network training, and carrying out unsupervised training on the network to obtain an unsupervised optical flow estimation model with higher speed and higher precision. The invention can enhance the feature extraction capability of the feature pyramid layer on the image, and performs shielding compensation processing on shielding pixels in the image so as to improve the precision of optical flow estimation.","['G06T7/269', 'G06F18/22', 'G06N3/045', 'G06N3/088', 'G06T7/246', 'G06T2207/10016', 'G06T2207/20016', 'Y02T10/40']"
US10613157B2,Smart fault detection device to anticipate impending faults in power transformers,"Certain embodiments may generally relate to a smart fault detection device for power grids, and a method of fault detection for power grids. A method may include receiving raw data samples of currents in grounding conductors and line conductors. The method may also include processing the raw data samples under at least one of a plurality of system operating modes. The method may also include monitoring normal operation and anticipating an impending fault while operating under at least one of the system operating modes. The method may further include extracting fault information based on the monitoring. The method may also include reporting the fault information to a supervisory control and data acquisition system human-machine interface. The method may further include anticipating faults based on an analysis of the raw data samples.","['G01R31/086', 'G01R31/50', 'G01R31/62', 'G01R31/088', 'Y02E40/70', 'Y02E60/00', 'Y02E60/728', 'Y04S10/00', 'Y04S10/22', 'Y04S10/265', 'Y04S10/52', 'Y04S10/522']"
US12056458B2,"Translation method and apparatus based on multimodal machine learning, device, and storage medium","A computer device acquires a semantic association graph associated with n source statements belonging to different modals. The semantic association graph includes n semantic nodes of the different modals, a first connecting edge used for connecting the semantic nodes of a same modal and a second connecting edge used for connecting the semantic nodes of different modals. The computer device extracts a plurality of first word vectors from the semantic association graph. The device encodes the plurality of first word vectors to obtain n encoded feature vectors. The device also decodes the n encoded feature vectors to obtain a translated target statement.","['G06F40/42', 'G06F40/58', 'G06F40/205', 'G06F40/30', 'G06F40/44', 'G06F40/51', 'G06N20/00']"
US11995803B1,Training and deployment of image generation models,"In some embodiments, a method receives a text prompt and executes a text encoder on the text prompt to generate an embedding representation. A set of base images is generated based on the embedding representation and parameters of a base image generation model. A high resolution model is executed to upsample one or more base images in the set of base images based on parameters of the high resolution model to generate a set of final images. The method ranks the set of base images or the set of final images using reward values that are generated by a reward model. The reward model is trained using human input that provided feedback on a quality of generated images using the base image generation model and the high resolution model. One or more final images are output based on the ranking in response to the text prompt.","['G06T5/70', 'G06T11/00', 'G06T2207/20081', 'G06T2207/20084']"
US12380714B2,Methods and apparatus to perform dense prediction using transformer blocks,"Methods, apparatus, systems and articles of manufacture disclosed herein perform dense prediction of an input image using transformers at an encoder stage and at a reassembly stage of an image processing system. A disclosed apparatus includes an encoder with an embedder to convert an input image to a plurality of tokens representing features extracted from the input image. The tokens are embedded with a learnable position embedding. The encoder also includes one or more transformers configured in a sequence of stages to relate the tokens to each other. The apparatus further includes a decoder that includes one or more of reassemblers to assemble the tokens into feature representations, one or more of fusion blocks to combine the feature representations to generate a final feature representation, and an output head to generate a dense prediction based on the final feature representation and based on an output task.","['G06V20/70', 'G06T1/20', 'G06N3/04', 'G06N3/045', 'G06T1/60', 'G06T3/4046', 'G06T5/00', 'G06T7/10', 'G06T9/002', 'G06V10/82', 'G06N3/048', 'G06T2207/20016', 'G06T2207/20021', 'G06T2207/20084', 'G06T2207/20221']"
CN111046221B,"Song recommendation method, device, terminal equipment and storage medium","The embodiment of the application discloses a song recommendation method, a device, terminal equipment and a storage medium, wherein the method comprises the following steps: obtaining a search object content text corresponding to a search record of a target user within a preset time period, and obtaining a content text feature vector corresponding to the search object content text, wherein the content text feature vector comprises a plurality of word vectors corresponding to a plurality of words composing the search object content text; obtaining a song text feature vector corresponding to each song included in a song library, wherein the song text feature vector comprises a plurality of word vectors corresponding to a plurality of words composing song information; and determining the text similarity between the content text of the search object and the song information of each song based on the content text feature vector and the song text feature vector corresponding to each song included in the song library, and determining at least one song from the song library to recommend to the target user according to the text similarity. By adopting the embodiment of the application, the accuracy of recommending songs for new users can be improved.","['G06F16/635', 'G06F16/686']"
US11934441B2,Generative ontology learning and natural language processing with predictive language models,"An ontology topic is selected and a pretrained predictive language model is primed to create a predictive primed model based on one or more ontological rules corresponding to the selected ontology topic. Using the predictive primed model, natural language text is generated based on the ontology topic and guidance of a prediction steering component. The predictive primed model is guided in selecting text that is predicted to be appropriate for the ontology topic and the generated natural language text. The generated natural language text is processed to generate extracted ontology rules and the extracted ontology rules are compared to one or more rules of an ontology rule database that correspond to the ontology topic. A check is performed to determine if a performance of the ontology extractor is acceptable.","['G06F16/367', 'G06F40/30', 'G06F40/20', 'G06N3/042', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/084', 'G06F40/242', 'G06F40/284', 'G06F40/56', 'G06N5/022', 'G06N5/025']"
US20250094700A1,Small sample fine-turning method and system and related apparatus,"The present application relates to the technical field of computers. Provided is a small sample fine-turning method, the method comprising: inputting a data set, and forming an input sample according to a fixed template; constructing a candidate tag word set and a candidate prompt template set; by means of reinforcement learning, searching an optimal tag word corresponding to the input sample from the candidate tag word set, and a prompt template corresponding to the input sample from the candidate prompt template set; and outputting a mapping relationship of the optimal tag word and an optimal prompt template format corresponding to the prompt template.","['G06F40/186', 'G06F16/335', 'G06F16/36', 'G06F40/30']"
US11556712B2,Span selection training for natural language processing,"Methods and systems for natural language processing include pretraining a machine learning model that is based on a bidirectional encoder representations from transformers model, using a span selection training data set that associates a masked word with a passage. A natural language processing task is performed using the span selection pretrained machine learning model.","['G06F40/30', 'G06F18/2113', 'G06F18/2148', 'G06F40/284', 'G06F40/56', 'G06K9/623', 'G06K9/6257', 'G06N20/00', 'G06N3/045', 'G06N3/084']"
US20220308866A1,Predictive Analytics Across DevOps Landscape,"The present invention extends to methods, systems, and computer program products for deriving unified insights ad logs from DevOps CI/CD tools and pipeline data. In general, a data transformer facilitates data normalization and serialization converting raw data across multiple DevOps tools and stores the data into a Data Lake in accordance with a customized schema. A continuous orchestrator sequences, aggregates and contextualizes the logs, providing an intuitive way of troubleshooting issues across a DevOps environment, historical data for compliance and audit purposes, and a build manifest for root cause analysis. The continuous orchestrator also processes the logs and leverages a KPI framework, providing intelligent dashboards across 90+ KPI's and a plurality of different dimensions (Planning, Development/pipelines, security, quality, operations, productivity and source code) to help customers make smart decisions and do more with less.","['G06F16/245', 'G06Q10/101', 'G06F11/302', 'G06F11/3082', 'G06F11/3438', 'G06F16/248', 'G06F16/285', 'G06F17/40', 'G06F3/0482', 'G06F3/0486', 'G06F8/10', 'G06F8/35', 'G06F8/71', 'G06F8/77', 'G06F9/5005', 'G06Q10/0637', 'H04L67/133', 'G06F11/3409']"
US11256873B2,"Data processing system and method for dynamic assessment, classification, and delivery of adaptive personalized recommendations","A data processing system and method for delivering a personalized recommendation to an individual, using computerized, dynamic assessment and classification of communication interaction records, is described. The system and method comprises a machine learning application to (1) classify a persona based on communication processing of communication interaction records captured over time, and (2) generate an adaptive personalized recommendation for use within a coaching, advisory or other personalized service. The system and method allows for improved assessments and identification of factors affecting achievement of a goal or outcome, thereby allowing for adaptive, personalized recommendations, improving delivery of personalized support services, and improved success rates for achieving a defined outcome. The system and method may also be used to create a collection of persona analytics information that may be used to visualize and evaluate trends associated with an individual or within a given population.","['G06F16/9535', 'G06F40/30', 'G06F16/24522', 'G06F16/2457', 'G06F16/248', 'G06F16/285', 'G06F40/284', 'G06Q30/0269', 'G06F40/20']"
CN108267172B,Intelligent robot inspection system for mine,"The invention discloses a mining intelligent robot inspection system and a method, wherein the mining intelligent robot inspection system comprises a track circulating rotation subsystem, a robot perception subsystem, a wireless charging subsystem and a remote control and deep learning subsystem; the track circulating rotation subsystem is used for driving the robot sensing subsystem to rotate around the track in a circulating manner; the robot perception subsystem is used for detecting and communicating the surrounding environment; the remote control and deep learning subsystem is used for receiving the detected data and sending a control command to the track circulation rotation subsystem and the robot perception subsystem. According to the technical scheme, on the premise of meeting the coal mine explosion-proof standard, according to the collection of the environmental parameters, the unknown risk is predicted by utilizing deep learning, and the robot inspection system is controlled by utilizing remote control, so that automatic wireless charging is realized, the production safety is improved, and the reliability of the inspection process is ensured.","['G01D21/02', 'G05B19/0428', 'G05D1/02', 'G05B2219/2612']"
US20220138193A1,Conversion method and systems from natural language to structured query language,"The present application discloses a conversion method and system from natural language to structured query language. The method includes obtaining a natural language question text; converting from the natural language question text to the structured query language according to similarities between the natural language question text and natural language questions in a preset dataset; and when there is no target natural language question in the preset dataset, converting the natural language question text to the structured query language by a conversion algorithm model.","['G06F40/35', 'G06F16/243', 'G06F16/24522', 'G06F18/22', 'G06F40/216', 'G06F40/289', 'G06F40/40', 'G06F40/44', 'G06F40/55', 'Y02D10/00']"
CN110852116B,"Non-autoregressive neural machine translation method, device, computer equipment and medium","The embodiment of the application discloses a non-autoregressive neural machine translation method, a device, computer equipment and a medium; the method comprises the steps of obtaining a source sentence of a source language and word vectors corresponding to words in the source sentence; coding the word vector corresponding to the word to obtain a coding vector of the concerned context information; determining a sentence to be translated according to the source sentence, wherein the sentence to be translated comprises words to be translated; reordering the words to be translated in the sentences to be translated according to the word vectors corresponding to the words to be translated and the coding vectors according to the structure of the target language to obtain pseudo translated sentences; translating the pseudo translation sentence into a target sentence of the target language according to a word vector corresponding to a word to be translated in the pseudo translation sentence and the coding vector; and outputting the target sentence. The scheme can improve translation quality.",['G06N20/00']
CN112257421B,Nested entity data identification method and device and electronic equipment,"The application discloses a method and a device for identifying nested entity data and electronic equipment, and relates to the technical field of data identification. The method comprises the following steps: arranging and combining the seed entity vocabularies of different entity categories to generate a short text data set; defining at least one entity category label for the short text in the short text data set, wherein each entity category label corresponds to the index information of the beginning and the end of the subfile in the short text; training a deep learning recognition model by using the defined short text data set as a training set; and identifying the nested entity data by using the identification model which reaches the standard after training. According to the method and the device, entity marking information is defined according to the mode of starting and ending index and entity category label of the statement, so that the marking of the content of the multi-nested entity is simpler to realize, the process and the workload of marking by identifying the nested entity are optimized, the time cost and the labor cost are saved, and the identification efficiency and the accuracy of the data of the nested entity can be improved.","['G06F40/295', 'G06F16/316', 'G06F16/3344', 'G06F16/35', 'G06F40/242', 'G06N3/045', 'G06N3/049', 'G06N3/08']"
CN111597778B,A method and system for automatic optimization of machine translation translation based on self-supervision,"The invention relates to a self-supervision-based automatic optimizing method and system for machine translation, and belongs to the field of machine translation. The method comprises the following steps: training a model by using a large-scale parallel corpus to enable the model to learn and replace operation; training the model by using artificial pseudo data to enable the model to learn and insert operation, so as to obtain a trained model; and thirdly, predicting candidate words by using the trained model for each word and inter-word space in the translation to be optimized, and finishing the post-editing operation of replacement and insertion. The invention uses bilingual parallel corpus as model pre-training input, so that the model can obtain good optimization performance on different machine translation systems, and is a universal automatic post-editing model for machine translation.","['G06F40/166', 'G06F40/232', 'G06F40/58', 'Y02T10/40']"
US11169786B2,Generating and using joint representations of source code,"Implementations are described herein for generating embeddings of source code using both the language and graph domains, and leveraging combinations of these semantically-rich and structurally-informative embeddings for various purposes. In various implementations, tokens of a source code snippet may be applied as input across a sequence-processing machine learning model to generate a plurality of token embeddings. A graph may also be generated based on the source code snippet. A joint representation may be generated based on the graph and the incorporated token embeddings. The joint representation generated from the source code snippet may be compared to one or more other joint representations generated from one or more other source code snippets to make a determination about the source code snippet.","['G06F8/42', 'G06F8/36', 'G06F8/38', 'G06F8/73', 'G06F8/75', 'G06F8/77', 'G06N20/00', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'G06N3/044']"
CN110223698B,Training a speaker recognition model of a digital assistant,"The invention provides a speaker recognition model for training a digital assistant. Techniques are provided for training a speaker recognition model for interaction with a digital assistant. In some examples, user authentication information is obtained at a first time. A user utterance representing a user request is received at a second time. Generating a voice print from the user utterance. It is determined whether a plurality of conditions are satisfied. The plurality of conditions includes a first condition that the user authentication information corresponds to one or more authentication credentials assigned to a registered user of the electronic device. The plurality of conditions further includes a second condition that an interval between the first time and the second time does not exceed a predefined time period. In accordance with a determination that the plurality of conditions are satisfied, updating a speaker profile assigned to the registered user based on the voiceprint.","['G10L17/22', 'G10L17/04', 'G10L15/22', 'G10L17/06', 'G10L17/24', 'G10L17/00', 'G10L2015/223']"
CN112328767B,Question-answer matching method based on BERT model and comparative aggregation framework,"The invention belongs to the field of natural language processing technology and automatic question-answering system, and particularly relates to a question-answering matching method based on a BERT model and a comparison aggregation framework, which comprises the following steps: acquiring data in real time, and preprocessing the data; inputting the preprocessed data into a trained comparison aggregation question-answer matching model to obtain matching scores of the questions and the answers, and outputting the best answers according to the scores; the comparison aggregation question-answer matching model comprises a BERT model, a Transformer encoder and a comparison aggregation frame; the invention utilizes the BERT model to obtain the word embedding related to the context, and solves the problem of insufficient interaction between the question sentence and the answer sentence in the prior method. And the encoder which solely uses the Transformer model further performs attention weight distribution on the word vectors for comparison, so that a comparison layer can fully explore the complex semantic matching relation between the question sequence and the answer sequence.","['G06F16/3329', 'G06F16/3344', 'G06F18/2415', 'G06F40/211', 'G06F40/289', 'G06N3/045', 'G06N3/08']"
US11398238B2,Speech recognition method in edge computing device,"Disclosed herein is a speech recognition method in a distributed network environment. A method of performing a speech recognition operation in an edge computing device includes receiving a natural language understanding (NLU) model from the cloud server, storing the received NLU model, receiving voice data spoken by a user from the client device, performing a natural language processing operation on the received voice data using the NLU model, performing speech recognition according to the natural language processing operation, and transmitting a result of the speech recognition to the client device.","['G10L15/18', 'G10L15/30', 'G10L15/063', 'G10L15/183', 'G10L15/34', 'G10L25/48', 'G10L25/30']"
US11037028B2,Computer-implemented method of creating a translation model for low resource language pairs and a machine translation system using this translation model,"A computer-implemented method for creating a translation model for low resource language pairs and applicable on noisy inputs utilizing several approaches: choosing particular input corpora covering in-domain noisy and clean texts as well as unrelated but larger general parallel texts, performing several chosen methods of creating synthetic parallel corpora and filtering, pre-processing, deduplicating and concatenating training corpora.","['G06K9/6257', 'G06F40/51', 'G06F18/2148', 'G06F40/45', 'G06F40/58', 'G06N3/02', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
CN111753727B,"Method, apparatus, device and readable storage medium for extracting structured information","The embodiment of the application discloses a method, a device, electronic equipment and a computer readable storage medium for extracting structured information, and relates to the technical fields of deep learning, image processing, natural language processing and cloud computing. One embodiment of the method comprises the following steps: acquiring an image to be processed, and identifying and obtaining a wireless table area in the image to be processed; performing semantic segmentation operation on the wireless table area by using a deep model which can extract multi-scale features and is used for segmentation to obtain each text block, so as to obtain each segmented text block; and extracting according to each text block to obtain target structural information. The embodiment provides an automatic structured information extraction scheme of a detail bill and a bill, and particularly aims at a wireless table area, by using a deep model which can extract multi-scale features for dividing and obtaining each text block, the text block dividing effect is better, and the accuracy of the extracted structured information is improved.","['G06V30/412', 'G06F18/23', 'G06F18/24', 'G06N3/045', 'G06N3/08', 'G06V10/267', 'G06V10/464']"
CN112527999B,Extraction type intelligent question-answering method and system for introducing knowledge in agricultural field,"The invention discloses a method and a system for extracting intelligent question-answering by introducing knowledge in the agricultural field, wherein the method for extracting intelligent question-answering comprises the following steps: s1, obtaining technical articles and constructing a document resource library; s2, searching related documents related to the problems in a document resource library, and constructing problem article combination pair data; s3, performing question and article pre-training language model Bert coding according to the question articles; s4, embedding knowledge graph data into the Bert model; s5, performing self-attention interaction layer operation on the question and the article coding vector, and splicing the question and the article coding vector with the self-attention interaction vector to obtain a spliced relation vector; s6, taking the spliced relation vector as an input vector of an answer prediction layer, predicting the answer position, and outputting the answer to a user. The extraction type intelligent question-answering method effectively improves the accuracy of answer extraction of the extraction type machine reading understanding model in the intelligent question-answering process.","['G06F16/3329', 'G06F40/289']"
WO2022198854A1,Method and apparatus for extracting multi-modal poi feature,"A method and apparatus for extracting a multi-modal POI feature, which relate to big data technology in the field of artificial intelligence. The method comprises: extracting a visual feature representation of a POI from an image of the POI by using an image feature extraction model; extracting a semantic feature representation from text information of the POI by using a text feature extraction model; extracting a spatial feature representation from spatial position information of the POI by using a spatial feature extraction model; and fusing the visual feature representation, the semantic feature representation and the spatial feature representation of the POI, so as to obtain a multi-modal feature representation of the POI. By means of the method, a feature vector representation that fuses multiple modalities is extracted for each POI, thereby providing a basis for subsequent calculation of the similarity between POIs.","['G06F16/9535', 'G06F16/29', 'G06F16/908', 'G06F16/909', 'G06F16/9537']"
CN111459491B,Code recommendation method based on tree neural network,"The invention discloses a code recommendation method based on a tree neural network, which comprises the following steps: preprocessing a data set formed by code blocks and constructing a training set; processing the code part above the code line to be recommended of each sample in the training set, analyzing the code part into an abstract syntax tree, extracting path information from a root node to the code line to be recommended, and adjusting the abstract syntax tree and annotation information of the samples; training an AST-Transformer model obtained by improving a Transformer model, taking an abstract syntax tree, annotation information and path information from a root node to a recommendation point as input, and taking a recommended code line sequence as output; and extracting the code written by the user and the related annotation information thereof, and predicting the next possible code. The method improves the transformer model to adapt to the tree structure, uses the annotation information in the code, better helps the model to learn semantic information, and makes high-quality recommendation.","['G06F8/436', 'G06F8/447', 'G06N3/045', 'G06N3/08']"
US11989237B2,Natural language interaction with automated machine learning systems,"An artificial intelligence (AI) interaction method, system, and computer program product include selecting an artificial intelligence model to respond to a query to generating a response to the query using the selected artificial intelligence model, and receiving the response to the query from the selected artificial intelligence model.","['G06F16/90332', 'G06N3/08', 'G06N5/04', 'G06N3/044', 'G06N3/045']"
US20240104336A1,Guided dialogue using language generation neural networks and search,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for enabling a user to conduct a dialogue. Implementations of the system learn when to rely on supporting evidence, obtained from an external search system via a search system interface, and are also able to generate replies for the user that align with the preferences of a previously trained response selection neural network. Implementations of the system can also use a previously trained rule violation detection neural network to generate replies that take account of previously learnt rules.","['G06F16/33295', 'G06N3/006', 'G06F16/3329', 'G06F16/338', 'G06F40/284', 'G06F40/35', 'G06N3/008', 'G06N3/042', 'G06N3/045', 'G06N3/0455', 'G06N3/0475', 'G06N3/09', 'G06N3/092', 'G06N3/084', 'G06N3/094', 'G06N5/01']"
US12271438B2,System and method for integrating user feedback into website building system services,"A website building system (WBS) includes a processor implementing a machine learning feedback-based proposal module and a database storing at least the web sites of a plurality of users of the WBS, and components of the websites. The module includes a plurality of per activity AI units and a feedback system. Each per activity AI unit supports one or more specific activity related to the WBS and provides at least one system suggestion to the users related to its specific activity. Each per activity AI unit includes at least one machine learning model suitable for the activity supported by its per activity AI unit. The feedback system provides a plurality of different kinds of feedback from the users and from rule engines for updating the machine learning models. The feedback system analyzes the feedback to determine which one of the at least one machine learning models to update.","['G06F16/958', 'G06F11/302', 'G06F11/3438', 'G06F16/972', 'G06N20/20', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06F2201/865']"
US11609746B2,Efficient streaming based lazily-evaluated machine learning framework,"Methods, systems, and computer products are herein provided for lazy evaluation of input data by a machine learning (ML) framework. An ML pipeline receives input data and compiles a chain of operators into a chain of dataviews configured for lazy evaluation of the input data. Each dataview in the chain represents a computation over data as a non-materialized view of the data. The ML pipeline receives a request for column data and selects a chain of delegates comprising one or more delegates for one or more dataviews in the chain to fulfill the request. The ML pipeline processes the input data with the selected chain of delegates. The ML pipeline performs delegate chaining on a dataview. A feature value for a feature column of the dataview is determined based on the delegate chaining and provided to an ML algorithm to predict column data.","['G06F8/31', 'G06F12/0207', 'G06F16/24535', 'G06F16/24568', 'G06F9/30036', 'G06F9/30079', 'G06F9/5016', 'G06F9/5022', 'G06F9/544', 'G06N20/00']"
CN112507065B,A Code Search Method Based on Annotation Semantic Information,"The invention relates to a code searching method based on annotated semantic information, which comprises the steps of obtaining a code sample stored in a form of 'natural language description, function code' from a collected code warehouse, carrying out data processing on the code sample, then coding the code sample through a neural network model coder, and training a neural network model; converting all code functions in the code warehouse and the input of the user into a vector form through a neural network model encoder, constructing the code vector warehouse, calculating a plurality of code vectors which are closest to the vector input and converted by the user in the code vector warehouse, and returning corresponding codes as retrieval results. The invention has the advantages that: the comment information in the code is used as the natural language label, so that the workload of manual labeling is reduced, and large-scale data generation can be realized. And secondly, semantic feature extraction is performed by adopting a transform neural network and the like, so that the defect that the traditional keyword matching needs one-to-one correspondence is overcome.","['G06F16/3344', 'G06F16/338', 'G06F18/22', 'G06F40/126', 'G06N3/084', 'Y02D10/00']"
US20220188654A1,System and method for clinical trial analysis and predictions using machine learning and edge computing,"A system and method for improving the efficiency of information flow of and during clinical trials and also using edge-based and cloud-based machine learning for analyzing clinical trial data from inception to completion subsequently protecting investments, assets, and human life. The system comprises a pharmaceutical research system that receives, pushes, and facilitates data packets containing clinical trial information across multiple sites and across multiple trial personnel while also using machine learning for a variety of tasks. A mobile application on edge devices uses edge-based machine learning to identify biomarkers and provides sponsors and clinicians with an expedient and secure communication means. The edge devices and the cloud-based machine learning communicate full-duplex and share information and machine learning models leading to an improvement in early adverse effects detection. Biomarkers predicting severe adverse effects trigger the system to send alerts, reports, and potential victims to medical personnel for immediate intervention.","['G06N3/084', 'G06N5/022', 'G06F16/951', 'G06F18/22', 'G06F18/2413', 'G06K9/6215', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06V10/82', 'G06V20/698', 'G16B15/30', 'G16B40/20', 'G16H10/20', 'G16H50/20', 'G16H50/30', 'G16H50/50', 'G06N5/02', 'G06V2201/04', 'G16B20/40']"
US11423878B2,"Intelligent voice recognizing method, apparatus, and intelligent computing device","Disclosed are an intelligent voice recognizing method, a voice recognizing apparatus, and an intelligent computing device. The an intelligent voice recognizing method according to an embodiment of the present disclosure receives a voice, acquires a sequential start language uttered sequentially with a utterance language from the voice, and sets the sequential start language as an additional start language other than a basic start language when the sequential start language is recognized as a start language of a voice recognizing apparatus, thereby being able to authenticate a user and recognize a voice even through a seamless scheme voice that is uttered in an actual situation. According to the present disclosure, one or more of the voice recognizing device, intelligent computing device, and server may be related to artificial intelligence (AI) modules, unmanned aerial vehicles (UAVs), robots, augmented reality (AR) devices, virtual reality (VR) devices, and 5G service-related devices.","['G10L15/005', 'G10L15/063', 'G10L17/24', 'G10L15/183', 'G10L15/30', 'G10L17/04', 'G10L17/12', 'H04L5/0044', 'H04W74/004', 'G10L15/22', 'G10L2015/0635', 'G10L2015/223']"
US11960867B1,Using natural language latent representation in automated conversion of source code from base programming language to target programming language,"Using a natural language (NL) latent presentation in the automated conversion of source code from a base programming language (e.g., C++) to a target programming language (e.g., Python). A base-to-NL model can be used to generate an NL latent representation by processing a base source code snippet in the base programming language. Further, an NL-to-target model can be used to generate a target source code snippet in the target programming language (that is functionally equivalent to the base source code snippet), by processing the NL latent representation. In some implementations, output(s) from the NL-to-target model indicate canonical representation(s) of variables, and in generating the target source code snippet, technique(s) are used to match those canonical representation(s) to variable(s) of the base source code snippet. In some implementations, multiple candidate target source code snippets are generated, and a subset (e.g., one) is selected based on evaluation(s).","['G06F8/51', 'G06F8/436', 'G06F40/279', 'G06F40/40', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N7/01']"
CN112530437B,"Semantic recognition method, device, equipment and storage medium","The application discloses a semantic recognition method, a semantic recognition device, semantic recognition equipment and a semantic recognition storage medium, and relates to the technical field of deep learning and natural language processing. The specific implementation scheme is as follows: acquiring a voice recognition result of the voice to be processed, wherein the voice recognition result comprises a newly added recognition result fragment and a historical recognition result fragment; acquiring semantic vectors of all history objects in the history recognition result fragments, and acquiring the semantic vectors of all the history objects and all the newly-added objects in the newly-added recognition result fragments by an input stream type semantic coding layer; and inputting the semantic vectors of each history object and the semantic vectors of each newly added object into a streaming semantic vector fusion layer and a semantic understanding multi-task layer which are sequentially arranged, and obtaining a semantic recognition result of the voice to be processed. Therefore, real-time semantic recognition of the voice of the user is realized, the response time of the man-machine voice interaction system is shortened, the interaction efficiency is improved, and the user experience is improved.","['G10L15/1815', 'G10L15/1822', 'G10L15/26', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/08', 'G06N3/09', 'G10L15/02', 'G10L15/063', 'G10L15/16', 'G10L15/22', 'G06N3/02', 'G06N7/01', 'G10L2015/027', 'Y02D10/00']"
US11256995B1,System and method for prediction of protein-ligand bioactivity using point-cloud machine learning,"A system and method that predicts whether a given protein-ligand pair is active or inactive, the ground-truth protein-ligand complex crystalline-structure similarity, and an associated bioactivity value. The system and method further produce 3-D visualizations of previously unknown protein-ligand pairs that show directly the importance assigned to protein-ligand interactions, the positive/negative-ness of the saliencies, and magnitude. Furthermore, the system and method make enhancements in the art by accurately predicting protein-ligand pair bioactivity from decoupled models, removing the need for docking simulations, as well as restricting attention of the machine learning between protein and ligand atoms only.","['G06N5/022', 'G16B15/30', 'G06F16/951', 'G06F18/22', 'G06F18/2413', 'G06K9/6215', 'G06N3/047', 'G06N3/08', 'G06N3/084', 'G16B40/20', 'G06N3/045', 'G06N5/02']"
CN113535963B,"Long text event extraction method and device, computer equipment and storage medium","The invention discloses a method and a device for extracting long text events, computer equipment and a storage medium, wherein the method comprises the following steps: acquiring a trigger word in a long text of an event to be extracted, and performing text truncation on the long text according to the trigger word to obtain a truncated text; classifying and predicting a plurality of event types corresponding to the truncated text by using a deep learning model; extracting corresponding event role information for each event type by combining a machine reading understanding technology and a pointer network model; and combining all the event role information into a target event based on a sequence generation algorithm, and outputting the target event as an event extraction result. According to the invention, the event classification, the event role extraction and the event combination are carried out on the long text, so that the event extraction efficiency and the extraction precision of the long text are improved.","['G06F16/353', 'G06F40/205', 'G06F40/289', 'G06N3/045']"
US10108726B2,Scenario-adaptive input method editor,"An input method editor (IME) described herein couples scenarios of the input of the user with specific network services to offer more relevant and richer candidates for higher input productivity. Data relating to a computer application in which the input candidates are to be input and/or context relating to a user-submitted query is collected and analyzed to determine a scenario. The input candidates may include text candidates and rich candidates. The IME may select a scenario-tuned and type specific engine to identify the text candidates and/or rich candidates. The scenario-tuned text candidate engines leverage scenario-tuned language models and lexicons, and the scenario-tuned rich candidate engines leverage scenario-relevant web services, such as image, mapping, and video search, when available and appropriate.","['G06F17/30867', 'G06F16/9535', 'G06F16/9537', 'G06F17/24', 'G06F17/276', 'G06F17/3087', 'G06F40/166', 'G06F40/274']"
US12271834B2,"Systems and methods for organizing, finding, and using data","A system and associated methods for organizing, representing, finding, discovering, and using data. Embodiments represent information and data in the form of a data structure termed a Feature Graph, that includes nodes and edges, where the edges serve to connect a node to one or more other nodes. A node in a Feature Graph may represent a variable, such as a measurable object, characteristic, or factor of a study. An edge in a Feature Graph may represent a measure of a statistical association between a node and one or more other nodes. Datasets that demonstrate or support the statistical association or measure the associated variable may be accessed through an identifier in a Feature Graph. An application may traverse a Feature Graph and aggregate and process data associated with a set of nodes or edges.","['G06N7/01', 'G06F16/24578', 'G06F16/9024', 'G06F18/24', 'G06F18/285', 'G06F18/29', 'G06F40/30', 'G06N20/00', 'G06N5/02', 'G06F18/211', 'G06F18/213']"
US12299796B2,Generation of story videos corresponding to user input using generative models,"The present disclosure provides systems and methods for video generation corresponding to a user input. Given a user input, a story video with content relevant to the user input can be generated. One aspect includes a computing system comprising a processor and memory. The processor can be configured to execute a program using portions of the memory to receive the user input, generate a story text based on the user input, generate a plurality of story images based on the story text, and output a story including the story text and a story video having content corresponding to the story text, wherein the story video includes the plurality of story images. Additionally or alternatively, the story video can include audio data and a plurality of generated animated videos, each animated video corresponding to a story image in the plurality of story images.","['G06T13/80', 'G06T13/00', 'G06F16/5846', 'G06F16/635', 'G06F3/167', 'G06F40/216', 'G06F40/30', 'G06F40/44', 'G06T13/20', 'G06T7/50', 'G10L13/02', 'G11B27/031', 'G11B27/28', 'G06T2200/24', 'G06T2207/10016', 'G06T2213/04']"
US10360327B2,Modifying a virtual processor model for hardware/software simulation,"A method or apparatus for transforming a provided virtual processor model to a user virtual processor model. The method in on embodiment comprises transforming a virtual processor model to simulate a user target processor, by receiving a transformable virtual processor model having a transformable instruction set and a transformable pipeline, and transforming the transformable virtual processor model to a user virtual processor model designed to simulate a user target processor.","['G06F30/20', 'G06F17/5022', 'G06F30/33', 'G06F17/5009', 'G06F30/3308', 'G06F2117/08', 'G06F2217/86']"
US20250265392A1,Method and apparatus for constructing digital twin hybrid model of main device of power system,"Disclosed in the present invention are a method and apparatus for constructing a digital twin hybrid model of a main device of a power system. The method comprises: performing information coding on an operation state and fault association knowledge graph of a main device of a power system to generate a density vector; embedding the density vector into a first layer of a ConvGRU neural network for performing initial training of a data-driven model to obtain a data-driven initial model; optimizing a convolutional base and a classifier of the data-driven initial model by means of historical collected data to obtain an optimized data-driven model; and enabling the data-driven model and a mechanism model of the main device of the power system to cooperatively operate in parallel, and performing gradient descent of a loss function of the neural network by means of a solver, so as to construct a digital twin hybrid model of the main device of the power system.","['G06F30/27', 'G06N3/0464', 'H02J3/00', 'G06F2113/04']"
CN107239874B,Rail transit-oriented electric energy quality and energy-saving analysis system,"The invention relates to the field of rail transit electric energy quality and energy-saving analysis, in particular to a rail transit-oriented electric energy quality and energy-saving analysis system, which comprises a file management system, an energy-saving technology system and a decision support system, wherein the system is based on a Visual Studio 2010 software development platform and uses a C# programming language and a database SQL 2008; on the other hand, the functions of inquiring, counting and analyzing the related electricity consumption of the detection unit are realized; on the basis of comprehensively analyzing the energy consumption data, the energy consumption of the electricity consumption unit is evaluated, and a reasonable basis is provided for comprehensively and systematically planning, implementing, checking, controlling and improving various electricity consumption management activities and implementing the whole process management.","['G06Q10/06313', 'G06Q10/0639', 'G06Q50/40', 'Y02P90/82']"
CN116127953B,"Chinese spelling error correction method, device and medium based on contrast learning","The invention discloses a Chinese spelling error correction method, a device and a medium based on contrast learning, wherein the method firstly collects original error correction data and carries out pretreatment; then constructing a data set of a Chinese spelling error correction model according to the preprocessed error correction data; the method comprises the steps of constructing a Chinese spelling error correction model comprising an enabling module, an encoder, an error detection network and an error correction network, inputting a text to be corrected into the Chinese spelling error correction model, and outputting a character coding vector sequence, a character error probability sequence and correct character probability distribution; secondly, training the Chinese spelling error correction model by using a data set based on contrast learning, updating parameters according to loss function values, and storing the trained Chinese spelling error correction model; and finally, inputting the text to be corrected into a trained Chinese spelling correction model for correcting errors so as to obtain corrected text. The invention can effectively improve the robustness and the error correction accuracy of the Chinese spelling error correction model, and has strong practicability.","['G06F40/232', 'G06N3/08']"
WO2022036866A1,Error-prevention and early-warning system for transformer substation operation,"An error-prevention and early-warning system for transformer substation operation. The system comprises: a speech recognition subsystem (102), which is configured to recognize a speech input; a two-order analysis subsystem (101), which is configured to analyze text content of a work order and an operation order of transformer substation operation, and transmit a keyword to an analysis and alarm subsystem (105); an intelligent positioning subsystem (103), which is configured to perform real-time positioning and distance measurement, and send position information to a three-dimensional model subsystem (104) and the analysis and alarm subsystem (105); the three-dimensional model subsystem (104), which is configured to mark an operation place and position information of an operation person in real time, perform operation route navigation, and display a suggested traveling route in real time; the analysis and alarm subsystem (105), which is configured to perform analysis and send corresponding alarm information; and a system management host (111), which is configured to manage the plurality of subsystems in a centralized manner.","['G08B29/185', 'G01C21/20', 'G01S5/02', 'G10L15/26', 'G10L15/30', 'H04W64/00']"
US12271803B2,Systems and methods for predicting and providing automated online chat assistance,"Methods and systems are presented for providing automated online chat assistance in an online chat session. One or more utterances transmitted from a user device of a user via the online chat session are obtained. The one or more utterances are provided to a first prediction model to predict an intent of a user. If it is determined that the first prediction model is unable to predict the intent of the user based on the one or more utterances, the one or more utterances are provided to a second prediction model. After predicting the intent of the user by the second prediction model, the intent is used by a chat robot to provide a dialogue with the user via the online chat session. The one or more utterances and the predicted intent are used to re-train the first prediction model.","['H04L65/4015', 'G06N3/04', 'G06N20/00', 'G06N3/08', 'G06N5/04', 'H04L51/02', 'H04L51/04', 'H04L65/1069', 'H04L65/70']"
US11580359B2,Pointer sentinel mixture architecture,"The technology disclosed provides a so-called â€œpointer sentinel mixture architectureâ€ for neural network sequence models that has the ability to either reproduce a token from a recent context or produce a token from a predefined vocabulary. In one implementation, a pointer sentinel-LSTM architecture achieves state of the art language modeling performance of 70.9 perplexity on the Penn Treebank dataset, while using far fewer parameters than a standard softmax LSTM.","['G06N3/0445', 'G06N3/044', 'G06F40/284', 'G06N3/045', 'G06N3/0454', 'G06N3/047', 'G06N3/0472', 'G06N3/08', 'G06N3/084', 'G06N7/005', 'G06N7/01']"
CN111755078B,"Drug molecule attribute determination method, device and storage medium","The application discloses a method, a device and a storage medium for determining drug molecule attributes, and belongs to the technical field of artificial intelligence. The method comprises the following steps: acquiring a text character string of a drug molecule to be detected; the text character string is used for describing a chemical structural formula of the drug molecule to be detected; acquiring three-dimensional structure information of the drug molecules to be detected according to the text character string; and determining the drug forming property of the drug molecules to be detected according to the three-dimensional structure information of the drug molecules to be detected. The embodiment of the application provides a new drug molecule attribute prediction scheme, which can acquire three-dimensional structure information of a drug molecule to be detected, wherein the three-dimensional structure information of the drug molecule provides position distribution of each atom in the drug molecule in a three-dimensional space, and the spatial structure of the drug molecule can influence the property of the drug molecule, so that the drug molecule attribute can be accurately predicted based on the three-dimensional structure information of the drug molecule, the discovery speed of a new candidate drug can be increased, and the research and development cost can be reduced.","['G16C20/30', 'G06F18/214', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/09', 'G06V30/40', 'G16C20/50', 'G16C20/70', 'Y02D10/00']"
WO2020186778A1,"Error word correction method and device, computer device, and storage medium","An error word correction method and device, a computer device, and a storage medium. The error word correction method comprises: obtaining a general natural language data set (101); converting each sentence comprised in the natural language data set into a Pinyin sequence to obtain obtaining Pinyin-sentence pairs of the general natural language data set (102); performing Pinyin replacement on some of the Pinyin-sentence pairs of the general natural language data set to obtain a first sample set (103); pre-training a neural network model using the first sample set to obtain a pre-trained neural network model (104); obtaining a plurality of Pinyin-sentence pairs comprising similar Pinyin and related to a specific field as a second sample set (105); performing fine tuning on the pre-trained neural network model using the second sample set to obtain a fine-tuned neural network model (106); and inputting a Pinyin sequence of a sentence to be corrected into the fine-tuned neural network model for correction to obtain a corrected sentence (107). By means of the method, error correction can be performed on special words identified as common words in language identification.","['G06F16/3343', 'G06F16/3344', 'G06F40/232', 'G06F40/279', 'G06N3/045', 'G10L15/08', 'G10L25/30', 'G10L2015/088']"
WO2022217849A1,Methods and systems for training neural network model for mixed domain and multi-domain tasks,"Methods and systems for training a neural network model using domain mixing and multi-teacher knowledge distillation are described. Tokens, including a unique token, are inputted to an encoder of the neural network model. A unique embedding vector encoded from the unique token is inputted to an adaptor network to generate domain probabilities. A domain mixing embedding vector, determined from the unique embedding vector, is inputted to a predictor of the neural network model, to generate a predicted output. A final loss is computed using a domain mixing loss computed from the domain probabilities and a ground-truth domain of the data sample, and using an output prediction loss computed from the predicted output and a ground-truth label of the data sample. Parameters of the neural network model and adaptor network are updated using the final loss.","['G06N3/045', 'G06N3/084', 'G06N3/08']"
US11734584B2,Multi-modal construction of deep learning networks,"Methods, systems, and computer program products for multi-modal construction of deep learning networks are provided herein. A computer-implemented method includes extracting, from user-provided multi-modal inputs, one or more items related to generating a deep learning network; generating a deep learning network model, wherein the generating includes inferring multiple details attributed to the deep learning network model based on the one or more extracted items; creating an intermediate representation based on the deep learning network model, wherein the intermediate representation includes (i) one or more items of data pertaining to the deep learning network model and (ii) one or more design details attributed to the deep learning network model; automatically converting the intermediate representation into source code; and outputting the source code to at least one user.","['G06N5/04', 'G06F30/20', 'G06N3/105', 'G06N5/022', 'G06F2111/10']"
US12045269B2,Apparatus and method for generating a digital assistant,An apparatus for generating a digital assistant is disclosed. The apparatus include at least a processor and a memory communicatively connected to the at least a processor. The memory instructs the processor to receive at least one user query from a user. The memory instructs the processor to extract a plurality of background data and a plurality of contextual data from the user dataset. The memory instructs the processor to receive at least one user query from a user. The memory instructs the processor to generate a query response as a function of the at least one user query and the contextual data using the digital assistant. The memory instructs the processor to display the query response using the digital assistant on a display device.,"['G06F16/3329', 'H04L51/02', 'G10L15/22', 'H04L51/10']"
US10742068B2,Real-time deviation detection of power system electrical characteristics using time-synchronized measurements,"Systems, methods, and products are described herein for identifying deviations within a power system. Using time-synchronized measurement devices, a set of voltages and currents associated with a plurality of electrical components within the power system are continuously measured. For each electrical component of the plurality of electrical components, a representative set of parameters are recursively determined based on the measured set of voltages and currents. For each electrical component, an electrical characteristic value is determined based on the representative set of parameters. For each electrical component, a deviation of the electrical component is identified based on comparison of the determined electrical characteristic value with a reference value of the electrical characteristic of the electrical component or based on identifying the deviation by means of a filtered rate of change. An alert of the deviation is provided for further characterization of an abnormality in the power system.","['H02J13/00001', 'G01R19/2513', 'H02J13/00002', 'H02J3/00', 'H02J3/0012', 'H02J3/242', 'G01R31/62', 'G06Q50/06', 'H02H3/042', 'H02H3/40', 'H02H7/26', 'H02J2203/20', 'H02J3/001', 'Y02E40/70', 'Y02E60/00', 'Y04S10/20', 'Y04S10/22', 'Y04S10/30', 'Y04S10/40', 'Y04S10/52']"
US11537950B2,Utilizing a joint-learning self-distillation framework for improving text sequential labeling machine-learning models,"This disclosure describes one or more implementations of a text sequence labeling system that accurately and efficiently utilize a joint-learning self-distillation approach to improve text sequence labeling machine-learning models. For example, in various implementations, the text sequence labeling system trains a text sequence labeling machine-learning teacher model to generate text sequence labels. The text sequence labeling system then creates and trains a text sequence labeling machine-learning student model utilizing the training and the output of the teacher model. Upon the student model achieving improved results over the teacher model, the text sequence labeling system re-initializes the teacher model with the learned model parameters of the student model and repeats the above joint-learning self-distillation framework. The text sequence labeling system then utilizes a trained text sequence labeling model to generate text sequence labels from input documents.","['G06N3/084', 'G06N20/00', 'G06F16/248', 'G06N3/044', 'G06N3/045', 'G06N7/01']"
CN111382580B,Encoder-decoder framework pre-training method for neural machine translation,"The invention discloses a coder-decoder framework pre-training method for neural machine translation, which comprises the following steps: constructing massive multilingual document-level monolingual corpus, and adding a special identifier in front of each sentence to represent the language type of the sentence; processing the sentence pairs to obtain training data; training the monolingual data of different languages to obtain a pre-training model parameter after convergence; establishing a parallel corpus, and initializing parameters of a neural machine translation model by using parameters of a pre-training model; the initialized neural machine translation model finely adjusts model parameters through parallel corpora to complete the training process; in the decoding stage, the source language sentences are encoded by using the encoder of the trained neural machine translation model, and the decoder decodes the source language sentences to generate target language sentences. The invention enables the model to have language modeling capability and language generation capability, applies the pre-training model to the neural machine translation model, can accelerate the convergence speed of the model and improve the robustness of the model.","['G06N3/08', 'Y02D10/00']"
US11288142B2,Recovery strategy for a stream processing system,"The technology disclosed relates to discovering multiple previously unknown and undetected technical problems in fault tolerance and data recovery mechanisms of modem stream processing systems. In addition, it relates to providing technical solutions to these previously unknown and undetected problems. In particular, the technology disclosed relates to discovering the problem of modification of batch size of a given batch during its replay after a processing failure. This problem results in over-count when the input during replay is not a superset of the input fed at the original play. Further, the technology disclosed discovers the problem of inaccurate counter updates in replay schemes of modem stream processing systems when one or more keys disappear between a batch's first play and its replay. This problem is exacerbated when data in batches is merged or mapped with data from an external data store.","['G06F11/1471', 'G06F11/14', 'G06F11/1438', 'G06F11/202', 'G06F11/2035', 'G06F11/2048', 'G06F2201/84']"
US9020921B2,Storage tape analytics user interface providing library analysis and monitoring alerts,"A method of accessing tape storage analytics data from tape libraries. The method includes generating user interfaces on client devices. Each user interface includes a first screen displaying graphics or analysis tables based on the tape storage analytics data. The method includes receiving user input selecting the displayed graphics or an item within a cell of the analysis table and modifying the user interface to display a second screen, which presents a subset of the tape storage analytics data corresponding to operation of at least one of one or more tape libraries or library components (e.g., robots and elevators), tape drives, and tape media within the tape libraries or, alternatively, activities involving any of a combination of these components. The receiving and modifying steps are repeated to navigate between data corresponding to a tape library, a tape drive, and media and to operations involving these and other tape library components.","['G11B27/36', 'G06F11/0727', 'G06F11/0748', 'G06F11/0766', 'G06F16/18', 'G11B27/002', 'G11B27/105', 'G11B27/34', 'G11B2220/90']"
US9019269B1,Interactive rendering of building information model data,"A system, apparatus and method for interactively rendering building information modeling data.","['G06T19/003', 'G06T15/506', 'G06T17/00', 'G06T2200/04', 'G06T2200/12', 'G06T2210/04', 'G06T2215/12']"
CN112990296B,Image-text matching model compression and acceleration method and system based on orthogonal similarity distillation,"The invention provides a method and a system for compressing and accelerating an image-text matching model based on orthogonal similarity distillation, wherein the method comprises the following steps: s1: acquiring a picture-text matching data set, and constructing a student network model and a teacher network model; s2: preprocessing and data loading are carried out on the image-text matching data set; s3: calculating a difference similarity matrix based on the similarity matrix of the student network model and the similarity matrix of the teacher network model; calculating a singular value based on the difference similarity matrix; constructing an orthogonal similarity soft distillation loss function and an orthogonal similarity hard distillation loss function based on the singular value; calculating a joint loss function; training a student network model based on a joint loss function; s4: performing performance test on the trained student network model to obtain a performance evaluation result of the image-text matching data set and the trained student network model; s5: and inputting the image or the text to be detected into the trained student network model, and outputting the text or the image.","['G06F18/22', 'G06F18/217', 'G06F18/41']"
US11120585B2,Systems and methods for image reconstruction,"The present disclosure relates to a system. The system may obtain a k-space dataset according to magnetic resonance (MR) signals acquired by a magnetic resonance imaging (MRI) scanner. The system may also generate, based on the k-space dataset using an image reconstruction model that includes a sequence sub-model and a domain translation sub-model, a reconstructed image by: inputting at least a part of the k-space dataset into the sequence sub-model; outputting, from the sequence sub-model, a feature representation of the k-space dataset; inputting the feature representation of the k-space dataset into the domain translation sub-model; and outputting, from the domain translation sub-model, the reconstructed image.","['G06T11/008', 'G06T11/003', 'G01R33/5602', 'G01R33/5608', 'G01R33/561', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06T7/0012', 'G06T2207/10088', 'G06T2207/20084', 'G06T2207/30004']"
US20240160861A1,Transformer translation system for deep learning using triple sentence pair,"A transformer translation system for deep learning using triple sentence pair is designed to do both a rough translation and a revision of rough translations, and utilize reinforcement learning using a deep learning facility to produce data of refined translations. And this transformer translation system can translate literary works, webtoons, subtitles, and the like, which can be commercially distributed or exported to overseas markets, taking into account feelings, nuances, atmosphere, jargons, tones of voice, writers' intentions, context, etc. so that consumers or readers can enjoy not mere plain translations but refined, high-quality translations. Furthermore, this transformer translation system for deep learning using triple sentence pair may automatically produce the final translations, which can be commercially viable, continuously for an indefinite time without limit.","['G06F40/51', 'G06F40/58', 'G06F9/451', 'G06N3/08']"
CN112989761B,Text classification method and device,"The application provides a text classification method and a text classification device, which relate to natural language processing technology and deep learning technology in the field of artificial intelligence, and the method comprises the following steps: acquiring a text to be classified and a preset text library; performing word segmentation and part-of-speech tagging on the text to be classified to obtain a plurality of word segmentation results and part-of-speech tagging results corresponding to each word segmentation result; matching each word segmentation result with a preset text library, and determining a target word segmentation result and first weight information corresponding to the target word segmentation result based on the matching result and the part-of-speech tagging result; determining second weight information corresponding to other word segmentation results except the target word segmentation result in the plurality of word segmentation results based on the first weight information; obtaining text characteristic information of the text to be classified according to the first weight information and the second weight information; and performing relevance identification processing on the text characteristic information based on the text classification model to obtain a text classification result of the text to be classified. The method and the device can improve the accuracy and efficiency of text classification.","['G06F40/117', 'G06F16/3344', 'G06F16/353', 'G06F40/216', 'G06F40/258', 'G06F40/289', 'G06N3/02', 'G06N3/08']"
US20220160433A1,Al-Based Automatic Tool Presence And Workflow/Phase/Activity Recognition,"A robotic system is configured to automatically identify surgical instruments used during a bronchoscopy procedure. The robotic system can include a video capture device, a robotic manipulator, sensors configured to detect a configuration of the robotic manipulator, and control circuitry communicatively coupled to the robotic manipulator. The control circuitry is configured to perform, using a machine learning classifier, a first analysis of a bronchoscopy video of a patient site to track a medical instrument in the bronchoscopy video. The control circuitry can then identify a set of possible instrument identifications for the medical instrument in the bronchoscopy video based on the first analysis and an identified phase of the bronchoscopy procedure. The control circuitry can then track a motion of the medical instrument in the bronchoscopy video and select an identification from the set of possible instrument identification for the medical instrument based at least on the tracked motion.","['A61B34/20', 'G06T7/0012', 'A61B17/29', 'A61B17/3403', 'A61B34/30', 'A61B90/90', 'G06N3/044', 'G06N3/045', 'G06T7/11', 'G06T7/20', 'G16H20/40', 'G16H30/40', 'G16H50/20', 'A61B2017/00203', 'A61B2017/2901', 'A61B2034/2048', 'A61B2034/2051', 'A61B2034/2059', 'A61B2034/2065', 'A61B2034/301', 'A61B2090/365', 'A61B34/25', 'A61B90/92', 'A61B90/98', 'G06T2207/10016', 'G06T2207/10068', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30061']"
US11410426B2,Classification of auditory and visual meeting data to infer importance of user utterances,"In non-limiting examples of the present disclosure, systems, methods and devices for generating summary content are presented. Voice audio data and video data for an electronic meeting may be received. A language processing model may be applied to a transcript of the audio data and textual importance scores may be calculated. A video/image model may be applied to the video data and visual importance scores may be calculated. A combined importance score may be calculated for sections of the electronic meeting based on the textual importance scores and the visual importance scores. A meeting summary that includes summary content from sections for which combined importance scores exceed a threshold value may be generated.","['G10L25/54', 'G06F16/345', 'G06F18/214', 'G06F18/256', 'G06K9/6256', 'G06N3/044', 'G06N3/0445', 'G06N3/08', 'G06V10/811', 'G06V10/82', 'G06V20/41', 'G06V20/47', 'G06V40/16', 'G06V40/172', 'G10L17/00', 'G10L17/18', 'H04N7/155', 'G06V2201/10', 'G10L15/16', 'G10L15/26']"
WO2022101515A1,Method for an explainable autoencoder and an explainable generative adversarial network,"An exemplary embodiment provides an autoencoder which is explainable. An exemplary explainable autoencoder may explain the degree to which each feature of the input attributed to the output of the system, which may be a compressed data representation. An exemplary embodiment may be used for classification, such as anomaly detection, as well as other scenarios where an explainable autoencoder is used as input to another machine learning system or when an explainable autoencoder is a component in an end-to-end deep learning architecture. An exemplary embodiment provides an explainable generative adversarial network that adds explainable generation, simulation and discrimination capabilities. The underlying architecture of an exemplary embodiment may be based on an explainable or interpretable neural network, allowing the underlying architecture to be a fully explainable white-box machine learning system.","['G06N3/08', 'G06N3/045', 'G06N3/047', 'G06N5/022', 'G06N5/045']"
US8849784B2,Storage tape analytics user interface with media validation,"A method of accessing tape storage analytics data from tape libraries. The method includes generating a user interface on client devices. The user interface includes a first screen displaying graphics or an analysis table based on the tape storage analytics data. The method includes receiving user input selecting the displayed graphics or an item within a cell of the analysis table and displaying a second screen, which presents a subset of the tape storage analytics data and provides data on operation of at least tape libraries or library components (e.g. robots, elevators, CAPs, pass through ports), tape drives, and tape media within the tape libraries, or alternatively, activities involving any or a combination of these components. The receiving and modifying steps are repeated to navigate between data corresponding to a tape library or its comprising components, a tape drive, media, or corresponding activities.","['G11B27/36', 'G11B27/002', 'G11B27/105', 'G11B27/34', 'G11B5/008', 'G11B2220/90']"
WO2020218634A1,Answering device determination method and apparatus,"An answering device determination method and an answering device determination apparatus are disclosed. An answering device determination method according to one embodiment of the present invention determines an answering device on the basis of a first step of receiving audio signals from devices, extracting, from the audio signals, distance information from which the distances between a user and the plurality of devices can be indicated, and determining the answering device that is to respond to a wake-up word, wherein the answering device is determined by comparing the distance information according to a predetermined condition, and/or a second step of determining the answering device on the basis of a result of applying, as an input, the distance information to a deep neural network (DNN) model, and thus the present invention can select a device capable of providing a voice service in response to the wake-up word uttered once by the user, by reflecting the user's intention, in an environment in which a plurality of devices recognizes the same wake-up word.","['G06F3/167', 'G10L15/10', 'G01S11/14', 'G06N3/006', 'G06N3/02', 'G06N3/045', 'G06N3/08', 'G06N7/01', 'G10L15/07', 'G10L15/16', 'G10L15/22', 'G10L25/51', 'G10L2015/223', 'G10L2015/225']"
US11321538B1,Ensemble natural language processing model with compliance verification,"The present disclosure relates to an ensemble network of natural language processing (NLP) models configured to determine how well a given document addresses one or more requirements set forth in a requirement-specifying document. The NLP models may extract relevant text from the documents and perform term-similarity measurements to determine how similar the text tokens from one document are to the other and generate a similarity score for each sentence and each section of each document. The similarity scores may then be used to determine whether the response document addresses the requirements. If the response document does not address particular requirements, then data flags may be generated to indicate that a corresponding section of the response document may need to be updated.","['G06F40/40', 'G06F40/30', 'G06F40/284', 'G06K9/6232', 'G06N3/045', 'G06N3/0454', 'G06V10/82', 'G06V30/40', 'G06F2207/4824', 'G06N20/10']"
CN112667717B,"Transformer substation inspection information processing method and device, computer equipment and storage medium","The application relates to a transformer substation inspection information processing method and device, computer equipment and a storage medium. The method comprises the following steps: receiving a polling instruction carrying a polling task point required by the transformer substation, wherein the polling task point is used for determining a polling path; triggering data acquisition equipment on the inspection equipment to acquire information according to the inspection path based on the inspection instruction to obtain multimedia information of the inspection task point; preprocessing multimedia information to obtain equipment state data of a transformer substation; carrying out format conversion on the equipment state data to obtain equipment state data in a preset data format; and synchronizing the equipment state data in the preset data format to a server corresponding to the transformer substation based on the preset data interactive communication protocol. By adopting the method, the inspection efficiency of the transformer substation can be improved.",[]
CN114743020B,A food recognition method combining label semantic embedding and attention fusion,"The invention provides a food identification method combining label semantic embedding and attention fusion, which comprises the following steps: the window attention fusion module adaptively selects and discriminates the region by using a self-attention mechanism of a transducer, and does not need additional labeling training of frames. The module fuses the window attention of the Swin transducer, cuts out and enlarges the attention area from the original image, and is used as the input of the next network to learn more distinguishing characteristics, and the names of the food categories contain important text information such as main components, places of production, cooking methods and the like, so that the module is easy to obtain and helpful for food identification. Therefore, the invention provides context sensitive semantic center loss, and utilizes the semantic embedding of the food label as the center of the feature space so as to guide the image expression to learn fine granularity semantic information. The combination of the two improves the food recognition accuracy.","['G06F18/22', 'G06F18/241', 'G06F18/2415', 'G06N3/047', 'G06N3/08']"
CN114821223B,Pre-training image text model processing method and image-text retrieval system,"The invention discloses a pre-training image text model processing method and an image-text retrieval system. The method comprises the steps of obtaining mask training sample pairs for masking words and image blocks in an image text sample pair, inputting the mask training sample pairs into a pre-training image text model, obtaining loss values output for masked words, the masked image blocks and image text tasks, wherein the pre-training image text model comprises a multi-stage downsampling encoder and a multi-stage upsampling decoder, and adjusting parameters in the pre-training image text model according to the loss values. The invention realizes the pixel-level reconstruction of the masked image blocks in the pre-training image language network by combining the block embedding of the image with the model structure of the multi-stage downsampling encoder and the up-sampling decoder corresponding step by step. Further, combining the self-built residual sub-network implementing the input image and text embedding with the encoder-decoder structure as above, an end-to-end multi-modal pre-training is implemented.",['G06F18/214']
US11635455B2,System and method for performing data transfers in an intelligent electronic device,"There is provided an intelligent electronic device for responding to user data and information requests regarding power usage and power quality for any metered point within a power distribution system. The intelligent electronic device includes a first network interface which receives client side information and data requests, which are processed in accordance with a network protocol and forwarded to a network interface via a network socket interface translator which translates management signals to facilitate the eventual data transfer. Protocol routines process the requests by constructing an internal data request in certain cases and forwards the internal data request to a data interface for translation from an internal data request format of the protocol routine format to a native database format. The database receives the translated request, and retrieves the requested data from a measuring unit of the electric power meter, and forwards the data back to the requesting client.","['G01R21/133', 'G01R22/063', 'G01D4/004', 'Y02B90/20', 'Y04S20/30']"
US12182721B2,Deep learning-based anomaly detection in images,"A method comprising: receiving, as input, training images, wherein at least a majority of the training images represent normal data instances; receiving, as input, a target image; extracting (i) a set of feature representations from a plurality of image locations within each of the training images, and (ii) target feature representations from a plurality of target image locations within the target image; calculating, with respect to a target image location of the plurality of target image locations in the target image, a distance between (iii) the target feature representation of the target image location, and (iv) a subset from the set of feature representations comprising the k nearest the feature representations to the target feature representation; and determining that the target image location is anomalous, when the calculated distance exceeds a predetermined threshold.","['G06N3/088', 'G06F18/24147', 'G06F18/2433', 'G06N3/045', 'G06V10/44', 'G06V10/761', 'G06V10/762', 'G06V10/82']"
US12010129B2,Methods and apparatus for using machine learning to classify malicious infrastructure,"Embodiments disclosed include methods and apparatus for detecting a reputation of infrastructure associated with potentially malicious content. In some embodiments, an apparatus includes a memory and a processor. The processor is configured to identify an Internet Protocol (IP) address associated with potentially malicious content and define each row of a matrix by applying a different subnet mask from a plurality of subnet masks to a binary representation of the IP address to define that row of the matrix. The processor is further configured to provide the matrix as an input to a machine learning model, and receive, from the machine learning model, a score associated with a maliciousness of the IP address.","['G06F21/552', 'G06F21/566', 'G06N3/045', 'G06N3/08', 'H04L63/0236', 'H04L63/14', 'H04L63/1408', 'H04L63/1416', 'H04L63/1425', 'H04L63/1441', 'H04L63/20', 'G06N20/00', 'G06N3/048']"
US9396037B2,Model-based data pipeline system optimization,"A computer-implemented method for optimizing a data pipeline system includes processing a data pipeline configuration manifest to generate a framework of the data pipeline system and a data flow logic package of the data pipeline system. The data pipeline configuration manifest includes an object-oriented metadata model of the data pipeline system. The computer-implemented method further includes monitoring performance of the data pipeline system during execution of the data flow logic package to obtain a performance metric for the data pipeline system, and modifying, with a processor, the framework of the data pipeline system based on the data pipeline configuration manifest and the performance metric.","['G06F9/5083', 'G06F11/3409', 'G06F16/254', 'G06F17/30563', 'G06F2209/5022']"
CN113190656B,A Chinese Named Entity Extraction Method Based on Multi-Annotation Framework and Fusion Features,"The invention discloses a Chinese named entity extraction method based on a multi-label framework and fusion characteristics. Then, word information and word segmentation mark information are introduced for each Chinese character through dictionary matching, and dictionary characteristics are constructed. On the basis, phonetic characters are annotated by using Chinese phonetic software according to the meaning of the Chinese characters in the matched words, so that phonetic features are constructed. And then, based on a dot-multiplying attention mechanism, combining dictionary features and pinyin features into Chinese character codes to obtain Chinese character semantic codes combining the dictionary features and the pinyin features, and improving the recognition capability of boundaries of Chinese named entities. Finally, combining the advantages of sequence labeling and index labeling, and utilizing a multi-task learning model to jointly learn two labeling tasks, so as to improve the accuracy of extracting the Chinese named entities.","['G06F16/3344', 'G06F16/3346', 'G06F16/35', 'G06F18/241', 'G06F18/2415', 'G06F40/216', 'G06F40/242', 'G06F40/295', 'G06N3/044', 'G06N3/047', 'G06N3/08', 'Y02D10/00']"
US12393875B2,Training encoder model and/or using trained encoder model to determine responsive action(s) for natural language input,"Systems, methods, and computer readable media related to: training an encoder model that can be utilized to determine semantic similarity of a natural language textual string to each of one or more additional natural language textual strings (directly and/or indirectly); and/or using a trained encoder model to determine one or more responsive actions to perform in response to a natural language query. The encoder model is a machine learning model, such as a neural network model. In some implementations of training the encoder model, the encoder model is trained as part of a larger network architecture trained based on one or more tasks that are distinct from a â€œsemantic textual similarityâ€ task for which the encoder model can be used.","['G06N20/00', 'G06F16/3329', 'G06F16/3344', 'G06F16/3346', 'G06F16/35', 'G06F40/30', 'G06N3/0442', 'G06N3/0455', 'G06N3/0464', 'G06N3/084', 'G06N5/04', 'G10L15/16', 'G10L15/1822', 'G10L15/22', 'G06N3/088', 'G06N3/09', 'G06N3/096']"
CN112527998B,"Reply recommendation method, reply recommendation device and intelligent equipment","The application discloses a reply recommending method, a reply recommending device, intelligent equipment and a computer readable storage medium. The answer recommendation method comprises the following steps: in the multi-round dialogue process, if receiving the user statement input in the round, extracting the entity words and the relation information of the user statement; inputting the history spliced sentences, the user sentences and the entity words into a trained natural language processing model to obtain at least one candidate answer output by the natural language processing model, wherein the history spliced sentences are generated based on each history user sentence and the history answer corresponding to each history user sentence in the multi-round dialogue process; screening target answers from the at least one candidate answer according to the entity words, the relation information and a preset knowledge graph; and outputting the target reply. By the scheme, meaningful replies can be recommended to the user on the basis of fully understanding the history dialogue.","['G06F16/3329', 'G06F40/295', 'Y02D10/00']"
US20220036194A1,Deep neural network optimization system for machine learning model scaling,"The present disclosure is related to techniques for optimizing artificial intelligence (AI) and/or machine learning (ML) models to reduce resource consumption while maintaining or improving AI/ML model performance. A sparse distillation framework (SDF) is provided for producing a class of parameter and compute efficient AI/ML models suitable for resource constrained applications. The SDF simultaneously distills knowledge from a compute heavy teacher model while also pruning a student model in a single pass of training, thereby reducing training and tuning times considerably. A self-attention mechanism may also replace CNNs or convolutional layers of a CNN to have better translational equivariance. Other embodiments may be described and/or claimed.","['G06N3/082', 'G06N3/04', 'G06N3/045', 'G06N3/063']"
US11354582B1,System and method for automated retrosynthesis,"A system and method for automated retrosynthesis which can reliably identify valid and practical precursors and reaction pathways. The methodology involves a k-beam recursive process wherein at each stage of recursion, retrosynthesis is performed using a library of molecule disconnection rules to identify possible precursor sets, validation of the top k precursor sets is performed using a transformer-based forward reaction prediction scoring system, the best candidate of the top k precursor sets is selected, and a database is searched to determine whether the precursors are commercially available. The recursion process is repeated until a valid chain of chemical reactions is found wherein all precursors necessary to synthesize the target molecule are found to be commercially available.","['G16B15/30', 'G06F16/951', 'G06F18/214', 'G06F18/22', 'G06K9/6215', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N3/084', 'G06N5/01', 'G06N5/022', 'G06N7/01', 'G06V30/10', 'G16B15/00', 'G16B40/00', 'G16B40/20', 'G16B45/00', 'G16B50/10', 'G16C20/10', 'G16C20/70', 'G06N3/006']"
US12248472B2,Systems and methods for dynamic query prediction and optimization,"Systems, apparatus, methods, and articles of manufacture provide for evaluation of and prediction for data queries (e.g., SQL statements). In one example implementation, a controller device trains a data model for classifying input SQL queries. The controller classifies each input SQL query by testing it against the data model, without requiring actual execution of the SQL query.","['G06F16/906', 'G06F16/24542', 'G06F16/215', 'G06F16/2425', 'G06N20/00', 'G06N20/20', 'G06N5/01', 'G06N5/04']"
US11893345B2,Inducing rich interaction structures between words for document-level event argument extraction,"Systems and methods for natural language processing are described. One or more embodiments of the present disclosure receive a document comprising a plurality of words organized into a plurality of sentences, the words comprising an event trigger word and an argument candidate word, generate word representation vectors for the words, generate a plurality of document structures including a semantic structure for the document based on the word representation vectors, a syntax structure representing dependency relationships between the words, and a discourse structure representing discourse information of the document based on the plurality of sentences, generate a relationship representation vector based on the document structures, and predict a relationship between the event trigger word and the argument candidate word based on the relationship representation vector.","['G06F40/284', 'G06F40/126', 'G06F40/211', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'G06N3/0455', 'G06N3/08', 'G06N5/02', 'G06N3/0464', 'G06N3/09']"
US12032922B2,Automated script generation and audio-visual presentations,Automatic generation of intelligent content is created using a system of computers including a user device and a cloud-based component that processes the user information. The system performs a process that includes receiving an input document and parsing the input document to generate inputs for a natural language generation model using a text analysis model. The natural language generation model generates one or more candidate presentation scripts based on the inputs. A presentation script is selected from the candidate presentation scripts and displayed. A text-to-speech model may be used to generate a synthesized audio presentation of the presentation script. A final presentation may be generated that includes a visual display of the input document and the corresponding audio presentation in sync with the visual display.,"['G06F40/30', 'G06F40/58', 'G06F16/24578', 'G06F3/0481', 'G06F40/205', 'G06F40/56', 'G10L13/02']"
US11429779B2,Method and system for intelligently suggesting paraphrases,"A method and system for providing replacement text segments for a given text segment may include receiving a request to provide the replacement text segment for the text segment in the document, examining a content characteristic of the document, and examining at least one of user-specific information, organization-specific information, or non-linguistic features of the document, before identifying at least one replacement text segment for the text segment, via a machine translation system, based on the content characteristic of the document and at least one of the user-specific information, the organization-specific information, or the non-linguistic features of the document. The method and system may include providing the identified replacement text segment for display to a user, receiving an input indicating a user's selection of the identified replacement text segment, and upon receiving the input, replacing the text segment in the document with the identified replacement text segment.","['G06F40/166', 'G06F18/2178', 'G06F3/0482', 'G06F40/16', 'G06F40/216', 'G06F40/295', 'G06F40/30', 'G06F40/44', 'G06F40/47', 'G06F40/56', 'G06K9/6263']"
CN108289708B,Circuit topology for a combined generator,"The present invention provides an apparatus, system, and method for managing Radio Frequency (RF) and ultrasonic signals output by a generator comprising a surgical instrument including an RF energy output and an ultrasonic energy output and circuitry configured to receive a combined RF and ultrasonic signal from the generator, wherein the circuitry may be configured to filter the frequency content of the combined signal and configured to provide a first filtered signal to the RF energy output and a second filtered signal to the ultrasonic energy output, or the circuitry may be configured to switch between outputs of the surgical instrument.","['A61B18/1445', 'A61B18/1206', 'A61B18/1233', 'A61B2017/320071', 'A61B2017/320094', 'A61B2017/320095', 'A61B2018/00178', 'A61B2018/00601', 'A61B2018/00607', 'A61B2018/0063', 'A61B2018/00642', 'A61B2018/00648', 'A61B2018/00684', 'A61B2018/00732', 'A61B2018/00779', 'A61B2018/00875', 'A61B2018/00958', 'A61B2018/00994', 'A61B2018/1273', 'A61B2018/128', 'A61B2018/1286', 'A61B2018/1293', 'A61B2018/1452', 'A61B2090/0811', 'H03F2200/18']"
US11669698B2,Method and system for automatic formality classification,"The present teaching relates to automatic formality classification and transformation of online text items. In one example, a request is received for determining a formality level of a text item in an online communication. One or more linguistic features are extracted from the text item. Contextual information with respect to the online communication is extracted. A formality level of the text item is determined based on the one or more linguistic features and the contextual information. The formality level represents a degree of formality of the text item. The formality level is provided as a response to the request.","['G06F16/345', 'G06F16/335', 'G06F40/253', 'G06F40/35', 'G06F40/56']"
US10706450B1,Artificial intelligence system for generating intent-aware recommendations,"The present disclosure is directed to training and using machine learning models to determine user intent from a search query, for example via a semantic parse that identifies particular catalog fields for items in an electronic catalog that would satisfy the user's current mission as reflected in their search query intent. The determined intent can then be used to filter recommendations and/or pre-select attribute-value input fields on detail pages displayed after the user navigates away from the search results page, until the mission is complete.","['G06Q30/0625', 'G06F16/9535', 'G06N20/00', 'G06Q30/0641', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G06N5/022', 'G06N7/01']"
US20240144141A1,"Systems, methods, kits, and apparatuses for using artificial intelligence for automation in value chain networks","A VCN process may receive information associated with a value chain network. A VCN process may provide the information to a set of Artificial Intelligence (AI)-based learning models, wherein at least one member of the set of AI-based learning models is trained on a training data set of a set of value chain network entities operating data to classify at least one of: an operating state, a fault condition, an operating flow, or a behavior of at least one value chain entity of the set of value chain network entities. A VCN process may determine a task to be completed for the value chain network based upon, at least in part, on an output of the set of AI-based learning models. A VCN process may execute the task to facilitate an improvement in the value chain network.","['G06Q10/06375', 'G06Q30/0202', 'G06Q30/0206']"
WO2023134073A1,"Artificial intelligence-based image description generation method and apparatus, device, and medium","The present application relates to the technical field of knowledge representation and reasoning of artificial intelligence and discloses an artificial intelligence-based image description generation method and apparatus, a device, and a medium. The method comprises: obtaining an image to be described; performing text region detection according to the image to be described; performing text recognition on each text region according to the image to be described to obtain texts to be analyzed; performing target feature extraction according to the image to be described; and performing image description generation on the basis of a multi-modal feature fusion method and according to the image to be described, each text to be analyzed and each target feature to obtain an image description result. On the basis of a multi-modal feature fusion method, understanding is performed on a text-associated environment of an understood image to generate an image description, and thereby the abundant information in the image is completely expressed in language, and the accuracy of image description is improved.","['G06F18/241', 'G06F18/253', 'G06N3/045', 'G06N3/08']"
CN119296516B,Domain-based speech recognition method and system based on RAG,"The invention discloses a method and a system for domain speech recognition based on RAG, wherein the method comprises the steps of obtaining an original domain document set, constructing a hierarchical document structure through theme classification, paragraph segmentation and sentence segmentation, performing vectorization processing to generate a multi-granularity vector matrix set, establishing a hierarchical knowledge vector index structure, performing multi-scale time-frequency analysis on a real-time audio stream, extracting and enhancing acoustic features to generate multi-scale feature tensors, performing self-adaptive blocking and progressive attention processing on the feature tensors to construct a probability estimation model, generating a candidate text sequence, performing multi-layer retrieval in a knowledge index based on multi-dimensional query vectors to generate relevant knowledge fragments, performing space mapping and dynamic attention processing on the features, and performing probability optimization decoding to obtain a final recognition text. The invention improves the accuracy and the robustness of the voice recognition in the professional field through knowledge enhancement and multi-level feature processing.","['G10L15/02', 'G06F16/325', 'G06F16/3344', 'G06F16/3346', 'G06F16/367', 'G10L15/04', 'G10L15/063', 'G10L15/16', 'G10L15/1815', 'G10L15/183', 'G10L15/26']"
US12314060B2,Value chain network planning using machine learning and digital twin simulation,"A VCN process may receive, by a value chain network digital twin, information associated with a value chain network. A VCN process may provide the information to a set of Artificial Intelligence (AI)-based learning models, wherein at least one member of the set of AI-based learning models is trained to classify at least one of: an operating state, a fault condition, an operating flow, or a behavior of the value chain network and at least one member of the set of AI-based learning models is trained to determine a task to be completed for the value chain network. A VCN process may provide at least one of an instruction for executing the task in the value chain network digital twin and a recommendation for executing the task in the value chain network digital twin.","['G05D1/0297', 'G05B19/4155', 'G05B19/41885', 'G05D1/223', 'G05D1/6987', 'G06N20/00', 'G06N3/08', 'G06Q10/06', 'G06Q10/0635', 'G06Q10/06375', 'G06Q10/06395', 'G06Q10/08', 'G06Q10/0833', 'G06Q10/087', 'G06Q10/0875', 'G06Q30/0201', 'G06Q30/0202', 'G06Q50/04', 'G06Q50/40', 'G05B2219/50391', 'G05D2101/15', 'G05D2107/70', 'G05D2109/10', 'G06N20/10', 'G06N20/20', 'G06N3/006', 'G06N3/044', 'G06N3/0455', 'G06N3/049', 'G06N3/084', 'G06N3/088', 'G06N5/01', 'G06N7/01', 'G06Q2220/00']"
US11561772B2,Low-code development platform,A computer-implemented low-code development platform is provided including a user interface and having access to a library of step macros configured for user configuration and interconnection via the user interface to generate executable code. Each step macro includes a step configuration generator and an execution code generator. The step configuration generator is configured to generate a step configuration file based on user-configurable data points configurable via the user interface. The execution code generator is configured to generate executable code in the form of a compiled step file configured for storage in memory and execution by a processor of a computing system. The execution code generator receives and inputs the step configuration file into a metaprogramming component configured to interpret the user-configurable data points of the step configuration file and to generate and output the compiled step file.,"['G06F8/34', 'G06F8/44', 'G06F8/71', 'G06F9/451', 'G06F9/45508', 'G06F8/36']"
US11816609B2,Intelligent task completion detection at a computing device,"Computerized systems and methods are provided for automatically detecting an indication that a task has been completed and associated user interface functionality. These systems and methods improve existing technologies by automatically detecting indications that tasks have been completed via new logic or rules and improving the functionality and computing resource consumption relative to existing machine learning models. These systems also improve the way computers operate by reducing computing resource consumption, such as memory, network latency, I/O, and the like.","['G06F40/284', 'G06Q10/063114', 'G06F3/0482', 'G06F40/289', 'G06F40/30', 'G06N20/00', 'G06Q10/1097', 'H04L51/046']"
CN106471570B,Multi-command single-speech input method,"A system and process for processing multi-part voice commands for a virtual assistant is disclosed. A voice input may be received from a user, the voice input including a plurality of executable commands within a single utterance. A text string may be generated from the speech input using a speech transcription process. The text string may be parsed into a plurality of candidate substrings based on domain keywords, imperative verbs, predetermined substring lengths, and so forth. For each candidate substring, a probability may be determined that indicates whether the candidate substring corresponds to an executable command. Such probabilities may be determined based on semantic coherence, similarity to user request templates, query services for determining manageability, and the like. If the probability exceeds a threshold, a user intent for each substring may be determined, a process associated with the user intent may be performed, and a confirmation may be provided to the user.","['G10L15/1822', 'G06F40/205', 'G10L15/1815', 'G10L15/26', 'G10L15/28', 'H04M3/4936', 'G10L2015/088', 'G10L2015/221', 'G10L2015/223', 'G10L2015/225', 'G10L2015/228', 'H04M2203/355']"
CN113569001B,"Text processing method, device, computer equipment and computer readable storage medium","The embodiment of the application provides a text processing method, a device, computer equipment and a computer readable storage medium, wherein the text processing method is based on an artificial intelligence technology and comprises the steps of obtaining a text to be processed, wherein the text to be processed comprises a text title, a text keyword and a text body, inputting the text to be processed into a long text recognition model for processing to obtain a target result, wherein the target result is used for indicating the practicability class of the text to be processed, the long text recognition model is obtained by pre-training an initial text recognition model by using first text data and then fine-tuning the pre-trained text recognition model by using second text data, the first text data comprises an incomplete text body, and the second text data comprises a sample text title, a sample text keyword, a sample text body and a corresponding reference practicability class label. The embodiment of the application can effectively improve the accuracy and the robustness of the practical identification of the chapter-level long text.","['G06F16/3331', 'G06F16/35', 'G06F40/289']"
AU2023200439B2,Actionable widget cards,"ACTIONABLE WIDGET CARDS ABSTRACT A messaging system and method includes a website building system hosting a website belonging to a publisher, the website having users; at least one database storing the website parameters, pre-defined rules concerning card definitions, pre-defined widget card parameters and a product classification taxonomy; at least one card product system to receive a trigger from the website, associated with an action related to a product and to generate an actionable widget card associated with the product based on the action, the website parameters, the pre defined rules, the pre-defined widget card parameters and the product classification taxonomy and where the widget card implements at least e-commerce related operations for the product between a publisher of the website with a user of the website.","['G06Q30/0643', 'G06F40/186', 'G06F8/34', 'G06F9/451', 'G06N20/00', 'G06N5/04', 'G06Q10/10', 'G06Q30/0251', 'G06Q30/0601', 'G06Q30/0613', 'G06Q30/0641', 'G06Q50/01']"
US11551244B2,Nowcasting abstracted census from individual customs transaction records,"A signal relationship is defined between a granular data value and a target data value. At least a portion of the granular data value corresponds to a granular latency value that is smaller than a target data latency value corresponding to the target data value. Granular data corresponding to the granular data value is interpreted. The granular data is aggregated in response to the signal relationship. A value of the target data value for a selected time reference is estimated, and the estimated value of the target data value is provided as a nowcasting prediction of the target data value.","['G06Q30/0202', 'G06F16/2246', 'G06F16/2365', 'G06F16/258', 'G06N3/08', 'G06N5/022', 'G06Q10/06313', 'G06Q10/0637']"
CN112016604A,A zero-resource machine translation method using visual information,"The invention discloses a zero-resource machine translation method using visual information. After training is completed, the method has the translation capability. The innovation point of the invention is that good translation performance is achieved under the situation that the parallel language materials do not exist and the corresponding pictures exist (zero resource situation). The invention takes the Transformer as a basic translation model and introduces a pre-training language model, thereby reducing the training time and improving the performance. The invention introduces a multi-modal module in a typical Transformer, so that a translation model can process multi-modal data in a fusion mode. The method optimizes the model parameters by using two training methods of mask sequence recovery and translation, so that the translation model can complete training in a zero-resource scene.","['G06F18/241', 'G06F18/2415', 'G06F40/58', 'G06N3/045', 'G06N3/047', 'G06N3/08']"
US11468324B2,Method and apparatus with model training and/or sequence recognition,"A processor-implemented method includes: using an encoder, determining, for each of a plurality of tokens included in an input sequence, a self-attention weight based on a token and one or more tokens that precede the token in the input sequence; using the encoder, determining context information corresponding to the input sequence based on the determined self-attention weights; and using a decoder, determining an output sequence corresponding to the input sequence based on the determined context information.","['G06N3/08', 'G06F40/40', 'G06F40/284', 'G06N3/04', 'G06N3/044', 'G06N3/0445', 'G06N3/045', 'G06N3/0454', 'G06N7/06', 'G10L15/063', 'G10L15/16']"
CN118210983B,"Intelligent self-adaptive retrieval enhancement system, method and storage medium","The invention relates to artificial intelligence and natural language processing technology, in particular to an intelligent self-adaptive retrieval enhancement system, method and storage medium, which can automatically integrate information of a plurality of data sources, intelligently identify and evaluate information conflicts through ICDA modules, ensure consistency and accuracy of generated content, ensure that a DKM module is responsible for automatically updating a knowledge base in real time, keep timeliness of the information, integrate new data and solve conflicts through a self-adaptive learning mechanism, reduce manual intervention, improve working efficiency and reduce cost, and an AGS module dynamically adjusts a generation strategy according to feedback and behaviors of users to generate personalized text content, thereby being beneficial to meeting user demands.","['G06F16/9535', 'G06F16/215', 'G06F16/367', 'G06F16/906', 'G06F16/951', 'G06F18/22', 'G06F18/253', 'G06F40/30', 'G06N5/022', 'G06N5/04']"
US11494396B2,Automated intelligent content generation,"Automatic generation of intelligent content is created using a system of computers including a user device and a cloud-based component that processes the user information. The system performs a process that includes receiving a user query for creating content in a content generation application and determining an action from an intent of the user query. A prompt is generated based on the action and provided to a natural language generation model. In response to the prompt, output is received from the natural language generation model. Response content is generated based on the output in a format compatible with the content generation application. At least some of the response content is displayed to the user. The user can choose to keep, edit, or discard the response content. The user can iterate with additional queries until the content document reflects the user's desired content.","['G06F16/248', 'G06F40/30', 'G06F16/243', 'G06F16/24578', 'G06F16/34', 'G06F16/4393', 'G06F16/93', 'G06F40/137', 'G06N3/0475', 'G06N3/02']"
US8930301B2,Systems and methods for content response prediction,"Techniques for predicting a user response to content are described. According to various embodiments, a configuration file is accessed, where the configuration file includes a user-specification of raw data accessible via external data sources and raw data encoding rules. In some embodiments, the raw data includes raw member data associated with a particular member and raw content data associated with a particular content item. Thereafter, source modules encode the raw data from the external data sources into feature vectors, based on the raw data encoding rules. An assembler module assembles one or more of the feature vectors into an assembled feature vector, based on user-specified assembly rules included in the configuration file. A prediction module performs a prediction modeling process based on the assembled feature vector and a prediction model, to predict a likelihood of the particular member performing a particular user action on the particular content item.","['G06N5/04', 'G06N20/00', 'G06F16/20', 'G06N5/02', 'G06N5/048', 'G06N7/01', 'G06N99/005', 'G06Q50/01', 'H04L51/00', 'H04L51/52']"
CN108885608B,Intelligent automated assistants in the home environment,"The invention provides an intelligent automatic assistant in a home environment. The present invention provides systems and processes for operating intelligent automated assistants. In one exemplary process, speech input representing a user request may be received. The process may determine one or more likely device features corresponding to the speech input. A data structure representing a set of devices having established locations may be retrieved. The process may determine one or more candidate devices from the set of devices based on the data structure. The one or more candidate devices may correspond to the speech input. The process may determine a user intent corresponding to the speech input based on the one or more possible device characteristics and one or more actual device characteristics of the one or more candidate devices. Instructions may be provided that cause a device of the one or more candidate devices to perform an action corresponding to the user intent.","['G06F40/35', 'H04L12/2823', 'H04L12/28', 'G06F16/00', 'G06F3/16', 'G06F3/167', 'G10L15/00', 'G10L15/1822', 'G10L15/22', 'H04L12/2816', 'H04W84/18', 'G10L2015/223']"
CN113609844B,Electric power professional word stock construction method based on hybrid model and clustering algorithm,"The invention relates to the field of artificial intelligence, in particular to a method for constructing an electric power professional word stock based on a mixed model and a clustering algorithm. Preprocessing an electric text and a parallel corpus, and then performing Word segmentation through a Word segmentation model, wherein mutual information, a left-right entropy algorithm and a TextRank algorithm perform Word combination on a resultant Word segmentation result, a TF-IDF algorithm and a Word2Vec Word clustering algorithm extract text keywords from the resultant Word segmentation result, the text Word is directly segmented by the information entropy Word segmentation algorithm, and the results are summarized and compared to obtain characteristic corpus words; selecting an electric professional vocabulary from the feature corpus words as a seed word; meanwhile, the derived electric text word stock is used as a candidate word to segment the electric text, and then word2vec algorithm is used to change words into word vectors; clustering to obtain similar words, and then filtering to obtain a power professional word stock. According to the invention, most of professional words in the non-electric power field can be filtered by using one clustering model, and the professional words are complete.","['G06F40/242', 'G06F16/35', 'G06F40/284', 'G06Q50/06']"
CN110019752B,multidirectional dialogue,"The present disclosure relates to multi-directional conversations. Systems and processes for providing a multi-directional dialog are provided. One example method includes, at an electronic device with one or more processors: receiving a first natural language input; determining a first intent based on the first natural language input, identifying a first dialog flow based on the first intent, outputting a natural language output associated with the first dialog flow, receiving a second natural language input; determining whether the second natural language input satisfies a dialog criterion associated with the first dialog flow, and in accordance with a determination that the second natural language input satisfies the dialog criterion, outputting a second natural language output associated with the first dialog flow.","['G06F16/90332', 'G06F3/016', 'G06F3/0488', 'G06F3/167', 'G06F40/30', 'G06F40/35', 'G06N20/00', 'G06Q10/00', 'G06Q10/10', 'G10L15/1815', 'G10L15/22', 'H04L51/02', 'H04M3/4936', 'G10L15/1822', 'G10L2015/223', 'G10L2015/228']"
CN104115077B,Co-location electrical architecture,"Disclose a kind of system and method for distribution system in handling facility.On the one hand, this method can include: received in computer system from monitoring system and use relevant data to the actual energy of the component of distribution systemï¼›The request about change distribution system is received in computer systemï¼›Using computer system, relevant data are used with a kind of system optimization function for distribution system and based on the request and to actual energy, a kind of revised distribution system design is providedï¼›According to revised distribution system change in design distribution system, to provide the distribution system in the facility after changeï¼›And it is received in computer system from monitoring system and uses relevant data to the actual energy of the component of the distribution system after change.","['G05F1/66', 'G01R21/1331', 'G06Q50/06']"
US11580094B2,Real-time anomaly determination using integrated probabilistic system,"An audio stream is detected during a communication session with a user. Natural language processing on the audio stream is performed to update a set of attributes by supplementing the set of attributes based on attributes derived from the audio stream. A set of filter values is updated based on the updated set of attributes. The updated set of filter values is used to query a set of databases to obtain datasets. A probabilistic program is executed during the communication session by determining a set of probability parameters characterizing a probability of an anomaly occurring based on the datasets and the set of attributes. A determination is made if whether the probability satisfies a threshold. In response to a determination that the probability satisfies the threshold, a record is updated to identify the communication session to indicate that the threshold is satisfied.","['G06F40/35', 'G06F16/2365', 'G06F16/2379', 'G06F40/216', 'G06F40/30', 'G06N3/045', 'G06N7/005', 'G06N7/01', 'G10L15/197', 'G06N3/084', 'G06N3/09', 'G10L15/26']"
US12259917B2,Method of retrieving document and apparatus for retrieving document,A method of retrieving a document according to an embodiment of the present application includes: acquiring a user retrieval query; calculating a user inquiry vector in a unit of sentence from the user retrieval query and acquiring a first document candidate group based on similarity between the calculated user inquiry vector and an embedding vector of a document stored in a retrieval database; acquiring a second document candidate group based on similarity between a text included in the user retrieval query and a text of the document stored in the retrieval database; and determining a summarization target document based on the first document candidate group and the second document candidate group.,"['G06F16/3329', 'G06F16/3347', 'G06F16/345', 'G06F16/35', 'G06N3/045', 'G06N3/096']"
CN113439301B,Methods and systems for machine learning,"A synthetic training data item is received, the synthetic training data item comprising a first symbol sequence representing a synthetic sentence output by a simulator. The synthetic training data item is processed using a machine learning model that outputs a second sequence of symbols representing the synthetic sentence. The synthetic training data item is modified by replacing the first symbol sequence with the second symbol sequence. There is a statistically significant mismatch between the first symbol sequence and a third symbol sequence, which may be output by an acoustic model that processes a set of acoustic features representing the utterance of the synthetic sentence, and there is no statistically significant mismatch between the second symbol sequence and the third symbol sequence. The modified synthetic training data item may be used to train a second machine learning model that processes data output by the acoustic model.","['G10L15/063', 'G06N20/00', 'G06N3/006', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0455', 'G06N3/047', 'G06N3/088', 'G06N3/09', 'G06N5/046', 'G06N7/01', 'G10L15/02', 'G10L15/16', 'G10L15/1822', 'G10L15/22', 'G10L2015/025', 'G10L2015/027', 'G10L2015/0635']"
CN111418007B,Multi-round prefabricated dialogue,"Systems and processes for providing multiple rounds of pre-made conversations are provided. An example method includes, at an electronic device: receiving a natural language input; determining whether the natural language input satisfies dialog criteria for a plurality of pre-made dialog rounds; in accordance with a determination that the natural language input satisfies the dialog criteria for a plurality of pre-made dialogs: identifying a natural language output of the plurality of rounds of pre-made dialog corresponding to the natural language input; and outputting the natural language output; and in accordance with a determination that the natural language input does not satisfy the dialog criteria for a plurality of pre-made dialogs: identifying a task associated with the natural language input; and performing the task associated with the natural language input.","['G10L15/22', 'G06F40/56', 'G10L15/26', 'G06F40/30', 'G10L2015/223']"
WO2023071680A1,"Endoscope image feature learning model training method and apparatus, and endoscope image classification model training method and apparatus","An endoscope image feature learning model training method and apparatus, and an endoscope image classification model training method and apparatus. The endoscope image feature learning model training method comprises: obtaining a first training data set, the first training data set comprising one or more endoscope images having an object to be recognized, and one or more endoscope images having no said object; inputting the first training data set into an endoscope image feature learning model; and performing unsupervised contrastive learning on the endoscope image feature learning model on the basis of the first training data set, so as to obtain a trained endoscope image feature learning model, wherein the endoscope image feature learning model comprises a plurality of contrastive learning sub modules, and each of the plurality of contrastive learning sub modules is configured to extract the feature representations of different scales of the same endoscope image in the first training data set, and perform contrastive learning on the basis of the extracted feature representations of different scales.","['G06T7/0012', 'G06F18/214', 'G06F18/22', 'G06F18/24', 'G06N3/088', 'G06T2207/10068', 'G06T2207/20081']"
CN110288994B,Detecting triggering of a digital assistant,"Detecting a trigger of a digital assistant is provided. A method for operating a digital assistant comprising: at each of a plurality of microphones of an electronic device, sampling an audio signal to obtain a first plurality of audio signals; processing at least a portion of the first plurality of audio signals using a beamforming technique to obtain a plurality of audio streams; determining, based on a plurality of audio streams, whether any of a first plurality of audio signals corresponds to a voice trigger, wherein the determining comprises: detecting an audio trigger from a first one or more audio streams of a plurality of audio streams; in accordance with a determination that any of the first plurality of audio signals corresponds to a speech trigger: initiating, by the electronic device, a session of the digital assistant; selecting a first set of microphones of the plurality of microphones based on the one or more audio streams; sampling a second plurality of audio signals using the first set of microphones; and in accordance with a determination that the first plurality of audio signals does not correspond to a voice trigger: the session of the initiating digital assistant is abandoned.","['G10L15/22', 'G10L15/04', 'G10L15/08', 'G10L15/18', 'G10L15/28', 'G10L15/30', 'G10L21/0216', 'H04R1/406', 'H04R3/00', 'H04R3/005', 'G10L15/1822', 'G10L2015/088', 'G10L2015/228', 'G10L2021/02166', 'G10L25/51', 'H04R2227/003', 'H04R2499/11', 'H04R27/00']"
CN110021300B,Far-field extension of digital assistant services,"The present application relates to a far-field extension of digital assistant services. The present disclosure provides an extended system and process for operating an intelligent automated assistant to provide digital assistant services. An example method includes: at an electronic device with one or more processors, a first speech input representing a user request is received from a first user. The method further comprises obtaining an identity of the first user; and providing, to at least one of the second electronic device or the third electronic device, a representation of the user request in accordance with the user identity. The method further comprises the following steps: receiving the response to the user request from the second electronic device or the third electronic device based on determining whether the second electronic device or the third electronic device or both will provide the response to the first electronic device. The method also includes providing a representation of the response to the first user.","['G10L15/20', 'G06F21/32', 'G10L15/16', 'G10L15/22', 'G10L15/30', 'G10L17/22', 'G06F3/167', 'G10L15/187', 'G10L15/26', 'G10L2015/223', 'G10L2015/226', 'G10L2015/228', 'H04L51/02']"
US9544329B2,Client/server security by an intermediary executing instructions received from a server and rendering client application instructions,"In an embodiment, a method comprises intercepting, from a server computer, a first set of instructions that define a user interface; executing, using a headless browser, the first set of instructions without presenting the user interface; rendering a second set of instructions, which when executed by a client application on a client computer, cause the client computer to present the user interface, wherein the second set of instructions are different than the first set of instructions; sending the second set of instructions to the client computer.","['H04L63/145', 'H04L63/0281', 'H04L63/08', 'H04L63/10', 'H04L63/1408', 'H04L63/20', 'H04L67/01', 'H04L67/02', 'H04L67/10', 'H04L67/2819', 'H04L67/42', 'H04L67/564', 'H04L67/2823', 'H04L67/565', 'H04L69/22']"
CN111901481B,"Computer-implemented method, electronic device, and storage medium","The present disclosure provides a computer-implemented method, an electronic device, and a storage medium. The present disclosure provides systems and processes for user configurable task triggers. In one example, at least one user input is received that includes a selection of at least one condition of a plurality of conditions and a selection of at least one task of a plurality of tasks. Stored context data corresponding to an electronic device is received. Determining whether the stored context data indicates an occurrence of the at least one selected condition. In response to determining that the stored context data indicates an occurrence of the at least one selected condition, performing the at least one selected task associated with the at least one selected condition.","['G06F3/016', 'G06F9/4881', 'G06F1/1626', 'G06F1/1643', 'G06F3/04817', 'G06F3/0482', 'G06F3/04842', 'G06F3/04847', 'G06F3/0488', 'G06F3/04883', 'G06F3/167', 'G06F9/4482', 'G06F9/451', 'G06F9/453', 'G06Q10/06311', 'G06Q10/109', 'G06F2209/484', 'H04M2250/12']"
CN107408387B,Virtual Assistant Activation,"The invention discloses a method, which comprises the following steps: at an electronic device having a display, a microphone, and an input device: receiving user input via the input device when the display is on, the user input satisfying a predetermined condition; in accordance with receiving the user input that satisfies the predetermined condition, sampling audio input received via the microphone; determining whether the audio input includes a voice trigger; and in accordance with a determination that the audio input comprises the spoken trigger, triggering a virtual assistant session.","['G10L15/22', 'G10L17/24', 'G06F3/167', 'G06F3/016', 'G06F3/041', 'G10L15/28', 'G10L25/87', 'H04L12/1827', 'H04L51/02', 'G10L2015/088', 'G10L2015/223', 'G10L2015/228', 'G10L25/78', 'H04L51/10']"
CN110058834B,Intelligent device arbitration and control,"The invention provides intelligent device arbitration and control. The present disclosure relates to systems and processes for arbitrating between and/or controlling electronic devices with virtual assistants. In one exemplary process, a first electronic device samples audio input with a microphone. The first electronic device broadcasts a first set of one or more values based on the sampled audio input. Further, the first electronic device receives a second set of one or more values based on the audio input from the second electronic device. Based on the first set of one or more values and the second set of one or more values, the first electronic device determines whether a response to the audio input should be made or whether a response to the audio input should be abandoned.","['G06F3/167', 'G10L15/22', 'G10L15/30', 'G10L17/00', 'G10L2015/223']"
CN107430501B,Competing devices that respond to voice triggers,"On a first electronic device having a display and a microphone: sampling an audio input using a first microphone; in accordance with sampling audio input using the first microphone, sending a stop instruction to a second electronic device having a second microphone, the second electronic device external to the first electronic device, wherein the second electronic device is configured to respond to audio input received using the second microphone, and wherein the stop instruction instructs the second electronic device to forgo responding to audio input received using the second microphone, wherein responding to audio input received using the second microphone comprises providing perceptible output.","['G06F3/167', 'G10L15/22', 'G10L15/32', 'G10L2015/088']"
CN113378815B,Scene text positioning and identifying system and training and identifying method thereof,"The invention discloses a model for positioning and identifying a scene text and a training and identifying method thereof, belonging to the technical field of computers. The device comprises a locator, a grouping module and a recognizer, wherein the locator is connected with the recognizer through the grouping module; the locator outputs a character frame, a character connection frame and a text frame, the character frame and the character connection frame locate the position of a text, the grouping module cuts character pictures according to the character frame and the text frame, the character pictures are grouped and sent into the identifier, the identifier outputs identification results of all groups, and finally, the location and the identification of a scene text are completed according to the location and the identification results; the invention can make the computer accurately and efficiently finish the task of detecting and identifying the text in the natural scene.","['G06F18/214', 'G06N3/045', 'G06N3/08']"
CN113673248B,Named entity identification method for testing and identifying small sample text,"The invention discloses a named entity recognition method for a small sample text for experimental identification, which comprises the following specific steps of carrying out a contrast experiment, and constructing a baseline model for entity recognition according to an experiment result; improving the baseline model to obtain an entity recognition model; the entity recognition model comprises a text pre-training model, a coding model and a decoding model which are connected in sequence; the entity recognition model is used for recognizing the entity, classifying the recognition error conditions, summarizing and carrying out semantic analysis, making a semantic correction rule aiming at the error conditions, and constructing a semantic correction rule module to realize semantic correction of the recognition result of the entity recognition model. Aiming at the problems of complex entity structure, long entity length, entity doping irrelevant noise and the like in specific fields such as test identification and the like, the method carries out classified statistical analysis on the error result of multi-model neural network identification, and works out a plurality of correction rules to correct the fusion result.","['G06F40/295', 'G06F18/214', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
US10687193B2,Assist device and system,"A wearable device monitors a wearer's physical activity and provides assistance to the wearer. The device includes physiologic sensors that provide sensor data of the wearer to a processor, a user interface that provides information to the wearer, a network interface that provides a network connection between the device and a remote computer, and a memory that stores instructions. The processor executes the instructions to collect physical activity data of the wearer using the sensors, to provide the collected activity data to the remote computer via the network interface to create or update a parameterized rule-based custom data model for the wearer, to receive the custom data model for comparison to collected activity data, and to communicate with the wearer via the user interface when a check of the wearer's activity data against the custom data model indicates that the wearer's activity is not consistent with the custom data model.","['H04W4/90', 'G06F1/163', 'G06F1/1635', 'G06F1/3231', 'G06F1/3287', 'H04L67/12', 'H04L67/18', 'H04L67/306', 'H04L67/52', 'H04L67/535', 'H04W4/024', 'H04W4/029', 'Y02D10/00']"
US12266353B2,System and method for artificial intelligence (AI) assisted activity training,"The disclosure relates to system and method for AI assisted activity training. The method includes presenting a plurality of activity categories to a user and receiving a voice-based input from the user. The method uses an Natural Language Processing model to process received voice-based input to extract selection activity and activity attribute. Contemporaneous to receiving voice-based input, method presents multimedia content in conformance with activity and activity attribute. In response to initiation of the multimedia content, method detects initiation of user activity performance. The method captures video of user activity, overlays, by a smart mirror a pose skeletal model corresponding to user activity performance over reflection of user on smart mirror and process video using AI model to extract user performance parameters. Feedback may be generated based on overlaid pose skeletal model and differential between user performance parameters and target set of performance parameters.","['A63B24/0006', 'A63B24/0062', 'A63B24/0075', 'A63B24/0087', 'A63B71/0622', 'G06F40/30', 'G06F40/35', 'G06N20/00', 'G06V10/82', 'G06V40/23', 'G10L15/18', 'G10L15/22', 'G16H20/30', 'G16H40/63', 'G16H40/67', 'G16H50/20', 'G16H50/70', 'A63B2024/0009', 'A63B2024/0015', 'A63B2024/0071', 'A63B2024/0096', 'A63B2071/0627', 'A63B2220/806', 'A63B2225/12', 'G10L15/005']"
US20240249081A1,Method and system for automated customized content generation from extracted insights,"Disclosed embodiments may provide techniques for generating customizable content based on extracted insights. A computer-implemented method can include accessing input data that includes initial content. The computer-implemented method can also include determining a content format of customized content to be generated by processing the input data. In some instances, the content format specifies how the customized content is to be formatted for a target recipient. The computer-implemented method can also include generating one or more prompts to be processed by a content machine-learning model for generating the customized content. The one or more prompts can be generated based on the input data and the content format. The computer-implemented method can also include applying the content machine-learning model to the one or more prompts to generate the customized content. The computer-implemented method can also include outputting the customized content.","['G06F40/40', 'G06F16/245']"
US12223435B2,System and method for molecular reconstruction from molecular probability distributions,"A system and method comprising a transmoler that identifies common substructures of a given 3D conformer and predicts its structural information. First, based on contrastive learning, substructure embeddings are learned in an unsupervised manner. Secondly, a novel oriented 3D object regressor predicts the dimensions and directions of each substructure in a conformer as well as its fingerprint embedding which are used to create differentiable junction tree molecular graphs. Lastly, using the junction tree graphs, molecular representations such as DeepSMILES are generated which represent new and novel molecules. The system may also generate conformers directly from a pocket. A pocket may be input to the model and the model learns to generate structures which can fit that pocket by conditioning the generative system. Furthermore, structure-based contrastive embeddings generated for transmoler can be recycled in structure-based generative modelling.","['G06N3/088', 'G06F16/951', 'G06F18/22', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N5/022', 'G06V10/751', 'G06V10/82', 'G16B15/00', 'G16B40/00', 'G16B45/00', 'G16B50/10', 'G16C20/50', 'G16C20/70', 'G06N3/044', 'G06N3/048', 'G06N3/084', 'G16C20/30', 'G16C20/90']"
CN111985161B,Reconstruction method of three-dimensional model of transformer substation,"The invention discloses a three-dimensional model reconstruction method of a transformer substation, which comprises the following steps: acquiring data of a three-dimensional model public database to form a basic training set; acquiring original images of substation equipment, marking and determining a transfer learning training set; processing the images in the basic training set and the transfer learning training set, extracting to obtain an equipment feature map, and constructing a deep convolution self-coding neural network based on the equipment feature map; training a depth convolution self-coding neural network by using the basic training set, and further training the depth convolution self-coding neural network by using the migration learning training set; modeling the transformer substation according to the trained deep convolution self-coding neural network to obtain a three-dimensional model of the transformer substation. By adopting the technical scheme, the three-dimensional model of the transformer substation can be constructed, the modeling efficiency of the transformer substation is improved, and the sampling workload in modeling is reduced.","['G06F30/27', 'G06F18/214', 'G06N3/045', 'G06N3/08', 'G06Q50/06', 'G06T17/00', 'Y04S10/50']"
CN110603520B,Integrating learning models into software development systems,"The subject technology provides for generating a Machine Learning (ML) model code from an ML document file, the ML document file being in a first data format, the ML document file being converted into code in an object-oriented programming language different from the first data format. The subject technology further provides for receiving additional code that invokes a function provided by the ML model code. The subject technology compiles ML model code and additional code, the compiled ML model code including object code corresponding to the compiled ML model code and the compiled additional code including object code corresponding to the additional code. The subject technology generates a package that includes compiled ML model code and compiled additional code. In addition, the subject technology sends the package to the runtime environment on the target device for execution.","['G06F8/35', 'G06N20/00', 'G06F16/245', 'G06F8/10', 'G06F8/30', 'G06F8/41', 'G06F8/71', 'G06F8/36', 'G06F8/60']"
US12367248B2,Hardware-aware machine learning model search mechanisms,"The present disclosure is related to framework for automatically and efficiently finding machine learning (ML) architectures that generalize well across multiple artificial intelligence (AI) and/or ML domains, AI/ML tasks, and datasets. The ML architecture search framework accepts a list of tasks and corresponding datasets as inputs, and may also include relevancy scores/weights for each item in the input. A combined performance metric is generated, where this combined performance metric quantifies the performance of the ML architecture across all the specified AI/ML domains, AI/ML tasks, and datasets. The system then performs a multi-objective ML architecture search with the combined performance metric, along with hardware-specific performance metrics as the objectives. Other embodiments may be described and/or claimed.","['G06N3/086', 'G06F16/953', 'G06N5/01']"
US9015684B1,Profiler-based optimization of automatically generated code,"A device generates code with a technical computing environment (TCE) based on a model and information associated with a target processor, registers an algorithm with the TCE, automatically sets optimization parameters applied during generation of the code based on the algorithm, executes the generated code, receives feedback based on execution of the generated code, and uses the feedback to automatically update the optimization parameters and to automatically regenerate the code with the TCE until an optimal code is achieved for the target processor.","['G06F8/443', 'G06F8/35', 'G06F11/3466', 'G06F9/449', 'G06F11/323', 'G06F11/3419', 'G06F11/3423', 'G06F11/3452', 'G06F2201/865', 'G06F2201/88']"
US20230111864A1,Streaming and filtering event objects into a data lake,"An asynchronous stream of security events is added to a data lake for enterprise security by identifying groups of related events related to a security threat, and creating rules to fold these related events into a single security event along with metadata. The folding rules may then be applied to security events in the event stream to compress data in the data lake and improve detection efficiency.","['G06F21/552', 'H04L63/1408', 'G06F21/53', 'G06F21/567', 'H04L63/1416', 'H04L63/1425', 'H04L63/1433', 'H04L63/1441', 'H04L63/145', 'H04L63/20']"
US11636936B2,Method and apparatus for verifying medical fact,"The present disclosure relates to the field of medical data processing based on natural language processing. Embodiments of the present disclosure disclose a method and apparatus for verifying a medical fact. The method may include: acquiring a description text of the medical fact; selecting a relevant paragraph related to the description text of the medical fact from a medical document; and inputting the description text of the medical fact and the corresponding relevant paragraph into a trained discrimination model for authenticity judgment, to obtain a verification result of the medical fact, the discrimination model being pre-trained based on a medical text paragraph pair extracted from the medical document, and being iteratively adjusted using a medical fact sample set including authenticity labeling information after the pre-training.","['G16H70/60', 'G16H20/10', 'G06N3/08', 'G06F16/3344', 'G06F40/20', 'G06F40/30', 'G06F40/58', 'G06N20/00', 'G16H10/60', 'G16H40/63', 'G16H40/67', 'G16H50/70', 'G06N3/045']"
AU2014219247B2,A system and method for inferring schematic and topological properties of an electrical distribution grid,"A system and method for inferring schematic and topological properties of an electrical distribution grid is provided. The system may include Remote Hubs, Subordinate Remotes, a Substation Receiver, and an associated Computing Platform and Concentrator. At least one intelligent edge transmitter, called a Remote Hub Edge Transmitter, may transmit messages on the electrical distribution grid by injecting a modulated current into a power main that supplies an electric meter. The Subordinate Remotes, Remote Hubs, the Substation Receiver, and the associated Computing Platform and Concentrator may contain processing units which execute stored instructions allowing each node in the network to implement methods for organizing the on-grid network and transmitting and receiving messages on the network. The Substation Receiver, Computing Platform and Concentrator may detect and infer schematic grid location attributes of the network and publish the detected and inferred attributes to other application systems including geospatial information systems maintaining the logical and physical network model.","['H04B3/54', 'G01R29/18', 'G05B13/02', 'H02J13/00007', 'H02J13/00034', 'H04B3/542', 'H04B3/546', 'H04L41/0803', 'H04L41/083', 'H04L41/12', 'H04L43/0847', 'G06Q50/06', 'H04B2203/5433', 'H04B2203/5466', 'H04L1/0009', 'H04L1/188', 'Y02E60/00', 'Y04S10/16', 'Y04S40/00', 'Y04S40/121']"
US11915133B2,Techniques for smooth region merging in image editing,Systems and methods seamlessly blend edited and unedited regions of an image. A computing system crops an input image around a region to be edited. The system applies an affine transformation to rotate the cropped input image. The system provides the rotated cropped input image as input to a machine learning model to generate a latent space representation of the rotated cropped input image. The system edits the latent space representation and provides the edited latent space representation to a generator neural network to generate a generated edited image. The system applies an inverse affine transformation to rotate the generated edited image and aligns an identified segment of the rotated generated edited image with an identified corresponding segment of the input image to produce an aligned rotated generated edited image. The system blends the aligned rotated generated edited image with the input image to generate an edited output image.,"['G06N3/08', 'G06F18/211', 'G06F18/214', 'G06F18/2163', 'G06F18/40', 'G06F3/04845', 'G06F3/04847', 'G06N20/20', 'G06N3/045', 'G06N3/047', 'G06T11/00', 'G06T11/001', 'G06T11/60', 'G06T3/0006', 'G06T3/0093', 'G06T3/02', 'G06T3/18', 'G06T3/40', 'G06T3/4038', 'G06T3/4046', 'G06T5/005', 'G06T5/20', 'G06T5/77', 'G06V10/28', 'G06V10/82', 'G06V10/98', 'G06V40/168', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2210/22']"
US11804211B2,Example-based voice bot development techniques,"Implementations are directed to providing a voice bot development platform that enables a third-party developer to train a voice bot based on training instance(s). The training instance(s) can each include training input and training output. The training input can include a portion of a corresponding conversation and a prior context of the corresponding conversation. The training output can include a corresponding ground truth response to the portion of the corresponding conversation. Subsequent to training, the voice bot can be deployed for conducting conversations on behalf of a third-party. In some implementations, the voice bot is further trained based on a corresponding feature emphasis input that attentions the voice bot to a particular feature of the portion of the corresponding conversation. In some additional or alternative implementations, the voice bot is further trained to interact with third-party system(s) via remote procedure calls (RPCs).","['G10L15/063', 'G10L13/027', 'G06F16/3329', 'G06F40/35', 'G06F40/56', 'G06N20/00', 'G06N3/044', 'G06N3/084', 'G10L15/02', 'G10L15/04', 'G10L15/22', 'H04L51/02', 'H04L67/133', 'H04M3/4217', 'H04M3/493', 'H04M3/4936', 'G06F3/167', 'G06Q30/016', 'G10L15/1822', 'G10L2015/0635', 'G10L25/30']"
US10049307B2,Visual object recognition,Technical solutions are described for training an object-recognition neural network that identifies an object in a computer-readable image. An example method includes assigning a first neural network for determining a visual alignment model of the images for determining a normalized alignment of the object. The method further includes assigning a second neural network for determining a visual representation model of the images for recognizing the object. The method further includes determining the visual alignment model by training the first neural network and determining the visual representation model by training the second neural network independent of the first. The method further includes determining a combined object recognition model by training a combination of the first neural network and the second neural network. The method further includes recognizing the object in the image based on the combined object recognition model by passing the image through each of the neural networks.,"['G06K9/66', 'G06V10/82', 'G06F18/2414', 'G06K9/00208', 'G06K9/00228', 'G06N3/04', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G06T7/0024', 'G06T7/30', 'G06T7/70', 'G06V10/454', 'G06V20/647', 'G06V30/19173', 'G06V40/161', 'G06V40/172', 'G06N3/048', 'G06T2207/20081']"
EP4205046A1,"Behavior modeling, verification, and autonomous actions and triggers of ml and ai systems","An exemplary embodiment may present a behavior modeling architecture that is intended to assist in handling, modelling, predicting and verifying the behavior of machine learning models to assure the safety of such systems meets the required specifications and adapt such architecture according to the execution sequences of the behavioral model. An embodiment may enable conditions in a behavioral model to be integrated in the execution sequence of behavioral modeling in order to monitor the probability likelihoods of certain paths in a system. An embodiment allows for real-time monitoring during training and prediction of machine learning models. Conditions may also be utilized to trigger system-knowledge injection in a white-box model in order to maintain the behavior of a system within defined boundaries. An embodiment further enables additional formal verification constraints to be set on the output or internal parts of white-box models.","['G06N3/08', 'G06F18/214', 'G06F18/2163', 'G06F18/217', 'G06N20/00', 'G06N3/006', 'G06N3/045', 'G06N7/01', 'G06N3/042']"
US11468460B2,Transactive control framework and toolkit functions,"Disclosed herein are representative embodiments of methods, apparatus, and systems for facilitating operation and control of a resource distribution system (such as a power grid). For example, embodiments of the disclosed technology can be used to improve the resiliency of a power grid and to allow for improved consumption of renewable resources. Further, certain implementations facilitate a degree of decentralized operations not available elsewhere.","['G06Q30/0206', 'G05B15/02', 'G06Q50/06', 'Y04S50/14']"
US20210397793A1,Intelligent Tone Detection and Rewrite,"A method and system for providing tone detection and modification for a content segment may include receiving a request to detect a tone for the content segment, inputting the content segment into a first machine-learning (ML) model to detect the tone for the content segment, obtaining the detected tone as a first output from the first ML model, inputting the content segment into a second ML model for modifying the tone from the detected tone to a modified tone, obtaining at least one rephrased content segment as a second output from the second ML model, the rephrased content segment modifying the tone of the content segment from the detected tone to the modified tone, and providing at least one of the detected tone or the at least one rephrased content segment for display to a user.","['G06F40/253', 'G06F40/30', 'G06F3/0481', 'G06F40/166', 'G06F40/205', 'G06F40/56', 'G06N20/00', 'G06Q10/10', 'G06F40/44']"
US9846752B2,System and methods for intuitive modeling of complex networks in a digital environment,"Systems and methods for modeling an electrical power system are disclosed. A computer and an analytics server are in network connection. The computer comprises a processor coupled with a memory. The memory is configured to maintain at least one engine element and a component database. The analytics server comprises a virtual system modeling engine and an analytics engine. The component database is operable to store power system components. The at least one engine element is operable to generate a virtual system model of the electrical power system and generate predicted output based on the virtual system model. The analytics engine is operable to monitor the predicted output and real-time output from at least one sensor of the electrical power system, and calibrate the virtual system model based on a difference between the predicted output and the real-time output.","['G06F17/5009', 'G06F30/20', 'G06F17/10', 'G06F2119/06', 'G06F2217/78']"
US12167100B2,"Method, apparatus, device and medium for generating captioning information of multimedia data","Embodiments of the present disclosure provide a method, an apparatus, a device, and a medium for generating captioning information of multimedia data. The method includes extracting characteristic information of multimedia data to be processed, wherein the multimedia data comprises a video or an image; and generating a text caption of the multimedia data based on the extracted characteristic information. According to the method provided in the embodiments of the present disclosure, the accuracy of the generated text caption of the multimedia data can be effectively improved.","['H04N21/4884', 'G06N20/00', 'G06F18/2431', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N5/022', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/41', 'G06V20/635', 'H04N21/23418', 'H04N21/234336', 'H04N21/26603', 'H04N21/44008', 'H04N21/4662', 'H04N21/4666', 'H04N21/8133', 'H04N21/84', 'H04N5/278']"
US11791048B2,Machine-learning-based healthcare system,"A method for providing a healthcare system. The method comprises a machine learning system: obtaining and labelling patient data to produce labelled patient data relating to one or more patients; determining one or more predictions based on the labelled patient data; receiving, from a user, one or more user inputs labelling each of the one or more predictions as a success or a failure; generating training data based on the labelled patient data and the one or more user inputs; and training the machine learning system based on the training data.","['G16H50/20', 'G16H10/20', 'G16H15/00', 'G16H20/00', 'G16H40/20', 'G16H50/30', 'G16H50/70']"
US20240273159A1,Emulating Web Browser in a Dedicated Intermediary Box,"Anonymity and privacy of a client device that fetches a content from a web server are improved by using an intermediate device located along the communication path between the client device and the web server. The primary or exclusive function of the intermediate device may be to serve as an intermediate device, and may be implemented as a stand-alone dedicated client device located at a residential premises, or may be integrated with another device, such as a router or a sensor unit, and may communicate using wired communication (such as LAN) or wireless communication (such as WLAN). The intermediate device may modify a content request from the client device in order to avoid identification or blocking by a web server that uses web tracking, such as fingerprinting. The modification may use a web browser, such as a headless browser, for emulating a different device or user.","['G06F16/955', 'G06F16/9566', 'G06F16/957', 'G06F16/9574', 'G06F16/958', 'G06F9/455', 'G06F9/4555', 'H04L63/0227', 'H04L63/0272', 'H04L63/029', 'H04L63/0876', 'H04L67/02', 'H04L67/56', 'H04L67/59']"
CN110399460B,"Dialogue processing method, device, equipment and storage medium","The invention provides a dialogue processing method, a device, equipment and a storage medium, wherein the method comprises the steps of obtaining input information of a current dialogue, encoding the input information, context information of a historical dialogue and related document information of the historical dialogue to obtain the context information of the current dialogue, performing first decoding on the input information of the current dialogue and the context information of the current dialogue to obtain intermediate decoding information of the current dialogue, and performing second decoding on the intermediate decoding information and the related document information of the current dialogue to obtain reply information of the current dialogue. By the method and the device, accurate and context-consistent reply information can be obtained.","['G06F16/3326', 'G06F16/3329', 'G06F16/3344']"
US11544886B2,Generating digital avatar,"In one embodiment, a method includes, by one or more computing systems: receiving one or more non-video inputs, where the one or more non-video inputs include at least one of a text input, an audio input, or an expression input, accessing a K-NN graph including several sets of nodes, where each set of nodes corresponds to a particular semantic context out of several semantic contexts, determining one or more actions to be performed by a digital avatar based on the one or more identified semantic contexts, generating, in real-time in response to receiving the one or more non-video inputs and based on the determined one or more actions, a video output of the digital avatar including one or more human characteristics corresponding to the one or more identified semantic contexts, and sending, to a client device, instructions to present the video output of the digital avatar.","['G06T13/40', 'G06T13/00', 'G06F40/216', 'G06F40/30', 'G06N20/00', 'G06N3/006', 'G06N3/042', 'G06N3/08', 'G06N5/022', 'G06N5/04', 'G06N5/041', 'G06V40/174', 'G06N3/044']"
US20230092027A1,"Method and apparatus for training medical image report generation model, and image report generation method and apparatus","A method for training a medical image report generation model that includes a visual feature extraction network, an encoding network, and a decoding network. The method includes: acquiring a sample medical image; extracting visual feature information of on a sample medical image through the visual feature extraction network, to obtain a visual feature sequence; concatenating a self-learning label based on the visual feature sequence, to obtain input information about the encoding network; encoding the input information through the encoding network, to obtain a visual encoding feature vector and an output task result; decoding the visual encoding feature vector through the decoding network, to obtain an output image report; and calculating a loss of the model based on the output image report and the output task result, and adjusting a parameter of the medical image report generation model according to the total loss function value.","['G06T7/0012', 'G16H10/60', 'G16H15/00', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'G16H50/70', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10104', 'G06T2207/10116', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004']"
US11610139B2,System and method for the latent space optimization of generative machine learning models,"A system and method for optimizing the latent space in generative machine learning models, and applications of the optimizations for use in the de novo generation of molecules for both ligand-based and pocket-based generation. The ligand-based optimizations comprise a tunable reward system based on a multi-property model and further define new measurable metrics: molecular novelty and uniqueness. The pocket-based optimizations comprise an initial multi-property optimization followed up by either a seed-based optimization or a relaxed-based optimization.","['G06N3/088', 'G06F16/951', 'G06F18/22', 'G06K9/6215', 'G06N3/0442', 'G06N3/045', 'G06N3/0455', 'G06N3/047', 'G06N3/0475', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06N3/092', 'G06N5/022', 'G06N7/01', 'G06V10/751', 'G06V10/82', 'G16B15/00', 'G16B15/30', 'G16B40/00', 'G16B40/20', 'G16B45/00', 'G16B50/10', 'G16C20/50', 'G16C20/70', 'G06N3/044', 'G06N3/048', 'G06N3/096', 'G16C20/30', 'G16C20/90']"
EP3913555A1,Power distribution network operation aided decision-making analysis system and method,"Provided is a power distribution network operation aided decision-making analysis system and method. The system includes a data layer, a platform layer, and an application layer. The data layer is configured to classify and store data acquired from external. The platform layer is configured to build multiple operation modules according to data types of the data classified and stored by the data layer. The application layer is configured to perform a risk analysis on a power distribution network and provide an optimization solution for a risk control measure according to the multiple operation modules built by the platform layer and the data classified and stored by the data layer and is further configured to display operation result information of the multiple operation modules.","['G06Q50/06', 'G06Q10/04', 'G06Q10/06', 'Y04S10/50']"
US20220405485A1,Natural language analysis of user sentiment based on data obtained during user workflow,"A text-based real-time communication interface, such as a chatbot, is presented to a user for the exchange of customer support information. A user's freeform text input is analyzed using machine learning algorithms to derive the meaning of the input text as well as to determine the user sentiment expressed therein. These determinations may be further supported by signals extracted from session-based activity, which signals can be used to infer the intended workflow of the user and whether or not that workflow was achieved. The expressed user sentiment is considered along with other historical or session-based user data to generate tailored questions and responses to be delivered in real-time to the user. The responses are displayed to the user along with information that routes the user to a workflow resolution.","['G06Q30/0613', 'G06F3/0482', 'G06F3/0484', 'G06F3/04842', 'G06F3/0488', 'G06F40/126', 'G06F40/166', 'G06F40/35', 'G06Q30/016', 'G06Q30/0203', 'G06Q10/02']"
CN114418030B,"Image classification method, training method and device for image classification model","The embodiment of the application provides an image classification method, an image classification model training method and an image classification model training device, and relates to the field of artificial intelligence. The method comprises the following steps: inputting an image to be classified into a coding layer of a pre-trained image classification model, and extracting vectors to obtain feature vectors, wherein the feature vectors comprise a plurality of local vectors, and each local vector corresponds to one sub-image in the image to be classified; screening the local vectors by using an attention mechanism layer in the coding layer to obtain residual local vectors; and obtaining a classification result of the image to be classified based on the residual local vector. The embodiment of the application firstly does not need an auxiliary network or introduces more parameters in the process of reducing the local vector, and can reduce the calculated amount, is also suitable for images with different resolutions and enhances the adaptability of the scheme because a lightweight image classification model can be trained from the beginning.","['G06F18/241', 'G06F18/214', 'G06N3/045', 'G06N3/084']"
US11862143B2,Systems and methods for processing speech dialogues,"The present disclosure is related to systems and methods for processing speech dialogue. The method includes obtaining target speech dialogue data. The method includes obtaining a text vector representation sequence, a phonetic symbol vector representation sequence, and a role vector representation sequence by performing a vector transformation on the target speech dialogue data based on a text embedding model, a phonetic symbol embedding model, and a role embedding model, respectively. The method includes determining a representation vector corresponding to the target speech dialogue data by inputting the text vector representation sequence, the phonetic symbol vector representation sequence, and the role vector representation sequence into a trained speech dialogue coding model. The method includes determining a summary of the target speech dialogue data by inputting the representation vector into a classification model.","['G06F40/30', 'G10L15/063', 'G06F18/24', 'G06F40/253', 'G06V10/7753', 'G10L15/02', 'G10L15/16', 'G10L15/18', 'G10L15/1822', 'G10L15/22', 'G10L2015/025', 'G10L2015/223']"
US11754997B2,"Devices, systems and methods for predicting future consumption values of load(s) in power distribution systems","Devices, systems and methods for predicting future consumption values of load(s) in power distribution systems are provided. The present disclosure provides for receiving a request for a load prediction for at least one meter; extracting time series data relating to the at least meter; retrieving future weather conditions for a particular location based on the at least one meter; and providing the extracted data and the future weather conditions to a prediction model that predicts load usage for the at least one meter. Additionally, the present disclosure provides for performing at least one action based on the prediction, wherein the action includes outputting at least one of a communication signal and/or at least one control signal to at least one client or at least one meter.","['G05B19/4155', 'H04Q9/02', 'H04Q9/00', 'G05B2219/2639', 'H04Q2209/60', 'H04Q2209/823', 'H04Q2213/08']"
US10540362B2,"Database, data structure and framework transformer apparatuses, methods and systems","The Database, Data Structure and Framework Transformer Apparatuses, Methods and Systems (â€œDDSFTâ€) transforms variable list request, population selection, base table transform extract data inputs via DDSFT components into transformed, merged data outputs. The DDSFT includes a database structure that stores data used in the framework operations. A macro-tool includes one or more macros that control a sequence of database queries that extract the data from the database structure and then perform transformations on the extracted data. The macro-tool includes a series of binary flags indicative of whether or not statements are executed.","['G06F16/254', 'G06F16/2428', 'G06F16/2456']"
US20190266246A1,Sequence modeling via segmentations,"In neural-network-based approaches to sequence modeling, an output sequence may be modeled via segmentations, the probability of the output sequence being constructed as a sum of products of output-segment probabilities, taken over all valid output-sequence segmentations. A set of artificial neural networks may model the distribution of the output-sequence probability with a recurrent neural network modeling the distributions of the individual output-segment probabilities, optionally in conjunction with a second recurrent neural network modeling concatenations of output segments. In various embodiments, this approach is applied to neural phrase-based machine translation.","['G06F17/289', 'G06F40/58', 'G06F40/44', 'G06F15/18', 'G06N20/00', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G06N7/005', 'G06N7/01', 'G10L15/28', 'G10L15/04', 'G10L15/16']"
CN114330475B,"Content matching method, apparatus, device, storage medium, and computer program product","The embodiment of the application provides a content matching method, a device, equipment, a computer readable storage medium and a computer program product, relating to the field of artificial intelligence, wherein the method comprises the following steps: determining a first feature vector of a text in the content to be queried and a second feature vector of each image, wherein the first feature vector is vectorized fine granularity representation of the text, and the second feature vector is used for representing visual features of each image; carrying out fusion processing on the second feature vectors of the images to determine the image feature vectors of the content to be queried; carrying out fusion weighting treatment between the image feature vector and the first feature vector to determine a feature embedding vector of the content to be queried; and matching similar content corresponding to the content to be queried from the plurality of contents according to the feature embedding vector of the content to be queried and the feature embedding vectors of the preset plurality of contents. The method realizes comprehensive characterization of the text and the image of the content, and improves the accuracy of content matching.",[]
CN111309915B,"Method, system, device and storage medium for training natural language of joint learning","The invention provides a natural language training method, a system, equipment and a storage medium for joint learning, wherein the method comprises the following steps: preprocessing training data, wherein each training data comprises a prediction slot position sequence and a prediction intention label which are mutually associated with an input text after being preprocessed; segmenting the input text of each training data, embedding words in an embedding layer of a pre-training model, and outputting the words to an encoder layer; obtaining a probability set of a prediction intention node and a prediction slot position node; obtaining the loss of the predicted intention nodes and the loss of the slot position nodes of the training according to the loss of the intention nodes and the loss and the accuracy of the slot position nodes; and obtaining a loss coefficient according to the matching relation between the slot position in the predicted slot position sequence and the Chinese word segmentation, and obtaining joint loss to update a unit in an encoder layer in the pre-training model. The invention accelerates the convergence of the intention identification by balancing the intention and the weight occupied in the slot position, and improves the slot position identification accuracy rate and the training speed.","['G06F16/355', 'G06N3/045', 'G06N3/084']"
CN114898121B,Automatic Generation Method of Concrete Dam Defect Image Description Based on Graph Attention Network,"The invention discloses an automatic generation method of concrete dam defect image description based on a graph attention network, which comprises the following steps: 1) Extracting local grid characteristics and whole image characteristics of the defect image by using a multi-layer convolutional neural network, and performing image coding; 2) Constructing a grid feature interaction diagram, and carrying out fusion coding on grid visual features and global image features of the defect image; 3) Global and local features are updated and optimized through a graph attention network, and improved visual features are fully utilized for defect description. The invention builds the grid feature interaction graph, updates node information by using the graph attention network, realizes the feature extraction task as the graph node classification task, does not increase calculation cost, and improves performance. The invention can capture the global image information of the defect image and capture the potential interaction of the local grid characteristics, deepen the understanding of the content of the defect image, and the generated description text can accurately and coherently describe the defect information.","['G06V10/46', 'G06N3/045', 'G06N3/08', 'G06V10/42', 'Y02A10/40', 'Y04S10/50']"
US11780472B2,Systems and methods for generating motion forecast data for a plurality of actors with respect to an autonomous vehicle,"A computing system can input first relative location embedding data into an interaction transformer model and receive, as an output of the interaction transformer model, motion forecast data for actors relative to a vehicle. The computing system can input the motion forecast data into a prediction model to receive respective trajectories for the actors for a current time step and respective projected trajectories for the actors for a subsequent time step. The computing system can generate second relative location embedding data based on the respective projected trajectories from the second time step. The computing system can produce second motion forecast data using the interaction transformer model based on the second relative location embedding. The computing system can determine second respective trajectories for the actors using the prediction model based on the second forecast data.","['B60W60/00272', 'B60W60/00274', 'B60W60/00276', 'G06N20/00', 'G06N3/045', 'G06N3/084', 'B60W2420/403', 'B60W2554/4049', 'B60W2556/40', 'B60W2556/50', 'G06N3/044']"
CN110309839B,A kind of method and device of iamge description,"The application provides a kind of method and device of iamge description, which comprises extracts model using multiple fisrt feature and carries out feature extraction to target image, obtains each fisrt feature and extract the characteristics of image that model generatesï¼›The characteristics of image that model generates is extracted to multiple fisrt feature and carries out fusion treatment, generates the corresponding global image feature of target imageï¼›Model is extracted using second feature, and the corresponding target detection feature of target image is obtained to target image progress feature extractionï¼›The corresponding global image feature of target image and target detection feature are input to translation model, using the translation sentence of generation as the descriptive statement of target image, to it is subsequent by the corresponding global image feature of target image and target detection feature be input to translation model generate translation sentence during, there is the global image feature of more rich image information as reference, keeps the translation sentence of output more accurate.","['G06V10/82', 'G06V10/424', 'G06F18/2414', 'G06F18/253', 'G06F40/40', 'G06V10/44', 'G06V10/457', 'G06V10/806', 'G06V20/70', 'G06V2201/07']"
CN115329127B,Multi-mode short video tag recommendation method integrating emotion information,"The invention discloses a multimode short video tag recommendation method fusing emotion information, which belongs to the technical field of video processing and comprises the steps of constructing a short video sample set, inputting a short video sample into an initial multimode tag recommendation model based on a multi-head attention mechanism and a self-encoder, enabling the short video sample to conduct feature extraction on images, audios and texts of the short video sample to obtain content features and emotion features, fusing by using an attention network to obtain a plurality of candidate video tags, taking a desired video tag as a target, taking the word features of the candidate video tag and the desired video tag as losses, training the initial multimode tag recommendation model to obtain a target multimode tag recommendation model, and inputting a current short video into the target multimode tag recommendation model to enable the current short video to generate a target video tag. By fusing the image features, the audio features and the text features, the method and the device can fully utilize the multi-mode information related to the video, and effectively improve the quality of the generated video tag.","['G06F16/735', 'G06F16/65', 'G06F16/75', 'G06F16/783', 'G06F16/7834', 'G06F16/7844', 'G06N3/08', 'G06V10/44', 'G06V10/764']"
US20230244869A1,Systems and methods for classification of textual works,"A system for classification of textual works includes a computing device configured to receive a first scholastic work, identify an author and a category of the first scholastic work, determine at least a work theme, calculate a reliability quantifier as a function of the at least a theme, the author, and the category, select the scholastic work as a function of the reliability quantifier, and derive, from the scholastic work, at least a correlation between a dietary practice and alleviation of a disease state.","['G06F40/30', 'G06F40/284', 'G06F18/214', 'G06V30/41', 'G06V30/414', 'G06V30/416', 'G06V30/418', 'G06V30/42']"
US11636931B2,Generation of customized personal health ontologies,"A computer system may generate a curated health ontology. The curated health ontology may be used by a requesting system. The curated health ontology generated from on a set of standard health codes combined with augmentation information, which supplements and/or otherwise changes the standard health codes. Once generated, the curated health ontology may be transmitted to the requesting system.","['G16H50/30', 'G16H10/60', 'G16H40/20', 'G16H40/63', 'G16H40/67', 'G16H50/70']"
US20220321016A1,"Multi-port power converters and power conversion systems, and methods for design and operation thereof","A multi-port power conversion system can have a multi-winding transformer and at least three ports. Each port can be coupled to the multi-winding transformer. Each port can have a semiconductor bridge and a coupling network. For each port, the semiconductor bridge can have two or more levels and can comprise at least two switches. The coupling network for each port can comprise at least one inductor. The semiconductor bridge can be coupled to the multi-winding transformer via the respective coupling network. The multi-port power conversion system can have a multi-active bridge (MAB) architecture that is universally applicable to AC-DC, DC-DC, DC-AC, and AC-AC conversion applications and extendable to any number of ports.","['H02M3/33573', 'H02M1/10', 'H02M1/4241', 'H02M1/44', 'H02M3/33561', 'H02M3/33576', 'H02M3/01', 'H02M5/2932', 'H02M7/23', 'H02M7/4837', 'H02M7/487', 'H02M7/493']"
US10866791B2,Transforming non-Apex code to Apex code,Source code of a first high-level language is received and analyzed. The source code of the first high-level language is transformed into source code of a second high-level language. The source code of the second high-level language is customized to a specific organization among a plurality of organizations that are hosted in a multitenant platform. At least a portion of the source code of the second high-level language is caused to be compiled into low-level code for execution in the multitenant platform.,"['G06F8/51', 'G06F8/315', 'G06F8/40', 'G06F8/425', 'G06F8/436']"
US11625540B2,"Encoder, system and method for metaphor detection in natural language processing","Provided is an encoder, system and method for metaphor detection in natural language processing. The system comprises an encoding module configured to convert words included in a sentence into BiLSTM representation vectors; a first encoder configured to generate a first entire representation vector of a WSD resolving task; a second encoder configured to generate a second entire representation vector of an MD task; and a multi-task learning module configured to perform knowledge transfer between the first and second encoders. Wherein, each of the first and second encoders includes a graph convolutional neural network (GCN) module configured to encode a link between a target word and a core word to generate GCN representation vectors; a control module configured to regulate the GCN representation vectors to generate an entire representation vector.","['G06F40/30', 'G06F40/205', 'G06F40/279', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
US12033612B2,"Speech synthesis method and apparatus, and readable storage medium","A speech synthesis method includes: converting a text input sequence into a text feature representation sequence; inputting the text feature representation sequence into an encoder including N encoding layers; the N encoding layers including an encoding layer Ei and an encoding layer Ei+1; the encoding layer Ei+1 including a first multi-head self-attention network; acquiring a first attention matrix and a historical text encoded sequence outputted by the encoding layer Ei, and generating a second attention matrix of the encoding layer Ei+1 according to residual connection between the first attention matrix and the first multi-head self-attention network and the historical text encoded sequence; and generating a target text encoded sequence of the encoding layer Ei+1 according to the second attention matrix and the historical text encoded sequence, and generating synthesized speech data matched with the text input sequence based on the target text encoded sequence.","['G10L13/02', 'G06N3/0455', 'G10L13/047', 'G10L19/04', 'G10L21/043', 'G06N3/0464', 'G06N3/08']"
US12352889B2,"Method, apparatus, and system for wireless sensing based on deep learning","Methods, apparatus and systems for wireless sensing based on deep learning are described. For example, a described method comprises: transmitting a wireless signal through a wireless multipath channel of a venue, wherein the wireless multipath channel is impacted by a motion of an object in the venue; receiving the wireless signal through the wireless multipath channel of the venue, wherein the received wireless signal differs from the transmitted wireless signal due to the wireless multipath channel and the motion of the object; obtaining a time series of channel information (TSCI) of the wireless multipath channel based on the received wireless signal; computing a plurality of autocorrelation functions based on the TSCI, each autocorrelation function (ACF) computed based on CI of the TSCI in a respective sliding time window; constructing at least one ACF vector, wherein each respective ACF vector is a vector associated with a respective ACF comprising multiple vector elements each associated with a respective time lag, each vector element being a value of the respective ACF evaluated at the respective time lag; rearranging the at least one ACF vector into rearranged ACF data, wherein each ACF vector is a one-dimensional (1D) ACF-block; and performing a wireless sensing task based on a task engine to do a processing using the rearranged ACF data as an input.","['G01S7/415', 'G01S13/56', 'G01S13/86', 'G01S13/88', 'H04L27/2647', 'H04L5/0051', 'H04L5/0026', 'H04L5/0048']"
US20210019674A1,Risk profiling and rating of extended relationships using ontological databases,"A system and method for understanding and analyzing risk for use in business and financial decisions. The system and method allow a user to query an individual or business and returns a profile and a rating associated with the risk of that entity. The profile consists of an advanced temporospatial weighted and directional knowledge graph that is generated by ingesting, processing, and transforming a vast amount of complex data for the purpose of human comprehension and further system analysis. Meanwhile, the rating is generated from a risk analysis algorithm that conducts a comprehensive analysis by categorizing and weighting all available risk factors. The system and method provide advanced insights and analytics into the inherent state, value, and risk associated with an entity and its relations.","['G06F15/76', 'G06F16/367', 'G06F16/9024', 'G06F16/90332', 'G06N20/00', 'G06N5/022', 'G06Q10/0635', 'G06V20/35', 'G06V20/41', 'G06V20/70', 'G06V30/274']"
US11335062B2,Automated apparel design using machine learning,"Aspects of the present disclosure provide systems, methods, and computer-readable storage media facilitating automated apparel design using deep learning techniques. For example, user instructions may be received as text data (or converted to text data from audio data representing user speech), and natural language processing (NLP) may be performed on the text data to interpret the user instructions. An apparel design may be generated in real-time/substantially real-time based on the user instructions. For example, the interpreted user instructions may be provided as input to at least one machine learning (ML) model that is configured to determine one or more visual apparel elements based on the user instructions and to generate the apparel design based on the visual apparel elements. One or more operations may be initiated based on the apparel design.","['G06T19/20', 'G06T17/20', 'A41D1/00', 'G06F3/167', 'G06F40/30', 'G06F40/35', 'G06N20/00', 'G06N3/006', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/088', 'G10L15/1822', 'G10L15/22', 'G10L15/26', 'G06F40/216', 'G06F40/289', 'G06F40/295', 'G06T2210/16', 'G06T2219/2004', 'G10L2015/223']"
US11275376B2,Large scale unmanned monitoring device assessment of utility system components,"Large-scale unmanned monitoring devices, such as unmanned aerial vehicles (UAV), drones or rovers capable of operating within environmental conditions not suitable for human personnel and lesser capable monitoring devices may inspect system components within an area of interest (AOI) such an electric power distribution system including generation, transmission, and distribution elements for autonomous detection of damage to the components. The large-scale unmanned monitoring devices may inspect the system components while the environmental conditions are occurring. Work orders for repairing the damage are autonomously generated and resources identified within the work orders are autonomously provisioned.","['G05D1/0094', 'G06Q10/06315', 'B64C39/024', 'G01N21/952', 'G01W1/00', 'G05D1/0027', 'G05D1/104', 'G06Q10/0631', 'G06Q10/20', 'B64C2201/12', 'B64U2101/31', 'Y04S10/50']"
US20180339597A1,Charging connector,"A charging station system for charging an electric vehicle includes a charging station having a controller configured to control charging of an electric vehicle, and an in-ground charging connector moveable between stowed and deployed configurations. The charging station is configured for connection to an MV electrical grid, and the controller is configured to charge a battery of an electric vehicle operationally engaging the charging station. The in-ground charging connector includes at least one charging post vertically movable between stowed and deployed positions. The at least one charging post is configured to operationally engage the electric vehicle in the deployed position to charge a battery of the electrical vehicle, and is generally disposed below a ground surface upon which the electric vehicle rests when the at least one post is in the stowed position.","['B60L53/16', 'B60L11/1818', 'B60L11/1833', 'B60L11/185', 'B60L53/11', 'B60L53/18', 'B60L53/305', 'B60L53/35', 'B60L53/36', 'B60L2230/16', 'B60L53/62', 'H02J7/0042', 'Y02T10/70', 'Y02T10/7072', 'Y02T90/12', 'Y02T90/14', 'Y02T90/16']"
US11561251B2,Remote autonomous inspection of utility system components utilizing drones and rovers,"Unmanned monitoring devices, such as unmanned aerial vehicles (UAV), drones or rovers may inspect system components within an area of interest (AOI) such an electric power distribution system including generation, transmission, and distribution elements for autonomous detection of damage to the components. Work orders for repairing the damage are autonomously generated and resources identified within the work orders are autonomously provisioned.","['G06Q10/06311', 'G01R31/086', 'B64C39/024', 'G01R31/085', 'G06N3/045', 'G06N3/08', 'G06Q50/06', 'B64C2201/127', 'B64U2101/30']"
CN207136905U,Surgery system and electric power and data interface assembly,"This disclosure relates to a kind of surgery system, includingï¼šPower supply, surgical instruments and electric power and data interface assembly.Electric power and data interface assembly include the transformer with armature winding and secondary windings, and the first modulator and the second demodulator are coupled to armature winding, and the second modulator and the first demodulator are coupled to secondary windings.Electric power signal is supplied to the first modulator by power supply.First modulator is configured to be based on the first data modulation electric power signal according to the modulation of the first kind.Modulated electric power signal is sent to the first demodulator by the first modulator by transformer.Second modulator is configured to be based on the second data modulation secondary signal according to the modulation of Second Type.Modulated secondary signal is sent to the second demodulator by the second modulator by transformer.The disclosure further relates to a kind of electric power and data interface assembly.","['A61B18/1233', 'H02J50/10', 'H02J7/0013', 'H02J7/0042', 'H02J7/0063', 'H02J7/00712', 'A61B17/295', 'A61B18/1445', 'A61B2017/320093', 'A61B2017/320094', 'A61B2017/320095', 'A61B2018/1226', 'H02J2310/22', 'H02J2310/23']"
CN111145728B,"Speech recognition model training method, system, mobile terminal and storage medium","The invention provides a method, a system, a mobile terminal and a storage medium for training a speech recognition model, wherein the method comprises the following steps: obtaining sample voice and sample text, and performing feature extraction on the sample voice to obtain voice features; respectively carrying out vector transformation on the voice characteristics and the sample text, and correspondingly carrying out coding calculation and matrix calculation according to a vector transformation result to obtain a voice coding result and a text matrix; performing normalization processing on the text matrix to obtain a probability vector, and performing loss calculation according to the probability vector and the sample text to obtain the total loss of the model; and transmitting the model total loss in the voice recognition model, and updating parameters of the voice recognition model until the voice recognition model is converged. The invention does not need to construct a pronunciation dictionary, reduces the labor cost and the time of model training, and improves the model training efficiency and the subsequent speech recognition efficiency by adopting an end-to-end architecture to update all parameters simultaneously.","['G10L15/063', 'G10L15/02', 'G10L2015/0633']"
US11573844B2,"Event-driven programming model based on asynchronous, massively parallel dataflow processes for highly-scalable distributed applications","An example method comprises receiving one or more published events by an event hook application program interface (API) from one or more client applications, passing a model to a web server configured to generate web containers in concurrent threads, receiving, by any number of worker nodes, each web container, each of the worker nodes including a system agent program for dynamically assigned functions, the web containers being provided to the any number of worker nodes for logical isolation of system agent execution in memory, and performing the dynamically assigned functions by the system agent program in a blackboard memory, the blackboard memory being a shared memory with non-blocking reads and writes and performing functionality, the dynamically assigned functions being executed in parallel and at least two of the dynamically assigned functions sharing context between inter-dependent processes.","['G06F9/542', 'G06F9/5072', 'G06F9/544']"
CN112185352B,"Voice recognition method, device and electronic equipment","The application provides a voice recognition method, a device and electronic equipment, which are suitable for the technical field of voice recognition in artificial intelligence and can realize end-side voice recognition, wherein the method comprises the following steps: and acquiring first voice data, and processing the first voice data by utilizing a voice recognition model to obtain a first voice recognition result. Wherein the speech recognition model is a model based on a transformer architecture and the speech recognition model includes an encoder network, a prediction network, and a joint network. Both the encoder network and the prediction network include convolutional networks. In an embodiment of the application, the encoder network and the prediction network of the speech recognition model both comprise convolutional neural networks. Therefore, the voice recognition model training time in the embodiment of the application is short.","['G10L15/063', 'G10L15/02', 'G10L15/04', 'G10L15/16', 'G10L15/26', 'G10L19/04']"
US20250191579A1,Data-Informed Decision Making Through a Domain-General Artificial Intelligence Platform,"A domain-general artificial intelligence platform or system and methods that enable data-informed decision making for anyone without the need for any coding ability are disclosed. This artificial intelligence platform has domain-generality, interoperability across heterogeneous sources of data, and controllability by tracking provenance. The artificial intelligence platform works by receiving a natural language query, converts the natural language query into executable code grounded in the deep semantic understanding of the underlying data, using a natural language artificial intelligence engine, runs the executable code on a distributed runtime engine to generate data output, and augments the data with a generated natural language report which becomes the ultimate output to the user.","['G10L15/16', 'G06F3/0481', 'G06F40/205', 'G06F40/30', 'G06F40/35', 'G06N3/0455', 'G06N3/084', 'G10L13/08', 'G06N3/091', 'G10L13/00', 'G10L15/26']"
US11989848B2,Browser optimized interactive electronic model based determination of attributes of a structure,"An interactive 3D electronic representation of a physical scene is executed in a browser. The browser has a limited computing capability compared to a native application or hardware usable by the computer. The interactive 3D representation is configured to minimize overall computing resources and processing time. Attributes of data items corresponding to surfaces and/or contents in the physical scene are extracted from the interactive 3D representation. Interactive verification of the attributes of the subset of data items is performed in the browser by: flattening a selected view of a ceiling, floor, or wall two dimensions; receiving user adjustments (if needed) to the dimensions and/or locations of the selected ceiling, floor, or wall; receiving user indications (if needed) of cut outs in the selected ceiling, floor, or wall; and updating the interactive 3D representation based on adjustments to the dimensions and/or locations, and/or the indications of cut outs.","['G06F3/04815', 'G06T19/20', 'G06F3/04845', 'G06V10/774', 'G06V10/7788', 'G06V10/82', 'G06V20/176', 'G06V20/20', 'G06F3/0482', 'G06T2210/04', 'G06T2219/012', 'G06T2219/2016']"
CN112528147B,"Content recommendation method and device, training method, computing device and storage medium","The disclosure provides a content recommendation method and device, a training method, a computing device and a storage medium, relates to the technical field of artificial intelligence, and particularly relates to deep learning, a neural network and big data. The content recommendation method comprises the following steps: generating a value of interest of the user to the candidate content item through the neural network based on the user information and the content information of the candidate content item; and in response to the value of interest of the candidate content item satisfying a predetermined condition, the control terminal outputs the candidate content item, wherein the user information includes historical interest information of the user, the historical interest information includes a value of a first interest feature of each of the historical interest content items of the user, and the content information of the candidate content item includes a value of the first interest feature of the candidate content item.","['G06F16/9535', 'G06N3/08']"
US20230099079A1,Scale-based user-physiological heuristic systems,"Certain aspects of the disclosure are directed to an apparatus including a scale and risk-assessment circuitry which is configured to assess a condition likely linked to the user. The scale includes a platform, and data-procurement circuitry for collecting signals specific to the user and cardio-physiological measurements. The scale includes processing circuitry to process data obtained by the data-procurement circuitry, therefrom generates cardio-related physiologic data, and sends an alert of the condition. The risk-assessment circuitry identifies a risk that the user has a condition based on the reference information and the user data provided by the scale and outputs generic information correlating to the condition to the scale that is tailored based on the identified risk.","['G01G19/50', 'A61B5/0075', 'A61B5/0205', 'A61B5/02055', 'A61B5/0535', 'A61B5/1102', 'A61B5/1128', 'A61B5/117', 'A61B5/282', 'A61B5/291', 'A61B5/318', 'A61B5/333', 'A61B5/339', 'A61B5/6887', 'A61B5/7225', 'A61B5/7246', 'A61B5/7275', 'A61B5/7282', 'A61B5/7465', 'A61B5/7495', 'A61B2090/064', 'A61B2560/0468', 'A61B2562/046', 'A61B5/0022', 'A61B5/021', 'A61B5/02438', 'A61B5/0816', 'A61B5/14532', 'A61B5/14542']"
US10338111B2,Method of monitoring operation of an electric power system and monitoring system,"In a method, operation of an electric power system which has a power utility automation system (1981-1984, 1991-1994) is monitored. The power utility automation system (1981-1984, 1991-1994) comprises a plurality of intelligent electronic devices (IEDs) (1981-1984, 1991-1994) communicating via a communication network. During operation of the electric power system, properties of the electric power system are monitored, the monitored properties comprising monitored data messages which are transmitted by the plurality of IEDs (1981-1984, 1991-1994) over the communication network. The monitored data messages are evaluated based on configuration information for the power utility automation system (1981-1984, 1991-1994) to detect a critical event. An alert signal is generated in response to detection of the critical event.","['G01R21/133', 'H04L41/0631', 'H04L63/14', 'H04L43/00', 'H04L43/12', 'Y04S40/00', 'Y04S40/166', 'Y04S40/168', 'Y04S40/20', 'Y04S40/24']"
US20220004642A1,Vulnerability analysis using contextual embeddings,"A method, a computer system, and a computer program product for vulnerability analysis using contextual embeddings is provided. Embodiments of the present invention may include collecting labeled code snippets. Embodiments of the present invention may include preparing the labeled code snippets. Embodiments of the present invention may include tokenizing the labeled code snippets. Embodiments of the present invention may include fine-tuning a model. Embodiments of the present invention may include collecting unlabeled code snippets. Embodiments of the present invention may include predicting a vulnerability of the unlabeled code snippets using the model.","['G06F21/577', 'G06F18/2155', 'G06F21/54', 'G06F21/563', 'G06F21/564', 'G06K9/6259', 'G06F2221/033', 'G06V10/82']"
US20220036564A1,Method of classifying lesion of chest x-ray radiograph based on data normalization and local patch and apparatus thereof,"Disclosed are a method of classifying lesions of chest x-ray radiographs based on data normalization and local patches and an apparatus thereof. The method includes converting an input chest x-ray radiograph into a normalized image, segmenting the converted normalized image into an organ area by using a first neural network based on a pre-learned segmentation model, generating local patches for the segmented organ area, and classifying a lesion in the input chest x-ray radiograph by using a second neural network based on a pre-learned classification model for the generated local patches.","['G06T7/143', 'G06T7/11', 'G06T7/0014', 'G06T2207/10116', 'G06T2207/20004', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30061']"
US9959514B2,Optimized asset maintenance and replacement schedule,"There are provided a system, a method and a computer program product for generating an optimal preventive maintenance/replacement schedule for a set of assets. The method includes receiving data regarding an asset, said data including a failure rate function of said asset, a cost of preventative maintenance (PM) of said asset, a cost of an asset failure, and a cost of replacing an asset. An optimal number K of preventative maintenance time intervals tk and an indication of a possible replacement is computed and stored for each asset by minimizing a mean cost-rate value function with respect to an electrical age of the asset. A first PM schedule is formed without consideration of labor and budget resource constraints. The method further generates a second maintenance schedule for a system of assets by minimizing a deviation from the optimal PM time intervals subject to the labor and budget resource constraints.","['G06Q10/06312', 'G01M99/008', 'G06Q10/20', 'Y02P90/80', 'Y02P90/86']"
CN108765187B,"Power failure fault auxiliary study and judgment method, device and system","The invention discloses a power failure fault auxiliary studying and judging method, a device and a system, wherein the method comprises the following steps: receiving an actively reported power failure event of a gateway of a platform area and/or a power failure event of a user, or receiving a repair report of the user; analyzing the user address of the received user repair report, and determining the user number or the station area number; and actively calling the power failure state of the gateway of the test platform area and/or the power failure state of a user to perform power failure fault auxiliary research and judgment. Through user address resolution positioning and power failure information auxiliary study and judgment, the success rate of the power failure fault study and judgment of the distribution room is effectively improved, the result is fed back to the power utilization user, accurate notification of the power failure information is achieved, and the high-quality service level of a power supplier and the power utilization experience of the user are effectively improved.",['G06Q50/06']
CN113316163B,Long-term network traffic prediction method based on deep learning,"The invention discloses a long-term network traffic prediction method based on deep learning, which comprises the following steps: firstly, acquiring a regional network flow sequence, and counting the flow value used by the regional network flow sequence at each moment; then preprocessing the flow matrix sequence to obtain input data of a Transformer model; secondly, establishing a Transformer model, and performing self-adaptive extraction of time correlation and space correlation on the two-dimensional matrix data by adopting the Transformer model; and finally, performing model training by adopting a self-adaptive training mechanism. The invention improves the accuracy of multi-step long-term prediction of network flow, is convenient for operators to plan network resources in the future in advance, and is beneficial to reasonable distribution of wireless resources.","['H04W24/00', 'G06N3/04', 'G06N3/08', 'H04L41/142', 'H04L41/147', 'H04W72/52']"
US10867087B2,Systems and methods for real-time DC microgrid power analytics for mission-critical power systems,"Systems and methods for performing power analytics on a microgrid. In an embodiment, predicted data is generated for the microgrid utilizing a virtual system model of the microgrid, which comprises a virtual representation of a topology of the microgrid. Real-time data is received via a portal from at least one external data source. If the difference between the real-time data and the predicted data exceeds a threshold, a calibration and synchronization operation is initiated to update the virtual system model in real-time. Power analytics may be performed on the virtual system model to generate analytical data, which can be returned via the portal.","['H02J13/00017', 'H02J13/00002', 'G06F30/00', 'G06F30/20', 'H02J13/00', 'H02J13/00001', 'H02J13/0003', 'H02J13/0006', 'H04L67/10', 'G05F1/66', 'G06F2119/06', 'G06F30/367', 'H02J2203/20', 'H02J3/00', 'Y02E60/00', 'Y04S10/40', 'Y04S40/124', 'Y04S40/20']"
US20240185588A1,Fine-tuning and controlling diffusion models,Systems and methods for fine-tuning diffusion models are described. Embodiments of the present disclosure obtain an input text indicating an element to be included in an image; generate a synthetic image depicting the element based on the input text using a diffusion model trained by comparing synthetic images depicting the element to training images depicting elements similar to the element and updating selected parameters corresponding to an attention layer of the diffusion model based on the comparison.,"['G06V10/774', 'G06N3/0455', 'G06N3/0464', 'G06N3/088', 'G06T11/00', 'G06V10/751', 'G06V10/778', 'G06V10/82']"
WO2022008677A1,Method for detecting and mitigating bias and weakness in artificial intelligence training data and models,"An exemplary embodiment may present methods for detecting bias both globally and locally by harnessing the white-box nature of the eXplainable artificial intelligence, eXplainable Neural Nets, Interpretable Neural Nets, eXplainable Transducer Transformers, eXplainable Spiking Nets, eXplainable Memory Net and eXplainable Reinforcement Learning models. Methods for detecting bias, strength, and weakness of data sets and the resulting models may be described. A first exemplary method presents a global bias detection which utilizes the coefficients of the explainable model to identify, minimize, and/or correct any potential bias within a desired error tolerance. A second exemplary method makes use of local feature importance extracted from the rule-based model coefficients to identify any potential bias locally. A third exemplary method aggregates the feature importance over the results/explanations of multiple samples. A fourth exemplary method presents a method for detecting bias in multi-dimensional data such as images. Further, a backmap reverse indexing mechanism may be implemented. A number of mitigation methods are also presented to eliminate bias from the affected models.","['G06N3/082', 'G06N5/045', 'G06F17/18', 'G06F18/10', 'G06F18/213', 'G06F18/2178', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N5/01', 'G06N5/022', 'G06V10/751', 'G06N3/048']"
US10170932B2,Energy storage modeling and control,"Systems and methods for optimal planning and real-time control of energy storage systems for multiple simultaneous applications are provided. Energy storage applications can be analyzed for relevant metrics such as profitability and impact on the functionality of the electric grid, subject to system-wide and energy storage hardware constraints. The optimal amount of storage capacity and the optimal operating strategy can then be derived for each application and be prioritized according to a dispatch stack, which can be statically or dynamically updated according to data forecasts. Embodiments can consist of both planning tools and real-time control algorithms.","['H02J15/00', 'G05B13/041', 'G05B19/0428', 'G05F1/67', 'G06Q10/06', 'H02J3/00', 'G05B2219/2639', 'Y02E40/70', 'Y02E40/76', 'Y04S10/50', 'Y04S10/545']"
US12313732B2,"Contextual visual-based SAR target detection method and apparatus, and storage medium","A contextual visual-based synthetic-aperture radar (SAR) target detection method and apparatus, and a storage medium, belonging to the field of target detection is described. The method includes: obtaining an SAR image; and inputting the SAR image into a target detection model, and positioning and recognizing a target in the SAR image by using the target detection model, to obtain a detection result. In the present disclosure, a two-way multi-scale connection operation is enhanced through top-down and bottom-up attention, to guide learning of dynamic attention matrices and enhance feature interaction under different resolutions. The model can extract the multi-scale target feature information with higher accuracy, for bounding box regression and classification, to suppress interfering background information, thereby enhancing the visual expressiveness. After the attention enhancement module is added, the detection performance can be greatly improved with almost no increase in the parameter amount and calculation amount of the whole neck.","['G06N3/045', 'G01S13/9027', 'G06N3/08', 'G06T3/4046', 'G06T7/60', 'G06T7/73', 'G06V10/225', 'G06V10/25', 'G06V10/40', 'G06V10/764', 'G06V10/766', 'G06V10/7715', 'G06V10/806', 'G06V10/82', 'G06T2207/10044', 'G06T2207/20081', 'G06T2207/20084', 'G06V2201/07']"
CN118312922B,Multi-mode network content security intelligent auditing system and method thereof,"The invention provides a multimode network content security intelligent auditing system and a method thereof, belonging to the network content security field, wherein the system comprises: the data access and preprocessing module is used for receiving the multi-mode network content data from different sources and preprocessing the multi-mode network content data; the multi-mode analysis module is used for extracting the characteristics of different modes of content from the multi-mode network content data; the strategy management module is used for configuring and managing auditing strategies of different-mode contents; the auditing engine module is used for comprehensively utilizing the rule engine and the machine learning model and auditing the multi-mode network content data according to the characteristics of different-mode content and auditing strategies; the auditing result disposal module is used for outputting and storing auditing results and triggering corresponding manual auditing or automatic disposal flow according to the auditing results. The invention comprehensively utilizes a plurality of artificial intelligence technologies to comprehensively audit texts, pictures, videos and audios, thereby greatly improving the content security management and control capability.","['G06F18/253', 'G06F18/213', 'G06F18/214', 'G06F18/241', 'G06F18/256', 'G06F18/259', 'G06N3/042', 'G06N3/045', 'G06N3/0495', 'G06N3/092', 'G06N3/094', 'G06N3/096', 'G06N3/0985', 'G06N5/022', 'G06N5/045']"
US20240211473A1,System and method for automated analysis of legal documents within and across specific fields,"Automated analysis of legal documents within and across different fields is disclosed. An extraction processor identifies and extracts knowledge from data contained in documents and transforms it into a common data form. An analysis processor develops local and global knowledge graphs containing the key entities, relationships and concepts encoded in the text.","['G06F16/245', 'G06F16/248', 'G06F16/9024', 'G06F40/30', 'G06N5/02', 'G06N5/04', 'G06N5/046']"
US12193414B2,"Animal visual identification, tracking, monitoring and assessment systems and methods thereof","An animal management system has one or more imaging devices, and a computing device coupled to the one or more image devices for receiving one or more images captured by the one or more imaging devices, processing at least one image using an artificial intelligence (AI) pipeline for: (i) detecting and locating in the image one or more animals, (ii) for each detected animal: (a) generating at least one section of the detected animal, (b) determining a plurality of key points in each section, (c) generating an embedding for each section based on the plurality of key points in the section, and (d) combining the embeddings for generating an identification of the detected animal with a confidence score. Key points and bounding boxes may also have associated confidence scores.","['G06Q50/02', 'A01K11/006', 'G06Q10/06', 'G06V10/25', 'G06V10/82', 'G06V20/63', 'G06V30/147', 'G06V40/10', 'A01K29/005']"
US20250131104A1,Methods and systems for natural language processing of graph database queries,"Methods and systems for translating a natural language user query into a graph database query are described. In some instances, the methods may comprise receiving a first input from a user comprising a natural language query regarding data in a graph database; processing the natural language query using a named entity recognition (NER) machine learning model to extract named entities from the natural language query and tag them according to an entity type; processing the tagged named entities using a semantic similarity algorithm to identify corresponding nodes and edges, and their associated properties, in the graph database; processing the natural language query using an intent classification machine learning model to determine a user intent for the natural language query; and applying a user intent-based template to the identified nodes and edges to formulate a graph database query that corresponds to the natural language query.","['G06F21/577', 'G06F16/24522', 'G06F16/9024', 'G06F16/90332', 'G06F40/169', 'G06F40/295', 'G06F21/552', 'G06F2221/034']"
US11741190B2,Multi-dimensional language style transfer,"In some embodiments, a style transfer computing system receives, from a computing device, an input text and a request to transfer the input text to a target style combination including a set of target styles. The system applies a style transfer language model associated with the target style combination to the input text to generate a transferred text in the target style combination. The style transfer language model comprises a cascaded language model configured to generate the transferred text. The cascaded language model is trained using a set of discriminator models corresponding to the set of target styles. The system provides, to the computing device, the transferred text.","['G06F40/253', 'G06F18/2148', 'G06F18/214', 'G06F18/2155', 'G06F18/24133', 'G06F40/205', 'G06F40/30', 'G06F40/40', 'G06V30/18057', 'G06V30/414', 'G06F2218/04', 'G06F40/289']"
US11657799B2,Pre-training with alignments for recurrent neural network transducer based end-to-end speech recognition,"Techniques performed by a data processing system for training a Recurrent Neural Network Transducer (RNN-T) herein include encoder pretraining by training a neural network-based token classification model using first token-aligned training data representing a plurality of utterances, where each utterance is associated with a plurality of frames of audio data and tokens representing each utterance are aligned with frame boundaries of the plurality of audio frames; obtaining first cross-entropy (CE) criterion from the token classification model, wherein the CE criterion represent a divergence between expected outputs and reference outputs of the model; pretraining an encoder of an RNN-T based on the first CE criterion; and training the RNN-T with second training data after pretraining the encoder of the RNN-T. These techniques also include whole-network pre-training of the RNN-T. A RNN-T pretrained using these techniques may be used to process audio data that includes spoken content to obtain a textual representation.","['G10L15/16', 'G10L15/063', 'G06N3/044', 'G06N3/0445', 'G06N3/08']"
CN113012686B,Neural Speech to Meaning,"The present disclosure relates to neuro-phonetic meaning. Neural speech-to-meaning systems are trained on speech audio that expresses a specific intent. The system receives voice audio and generates an indication of when the audio in the voice matches the intent. The intent may include a variable, which may have a wide range of values, such as the name of the place. The neuro-phonetic meaning system simultaneously recognizes enumerated values of variables and overall intent. The identified variable value may be used as an argument to an API request made in response to the identified intent. Thus, neural speech-to-meaning support provides a voice virtual assistant for a user to service based on API hits.","['G10L15/16', 'G06F3/167', 'G10L15/063', 'G10L15/1815', 'G10L15/1822', 'G10L15/183', 'G10L15/22', 'G10L15/26', 'G10L15/30', 'G06F40/30', 'G06F40/40', 'G10L2015/223']"
US11726997B2,Multiple stage filtering for natural language query processing pipelines,Multiple stage filtering may be implemented for natural language query processing pipelines. Natural language queries may be received at a natural language query processing system and processed through a query language processing pipeline. The query language processing pipeline may filter candidate linkages for a natural language query before performing further filtering of the candidate linkages in the natural language query processing pipeline as part of generating an intermediate representation used to execute the natural language query.,"['G06F16/24522', 'G06F16/243', 'G06F40/295', 'G06N20/00', 'G06N3/045', 'G06N3/08']"
US12007988B2,Interactive assistance for executing natural language queries to data sets,Interactive assistances for executing natural language queries to data sets may be performed. A natural language query may be received. Candidate entity linkages may be determined between an entity recognized in the natural language query and columns in data sets. The candidate linkages may be ranked according to confidence scores which may be evaluated to detect ambiguity for an entity linkage. Candidate entity linkages may be provided to a user via an interface to select an entity linkage to use as part of completing the natural language query.,"['G06F16/2423', 'G06F16/243', 'G06F16/24522', 'G06F40/295', 'G06N20/00', 'G06N3/0455', 'G06N3/08']"
US11726994B1,Providing query restatements for explaining natural language query results,Query restatements may be provided for explaining natural language query results. A natural language query is received at a natural language query processing system. An intermediate representation of the natural language query is generated for executing the natural language query. The intermediate representation is translated into a natural language restatement of the natural language query. The natural language restatement is provided with a result of the natural language query via an interface of the natural language query processing system.,"['G06F16/24522', 'G06F16/243', 'G06F16/24573', 'G06F16/248', 'G06F16/287']"
US20200151392A1,System and method automated analysis of legal documents within and across specific fields,"A system for automated analysis of legal documents within and across different fields is constructed using a computer system comprising at least one memory, at least one processor, and at least a first plurality of programming instructions stored in the at least one memory and operating on the at least one processor configured to allow the operation on the computer system of additional programming instructions, an extraction processor to identify, extract knowledge from data contained in the legal document and transform it into a common data form. The analysis processor develop a local and global knowledge graphs containing the key entities, relationships and concepts encoded in the text.","['G06F40/30', 'G06F16/254', 'G06F16/93', 'G06F40/284', 'G06F40/295', 'G06N20/00', 'G06N5/02', 'G06N5/022', 'G06N5/04', 'G06Q50/18']"
US12400302B2,"Image processing method, image processing apparatus, electronic device and computer-readable storage medium","An image processing method, an image processing apparatus, an electronic device, and a computer-readable storage medium, relating to the technical field of image processing are provided. The image processing method may include performing blur classification on pixels of an image to obtain a classification mask image; and determining a blurred area of the image based on the classification mask image.","['G06T7/0002', 'G06T5/75', 'G06N3/045', 'G06N3/08', 'G06T5/70', 'G06V10/764', 'G06V10/806', 'G06V10/809', 'G06V10/82', 'G06V10/993', 'G06T2207/20084', 'G06T2207/30168']"
CN112347612B,Modeling method and system for physical loop of secondary system of direct-current converter station,"The application discloses a physical loop modeling method and a physical loop modeling system of a secondary system of a direct current converter station, wherein the method comprises the steps of counting port types, interface types and purposes of a board card in the secondary system of the direct current converter station, and configuring a BPCD file of the port of the board card; modeling a board card port of an object in a secondary system; according to the constitution and internal connection of the screen cabinet in the secondary system, establishing a screen cabinet CPCD file; carrying out hierarchical construction and real loop configuration on different types of cabinet CPCD files to complete optical cable and cable arrangement and generate a substation SPCD file at a factory level; and analyzing the SPCD file of the transformer substation and the signal loop file to realize the virtual-real correspondence. The invention adds CPCD, configuration definition of the board card port BPCD file is used for assisting configuration of the SPCD file, and simultaneously, the invention also provides establishment of a secondary equipment board card library such as a control and protection host, thereby realizing accurate positioning of the fault of the secondary equipment board card and remote alarm and early warning.","['G06F30/20', 'G06Q50/06']"
CN113283427B,"Text recognition method, device, equipment and medium","The present disclosure provides a text recognition method, apparatus, device, and medium, wherein the method comprises: acquiring a text image to be recognized; inputting a text image to be recognized into a text recognition model obtained by pre-training; the text recognition model comprises a feature encoder and at least two feature decoders which are connected in series; performing feature extraction on a text image to be recognized through a feature encoder to obtain text character features; the text character features are subjected to multi-step iterative analysis through at least two feature decoders which are continuously connected, so that a text recognition result is obtained.","['G06V20/62', 'G06F18/241', 'G06F40/242', 'G06F40/289', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06V10/449', 'G06V10/768', 'G06V30/10']"
US11580728B2,Electrical power grid modeling,"Methods, systems, and apparatus, including computer programs encoded on a storage device, for electric grid asset detection are enclosed. An electric grid asset detection method includes: obtaining overhead imagery of a geographic region that includes electric grid wires; identifying the electric grid wires within the overhead imagery; and generating a polyline graph of the identified electric grid wires. The method includes replacing curves in polylines within the polyline graph with a series of fixed lines and endpoints; identifying, based on characteristics of the fixed lines and endpoints, a location of a utility pole that supports the electric grid wires; detecting an electric grid asset from street level imagery at the location of the utility pole; and generating a representation of the electric grid asset for use in a model of the electric grid.","['H02J13/00001', 'G06V20/182', 'G01S17/89', 'G06F16/29', 'G06F16/587', 'G06F18/24133', 'G06F30/18', 'G06K9/6271', 'G06T11/206', 'G06T17/05', 'G06T5/30', 'G06T7/12', 'G06T7/181', 'G06T7/50', 'G06T7/60', 'G06T7/73', 'G06T7/75', 'G06V20/176', 'H02J3/00', 'G06Q10/04', 'G06Q50/06', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/10032', 'G06T2207/10048', 'G06T2207/30184', 'G06V20/194', 'H02J2203/20', 'Y04S10/40']"
US20230009814A1,Method for training information recommendation model and related apparatus,"Embodiments of this application provide a for training an information recommendation model. The method includes: obtaining historical user behavior data in a plurality of product domains; generating candidate sample data of one or more target product domains according to the historical user behavior data by using a generative model; performing user-specific authenticity sample discrimination on candidate sample data of the target product domains and actual user click sample data by using a discriminative model, to obtain a discrimination result; and performing adversarial training on the generative model and the discriminative model according to the discrimination result, to obtain a trained generative adversarial network as an information recommendation model for a to-be-expanded product domain in the plurality of product domains. According to the method, the training effect of the generative model may be improved, the accuracy of generating the pseudo sample is improved, thereby further improving the recommendation effect.","['G06F16/9535', 'G06Q30/0631', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06Q30/0201', 'G06Q30/0202']"
US20210201144A1,Systems and methods for artificial intelligence enhancements in automated conversations,"Systems and methods for generating custom client intents in an AI driven conversation system are provided. Additionally, systems and methods for contact updating in a conversation between an original contact and a dynamic messaging system is provided. Additional systems and methods allow for annotation of a response in a training desk. In additional embodiments, systems and methods for model deployment in a dynamic messaging system are provided. In yet additional embodiments, systems and methods for improved functioning of a dynamic messaging system are provided. Further, systems and methods for an automated buying assistant are provided. An additional set of embodiments include systems and methods for automated task completion.","['G06F40/205', 'G06F40/35', 'G06N3/044', 'G06N3/0445', 'G06N3/045', 'G06N3/08']"
EP3144930A1,"Apparatus and method for speech recognition, and apparatus and method for training transformation parameter","Provided are a method and an apparatus for speech recognition, and a method and an apparatus for training transformation parameter. A speech recognition apparatus includes an acoustic score calculator configured to use an acoustic model to calculate an acoustic score of a speech input, an acoustic score transformer configured to transform the calculated acoustic score into an acoustic score corresponding to standard pronunciation by using a transformation parameter, and a decoder configured to decode the transformed acoustic score to output a recognition result of the speech input.","['G10L15/10', 'G10L15/08', 'G10L15/14', 'G10L15/063', 'G10L15/07', 'G10L15/18', 'G10L17/04', 'G10L17/12', 'G10L21/003', 'G10L15/183', 'G10L2015/0635', 'G10L2015/226', 'G10L2015/227', 'G10L2015/228']"
US11831682B2,Highly scalable distributed connection interface for data capture from multiple network service and cloud-based sources,"A system and method for a highly scalable distributed connection interface for data capture from multiple network service sources. The connection interface is designed to enable simple to initiate, performant and highly available input/output from a large plurality of external networked service's and application's application programming interfaces (API) to the modules of an integrated predictive business operating system. To handle the high volume of information exchange, the connection interface is distributed and designed to be scalable and self-load-balancing. The connection interface possesses robust expressive scripting capabilities that allow highly specific handling rules to be generated for the routing, transformation, and output of data within the business operating system.","['H04L63/20', 'G06F16/2477', 'G06F16/951', 'H04L41/0806', 'H04L41/147', 'H04L41/149', 'H04L43/06', 'H04L63/1408', 'H04L63/1425', 'H04L63/1441', 'H04L67/02', 'H04L41/0816', 'H04L41/145', 'H04L41/5019', 'H04L43/045', 'H04L43/0876', 'H04L63/1433', 'H04L67/10', 'H04L67/12', 'H04L67/53']"
US11176462B1,System and method for prediction of protein-ligand interactions and their bioactivity,"A system and method for computationally tractable prediction of protein-ligand interactions and their bioactivity. According to an embodiment, the system and method comprise two machine learning processing streams and concatenating their outputs. One of the machine learning streams is trained using information about ligands and their bioactivity interactions with proteins. The other machine learning stream is trained using information about proteins and their bioactivity interactions with ligands. After the machine learning algorithms for each stream have been trained, they can be used to predict the bioactivity of a given protein-ligand pair by inputting a specified ligand into the ligand processing stream and a specified protein into the protein processing stream. The machine learning algorithms of each stream predict possible protein-ligand bioactivity interactions based on the training data.","['G06V10/761', 'G06F16/951', 'G06F18/22', 'G06K9/6215', 'G06N3/044', 'G06N3/045', 'G06N5/022', 'G16B15/00', 'G16B40/00', 'G16B45/00', 'G16B50/10', 'G06N3/08', 'G06N5/02', 'G06V30/40']"
CN110795525B,"Text structuring method, text structuring device, electronic equipment and computer readable storage medium","The embodiment of the application relates to the technical field of text processing, and discloses a text structuring method, a device, electronic equipment and a computer readable storage medium, wherein the text structuring method comprises the following steps: determining a target entity keyword of a text to be structured and a candidate entity value of the target entity keyword based on a predefined template, wherein the predefined template is determined according to layout information and entity information of the text to be structured, and the entity information comprises entity keywords and format information of the entity value of the entity keywords; determining a target entity value of the target entity keyword from the candidate entity values according to the entity dependency relationship model and the predefined template; and structuring the target entity keywords and the target entity values based on the predefined templates to generate a target structure. The method of the embodiment of the application can flexibly select the corresponding predefined templates, thereby efficiently completing text structuring and realizing the full coverage of different types of texts to be structured with different formats.","['G06F16/313', 'Y02D10/00']"
CN113822192B,"Method, equipment and medium for identifying emotion of on-press personnel based on multi-mode feature fusion of Transformer","The invention relates to a method, equipment and medium for identifying emotion of a person under escort based on multi-mode feature fusion of a Transformer, which comprises the following steps: (1) data preprocessing: preprocessing text data, voice data, micro expression data and limb action data respectively to enable the text data, the voice data, the micro expression data and the limb action data to meet the input requirements of models corresponding to different modes; (2) feature extraction: extracting emotion information contained in the data of the four modes respectively to obtain corresponding feature vectors; (3) feature fusion: feature fusion is carried out on the feature vectors by adopting a trans-modal converter: and (4) training the model to obtain an optimal emotion recognition model. The invention overcomes the long-term dependence among different modes, effectively captures the complementary information among different modes and the mutual influence among the complementary information, enables the obtained combined feature vector to more comprehensively represent the correct emotion state of the person under the control of the user, and improves the accuracy of emotion recognition.","['G06F18/24', 'G06F18/253', 'G06F40/242', 'G06F40/289', 'G06N3/044', 'G06N3/045', 'G06N3/049', 'G06N3/08', 'Y02D10/00']"
CN110347835A,"Text Clustering Method, electronic device and storage medium","Disclosed herein a kind of Text Clustering Methods, this method comprises: receiving the text cluster instruction that user issuesï¼›Pre-training is carried out to predetermined opriginal language model using the corpus to be clustered, obtains target language modelï¼›Each text in the corpus to be clustered is sequentially input and carries out feature extraction in the target language model, result is exported according to model and obtains the sentence vector of each text in the corpus to be clustered, generates sentence vector set to be clusteredï¼›And using default clustering algorithm, the corpus to be clustered is clustered based on the sentence vector set to be clustered, corresponding sentence vector of all categories is obtained, and determine the cluster result of the corpus to be clustered.The present invention is also disclosed that a kind of electronic device and computer storage medium.Using the present invention, the accuracy and efficiency of text cluster can be improved.","['G06F16/35', 'G06F16/36', 'G06Q40/08']"
US11809828B2,Systems and methods of data augmentation for pre-trained embeddings,"Systems and methods are provided for generating textual embeddings by tokenizing text data and generating vectors to be provided to a transformer system, where the textual embeddings are vector representations of semantic meanings of text that is part of the text data. The vectors may be averaged for every token of the generated textual embeddings and concatenating average output activations of two layers of the transformer system. Image embeddings may be generated with a convolutional neural network (CNN) from image data, wherein the image embeddings are vector representations of the images that are part of the image data. The textual embeddings and image embeddings may be combined to form combined embeddings to be provided to the transformer system.","['G06F40/30', 'G06F17/18', 'G06F18/214', 'G06F18/2431', 'G06F18/251', 'G06F40/151', 'G06N20/10', 'G06N3/04', 'G06N3/045', 'G06N3/08', 'G06V10/40', 'G06V10/764', 'G06V10/803', 'G06V10/82', 'G06N20/00']"
US12314659B2,"Answer generating device, answer learning device, answer generating method, and answer generating program","An encoding unit transforms a piece of text divided into a plurality of spans that are subdivided units of the piece of text and a question which have been input into a vector representation sequence representing a meaning of a span and the question based on the piece of text and the question which have been input using a pre-trained encoding model for transforming input text into a vector representation sequence representing a meaning of the input text. For each of the spans, an evidence extraction unit estimates an evidence score indicating the degree to which the span is suitable as the evidence for extracting the answer using a pre-trained extraction model for calculating the evidence score based on the vector representation sequence.","['G06F40/30', 'G06F40/20', 'G06F16/90', 'G06F40/151', 'G06N3/044', 'G06N3/084']"
US10209282B2,Remote monitoring system and method for electricity demand of fused magnesium furnace group,"A remote monitoring system and method for electricity demand of a fused magnesium furnace group. The system has a data acquisition device, a local PC, a cloud server and a remote PC. The data acquisition device has a voltage transformer, a current transformer, an active power transducer, a first slave computer, a plurality of multi-purpose electronic measuring instruments and a second slave computer. The method includes acquiring smelting current and smelting power of each fused magnesium furnace and electricity demand of the furnace group, controlling the switch off/on of each fused magnesium furnace according to the smelting current and the smelting power of each fused magnesium furnace and the electricity demand of the furnace group, sending basic monitoring data to the local PC, and achieving data exchange between the local PC and the remote PC through the Zookeeper technology.","['G01R21/133', 'G01R19/2516', 'G01R21/06']"
US20180339601A1,Charging station system and method,"A charging station system for charging an electric vehicle includes a charging station having a controller configured to control charging of an electric vehicle. The charging station is configured for connection to an MV electrical grid, and the controller is configured to pulse current charge a battery of an electric vehicle operationally engaging the charging station.","['B60L11/1833', 'B60L3/0046', 'B60L11/1816', 'B60L53/11', 'B60L53/14', 'B60L53/16', 'B60L53/18', 'B60L53/30', 'B60L53/36', 'B60L53/50', 'B60L53/62', 'B60L58/25', 'Y02T10/70', 'Y02T10/7072', 'Y02T90/12', 'Y02T90/14']"
AU2023204143B2,Battery state detection system and method,"#$%^&*AU2023204143B220250710.pdf##### ABSTRACT A battery charger and method is disclosed for detecting when a battery has a low state of health while simultaneously charging or maintaining the battery. A battery charger includes a processor; a non-transitory memory device; a power management device to receive an input power and to output a charging current; a pair of electrical conductors to electrically couple with a battery, and a display electrically coupled to the processor. The display being configured to indicate a bad battery indicator when the battery has a low state of health and whether the battery is good to start. ABSTRACT A battery charger and method is disclosed for detecting when a battery has a low state of 2023204143 29 Jun 2023 health while simultaneously charging or maintaining the battery. A battery charger includes a processor; a non-transitory memory device; a power management device to receive an input power and to output a charging current; a pair of electrical conductors to electrically couple with a battery, and a display electrically coupled to the processor. The display being configured to indicate a bad battery indicator when the battery has a low state of health and whether the battery is good to start.","['H01M10/4285', 'G01R31/371', 'G01R31/3835', 'G01R31/392', 'H01M10/446', 'H01M10/48', 'H02J7/00032', 'H02J7/00047', 'H02J7/0036', 'H02J7/0049', 'H02J7/005', 'H02J7/0069', 'H02J7/00712', 'H02J7/00714', 'H02J7/00718', 'H02J7/007192', 'G01R1/203', 'G01R13/02', 'G01R19/0069', 'G01R31/006', 'G01R31/364', 'G01R31/378', 'G01R31/379', 'G01R31/3842', 'H01R13/025', 'H01R13/6616', 'H01R13/6625', 'H01R13/6683', 'H01R13/6691', 'H02J7/0048', 'H02J7/007194', 'Y02E60/10']"
US11836037B2,Systems and methods for artificial intelligence-based root cause analysis of service incidents,"Some embodiments of the current disclosure disclose methods and systems for analyzing root causes of an incident disrupting information technology services such as cloud services. In some embodiments, a set of problem review board (PRB) documents including information about said incidents may be parsed using a natural language processing (NLP) neural model to extract structured PRB data from the unstructured investigative information contained in the PRB documents. The structured PRB data may include symptoms of the incident, root causes of the incident, resolutions of the incidents, etc., and a causal knowledge graph causally relating the symptoms, root causes, resolutions of the incidents may be generated.","['G06F16/367', 'G06F11/079', 'G06F11/0706', 'G06F11/0751', 'G06F40/216', 'G06F40/279', 'G06F40/30', 'G06F11/0778']"
US11392833B2,Neural acoustic model,"An audio processing system is described. The audio processing system uses a convolutional neural network architecture to process audio data, a recurrent neural network architecture to process at least data derived from an output of the convolutional neural network architecture, and a feed-forward neural network architecture to process at least data derived from an output of the recurrent neural network architecture. The feed-forward neural network architecture is configured to output classification scores for a plurality of sound units associated with speech. The classification scores indicate a presence of one or more sound units in the audio data. The convolutional neural network architecture has a plurality of convolutional groups arranged in series, where a convolutional group includes a combination of two data mappings arranged in parallel.","['G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G10L15/16', 'G10L15/22', 'G06N3/048']"
US20210240853A1,De-identification of protected information,"The present disclosure is directed to methods and apparatus for centralized de-identification of protected data associated with subjects. In various embodiments, de-identified data may be received (1102) that includes de-identified data set(s) associated with subject(s) that is generated from raw data set(s) associated with the subjects. Each of the raw data set(s) may include identifying feature(s) that are usable to identify the respective subject. At least some of the identifying feature(s) may be absent from or obfuscated in the de-identified data. Labels associated with each of the de-identified data sets may be determined (1104). At least some of the de-identified data sets may be applied (1108) as input across a trained machine learning model to generate respective outputs, which may be compared (1110) to the labels to determine a measure of vulnerability of the de-identified data to re-identification.","['G06F21/6254', 'G16H10/60', 'G06N20/00', 'G16H10/20']"
US12046063B2,Table information extraction and mapping to other documents,"The accuracy of existing machine learning models, software technologies, and computers are improved by using one or more machine learning models to map data inside structural elements, such as rows or columns, as found within a document to data objects of other documents, where the data objects are at least partially indicative of candidate categories that the data can belong to.","['G06V30/142', 'G06V30/412', 'G06F18/22', 'G06F40/30', 'G06V30/1448', 'G06V30/413', 'G06V30/414', 'G06V2201/07', 'G06V30/18067']"
US12135940B2,Method for keyword extraction and electronic device implementing the same,"A method for keyword extraction, an apparatus, an electronic device, and a computer-readable storage medium, which relate to the field of artificial intelligence are provided. The method includes collecting feature information corresponding to an image to be processed, the feature information including text representation information and image visual information and then extracting keywords from the image to be processed based on the feature information. The text representation information includes text content and text visual information corresponding to each text line in the image to be processed. The method for keyword extraction, apparatus, electronic device, and computer-readable storage medium provided in the embodiments of the disclosure may extract the keywords from an image to be processed.","['G06F16/5846', 'G06F40/295', 'G06F40/279', 'G06V10/22', 'G06V10/82', 'G06V30/19173', 'G06V30/40', 'G06V30/416', 'G06F40/242', 'G06V30/10']"
WO2021218322A1,"Paragraph search method and apparatus, and electronic device and storage medium","A paragraph search method and apparatus, and an electronic device and a storage medium. According to the method, a text data set can be expanded on the basis of a Transformer model, so that the search comprehensiveness is improved; regression analysis is performed on the expanded data set on the basis of a BERT model to obtain a basic data set, so as to facilitate deep understanding of semantics; in response to a received question to be searched, an initial text representation of said question is determined; the initial text representation is adjusted on the basis of a named entity recognition model to obtain a target text representation of said question, so as to highlight an important word; a BM25 algorithm is used to search in the basic data set on the basis of the target text representation; and a classification model trained on the basis of a BERT algorithm is used to screen initial paragraphs, and a paragraph is output. By means of a convention and depth combined data processing form, the search speed and the accuracy of a query result are improved. In addition, the present application further relates to a blockchain technology, and the text data set can be stored in a blockchain.","['G06F40/216', 'G06F16/3329', 'G06F16/334', 'G06F16/35', 'G06F18/214', 'G06F40/194', 'G06F40/295']"
US12079821B2,Assistance for customer service agents,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for implementing assistance for customer service agents are disclosed. In one aspect, a method includes the actions of receiving, by a computing device, customer interaction data that reflects an interaction between a first user and a second user. The actions further include receiving, by the computing device, a customer summary file that reflects characteristics of the first user. The actions further include, based on the customer interaction data and the customer summary file, determining, by the computing device, instructions for the second user to continue interacting with the first user during the interaction between the first user and the second user. The actions further include, based on determining the instructions, providing, for output to the second user, the instructions for the second user to continue interacting with the first user.","['G06Q30/016', 'G06N20/00']"
US9069561B2,Methods and tools for data-driven application engineering,"The invention generally relates to enterprise computer applications, and more particularly to methods and tools for data-driven engineering of computer applications. A method of generating a business application includes inputting requirement data into a requirement model, inputting platform data into a platform model, generating a design model based on the requirement model and the platform model, generating source code of the business application from the design model, and outputting the source code.","['G06F8/35', 'G06F8/00', 'G06F8/10', 'G06F8/20']"
US20190108447A1,Multifunction perceptrons in machine learning environments,"A mechanism is described for facilitating multifunction perceptron-based machine learning in computing environments, according to one embodiment. A method of embodiments, as described herein, includes generating a multifunction perceptron architecture having a plurality of neurons to perform one or more neuron functions in a machine learning environment, wherein the plurality of neurons includes one or more of splitter neurons, mixer neurons, and counter neurons, wherein the plurality of neurons include heterogenous neurons.","['G06V10/82', 'G06N3/04', 'G06N3/045', 'G06N3/08', 'G06N3/088', 'G06N7/01', 'G06V40/28', 'G06F18/214', 'G06F18/217', 'G06K9/6256', 'G06N3/044', 'G06N3/086', 'G10L15/063']"
US10909446B2,Systems and methods for selecting global climate simulation models for training neural network climate forecasting models,"Methods and systems for generating a multi-model ensemble of global climate simulation data from a plurality of global climate simulation models (GCMs), to be used in training a neural network (NN)-based climate forecasting model, are disclosed. The methods and systems perform steps of computing a GCM validation measure for each GCM; selecting a validated subset of the GCMs, by comparing each computed GCM validation measure to a validation threshold determined based on observational historical climate data; computing a forecast skill score for each validated GCM, based on a first forecast function; selecting a validated and skillful subset of GCMs; generating one or more candidate ensembles by combining simulation data from at least two validated and skillful GCMs; computing an ensemble forecast skill score for each candidate ensemble, based on a second forecast function; and selecting a best-scored candidate ensemble. Embodiments of the present invention enable accurate climate forecasting without the need to run new dynamical global climate simulations on supercomputers.","['G06N3/0445', 'G01W1/10', 'G06F18/214', 'G06F18/217', 'G06F18/25', 'G06F30/27', 'G06K9/6256', 'G06K9/6262', 'G06N3/044', 'G06N3/045', 'G06N3/063', 'G06N3/08', 'G06V10/764', 'G06V10/80', 'G06V10/82', 'G01W2201/00', 'Y02A90/10']"
WO2022012605A1,Pre-trained deep neural network model-based user churn prediction system,"Disclosed herein is a pre-trained deep neural network model-based user churn prediction system, comprising: a course teaching platform module provides a teacher object and a student object with a work terminal platform related to a pre-determined educational course; a corpus collection module collects a voice dialogue, text dialogue and teaching video of the teacher object and the student object in a classroom during the operation of the work terminal platform; a churn status module marks classroom corpuses of lost users as churn, and marks the remaining class corpuses as non-churn; a prediction algorithm module, according to classroom corpuses collected by the corpus collection module and the classroom corpuses marked by the churn status module, calculates a user churn prediction result by means of a trained deep neural network model; and a real-time monitoring module monitors the course teaching platform module in real time, and displays the user churn prediction result calculated by the prediction algorithm module to the teacher object. The described user churn prediction system achieves the effect of adjusting an educational counseling policy.","['G06Q10/04', 'G06Q50/205']"
US20220197233A1,Wind power prediction method and system for optimizing deep transformer network,"A wind power prediction method and system for optimizing a deep Transformer network by whale optimization algorithm are disclosed. The sequence data of wind power and related influence factors are taken as sample data which is divided into a training set and a test set, where the data is trained and predicted by a Transformer network model established according to values of the initialized hyper-parameters, and an average absolute error of wind power prediction is taken as a fitness value of each whale group. A local optimal position is determined according to the initial fitness value of individual whale group, and the current optimal position is updated by utilizing whale group optimization, and the best prediction effect is obtained by comparing the local optimal solution with the global optimal solution. An optimal hyper-parameter combination is obtained after multiple iterations of the whale optimization algorithm, and the wind power is predicted.","['G05B19/042', 'G06F30/27', 'G06N3/006', 'G06N3/045', 'G06N3/08', 'H02J3/004', 'H02J3/381', 'G05B2219/2619', 'G05B2219/2639', 'H02J2203/20', 'H02J2300/28']"
US11729198B2,Mapping a vulnerability to a stage of an attack chain taxonomy,"In an embodiment, a semantic model and a semantic model training method that obtains a textual description of one or more features associated with a first vulnerability that has been used in one or more attacks. Text is parsed from the first textual description in accordance with one or more rules. The system determines a first label for the first vulnerability that is associated with one or more of a plurality of stages of an attack chain taxonomy. The model is generated or refined to map the parsed text to the first label associated with the one or more stages of the attack chain taxonomy.","['H04L63/1433', 'G06F40/205', 'G06F40/30', 'G06N20/00', 'G06N20/10', 'G06N3/044', 'G06N5/022', 'G06N5/04']"
CN114818826B,Fault diagnosis method based on lightweight Vision Transformer module,"The invention discloses a fault diagnosis method based on a lightweight Vision Transformer module, which comprises the following steps: obtaining vibration signals of mechanical equipment with various fault types as a data set, and dividing a training set and a testing set; encoding a one-dimensional vibration signal of the mechanical equipment into a two-dimensional image by using a two-dimensional image encoding method; constructing a network model by using a lightweight Vision Transformer module and extracting characteristics of a bearing dataset through the constructed network model; training the fault diagnosis model by using the converted bearing training data set; inputting the test data set into a trained fault diagnosis model to obtain a predicted classification result, and judging the fault condition of the mechanical equipment to be tested according to the classification result. The method has the advantages of strong feature extraction capability, high diagnosis accuracy and the like.","['G06F2218/08', 'G06F18/214', 'G06F18/24', 'G06N3/045', 'G06F2218/02', 'G06F2218/12']"
WO2020034419A1,Method for dynamics modeling of multi-axis robot based on axis invariant,"Provided are a method for dynamics modeling of a multi-axis robot based on an axis invariant and a computation method applicable to control of a multi-axis robot device. The multi-axis robot device comprises a link set and a joint set. The invention employs an axis set to establish a dynamical equation, and known vectors are substituted in the dynamical equation to derive unknown vectors, thereby solving mechanical problems encountered by multi-axis robot devices.","['B25J9/1607', 'B25J9/1605', 'B25J9/1671', 'G05B17/02', 'G05B2219/40527', 'G05B2219/50391', 'G06F2111/10']"
WO2020034420A1,Axis-invariant-based inverse kinematics modeling and solving method for multi-axis robot,"An axis-invariant-based inverse kinematics modeling and solving method for a multi-axis robot. The method is used for controlling a multi-axis robot device. A multi-axis robot system includes a link sequence and a joint sequence. The method comprises: using an axis sequence to correspondingly describe the multi-axis robot system, and using an axis invariant corresponding to an axis in the axis sequence and an axis chain to compute control parameters of the multi-axis robot system; and the multi-axis robot system constructing isomorphic systems in a one-to-one mapping relationship with axes by means of an axis primitive on a topology and an axis invariant primitive on metrics, and using the computed control parameters to control the multi-axis robot system.","['B25J9/1697', 'B25J9/1607', 'B25J9/0057', 'B25J9/1602', 'B25J9/1605', 'B25J9/1664', 'G05B2219/39077']"
CN113066028B,An Image Dehazing Method Based on Transformer Deep Neural Network,"The invention discloses an image defogging method based on a Transformer deep neural network, which mainly comprises the following steps of: s1, acquiring a fog/fog-free image construction data set paired in the same scene; s2, constructing an image defogging model, wherein the model consists of three branch networks; s3, inputting the foggy image I (x) into three branch networks of the image defogging model respectively to obtain a transmissivity image t (x), an atmospheric light image A (x) and a fogless image J (x) respectively; s4, reconstructing the input foggy image I '(x), I (x) and I' (x) according to the atmospheric scattering model by using the transmittance image t (x), the atmospheric light image A (x) and the fogless image J (x) to form training of a reconstruction loss constraint whole image defogging model. The image defogging method based on the Transformer is used for defogging the image, and the inherent self-attention mechanism of the Transformer is utilized, so that the global information can be captured more effectively, and the characteristics can be extracted better. Meanwhile, the traditional prior auxiliary deep neural network is utilized to achieve a better defogging effect.","['G06T5/70', 'G06N3/045', 'G06N3/08', 'G06T5/73', 'G06T2207/20081', 'G06T2207/20084']"
US11256994B1,System and method for prediction of protein-ligand bioactivity and pose propriety,"A system and method that predicts whether a given protein-ligand pair is active or inactive and outputs a pose score classifying the propriety of the pose. A 3D bioactivity platform comprising a 3D bioactivity module and data platform scrapes empirical lab-based data that a docking simulator uses to generate a dataset from which a 3D-CNN model is trained. The model then may receive new protein-ligand pairs and determine a classification for the bioactivity and pose propriety of that protein-ligand pair. Furthermore, gradients relating to the binding affinity in the 3D model of the molecule may be used to generate profiles from which new protein targets may be determined.","['G06N5/022', 'G16B15/30', 'G06F16/951', 'G06F18/22', 'G06K9/6215', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06V10/82', 'G06V20/69', 'G16B15/00', 'G16B40/20', 'G16B5/00', 'G06N3/048', 'G06N3/084', 'G06N5/02', 'G06V30/10']"
US11615324B2,System and method for de novo drug discovery,"A system and method for de novo drug discovery using machine learning algorithms. In a preferred embodiment, de novo drug discovery is performed via data enrichment and interpolation/perturbation of molecule models within the latent space, wherein molecules with certain characteristics can be generated and tested in relation to one or more targeted receptors. Filtering methods may be used to determine active novel molecules by filtering out non-active molecules and contain activity predictors to better navigate the molecule-receptor domain. The system may comprise neural networks trained to reconstruct known ligand-receptors pairs and from the reconstruction model interpolate and perturb the model such that novel and unique molecules are discovered. A second preferred embodiment trains a variational autoencoder coupled with a bioactivity model to predict molecules exhibiting a range of desired properties.","['G06N5/022', 'G06N3/088', 'G06F16/951', 'G06F18/22', 'G06K9/6215', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06N3/084', 'G06V10/82', 'G06V20/698', 'G16C60/00', 'G06N20/10', 'G16C20/30', 'G16C20/70']"
WO2024109308A1,Key video data extraction method based on multi-dimensional semantic information,"A key video data extraction method based on multi-dimensional semantic information. The method comprises: first, carrying out time domain sampling and preprocessing on an inputted video; then constructing a video background based on a Gaussian mixture model; next, in a non-background area, using a single-stage target detection network to extract and screen key targets in a video frame; using a target tracking algorithm to track the key targets and thus obtain a target bounding box sequence; calculating target motion information, calculating a quality score of an image block in each tracking bounding box, and selecting the image block having the largest quality score as a typical target image; using a target fine-grained attribute extraction model to extract target color and model sub-class information; using a Transformer-based video description generation model to generate a textual abstract of the key targets; and finally, constructing a key target multi-dimensional representation structure, and storing as key data the video background and multi-dimensional representations of all targets. In the invention, required storage space can be greatly reduced and data information density is improved.","['G06V20/40', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06T7/20', 'G06V10/20', 'G06V10/25', 'G06V10/40', 'G06V10/56', 'G06V10/764', 'G06V10/82', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30241', 'Y02D10/00']"
EP4113357A1,"Method and apparatus for recognizing entity, electronic device and storage medium","The disclosure provides a method for recognizing an entity, an apparatus for recognizing an entity, an electronic device and a storage medium, and relates to the field of computer technology, in particular to the field of artificial intelligence technology such as cloud computing, knowledge graph, and natural language processing. The method includes: obtaining message data to be processed; obtaining entity mention information by processing the message data to be processed according to a multi-pattern matching method; determining one or more candidate entities associated with the entity mention information and entity description information corresponding to the one or more candidate entities; and determining a target entity mentioned in the entity mention information according to the message data to be processed and the entity description information.","['G06F40/279', 'G06F40/295', 'G06F16/254', 'G06F16/288', 'G06F16/313', 'G06F16/3344', 'G06F16/35', 'G06F16/367', 'G06F40/216', 'G06F40/247', 'G06F40/284']"
CN112784345B,"Method, system, terminal and medium for checking consistency of graph model based on knowledge graph","The invention provides a method and a system for checking graph-model consistency based on a knowledge graph, which comprises the steps of constructing a drawing body according to drawing structural information; establishing a model body from a BIM model file; semantically fusing the drawing body and the model body to form a drawing model fusion knowledge graph; and extracting related examination points from the graph model fusion knowledge graph to complete consistency examination. A corresponding terminal and computer storage medium are also provided. The invention adopts the overall concept of respectively extracting information from the building model and the design drawing file to form a knowledge map so as to carry out consistency examination, can accurately extract the marking information in the design drawing, including the attribute information of all components, and accurately compare the information with the information in the building model, and can give the examination result aiming at the component type in the examination result.","['G06F30/13', 'G06F16/367']"
CN111081220B,"Vehicle-mounted voice interaction method, full-duplex dialogue system, server and storage medium","The embodiment of the invention provides a vehicle-mounted voice interaction method, a full-duplex dialogue system, a server and a storage medium, wherein the method comprises the following steps: receiving a voice request and inputting the voice request into an NLU model and a noise model; obtaining NLU information through an NLU model and inputting the NLU information into an NLU rule template; performing rule matching on the request by using an NLU rule template to obtain a first marking result; obtaining a second marking result through a noise model; if the two marking results are not consistent, matching the request according to the product rule; and determining according to the matching result and returning the feedback of the voice request to the client according to the final marking result of the request. The invention realizes that normal requests in a full-duplex dialogue system are not filtered by a method of combining two models with product rule matching filtering. Furthermore, the weighted proportion of the dual-model fusion can be dynamically adjusted by fusing the text anti-noise model, the request marking is more accurate, and the experience of the user using the voice assistant in the vehicle is improved.","['G10L15/18', 'G10L15/063', 'G10L15/08', 'G10L15/26', 'G10L17/22']"
CN113220919B,A cross-modal retrieval method and model for dam defect image text,"The invention discloses a dam defect image text cross-modal retrieval method based on cross attention, which comprises the following steps: 1) text-image feature extraction; 2) extracting text-image interactive features, inputting image feature maps and text feature vectors into a cross attention module, and extracting an interactive feature matrix of each image feature map and the whole text; 3) the method comprises the steps of text-image characteristic graph global similarity calculation, wherein an interactive characteristic matrix is calculated by using a cosine similarity to obtain a similarity matrix, then the global similarity is calculated respectively from a text retrieval task and an image retrieval task, and in the text retrieval task, an image characteristic graph with higher quality is endowed with higher weight when the global similarity is calculated; 4) the minimization of loss function optimizes the training results: and optimizing the training result by adopting the improved hinge ternary ordering function as a loss function and minimizing the ordering function. The invention obtains better retrieval effect in the tasks of retrieving images by using the dam image retrieval text and the dam defect description text.","['G06F16/58', 'G06F16/38', 'G06F18/22', 'G06F40/126', 'G06F40/242', 'G06F40/289', 'G06V10/25']"
US11361489B2,Dense captioning with joint interference and visual context,"A dense captioning system and method is provided for analyzing an image to generate proposed bounding regions for a plurality of visual concepts within the image, generating a region feature for each proposed bounding region to generate a plurality of region features of the image, and determining a context feature for the image using a proposed bounding region that is a largest in size of the proposed bounding regions. For each region feature of the plurality of region features of the image, the dense captioning system and method further provides for analyzing the region feature to determine for the region feature a detection score that indicates a likelihood that the region feature comprises an actual object, and generating a caption for a visual concept in the image using the region feature and the context feature when a detection score is above a specified threshold value.","['G06V20/70', 'G06F18/2411', 'G06F18/24143', 'G06K9/6269', 'G06K9/6274', 'G06T11/60', 'G06T7/11', 'G06V10/25', 'G06V10/764', 'G06V10/768', 'G06V10/82', 'G06V20/20', 'G06T2210/12']"
US11538143B2,Fully convolutional transformer based generative adversarial networks,Systems and methods for detecting anomaly in video data are provided. The system includes a generator that receives past video frames and extracts spatio-temporal features of the past video frames and generates frames. The generator includes fully convolutional transformer based generative adversarial networks (FCT-GANs). The system includes an image discriminator that discriminates generated frames and real frames. The system also includes a video discriminator that discriminates generated video and real video. The generator trains a fully convolutional transformer network (FCTN) model and determines an anomaly score of at least one test video based on a prediction residual map from the FCTN model.,"['G06N3/08', 'G06F18/22', 'G06F18/2433', 'G06K9/6215', 'G06N20/10', 'G06N20/20', 'G06N3/045', 'G06N3/0454', 'G06N3/047', 'G06N3/084', 'G06N5/046', 'G06T7/0002', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/46', 'G06T2207/20081', 'G06T2207/20084']"
CN114694220B,Double-flow face counterfeiting detection method based on Swin Transformer,"The invention relates to a double-flow face fake detection method based on a Swin Transformer, which utilizes deep learning to detect a face fake image. A deep learning network model is integrally built, and the network model is divided into three parts: a dual stream network, a feature extraction network, and a classifier. Since the presently disclosed face-counterfeit data sets are all video, the video needs to be cropped into a frame picture using OpenCV. In addition, since the frame picture contains a lot of background information, a face region needs to be cut out by using a face positioning algorithm. And then inputting the obtained face region image into a double-flow network and a feature extraction network to extract and learn features. And finally, inputting the learned characteristics into a classifier to identify the true and false of the face image. The invention is applied to solve the problem of weak generalization capability, which is a part of limitations of the existing face counterfeiting detection scheme, and simultaneously improves the compression resistance of the model through the double-flow framework, so that the model better accords with the common face video quality in daily life.","['G06F18/214', 'G06F18/24', 'G06N3/045', 'G06N3/08']"
US11887270B2,Multi-scale transformer for image analysis,"The technology employs a patch-based multi-scale Transformer (300) that is usable with various imaging applications. This avoids constraints on image fixed input size and predicts the quality effectively on a native resolution image. A native resolution image (304) is transformed into a multi-scale representation (302), enabling the Transformer's self-attention mechanism to capture information on both fine-grained detailed patches and coarse-grained global patches. Spatial embedding (316) is employed to map patch positions to a fixed grid, in which patch locations at each scale are hashed to the same grid. A separate scale embedding (318) is employed to distinguish patches coming from different scales in the multiscale representation. Self-attention (508) is performed to create a final image representation. In some instances, prior to performing self-attention, the system may prepend a learnable classification token (322) to the set of input tokens.","['G06T7/0002', 'G06T3/04', 'G06T3/0012', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/082', 'G06N3/09', 'G06T3/40', 'G06T2207/10004', 'G06T2207/20016', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30168']"
CN111241855B,"Text translation method, text translation device, storage medium and computer equipment","The application relates to a text translation method, which comprises the following steps: acquiring a word sequence and a chapter text of a source text lacking the representative words; coding the word sequence to obtain a source end vector sequence; decoding the source end vector sequence to obtain candidate translations, a target end vector sequence corresponding to each candidate translation and a translation score; respectively reconstructing the word sequence and the source end vector sequence and each target end vector sequence to obtain a reconstructed hidden layer vector sequence which corresponds to each candidate translation and comprises word information of the missing representative words; encoding a text of the discourse to obtain a discourse vector, and fusing the discourse vector and each reconstructed implicit vector sequence respectively to obtain a corresponding first fusion vector sequence; respectively carrying out reconstruction evaluation processing on the word sequence and each first fusion vector sequence to obtain a reconstruction score; and carrying out weighted summation on the translation score and the reconstruction score to obtain a comprehensive score, and screening the target translation from the candidate translations according to the comprehensive score. The method and the device can improve translation quality.",[]
US20240330589A1,Adapting foundation models for information synthesis of wireless communication specifications,"Existing approaches to understanding, developing, and researching modern wireless communication technologies involve time intensive and arduous processes of sifting through numerous webpages and technical specification documents, gathering the required information and synthesizing it. The present disclosure describes a conversational artificial intelligence tool for information synthesis of wireless communication specifications. The system builds on recent advancements in foundation large language models (LLMs) and consists of three key additional components: a domain-specific database, a context extractor, and a feedback mechanism. The system appends user queries with concise contextual information extracted from a database of wireless technical specifications and incorporates tools for expert feedback and data contribution. On evaluation using a benchmark dataset of expert queries and responses, the system provided more relevant and accurate answers on topics related to modern wireless communication specifications with a BLEU (BiLingual Evaluation Understudy) score of 0.28 compared to 0.03 achieved by current state-of-the-art LLM-based systems.","['G06F40/284', 'G06F40/205', 'G06F40/30', 'H04W24/02']"
US11423507B2,Power-based and target-based graphics quality adjustment,"An embodiment of an electronic processing system may include an application processor, persistent storage media communicatively coupled to the application processor, a graphics subsystem communicatively coupled to the application processor, a power budget analyzer to identify a power budget for one or more of the application processor, the persistent storage media, and the graphics subsystem, a target analyzer communicatively coupled to the graphics subsystem to identify a target for the graphics subsystem, and a parameter adjuster to adjust one or more parameters of the graphics subsystem based on one or more of the identified power budget and the identified target.","['G06T1/20', 'G06F1/3206', 'G06F1/3265', 'G06T1/60', 'G09G5/006', 'H04N19/156', 'G06F1/3212', 'G06T2200/16', 'G06T2210/52', 'G09G2330/021', 'G09G2340/0407', 'Y02D10/00']"
US11995560B2,Method and apparatus for generating vector representation of knowledge graph,"The present disclosure discloses a method and an apparatus for generating a vector representation of a knowledge graph, and relates to a field of a field of artificial intelligence technologies. The detailed implementing solution is: obtaining a knowledge graph, the knowledge graph including a plurality of entity nodes; obtaining a context type and context data corresponding to the knowledge graph; and generating vector representations corresponding to the plurality of entity nodes by a context model based on the context data and the context type.","['G06N5/02', 'G06N5/022', 'G06F16/31', 'G06F16/316', 'G06F16/3347', 'G06F16/35', 'G06F16/36', 'G06F16/367', 'G06F16/9017', 'G06F16/9024', 'G06F16/906', 'G06F40/279', 'G06N3/042', 'G06N3/045', 'G06N3/08']"
US10977097B2,Notifying entities of relevant events,"The present invention extends to systems and methods for notifying entities of relevant events. A boundary geometry, a user event truthfulness preference, a first event type, a second event type, and an area are combined into a rule formula. The first event type and a first event truthfulness associated with a detected event are accessed. The second event type and a second event truthfulness associated with another detected event are accessed. It is determined that the first event type and the second event type occurred in combination in the area within a boundary defined by the boundary geometry and that the first event truthfulness and the second event truthfulness both satisfy the user event truthfulness preference. An entity is automatically electronically notified in accordance with notification preferences that the rule formula was satisfied.","['G06F9/542', 'G06Q10/0637', 'G08B21/0423', 'G08B21/182', 'G08B27/001', 'G08B29/186']"
US9031824B2,Real-time predictive systems for intelligent energy monitoring and management of electrical power networks,"A system for intelligent monitoring and management of an electrical system is disclosed. The system includes a data acquisition component, a power analytics server and a client terminal. The data acquisition component acquires real-time data output from the electrical system. The power analytics server is comprised of a real-time energy pricing engine, virtual system modeling engine, an analytics engine, a machine learning engine and a schematic user interface creator engine. The real-time energy pricing engine generates real-time utility power pricing data. The virtual system modeling engine generates predicted data output for the electrical system. The analytics engine monitors real-time data output and predicted data output of the electrical system. The machine learning engine stores acid processes patterns observed from the real-time data output and the predicted data output to forecast an aspect of the electrical system.","['G06N20/00', 'G05B13/048', 'G06N5/04', 'G06N99/005', 'G06Q10/04', 'G06Q30/0206', 'G06Q30/0283', 'G06Q50/06', 'Y02P80/14', 'Y04S10/50', 'Y04S10/60', 'Y04S50/14']"
US20210278811A1,Real-time predictive systems for intelligent energy monitoring and management of electrical power networks,"A system for intelligent monitoring and management of an electrical system is disclosed. The system includes a data acquisition component, a power analytics server and a client terminal. The data acquisition component acquires real-time data output from the electrical system. The power analytics server is comprised of a real-time energy pricing engine, virtual system modeling engine, an analytics engine, a machine learning engine and a schematic user interface creator engine. The real-time energy pricing engine generates real-time utility power pricing data. The virtual system modeling engine generates predicted data output for the electrical system. The analytics engine monitors real-time data output and predicted data output of the electrical system. The machine learning engine stores and processes patterns observed from the real-time data output and the predicted data output to forecast an aspect of the electrical system.","['G05B13/048', 'G06N20/00', 'G06N5/04', 'G06Q10/04', 'G06Q30/0206', 'G06Q30/0283', 'G06Q50/06', 'Y02P80/14', 'Y04S10/50', 'Y04S50/14']"
CN108289706B,Protection technology for generators that digitally generate electrosurgical and ultrasonic electrical signal waveforms,"The invention provides a method of generating an electrical signal waveform. The generator includes a digital processing circuit, a memory circuit defining a look-up table in communication with the digital processing circuit, a digital synthesis circuit in communication with the digital processing circuit and the memory circuit, and a digital-to-analog converter (DAC) circuit. The method comprises the following steps: generating a first digital electrical signal waveform and a second digital electrical signal waveform, combining the first waveform and the second waveform to form a combined waveform, modifying the combined waveform to form a modified waveform. The peak amplitude of the modified waveform does not exceed a predetermined amplitude value. The method includes generating a second waveform that is a function of the first waveform. The method includes modifying a frequency of the first waveform to form a frequency modified first waveform and combining the frequency modified first waveform and a second waveform to form a combined waveform.","['A61B18/1206', 'A61B17/320092', 'A61B18/1445', 'G06F1/0321', 'G06F1/0342', 'H03M1/66', 'A61B2017/320069', 'A61B2018/00607', 'A61B2018/00678', 'A61B2018/00684', 'A61B2018/00994']"
US11928444B2,Editing files using a pattern-completion engine implemented using a machine-trained model,"A technique is described herein for assisting a user in editing a file. The technique involves producing current context information that includes an input message and selected file content. The input message describes a user's editing objective, while the selected file content describes a portion of the file to which the editing objective is to be applied. The technique then requests a pattern-completion engine to generate edit information based on the current context information. The edit information describes one or more changes to the selected file content that satisfy the objective of the user. The pattern-completion engine uses a machine-trained autoregressive text-completion model that is trained on revision history information. The model can be trained in a process that incorporates various tests to ensure that the edit information that is generated works as expected, satisfies various performance metrics, and fulfills the editing objectives of the user.","['G06F40/103', 'G06F8/33', 'G06F40/166', 'G06F40/20', 'G06F40/30']"
US12386913B2,Utilizing machine-learning models to generate identifier embeddings and determine digital connections between digital content items,"The present disclosure relates to systems, methods, and non-transitory computer-readable media that utilize machine learning models to generate identifier embeddings from digital content identifiers and then leverage these identifier embeddings to determine digital connections between digital content items. In particular, the disclosed systems can utilize an embedding machine-learning model that comprises a character-level embedding machine-learning model and a word-level embedding machine-learning model. For example, the disclosed systems can combine a character embedding from the character-level embedding machine-learning model and a token embedding from the word-level embedding machine-learning model. The disclosed systems can determine digital connections between the plurality of digital content items by processing these identifier embeddings for a plurality of digital content items utilizing a content management model. Based on the digital connections, the disclosed systems can surface one or more digital content suggestions to a user interface of a client device.","['G06F16/958', 'G06F16/14', 'G06F40/284', 'G06F40/30', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G06N5/02']"
US11798547B2,Voice activated device for use with a voice-based digital assistant,"A voice activated device for interaction with a digital assistant is provided. The device comprises a housing, one or more processors, and memory, the memory coupled to the one or more processors and comprising instructions for automatically identifying and connecting to a digital assistant server. The device further comprises a power supply, a wireless network module, and a human-machine interface. The human-machine interface consists essentially of: at least one speaker, at least one microphone, an ADC coupled to the microphone, a DAC coupled to the at least one speaker, and zero or more additional components selected from the set consisting of: a touch-sensitive surface, one or more cameras, and one or more LEDs. The device is configured to act as an interface for speech communications between the user and a digital assistant of the user on the digital assistant server.","['G10L15/22', 'G06F3/167', 'G10L13/00', 'G10L2015/223']"
US11423538B2,Computer-implemented machine learning for detection and statistical analysis of errors by healthcare providers,"For training data pairs comprising training text (a radiological report) and training images (radiological images associated with the radiological report), a first encoder network determines word embeddings for the training text. A concept is generated from the operation of layers of the first encoder network, which is regularized by a first loss between the generated concept and a labeled concept for the training text. A second encoder network determines features for the training image. A heatmap is generated from the operation of layers of the second encoder network, which is regularized by a second loss between the generated heatmap and a labeled heatmap for the training image. A categorical cross entropy loss is calculated between a diagnostic quality category (classified by an error encoder) and a labeled diagnostic quality category for the training data pair. A total loss function comprising the first, second, and categorical cross entropy losses is minimized.","['G16H50/20', 'G06F18/214', 'G06F18/251', 'G06K9/6256', 'G06N3/044', 'G06N3/0445', 'G06N3/045', 'G06N3/048', 'G06N3/084', 'G06N7/01', 'G06T7/0012', 'G06V10/803', 'G06V10/82', 'G06V10/98', 'G06V10/993', 'G06V30/133', 'G06V30/19147', 'G06V30/1916', 'G06V30/19173', 'G06V30/333', 'G06V30/40', 'G16H30/40', 'G06F18/22', 'G06K9/6215', 'G06T2207/20081', 'G06T2207/20084', 'G06V2201/03', 'G16H30/20']"
CN111444298B,An Address Matching Algorithm Based on POI Knowledge Graph Pre-training,"The invention discloses an address matching algorithm based on interest point knowledge graph pre-training, which comprises the following steps: after the interest point address is obtained, distinguishing administrative regions with different granularities to obtain the marked interest point address; inputting the marked interest point address into a language model by randomly covering a part of administrative regions, outputting a predicted interest point address, calculating a loss function by using the interest point address and the predicted interest point address, and obtaining the language model outputting the accurate interest point address after multiple iterations; connecting a full connection layer behind the language model, and performing integral parameter fine adjustment on the model and the full connection layer by using the labeled address matching task data set to obtain the fine-adjusted language model and the full connection layer; inputting the marked original interest point address to be predicted into the language model and the full-connection layer after fine adjustment to obtain the predicted address of the interest point to be predicted, and performing similarity calculation on the original interest point address to be predicted and the predicted address of the interest point to be predicted to complete address matching.","['G06F16/29', 'G06F16/3346', 'G06F16/3347', 'G06F16/367']"
US12282563B2,System and method for automatically detecting a security vulnerability in a source code using a machine learning model,"There is disclosed a method of automatically detecting a security vulnerability in a source code using a machine learning model, characterized in that the method comprises: obtaining the source code from a client codebase, wherein the client codebase is a complete or an incomplete body of the source code for a given software program or an application; and using a machine learning (ML) model to perform a ML based analysis on an abstract syntax tree (AST) for detecting a first security vulnerability over a static source code, the machine learning based analysis comprise (i) flattening the abstract syntax tree (AST) into a sequence of structured tokens, wherein the sequence of structured tokens comprises a semantic structure and a syntactic structure of the source code, (ii) implementing a natural language processing technique on the sequence of structured tokens for mapping the sequence of structured tokens to one or more integers, (iii) pre-training the machine learning model using an unlabeled source code as an input to predict a subsequent sub-token in the sequence of structured tokens and (iv) training the machine learning model on a labeled source code to predict a presence or an absence of the first security vulnerability.","['G06F21/57', 'G06F16/9024', 'G06F16/9027', 'G06F21/563', 'G06F21/577', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N5/01']"
US11336689B1,"Detecting phishing websites via a machine learning-based system using URL feature hashes, HTML encodings and embedded images of content pages","Disclosed is phishing classifier that classifies a URL and content page accessed via the URL as phishing or not is disclosed, with URL feature hasher that parses and hashes the URL to produce feature hashes, and headless browser to access and internally render a content page at the URL, extract HTML tokens, and capture an image of the rendering. Also disclosed are an HTML encoder, trained on HTML tokens extracted from pages at URLs, encoded, then decoded to reproduce images captured from rendering, that produces an HTML encoding of the tokens extracted, and an image embedder, pretrained on images, that produces an image embedding of the image captured. Further, phishing classifier layers, trained on the feature hashes, the HTML encoding, and the image embedding, process the URL feature hashes, HTML encoding and image embeddings to produce a likelihood score that the URL and the page accessed presents a phishing risk.","['H04L63/0281', 'H04L63/1408', 'H04L63/1483', 'H04L63/168', 'H04L67/02']"
US11710070B2,Machine learned model framework for screening question generation,"In an example embodiment, a screening question-based online screening mechanism is provided to assess job applicants automatically. More specifically, job-specific questions are automatically generated and asked to applicants to assess the applicants using the answers they provide. Answers to these questions are more recent than facts contained in a user profile and thus are more reliable measures of an appropriateness of an applicant's skills for a particular job.","['G06N20/00', 'G06N3/042', 'G06N3/045', 'G06N3/08', 'G06N5/04', 'G06Q10/1053', 'G06N5/022']"
US12051980B2,Variable inverter/rectifier/transformer,"Described is a hybrid electronic and magnetic structure that enables a transformer with fractional and reconfigurable effective turns ratios (e.g. 12:0.5, 12:2/3, 12:1, and 12:2) and hereinafter referred to as a Variable-Inverter-Rectifier-Transformer (VIRT). A VIRT is valuable in converters having wide operating voltage ranges and high step-up/down, as it offers a means to reduce turns count and copper loss within a transformer while facilitating voltage doubling and quadrupling. Such characteristics are beneficial for reducing the size of a transformer stage in many power electronics applications, such as USB wall chargers. In embodiments, a VIRT comprises a plurality of switching cells distributed around a magnetic core and coupled to half-turns wound through that core. By controlling operating modes of the switching cells, it is possible to gain control over flux paths and current paths in the transformer.","['H02M3/33584', 'H01F27/2804', 'H01F27/38', 'H01F2027/2819']"
CN111401077B,Language model processing method and device and computer equipment,"The application relates to a language model processing method, a language model processing device, computer equipment and a storage medium. The method comprises the following steps: obtaining a statement sample; performing word segmentation on the sentence sample to obtain a sample word segmentation sequence; inputting the sample word segmentation sequence into a pre-training language model; processing the sample word segmentation sequence through a plurality of processing layers which are sequentially connected in the pre-training language model to obtain a first layer processing result corresponding to each processing layer, wherein the first layer processing results output by the plurality of processing layers which are sequentially connected are respectively expressed by languages of different levels; based on the language representation corresponding to the processing layer, performing feature enhancement processing on the corresponding first layer processing result to obtain a second layer processing result corresponding to the processing layer; and adjusting the pre-training language model according to the second layer of processing results, and continuing training until a trained language model is obtained. By adopting the method, the recognition accuracy of the trained language model can be improved.","['G06F16/3334', 'G06F16/334', 'G06F16/338']"
CN114036300B,"Training method and device of language model, electronic equipment and storage medium","The application provides a training method, a device, electronic equipment and a storage medium of a language model, which comprise the steps of obtaining initial text data related to a preset natural language processing task according to the preset natural language processing task, obtaining a text loading template corresponding to the preset natural language processing task and used for loading training text samples, loading the initial text data according to the text loading template to obtain training text samples used for training the language model, wherein the training text samples comprise identification information training texts and non-identification information training texts, and iteratively updating the initial language model by using the training text samples to generate the target language model. According to the application, the training text with the identification information and the training text without the identification information are used for synchronously training the language model, so that the model precision can be improved.","['G06F16/35', 'G06F40/186', 'G06F40/279']"
US10850840B2,Drone and rover preplacement for remote autonomous inspection of utility system components,"Unmanned monitoring devices, such as unmanned aerial vehicles (UAV), drones or rovers may survey an area of interest (AOI) such an electric power distribution system including generation, transmission, and distribution elements to automatically determine components in the AOI to be monitored. Types, placements locations, and pathways taken by other unmanned monitoring devices to facilitate the monitoring of the components of the AOI are also automatically determined.","['B64C39/024', 'H02G1/02', 'B64C2201/126', 'B64C2201/141', 'B64U2101/30', 'B64U2201/10']"
US11636341B2,Processing sequential interaction data,"This disclosure relates to processing sequential interaction data through machine learning. In one aspect, a method includes obtaining a dynamic interaction graph constructed based on a dynamic interaction sequence. The dynamic interaction sequence includes interaction feature groups corresponding to interaction events. Each interaction feature group includes a first object, a second object, and an interaction time of an interaction event that involved the first object and the second object. The dynamic interaction graph includes multiple nodes including, for each interaction feature group, a first node that represents the first object of the interaction feature group and a second node that represents the second object of the interaction feature group. A current sequence corresponding to a current node to be analyzed is determined. The current sequence is input into a Transformer-based neural network model. The neural network model determines a feature vector corresponding to the current node.","['G06F16/951', 'G06N3/08', 'G06F16/212', 'G06N3/04', 'G06N3/045', 'G06N3/084', 'G06N5/022']"
US11086687B2,Managing resource allocation in a stream processing framework,"The technology disclosed herein relates to method, system, and computer program product (computer-readable storage device) embodiments for managing resource allocation in a stream processing framework. An embodiment operates by configuring an allocation of a task sequence and machine resources to a container, and by running the task sequence, wherein the task sequence is configured to be run continuously as a plurality of units of work corresponding to the task sequence. Some embodiments further include changing the allocation responsive to a determination of an increase in data volume. A query may be taken from the task sequence and processed. Responsive to the query, a real-time result may be returned. Query processing may involve continuously applying a rule to the data stream, in real time or near real time. The rule may be set via a query language. Additionally, the data stream may be partitioned into batches for parallel processing.",['G06F9/5083']
CN110347799B,Language model training method and device and computer equipment,"The application relates to a language model training method, a device and computer equipment, wherein the method comprises the following steps: acquiring a text to be processed and a corresponding tag word; inputting the text to be processed into a language model for processing to obtain a first probability corresponding to a first word to be selected and a second probability corresponding to a second word to be selected, wherein the first word to be selected is a word in a preset word list, and the second word to be selected is a word in the text to be processed; processing according to the first probability and the second probability to obtain a target word; and adjusting parameters of the language model according to the difference between the target word and the tag word until the training stopping condition is met, so as to obtain the trained target language model. The scheme provided by the application can improve the accuracy of language model prediction.","['G06F16/3344', 'G06F16/35', 'G06F40/289', 'G06F40/30']"
CN111930942B,"Text classification method, language model training method, device and equipment","The application discloses a text classification method, a language model training method, a device and equipment, and relates to the technical field of artificial intelligence and natural language processing. The method comprises the following steps: acquiring an input text; acquiring relation characterization vectors between words of an input text, between words and between words, wherein the relation characterization vectors are used for representing semantic relations; determining an associated feature vector based on the relationship characterization vector, the associated feature vector being used to characterize semantic relevance between different dimensional features of the relationship characterization vector; based on the associated feature vector, a classification result of the input text is determined. The method and the device realize the acquisition of rich relation characterization vectors used for representing semantic relations, and through the acquisition of semantic relativity among different dimensional characteristics of the relation characterization vectors, so as to mine and enrich deeper semantic information, fully improve semantic understanding capability of input texts, and further improve final classification accuracy.","['G06F16/355', 'G06F40/205', 'G06F40/30', 'G06N20/00']"
CN112732919B,Intelligent classification label method and system for network security threat information,"The invention provides an intelligent label classification method and system for network security threat intelligence. The method comprises the steps of preprocessing text data of social network text data related to network threats, acquiring a vector representation form, and inputting a BERT pre-training model for training; converting the full connection layer and the linear classifier of the feedforward neural network into vectors consistent with the vector dimension of the classification label; calculating and updating model parameters according to a cross entropy loss function until the model is converged to obtain a threat intelligence classification model; and searching, collecting, preprocessing and classifying the labels of the social network text data in sequence according to the threat intelligence classification model to obtain a classification label result. According to the scheme, the pre-training model is adopted to learn the relation between the context semantic information and the sentences of the text data, the semantic representation is obtained, the accuracy of the generated threat information classification model is high, the training efficiency can be improved, and the direct model training time is shortened.","['G06F16/353', 'G06F16/355', 'G06F16/951', 'G06N3/045', 'G06N3/088', 'H04L63/30']"
CN106253487B,Intelligent power server applied to intelligent substation protection control system,"The present invention relates to the intelligent electric power servers for being applied to intelligent substation Protection control system.The spaced conjunction intelligence integrated unit of institute is directly accessed intelligent electric power server in intelligent substation in the present invention, and each conjunction intelligence integrated unit exchanges communication network by embedded high bandwidth and accesses a transmission port on intelligent electric power serverï¼›Intelligent electric power server realizes the exchange and motor function of the monitoring and protection, whole station information of whole station primary equipment.Using technical solution of the present invention, the quantity of secondary device and substation's occupied area in substation can be reduced, reduce the construction maintenance difficulty of substation, and to realize that cloud control, cloud service, energy big data and the energy internet of substation provide solution, popularization of the invention simultaneously will push the fusion and deployment of distribution and micro-capacitance sensor construction new technology, the technology greatly improves the overall stability and reliability of substation, provides technical guarantee for the diversity access of the power grid energy.","['H02J13/0013', 'H02J13/00016', 'G06F1/26', 'H02J13/00019', 'H02J13/00034', 'H02J13/0004', 'H02J13/0062', 'H02J13/00017', 'Y02E60/00', 'Y04S10/16', 'Y04S10/18', 'Y04S40/124']"
CN108931972B,A kind of substation secondary device condition intelligent diagnostic method based on model-driven,"The invention discloses a kind of the substation secondary device condition intelligent diagnostic method based on model-driven, calculating primary equipment operating status topological based on primary equipmentï¼›Based on IEC61850 model configuration file, substation secondary device topology is established, and combines the listed information analysis secondary device operating status of secondary deviceï¼›Rule is moved back according to the throwing of substation secondary device soft pressing plate, intelligent diagnostics service logic model library is established by standardization, and be described using the logic description language based on XMLï¼›Service logic model is obtained by load intelligent diagnostics model library, and it is automatically performed the mapping of service logic model and monitoring system of electric substation database, intelligent diagnostics model instance is generated, the related real time data for being then based on monitoring system of electric substation database completes the diagnosis of secondary device stateï¼›The present invention pushes the standardization of substation secondary device condition diagnosing function, standardization.",['G05B23/0245']
CN111164654B,Adaptive fovea encoder and global motion predictor,"Embodiments of an adaptive video encoder may include techniques to determine information related to a head-mounted device including at least one of focus-related information and motion-related information, and to determine one or more video encoding parameters based on the information related to the head-mounted device. Other embodiments are disclosed and claimed.","['G02B27/0093', 'G02B27/017', 'G06F3/011', 'G06F3/012', 'G06F3/013', 'G06F3/017', 'G06F3/0346', 'H04N19/00', 'H04N19/124', 'H04N19/154', 'H04N19/162', 'H04N19/167', 'H04N19/176', 'H04N19/189', 'H04N19/52', 'H04N19/527', 'H04N21/234345', 'H04N21/234354', 'H04N21/42202', 'H04N21/44218', 'H04N21/4728', 'H04N21/6587', 'G02B2027/0138', 'G02B2027/0187', 'H04N13/161', 'H04N19/30', 'H04N19/46', 'H04N19/50', 'H04N19/85']"
US12205691B2,Systems and methods for computing with private healthcare data,Techniques are provided for computing with private healthcare data. The techniques include a method comprising constructing an isolated memory partition that forms a secure enclave and pre-provisioning software within the secure enclave. The pre-provisioned software is configured to receive at least one of input data or the instructions for the one or more application computing processes in an encrypted form; decrypt the at least one of input data or instructions using one or more cryptographic keys; execute the one or more application computing processes based on the decrypted at least one of input data or instructions to generate output data; generate a proof of execution that indicates that the one or more application computing processes operated on the received input data; encrypt the output data using the one or more cryptographic keys; and provide external access to the encrypted output data and the proof of execution.,"['G16H10/60', 'G06F21/53', 'G06F21/602', 'G06F21/6245', 'G06F21/6254', 'G06F21/64']"
US20220398635A1,Holistic analysis of customer sentiment regarding a software feature and corresponding shipment determinations,"A preliminary software feature is applied in a testing rollout to a discrete subset of customers. Survey data may be collected from those customers through a variety of sources, such as chatbot text, session workflow, historical user data, social media data, email survey data, user profile data, messaging threads, and the like. This survey data is analyzed using machine learning algorithms to derive the meaning of input text as well as to determine the user sentiment expressed therein. The outputs of this analysis are normalized across sources and aggregated at a feature-level to generate overall metrics of customer satisfaction with the feature. A holistic analysis is performed on this customer sentiment data to obtain an aggregate or combined user satisfaction score. This score is applied against a set of guardrails to determine whether to ship the feature to a broader customer base.","['G06Q30/0282', 'G06Q30/0203', 'G06Q30/0204', 'G06Q50/01']"
US11475590B2,Keypoint based pose-tracking using entailment,"Aspects of the present disclosure describe systems, methods and structures for an efficient multi-person posetracking method that advantageously achieves state-of-the-art performance on PoseTrack datasets by only using keypoint information in a tracking step without optical flow or convolution routines. As a consequence, our method has fewer parameters and FLOPs and achieves faster FPS. Our method benefits from our parameter-free tracking method that outperforms commonly used bounding box propagation in top-down methods. Finally, we disclose tokenization and embedding multi-person pose keypoint information in the transformer architecture that can be re-used for other pose tasks such as pose-based action recognition.","['G06T7/73', 'G06T7/74', 'G06T7/246', 'G06T2207/10016', 'G06T2207/20084', 'G06T2207/30196']"
US12216799B2,Systems and methods for computing with private healthcare data,"Techniques are provided for computing with private healthcare data. The techniques include a de-identification method including receiving a text sequence; providing the text sequence to a plurality of entity tagging models, each of the plurality of entity tagging models being trained to tag one or more portions of the text sequence having a corresponding entity type; tagging one or more entities in the text sequence using the plurality of entity tagging models; and obfuscating each entity among the one or more tagged entities by replacing the entity with a surrogate, the surrogate being selected based on one or more attributes of the entity and maintaining characteristics similar to the entity being replaced.","['G06F21/6254', 'G06F21/53', 'G06F21/602', 'G06F21/6245', 'G06N20/00', 'G06N3/044', 'G06N3/047', 'G06N3/088', 'G16H10/60', 'G16H15/00', 'G16H50/70', 'G16H70/40', 'G16H70/60', 'G06N3/045', 'G06N5/02']"
WO2022195285A1,Image processing using machine learning,"A method of processing image data, comprises receiving one or more target images and at least one reference source. The method processes the at least one reference source and the one or more target images to extract features using a convolutional neural network. In addition the method processes the features of the one or more target images and the at least one reference source using a transformer network to provide an attention output. The attention output is provided as an input to a convolutional neural network decoder. In addition skip connections are provided from the convolutional neural network encoder to provide features of the one or more target images to the convolutional neural network decoder at one or more decoder layers. Finally, the extracted features of the one or more target images and the attention output are processed using the convolutional neural network decoder to produce one or more output images. The output images take the style of the reference source. In one example, this style is by taking colourisation from the reference source. In another example interpolation is provided between two images and the reference source itself comprises two images.","['G06T5/77', 'G06T11/00', 'G06N3/045', 'G06N3/084', 'G06T5/50', 'G06T11/001', 'G06T2207/20084']"
US20220067513A1,Efficient softmax computation,"Solutions improving efficiency of Softmax computation applied for efficient deep learning inference in transformers and other neural networks. The solutions utilize a reduced-precision implementation of various operations in Softmax, replacing ex with 2x to reduce instruction overhead associated with computing ex, and replacing floating point max computation with integer max computation. Further described is a scalable implementation that decomposes Softmax into UnNormalized Softmax and Normalization operations.","['G06F7/544', 'G06N3/047', 'G06F7/22', 'G06F7/50', 'G06F7/552', 'G06N3/045', 'G06N3/061', 'G06N3/063', 'G06N3/08']"
US12301171B2,Power amplifier antenna structure,"Integrated Doherty power amplifiers are provided herein. In certain implementations, a Doherty power amplifier includes a carrier amplification stage that generates a carrier signal, a peaking amplification stage that generates a peaking signal, and an antenna structure that combines the carrier signal and the peaking signal. The antenna structure radiates a transmit wave in which the carrier signal and the peaking signal are combined with a phase shift.","['H03F1/0288', 'H01P5/16', 'H01Q21/065', 'H01Q23/00', 'H01Q9/0407', 'H01Q9/045', 'H03F3/195', 'H03F3/213', 'H03F3/245', 'H01L2224/05554', 'H01L2224/48091', 'H01L2224/48227', 'H01L2224/49171', 'H01L2924/15184', 'H01L2924/181', 'H03F2200/451']"
US11640527B2,Near-zero-cost differentially private deep learning with teacher ensembles,"Systems and methods are provided for near-zero-cost (NZC) query framework or approach for differentially private deep learning. To protect the privacy of training data during learning, the near-zero-cost query framework transfers knowledge from an ensemble of teacher models trained on partitions of the data to a student model. Privacy guarantees may be understood intuitively and expressed rigorously in terms of differential privacy. Other features are also provided.","['G06N3/08', 'G06N3/04', 'G06N3/045', 'G06N3/0454', 'G06N3/082', 'G06T2207/00', 'G06T2207/20081', 'G06T2207/20084']"
CN113780149B,An efficient method for extracting building targets from remote sensing images based on attention mechanism,"The invention discloses a remote sensing image building target efficient extraction method based on an attention mechanism. The method comprises the following specific steps: 1. reading in image data and preprocessing; 2. constructing a remote sensing image building target efficient extraction network based on an attention mechanism; 3. training a neural network to obtain model parameters; 4. and extracting the building target by the remote sensing image. The network model of the invention constructs global context features on sparse features of shallow feature maps by using a transducer. The application of the transducer on the shallow feature map can well keep local details for identifying boundaries; only using sparse semantic words can significantly improve the speed of the network and reduce the memory consumption of the computer; meanwhile, the constructed global receptive field can greatly reduce the interference of complex background. The remote sensing image building extraction method disclosed by the invention is used for inputting the remote sensing image and outputting the remote sensing image as the binary mask of the building target, has high automation degree and high information analysis speed, and can greatly improve the efficiency and reduce the cost.","['G06N3/045', 'G06N3/08', 'Y02D10/00']"
WO2023065619A1,Multi-dimensional fine-grained dynamic sentiment analysis method and system,"Provided in the present disclosure are a multi-dimensional fine-grained dynamic sentiment analysis method and system. The method comprises: predicting and completing missing data by using the relevant experience of multi-modal data; performing entity-level sentiment analysis on text, so as to obtain sentiments for different entities, and then performing secondary segmentation on the multi-modal data by using a result of entity segmentation; respectively calculating features of multiple modals, i.e. a text modal, an audio modal and an image modal; by means of a fused feature of the multi-modal data, predicting a sentiment by using a trained classification model; and on the basis of historical sentiment data of an entity, predicting a future sentiment trend of the entity.","['G06F16/9536', 'G06N3/045', 'G06N3/049', 'G06N3/08', 'G06Q50/01']"
US9842000B2,Managing processing of long tail task sequences in a stream processing framework,"The technology disclosed relates to managing processing of long tail task sequences in a stream processing framework. In particular, it relates to operating a computing grid that includes a plurality of physical threads which processes data from one or more near real-time (NRT) data streams for multiple task sequences, and queuing data from the NRT data streams as batches in multiple pipelines using a grid-coordinator that controls dispatch of the batches to the physical threads. The method also includes assigning a priority-level to each of the pipelines using a grid-scheduler, wherein the grid-scheduler initiates execution of a first number of batches from a first pipeline before execution of a second number of batches from a second pipeline, responsive to respective priority levels of the first and second pipelines.","['G06F9/5038', 'G06F9/5072', 'G06F16/24568', 'G06F17/30516', 'G06F9/5088']"
US8572566B2,Systems and methods for analyzing changes in application code from a previous instance of the application code,"The present application is directed towards systems and methods for analyzing and transforming changes in customized code of an enterprise resource planning (ERP) application from a previous instance of transformation of the customized code of the ERP application. Customized functions, objects, databases, and code of the application may be analyzed to identify changes in application code from a previous instance of the application code. Changed code may be further analyzed to determine which portions violate a predetermined set of coding rules of the application. Portions that violate the predetermined set of coding rules may be enumerated by one of developer, time, or type of object. Transformation rules may be applied to the code that violates the predetermined coding rules to generate a transformed instance of the application that is in conformance with the coding rules.","['G06F8/65', 'G06F8/71']"
US12008333B2,"Computer implemented methods for the automated analysis or use of data, including use of a large language model","Methods are provided, such as a method of interacting with a large language model (LLM), including the step of a processing system using a structured, machine-readable representation of data that conforms to a machine-readable language, such as a universal language, to provide new context data for the LLM, in order to improve the output, such as continuation text output, generated by the LLM in response to a prompt; and such as a method of interacting with a LLM, including the step of providing continuation data generated by the LLM to a processing system that uses a structured, machine-readable representation of data that conforms to a machine-readable language, such as a universal language, in which the processing system is configured to analyse the continuation output generated by the LLM in response to a prompt to enable an improved version of that continuation output to be provided to a user. Related computer systems are provided.","['G06F40/56', 'G06F40/205', 'G06F40/30', 'G06N3/0442', 'G06N3/0895']"
US20240412126A1,Talent platform exchange and recruiter matching system,"According to various aspects, systems and methods are provided for automatically matching recruiters to job openings. The system may train one or more models that determine a measure of compatibility between a recruiter and a job opening. The system may train the model(s) using stored records of activity tracked by the system. The measure of compatibility may be used to determine whether a recruiter is likely to place a candidate for a job opening. Some embodiments provide a system that can collect objective data about recruiters and hiring parties, and use the objective data to train the model(s).","['G06Q10/063112', 'G06Q10/1053']"
US9965330B2,Maintaining throughput of a stream processing framework while increasing processing load,"The technology disclosed relates to maintaining throughput of a stream processing framework while increasing processing load. In particular, it relates to defining a container over at least one worker node that has a plurality workers, with one worker utilizing a whole core within a worker node, and queuing data from one or more incoming near real-time (NRT) data streams in multiple pipelines that run in the container and have connections to at least one common resource external to the container. It further relates to concurrently executing the pipelines at a number of workers as batches, and limiting simultaneous connections to the common resource to the number of workers by providing a shared connection to a set of batches running on a same worker regardless of the pipelines to which the batches in the set belong.","['G06F9/505', 'G06F3/0613', 'G06F3/0631', 'G06F3/067', 'G06F9/5061']"
US20200293032A1,Extremely fast substation asset monitoring system and method,"embodiments are directed to a system, method, and article for monitoring a power substation asset. During an offline analysis mode, training data may be acquired and processing, and one or more classifiers may be generated for an online anomaly detection and localization mode. During the online anomaly detection and localization mode, power system related data may be received from field devices, a state of a substation system and of the power substation asset component and an unclassified state of one or instances may be generated based on the one or more classifiers. An alert may be generated to indicate the state of the substation system and of the power substation asset.","['G01R19/2513', 'G05B23/0254', 'G01R31/086', 'G05B15/02', 'G05B23/0208', 'G05B23/0221', 'G05B23/024', 'G05B23/027', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/084', 'G06N3/088', 'G06N5/04', 'H02J13/00', 'H02J13/00002', 'H02J13/00034', 'H02H1/0092', 'H02J13/00001', 'H02J2203/20', 'Y02B90/20', 'Y02E40/70', 'Y02E60/00', 'Y04S10/22', 'Y04S10/30', 'Y04S10/40', 'Y04S10/50', 'Y04S20/00', 'Y04S40/20']"
CN118413402B,Malicious domain name detection method based on large language model,"The invention relates to a malicious domain name detection method based on a large language model, which solves the defect that the malicious domain name is difficult to detect compared with the prior art. The invention comprises the following steps: constructing a pre-training data set and a fine-tuning training data set; setting a URL-BERT model; pre-training of URL-BERT model; fine tuning of URL-BERT model; obtaining a domain name to be detected; obtaining a malicious domain name detection result. The invention adopts the big language model BERT to process the malicious domain name, and utilizes the strong semantic understanding capability of the big language model to better capture the implicit information and the context in the domain name and improve the identification accuracy of the malicious domain name.","['G06F18/214', 'G06F16/955', 'G06F18/2431', 'G06F18/253', 'G06N3/0464', 'H04L61/4511', 'H04L63/1408', 'H04L9/40']"
CN114556354B,Automatically determine and present personalized action items from events,"A computerized system is provided for automatically determining action items for an event such as a meeting. The determined action item may be personalized for a particular user, such as a meeting attendee, and may include contextual information that enables the user to understand the action item. Specifically, personalized action items may be determined based in part on determining and utilizing specific factors in connection with an event dialog, such as the language style of the event speaker, user roles in the organization, historical patterns of communication, event purpose, name or location, event participants or other contextual information. A particular statement is evaluated to determine if the statement may or may not be an action item. The context information may be determined for the action item and then provided to a particular user during or after the event.","['G06F40/20', 'G06F40/30', 'G06N20/20', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/096', 'G06N5/046', 'G06Q10/109', 'G06Q10/1093', 'G06N3/044', 'G06N3/047', 'G06N7/01']"
US11775572B2,Directed acyclic graph based framework for training models,"Techniques for chatbots, and more particularly, to techniques for using a directed acyclic graph (DAG) based framework to build and train models. In one particular aspect, a computer implemented method is provided that includes generating, by a DAG based framework, a first model and a second model, executing the first model for a chatbot in run-time and second model for the chatbot in design-time, collecting attributes for intent classification associated with a set of utterances with the chatbot running the first model and the second model, evaluating, using one or more metrics, performance of the first model and the second model based on an analysis of the attributes for the intent classification, determining whether the performance of the second model is improved as compared to the performance of the first model, and executing the first model or the second model for the chatbot in run-time based on the performance determination.","['G06N3/008', 'G06F16/3347', 'G06F16/9024', 'G06F18/24', 'H04L51/02']"
CN112560479B,"Summary extraction model training method, summary extraction method, device and electronic equipment","The application discloses a abstract extraction model training method, an abstract extraction device, electronic equipment and a storage medium, and relates to the artificial intelligence fields such as the natural language processing field, the deep learning field and the like. The specific implementation scheme is as follows: acquiring a document sample and tag information of the document sample; determining a vector representation of each sentence in the document sample; generating a graph representation corresponding to the document sample according to the vector representation of each sentence in the document sample; acquiring a chapter integral vector representation of a document sample, and inputting the chapter integral vector representation and the diagram representation into a neural network model to acquire an importance score of a candidate sentence and a candidate abstract sentence; training the neural network model according to the importance scores of the candidate sentences, the candidate abstract sentences and the label information to obtain model parameters so as to form an abstract extraction model. Therefore, the single-document summarization technology and the multi-document summarization technology are organically unified by adopting the graph representation technology, so that a single-document and multi-document unified summarization model is formed.","['G06F40/289', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
CN111078836B,"Machine reading comprehension method, system and device based on external knowledge enhancement","The invention belongs to the technical field of natural language processing, in particular relates to a machine reading understanding method, a system and a device based on external knowledge enhancement, and aims to solve the problem that the answer prediction accuracy is low because the existing machine reading understanding method does not utilize diagram structure information among triples. Generating a context representation of each entity in the question and original text; based on an external knowledge base, acquiring a triplet set of each entity in the question and original text and a triplet set of each entity adjacent to each entity in the original text; based on the triplet set, acquiring knowledge subgraphs of all entities through an external knowledge graph; updating the fusion knowledge subgraph through a graph attention network to acquire knowledge representation; the context representation and the knowledge representation are spliced through a sentinel mechanism, and answers of questions to be answered are obtained through a multi-layer perceptron and a softmax classifier. The invention improves the accuracy of answer prediction by utilizing the graph structure information among the triples.","['G06F16/367', 'G06F16/3344', 'Y02D10/00']"
US11941366B2,Context-based multi-turn dialogue method and storage medium,The present disclosure discloses a context-based multi-turn dialogue method. The method includes: obtaining to-be-matched historical dialogue information; performing a word feature extraction based on the to-be-matched historical dialogue information to obtain a historical dialogue word embedding; obtaining candidate answer information; performing the word feature extraction based on the candidate answer information to obtain a candidate answer word embedding; obtaining a historical dialogue partial matching vector and a candidate answer partial matching vector by performing partial semantic relationship matching based on the historical dialogue word embedding and the candidate answer word embedding; obtaining a candidate answer matching probability by performing a matching probability calculation based on the historical dialogue partial matching vector and the candidate answer partial matching vector; and determining matched answer information based on the candidate answer information and the candidate answer matching probability.,"['G06F16/3329', 'G06F40/35', 'G06F40/284', 'G06N3/006', 'G06N3/044', 'G06N3/045', 'G06N3/048', 'G06N3/049', 'G06N5/041', 'Y02D10/00']"
US8930897B2,Data integration tool,"Computer-implemented systems and methods are disclosed for providing proactive validations of transformation scripts. In one implementation, a method is provided that includes associating, with at least one processor, the transformation script with ontology parameters. The method also includes initiating a debugging operation of the transformation script having at least one condition and importing, from a data source, at least one data item for transformation. The method further includes determining, as part of the debugging operation, whether the at least one condition that uses the at least one data item is valid based on the ontology parameters and notifying a user, on a display device, of a result associated with the determination.","['G06F16/367', 'G06F11/362', 'G06F11/3688', 'G06F11/3692', 'G06F16/2365', 'G06F16/288', 'G06F16/289']"
US20200412767A1,Hybrid system for the protection and secure data transportation of convergent operational technology and informational technology networks,"A system and method for monitoring, protecting, and transporting data on convergent networks of information (IT) and operational technologies (OT). The system and method provide a complete hybrid on-premise/cloud-based cybersecurity solution that includes analyst tools, host and network visibility, data provenance, and threat adaptation and mitigation while simultaneously providing an optional upstreaming pseudonymized feed of data for additional insight and optimization. The system and method comprise monitoring tools providing information regarding cybersecurity, asset information, and network topology which may further be used to identify, report, and adapt to malicious actors and actions within an organization's network. Furthermore, the system and method may comprise cyber physical graphs and other transformative metadata visualizations delivering contextual and visual information to quantifiably enhance machine and human operations and decisions.","['H04L63/1425', 'G06F16/2477', 'G06F16/951', 'H04L63/1441', 'H04L63/20', 'H04W12/009', 'H04W12/121']"
US11743275B2,Machine learning based anomaly detection and response,"The technology relates to machine responses to anomalies detected using machine learning based anomaly detection. In particular, to receiving evaluations of production events, prepared using activity models constructed on per-tenant and per-user basis using an online streaming machine learner that transforms an unsupervised learning problem into a supervised learning problem by fixing a target label and learning a regressor without a constant or intercept. Further, to responding to detected anomalies in near real-time streams of security-related events of tenants, the anomalies detected by transforming the events in categorized features and requiring a loss function analyzer to correlate, essentially through an origin, the categorized features with a target feature artificially labeled as a constant. An anomaly score received for a production event is determined based on calculated likelihood coefficients of categorized feature-value pairs and a prevalencist probability value of the production event comprising the coded features-value pairs.","['H04L63/1416', 'G06F21/554', 'G06F21/6209', 'G06N20/00', 'G06N5/02', 'G06N7/01']"
US11900940B2,Processing speech signals of a user to generate a visual representation of the user,"A computing system for generating image data representing a speaker's face includes a detection device configured to route data representing a voice signal to one or more processors and a data processing device comprising the one or more processors configured to generate a representation of a speaker that generated the voice signal in response to receiving the voice signal. The data processing device executes a voice embedding function to generate a feature vector from the voice signal representing one or more signal features of the voice signal, maps a signal feature of the feature vector to a visual feature of the speaker by a modality transfer function specifying a relationship between the visual feature of the speaker and the signal feature of the feature vector; and generates a visual representation of at least a portion of the speaker based on the mapping, the visual representation comprising the visual feature.","['G10L21/10', 'G10L15/22', 'G06T11/60', 'G10L13/00', 'G10L15/02', 'G10L15/26', 'G10L2021/105']"
CN109643291B,Method and apparatus for efficient use of graphics processing resources in virtualized execution environments,"An apparatus and method for an efficient multi-GPU virtualization environment are described. For example, one embodiment of an apparatus includes: a plurality of Graphics Processing Units (GPUs) for sharing by a plurality of Virtual Machines (VMs) within a virtualized execution environment; a shared memory for sharing between the plurality of VMs executing within the virtualized graphics execution environment and a GPU; the GPU for collecting performance data related to commands executed within a command buffer submitted by the VM, the GPU for storing the performance data within the shared memory; and a GPU scheduler and/or driver to schedule a subsequent command buffer to the GPU based on the performance data.","['G06T15/005', 'G06F9/4881', 'G06F9/455', 'G06F9/45558', 'G06F9/505', 'G06F9/5077', 'G06T17/10', 'G06F2009/45579', 'G06T15/04', 'G06T15/80']"
US20230370491A1,System and method for cyber exploitation path analysis and response using federated networks,"A system and method for cyber exploitation path analysis and response using federated networks to minimize network exposure and maximize network resilience, with the ability to simulate complex and large scale network traffic through the use of federated training networks, by gathering network entity information, establishing baseline behaviors for each entity, and monitoring each entity for behavioral anomalies that might indicate cybersecurity concerns. Further, the system and method involve incorporating network topology information into the analysis by generating a model of the network, annotating the model with risk and criticality information for each entity in the model and with a vulnerability level between entities, and using the model to evaluate cybersecurity risks to the network. Lastly, network attack path analysis and automated task planning for minimizing network exposure and maximizing resiliency is performed with machine learning, generative adversarial networks, hierarchical task networks, and Monte Carlo search trees.","['H04L63/1433', 'G06N3/0475', 'G06N3/094', 'G06N3/098', 'G06N5/01', 'H04L63/102', 'H04L63/1416', 'H04L63/1425', 'H04L63/20', 'G06N20/00', 'G06N3/084']"
CN113673489B,Video group behavior identification method based on cascade Transformer,"The invention relates to the field of computer vision and deep learning, in particular to a video group behavior identification method based on a cascade Transformer, which comprises the steps of firstly collecting and generating a video data set, extracting three-dimensional space-time characteristics of the video data set through a three-dimensional backbone network, and selecting a key frame image space characteristic diagram; preprocessing the key frame image space characteristic graph, sending the preprocessed key frame image space characteristic graph into a human body target detection Transformer, and outputting a human body target frame in the key frame image; then, mapping sub-feature maps corresponding to the screened human body target frames on the key frame image feature map, calculating query/key/value by combining the surrounding frame feature maps of the key frame image, inputting a group behavior identification Transfomer, and outputting a group level space-time coding feature map; and finally, classifying the group behaviors through a multilayer perceptron. The method has the effect of effectively improving the group behavior recognition accuracy.","['G06F18/214', 'G06F18/24', 'G06F18/253', 'G06N3/045', 'G06N3/084']"
US20230370490A1,System and method for cyber exploitation path analysis and task plan optimization,"A system and method for cyber exploitation path analysis and task plan optimization to minimize network exposure and maximize network resilience. The system and method involve gathering network entity information, establishing baseline behaviors for each entity, and monitoring each entity for behavioral anomalies that might indicate cybersecurity concerns. Further, the system and method involve incorporating network topology information into the analysis by generating a model of the network, annotating the model with risk and criticality information for each entity in the model and with a vulnerability level between entities, and using the model to evaluate cybersecurity risks to the network. Lastly, network attack path analysis and automated task planning for minimizing network exposure and maximizing resiliency is performed with machine learning, generative adversarial networks, hierarchical task networks, and Monte Carlo search trees.","['H04L63/1433', 'G06N3/0475', 'G06N3/094', 'G06N3/098', 'G06N5/01', 'H04L63/102', 'H04L63/1416', 'H04L63/1425', 'H04L63/20', 'G06N20/00', 'G06N3/084']"
CN113205817B,"Speech semantic recognition method, system, device and medium","The invention provides a speech semantic recognition method, a system, equipment and a medium, comprising the following steps: acquiring a voice signal of a user; converting the voice signal into text data by using an automatic voice recognition technology; judging whether the text data conforms to a preset sentence pattern, if so, responding according to a voice signal in the text data; if not, analyzing the text data by using natural language processing to obtain a named entity, carrying out syntactic analysis on the text data to obtain an intention keyword, and obtaining an intention identification result according to the relation between the intention keyword and the named entity. According to the invention, by combining the named entity and the intention keyword in the voice information in the text data, the intention of the user can be accurately understood when the user does not adopt a speech technology system for interaction, so that the accuracy of semantic recognition and the experience degree of the user are improved.","['G10L15/26', 'G06F40/216', 'G06F40/253', 'G06F40/289', 'G06F40/295', 'G06F40/30', 'G10L15/02', 'G10L15/063']"
TWI843770B,Systems and power amplifier devices for enhancing efficacy of ultrasound treatment,"Embodiments are provided that enhance ultrasound efficacy by for example, high efficiency, signal measurement, calibration, and assurance systems with a control system radiofrequency (RF) driver configured to drive one or more focused ultrasound transducers. The RF driver can comprise one or more power amplifiers including one or more III-V semiconductors, (e.g., gallium nitride GaN, GaAs, GaSb, InP, InAs, InSb, InGaAs, AlSb, AlGaAs, and/or AlGaN) field-effect transistors to efficiently provide high power with distinct narrow-band RF signals over a wide frequency range. The RF driver can include a power measurement and/or calibration system to monitor the amplitude and phase of the RF signal output from the power amplifier and estimate the amount of RF power delivered to the ultrasound transducers.","['A61N7/02', 'A61B18/04', 'H03F3/193', 'H03F3/2173', 'A61B2018/00452', 'A61B2018/00702', 'A61B2018/00827', 'A61B2018/00892', 'A61B2560/0204', 'A61N2007/0021', 'A61N2007/0034', 'A61N2007/0086', 'A61N2007/0095', 'B06B1/0207', 'B06B1/0644', 'B06B2201/76', 'H03F1/565']"
CN113569465B,A joint estimation system and estimation method of track vector and target type based on deep learning,"The invention discloses a flight path vector and target type joint estimation system and method based on deep learning, wherein the system comprises: the track observation data set construction module is used for constructing a training data set based on track coordinates, category data, observation information and auxiliary information; the observation feature extraction module is used for extracting observation features of the training data set based on the convolutional neural network to obtain observation feature vectors corresponding to the observation time slots; the auxiliary information embedding module is used for converting factors influencing the target track into low-dimensional actual vectors by adopting an embedding method, and splicing the low-dimensional actual vectors to obtain auxiliary information characteristic vectors as output; the track characteristic extraction module is used for learning the mapping relation between the observation characteristic vector and the auxiliary information characteristic vector to the track vector characteristic vector by using a neural network; and the multi-task learning module is used for performing combined learning on each module through a minimum joint loss function and jointly outputting a track vector and a target type. The invention obviously improves the track estimation performance.","['G06F30/27', 'G06N3/045', 'G06N3/047', 'G06N3/084']"
US11893071B2,"Content recommendation method and apparatus, electronic device, and storage medium","This application provides a content recommendation method and apparatus, an electronic device, and a storage medium. The content recommendation method includes obtaining content feedback information of a target object and content feature information of content that is to be recommended in response to a content recommendation request of the target object, the content feedback information comprising explicit feedback information and implicit feedback information and object portrait information of the target object; performing feature interaction according to the explicit feedback information and the implicit feedback information in the content feedback information, and obtaining behavior preference information; performing feature extraction based on the behavior preference information, the content feedback information, and the content feature information, and obtaining a predicted click-through rate (CTR); and determining, according to the predicted CTR, recommended content from the pieces of content that is to be recommended, and transmitting the recommended content to a terminal device.","['G06F16/9536', 'G06F16/9535', 'G06F16/958', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'H04L67/02', 'H04L67/535', 'H04N21/4668']"
US12293288B2,Systems and methods of data preprocessing and augmentation for neural network climate forecasting models,"Methods and systems for training a neural network (NN)-based climate forecasting model on a pre-processed multi-model ensemble of global climate simulation data from a plurality of global climate simulation models (GCMs), are disclosed. The methods and systems perform steps of determining a common spatial scale and a common temporal scale for the multi-model ensemble of global climate simulation data; spatially re-gridding the multi-model ensemble to the common spatial scale; temporally homogenizing the multi-model ensemble to the common temporal scale; augmenting the spatially re-gridded, temporally homogenized multi-model ensemble with synthetic simulation data generated from the spatially re-gridded, temporally homogenized multi-model ensemble; and training the NN-based climate forecasting model using the spatially re-gridded, temporally homogenized, and augmented multi-model ensemble of global climate simulation data. Embodiments of the present invention enable accurate climate forecasting without the need to run new dynamical global climate simulations on supercomputers.","['G06N3/08', 'G01W1/10', 'G06N3/045', 'G06N3/044', 'Y02A90/10']"
US12119115B2,Systems and methods for self-supervised learning based on naturally-occurring patterns of missing data,"Disclosed is a method comprising accessing, by a machine learning system, a set of data records for a plurality of users, the data records representative of physical statistics measured for each of the plurality of users over a time period. At least a subset of the data records comprises patterns of missing data for at least a portion of the time period. The method also comprises generating a set of masked data records by masking a subset of the data records in accordance with a pattern of natural missingness from a data record. The method also comprises generating, by the machine learning system, a set of learned representations from at least the set of masked data records. Finally, the method comprises fine tuning, by the machine learning system, a machine learning model using the set of learned representations, the machine learning model configured to perform a downstream machine learning task.","['G16H50/30', 'G16H40/67', 'G16H50/20', 'G16H50/70']"
CN109643443B,Cache and compression interoperability in graphics processor pipelines,"Several embodiments are described herein that provide enhanced data caching in combination with adaptive and dynamic compression to improve storage efficiency and reduce data transfer bandwidth during input and output from a GPU. The techniques described herein may reduce the need to access off-chip memory, resulting in improved performance and reduced GPU operating power. One embodiment provides a graphic processing apparatus including: a shader engine; one or more cache memories; cache control logic to control at least one of the one or more cache memories; and a codec unit coupled with the one or more cache memories, the codec unit configurable to perform lossless compression of read-only surface data immediately after storing or evicting the read-only surface data to or from the one or more cache memories.","['G06F12/0875', 'G06F12/0207', 'G06T1/20', 'G06T1/60', 'G06T15/005', 'G06F12/084', 'G06F12/0842', 'G06F2212/1024', 'G06F2212/302', 'G06F2212/401', 'G06F2212/455', 'G06F3/0655', 'G06F40/12', 'Y02D10/00']"
US10895965B2,Indicating potential focus in a user interface,"Methods and apparatus, including computer program products, implementing and using techniques for an integrated user interface an interface that receives user input through the user interface. User selection input received through the user interface is used to sequentially update a selection register and sequential user selections are differentiated within the user interface based on their status within the selection register.","['G06F3/0484', 'G06F3/02', 'G06F3/038', 'G06F3/04812', 'G06F3/0482', 'G06F3/0486', 'G06F9/451', 'G06F2203/0381']"
CN111340509B,False transaction identification method and device and electronic equipment,"The embodiment of the specification provides a false transaction identification method and device and electronic equipment. The method comprises the following steps: acquiring original transaction graph data corresponding to a predetermined platform, and extracting sub-graph data from the original transaction graph data according to a target node; taking the sub-graph data as input, and performing information aggregation operation and feature vector extraction operation by utilizing a plurality of preset continuous neural network structures to obtain a target feature vector; the information aggregation operation is used for determining an attention matrix according to the correlation among the nodes and aggregating neighborhood node information of a target node according to the attention matrix, wherein the correlation among the nodes comprises a first correlation determined according to the node information and a second correlation determined according to the side information; and inputting the target feature vector into a classifier so as to judge whether the transaction to be identified is a false transaction according to an output result of the classifier.","['G06Q20/4016', 'G06N3/045']"
CN215870792U,Power supply system for wellsite electric drive equipment,"The present invention provides a power supply system for a wellsite electric drive, comprising a combined power module configured to be electrically connected to the wellsite electric drive, the combined power module comprising at least one generator and at least one distribution substation, the at least one generator being disposed in parallel or grid-connected relation to the at least one distribution substation for supplying power to the wellsite electric drive. The power supply system disclosed by the utility model supplies power to the well site electric driving equipment in a mode of combining a power grid and a generator, can adapt to different well site operation conditions, and can meet the power requirements of various well site electric driving equipment during operation.","['H02J3/381', 'H02J13/00004', 'H02J2300/10', 'H02J2310/18']"
CN111722714B,AR technology-based digital substation metering operation and detection auxiliary method,"The invention discloses an AR technology-based digital substation metering operation and detection auxiliary method, which comprises the following steps: during the digital substation operation and inspection process, a worker wears AR intelligent glasses, and sends the two-dimension codes of field devices, the pictures and videos of the field devices, the parameters of the field devices and the voice information of the worker to a mobile phone operation and inspection APP terminal for offline analysis, or the mobile phone operation and inspection APP terminal uploads the information to a Web background system for network cloud analysis to obtain a solution, and then issues a command to the AR intelligent glasses to guide the worker to perform fault processing and next operation and inspection. According to the invention, the AR glasses, the mobile phone operation and detection APP end and the Web background system are connected together in a wireless and wired mode, so that data information interaction is realized, the safety and the instantaneity of inspection operation are improved, the development process of the intelligent substation for realizing comprehensive automation is promoted to be presented in an augmented reality mode, and the method has a large application value for metering operation and detection work of the digital substation.","['G06F3/011', 'G06F16/583', 'G06F18/24', 'G06K7/10722', 'G06N3/045', 'G06N3/08', 'G06N5/042', 'G06Q10/20', 'G06Q50/06', 'G06V20/20', 'G10L25/51', 'H04L67/02', 'H04L67/06']"
US12086546B2,Enterprise knowledge graphs using enterprise named entity recognition,"Examples described herein generally relate to a computer system including a knowledge graph storing a plurality of entities. A mining of a set of enterprise source documents within an enterprise intranet is performed, by an enterprise named entity recognition (ENER) model, to determine a plurality of entity names. An entity record is generated within a knowledge graph for a mined entity name from the linked entity names based on an entity schema and ones of the set of enterprise source documents associated with the mined entity name. The entity record includes attributes aggregated from the ones of the set of enterprise source documents associated with the mined entity name.","['G06F16/313', 'G06F16/906', 'G06F40/295', 'G06N5/02', 'G06F40/205']"
US10079915B2,"Methods of processing data corresponding to a device that corresponds to a gas, water, or electric grid, and related devices and computer program products","Methods of operating a communication node are provided. A method of operating a communication node may include receiving data from an electric grid device via a network interface. The method may include processing the data from the electric grid device at the communication node. Moreover, the method may include transmitting a filtered portion of the data to an electric utility head end system, after processing the data at the communication node. In some embodiments, a method of operating a communication node may include using a message broker controlled by a virtual machine in the communication node to provide a protocol to interface with a field message bus that includes a standards-based or open-source Application Programming Interface (API). Related communication nodes and computer program products are also described.","['H04L69/03', 'H04L67/12', 'G01D4/004', 'Y02B70/30', 'Y02B70/3266', 'Y02B90/20', 'Y02B90/242', 'Y04S20/242', 'Y04S20/30', 'Y04S20/322', 'Y04S40/18']"
US10725441B2,Energy management with multiple pressurized storage elements,"Disclosed techniques include energy management with multiple pressurized storage elements. Energy is obtained from one or more energy sources. Energy requirements are modeled over a first time period and a second time period. A first subset of the energy that was obtained is allocated for storage in a first energy store based on the modeling. A second subset of the energy that was obtained is allocated for storage in a second energy store based on the modeling, where the second energy store comprises a pressurized storage element. Energy is routed to the first energy store from the second energy store based on the modeling. Recovering energy further includes using the energy routed to the first energy store or the second energy store, based on the modeling.","['H02J3/003', 'G05B13/028', 'G05B13/048', 'G06Q10/06314', 'G06Q50/06', 'H02J15/006', 'H02J3/00', 'G05B2219/2639', 'H02J15/003', 'H02J2203/20', 'H02J3/28', 'H02J3/32']"
US12347158B2,"Pre-training method, image and text retrieval method for a vision and scene text aggregation model, electronic device, and storage medium","A pre-training method for a Vision and Scene Text Aggregation model includes: acquiring a sample image-text pair; extracting a sample scene text from a sample image; inputting a sample text into a text encoding network to obtain a sample text feature; inputting the sample image and an initial sample aggregation feature into a visual encoding subnetwork and inputting the initial sample aggregation feature and the sample scene text into a scene encoding subnetwork to obtain a global image feature of the sample image and a learned sample aggregation feature; and pre-training the Vision and Scene Text Aggregation model according to the sample text feature, the global image feature of the sample image, and the learned sample aggregation feature.","['H04N19/176', 'G06F16/332', 'G06F16/38', 'G06F16/532', 'G06F16/5846', 'G06F16/5866', 'G06F18/253', 'G06N3/045', 'G06N3/08', 'G06V10/42', 'G06V10/774', 'G06V10/806', 'G06V10/82', 'G06V20/63', 'G06F40/30', 'Y02D10/00']"
US12335310B2,System and method for collaborative cybersecurity defensive strategy analysis utilizing virtual network spaces,A system and method for collaborative cybersecurity defensive strategy analysis that predicts the evolution of new cybersecurity attack strategies and creates a virtual network space that provides a virtual reality environment for collaborative insights into network dynamics during a cyberattack. makes recommendations for cybersecurity improvements to networked systems based on a cost/benefit analysis. The system and method use machine learning algorithms to run simulated attack and defense strategies against a virtual network space model of the networked system created using a virtual network space manager. A simulation interaction server can facilitate secure sharing of virtual network spaces and simulations between and among various real and virtual actors to provide a collaborative space where one or more organization's network can be tested for resilience and mitigation. Recommendations are generated based on an analysis of the simulation results against a variety of cost/benefit indicators.,"['G06F16/2477', 'G06F16/9024', 'G06F16/951', 'H04L63/1425', 'H04L63/1441', 'H04L63/20']"
US20220210200A1,Ai-driven defensive cybersecurity strategy analysis and recommendation system,"A system and method for automated cybersecurity defensive strategy analysis that predicts the evolution of new cybersecurity attack strategies and makes recommendations for cybersecurity improvements to networked systems based on a cost/benefit analysis. The system and method use machine learning algorithms to run simulated attack and defense strategies against a model of the networked system created using a directed graph. Recommendations are generated based on an analysis of the simulation results against a variety of cost/benefit indicators. The recommendation engine runs continuously, makes suggestions, and takes adjustably autonomous actions to go further and actuate parts of the system using an orchestration service employing a distributed computational graph and actuation plugins based on generated plans. Actions are validated as required or as prudent from appropriate simulation modeling services.","['H04L63/1425', 'G06F16/2477', 'G06F16/951', 'G06F21/552', 'G06F21/554', 'G06F21/56', 'G06F21/577', 'H04L63/20', 'H04L63/1441']"
JP2024023421A,2-level phonetic prosody transcription,"To provide a two-level speech prosody transfer method.SOLUTION: A method (500) includes the steps of receiving input text utterances (320) that are to be synthesized into expressive speech (152) having intended prosody and target speech, and using a first text-to-speech (TTS) model (212) to generate an intermediate synthesized speech representation (202) for the input text utterances. The intermediate synthesized speech representation possesses the intended prosody. The method also includes the step of providing the intermediate synthetic speech representation to a second TTS model (220) that includes an encoder portion (300) and a decoder portion (400). The encoder portion is configured to encode the intermediate synthetic speech representation into an utterance embedding (204) that specifies the intended prosody. The decoder portion is configured to process the input text utterance and utterance embedding to generate an output audio signal (280) of the expressive speech having the intended prosody specified by the utterance embedding and target speech speaker characteristics.SELECTED DRAWING: Figure 2A","['G10L13/10', 'G06N3/044', 'G06N3/045', 'G06N3/0455', 'G06N3/047', 'G06N3/082', 'G06N3/084', 'G06N3/088', 'G06N3/096', 'G10L13/02', 'G10L13/047', 'G10L17/18']"
US12307336B2,Data manufacturing frameworks for synthesizing synthetic training data to facilitate training a natural language to logical form model,"Techniques are disclosed herein for synthesizing synthetic training data to facilitate training a natural language to logical form model. In one aspect, training data can be synthesized from original under a framework based on templates and a synchronous context-free grammar. In one aspect, training data can be synthesized under a framework based on a probabilistic context-free grammar and a translator. In one aspect, training data can be synthesized under a framework based on tree-to-string translation. In one aspect, the synthetic training data can be combined with original training data in order to train a machine learning model to translate an utterance to a logical form.","['G06N20/00', 'G06F16/24522', 'G06F16/3329', 'G06F16/90332', 'G06F40/211', 'G06F40/237', 'G06F40/284', 'G06F40/40', 'G06F40/47', 'G06F40/56', 'G06F40/58', 'G06N3/084', 'G06N5/046', 'H04L51/02', 'G06F40/205', 'G06F40/35', 'G06N3/0442', 'G06N3/0455', 'G06N3/096']"
US8797906B2,Method and system for wireless message-based advertising,Systems and methods for message-based advertising in a wireless communications network are described. A digital message is contained in a data packet set in transit from a terminal of a sender to a terminal of a recipient. At least one of the terminals is implemented as a mobile telephone. The digital message is received through a network connection. A processor determines a context for the digital message and matches the message context to content of advertisements stored in an advertisement database in order to identify at least one relevant advertisement. The data packet set is transmitted to the terminal of the recipient. A processor retrieves the relevant advertisement(s) from the advertisement database and generates a confirmation message containing a delivery status of the data packet set. The confirmation message includes the relevant advertisement and is transmitted to the terminal of the sender.,"['G06Q30/0267', 'G06Q30/02', 'G06Q30/0277', 'H04L51/58', 'H04M3/42382', 'H04M3/4878', 'H04L51/23', 'H04M2203/2038', 'H04M2203/2083', 'H04M2207/18', 'H04M3/42068', 'H04M3/4211']"
US11373045B2,Determining context and intent in omnichannel communications using machine learning based artificial intelligence (AI) techniques,"A system for determining context and intent in a conversation using machine learning (ML) based artificial intelligence (AI) in omnichannel data communications is disclosed. The system may comprise a data store to store and manage data within a network, a server to facilitate operations using information from the one or more data stores, and a ML-based AI subsystem to communicate with the server and the data store in the network. The ML-based AI subsystem may comprise a data access interface to receive data associated with a conversation with a user via a communication channel. The ML-based AI subsystem may comprise a processor to provide a proactive, adaptive, and intelligent conversation by applying hierarchical multi-intent data labeling framework, training at least one model with training data, and generating and deploying a production-ready model based on the trained and retained at least one model.","['G06F40/30', 'G06F40/35', 'G06N20/20', 'G06N3/045', 'G06N3/08', 'G06N5/027', 'G10L15/063', 'G10L15/16', 'G10L15/18', 'G06F40/295', 'G10L15/07', 'G10L15/1815', 'G10L15/1822', 'G10L15/183', 'G10L2015/227']"
CN111462750B,End-to-end task-based dialogue system and method with semantic and knowledge enhancement,"The application discloses a semantic and knowledge enhanced end-to-end task type dialogue system and a method, wherein the system comprises the following steps: the dialogue history semantic information coding module is used for carrying out sentence level semantic information processing and dialogue level semantic information processing on dialogue history information so as to decode and generate replies; the external database access module is used for determining the database accessed in the external database and determining attribute entries in the accessed database; and the dialogue reply decoding module is used for finishing dialogue by comprehensively considering dialogue history, a database and reply generation by using a copy mechanism and a gating mechanism in the decoding process. The system directly realizes the process of generating mapping from the historical text to the reply by combining the information of the external knowledge base and designing the efficient and quick end-to-end dialogue information coding model, and has the advantages of strong mobility, high training speed and high accuracy of retrieving the knowledge base.","['G06F16/245', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G10L15/18', 'Y02D10/00']"
CN112926303B,Malicious URL detection method based on BERT-BiGRU,"The invention provides a method for detecting malicious URLs based on BERT-BiGRU, which solves the problem that the detection and identification accuracy of the malicious URLs is low at present. The invention firstly uses a transducer feature extractor of the BERT model to replace the natural mode of the traditional common CNN or RNN, and utilizes the feature extractor to extract the dynamic feature characteristics of word vectors and the advantage of parallel computation, and in addition, the URL text features with stronger correlation can be obtained due to the multi-head self-attention mechanism. And inputting the feature vector obtained by training into a BiGRU network structure, and training a GRU model in two directions to enable each word in the sequence to completely depend on the context information, and then splicing the neural network vectors to obtain a result vector, and combining the result vector with a softmax classifier through a full connection layer to realize URL classification. The method can improve the accuracy and stability of URL classification by combining the model methods, and has good implementation property and effectiveness.","['G06F40/205', 'G06N3/044', 'G06N3/047', 'G06N3/084', 'Y02D10/00']"
CN109923519B,Mechanism for accelerating graphics workload in a multi-core computing architecture,A processing apparatus is described. The device comprises: a plurality of processing cores including a first processing core and a second processing core; a first Field Programmable Gate Array (FPGA) coupled to the first processing core to accelerate execution of a graphics workload being processed at the first processing core; and a second FPGA coupled to the second processing core to accelerate execution of the workload processed at the second processing core.,"['G06F9/30', 'G06T1/20', 'G06F13/00', 'G06F13/1652', 'G06F13/1657', 'G06F9/4881', 'G06F9/5027', 'G06F9/505', 'G06T1/60', 'G06T2200/28', 'Y02D10/00']"
CN110428580B,Household safety intelligent control method,"The invention discloses an intelligent control method for family safety, and belongs to the technical field of fire safety. A household safety intelligent control method comprises a set of household electrical fire and other fire monitoring control methods, a fire escape device application method, a household electrical appliance intelligent safety control method, a household gas safety monitoring processing method, a household water safety processing method and a household anti-theft processing control method, wherein six types of household safety control methods are adopted, carriers of the six types of household safety control methods are a plurality of intelligent devices, the intelligent devices are integrated into a main control chip and a wireless communication module through embedded software and corresponding algorithms, one-to-one or one-to-many data exchange among the devices is realized, an independent network system is formed, and data in the system is compared with safety data in the embedded algorithms after being calculated, so that hidden danger or dangerous data are screened.","['G05B19/4186', 'G08B19/00', 'H04L67/025', 'H04L67/125']"
US11816465B2,"Devices, systems and methods for tracking and upgrading firmware in intelligent electronic devices","Devices, systems and methods for tracking and upgrading firmware in intelligent electronic devices (IEDs) are provided. The devices, systems and methods provide for tracking firmware versions of at least one or a fleet of IEDs, e.g., electronic power or revenue meters, notifying a user that an update to an existing firmware is available and providing the ability to automatically upload the current or latest version of the firmware to all IEDs.","['G06F8/65', 'G06F8/71', 'G06F9/4401', 'G06F9/4406']"
CN107436014B,System and method for mobile application of HVAC installation and diagnostics,"The present invention relates to a system and method for mobile applications for heating, ventilation and air conditioning (HVAC) installation and diagnostics. A monitoring system for monitoring a heating, ventilation and air conditioning system of a building is provided. The monitoring system comprises a monitoring server and a monitoring client. The monitoring server is configured to: receiving a total current value from the monitoring device, wherein the total current value represents a total current flowing through the HVAC system; determining a commanded operating mode of the HVAC system in response to the total current value, wherein the operating mode of the HVAC system comprises at least one of an idle mode and an on mode; and analyzing a system condition of the HVAC system based on the determined commanded operating mode. The monitoring client is configured to: instructing the monitoring server to perform a verification test based on data received as a result of monitoring of the HVAC system; displaying the result of the verification test; displaying real-time system performance data; and displaying an alarm during a system failure.","['F24F11/30', 'F24F11/38', 'F24F11/39', 'F24F11/46', 'F24F11/52', 'F24F11/62', 'F24F11/63', 'F24F11/70', 'G05B19/048', 'F24F11/32', 'F24F11/57', 'F24F2110/00', 'F24F2110/10', 'F24F2140/60', 'G05B2219/2614']"
US11106711B2,Natural language data analytics platform,"A system for natural language analytics, stored and operating on a network-connected computing device, comprising a natural language application data importer, further comprising a natural language application data importer, a natural language application data augmenter that enriches the data and an analytics component which provides a means of querying structured as well as unstructured data and which also contains a method for providing adaptive natural language analytics.","['G06F16/3329', 'G06F40/20', 'G06F16/22', 'G06F16/3344', 'G06F16/51', 'G06N20/00']"
US20200082290A1,Adaptive anonymization of data using statistical inference,"Techniques that facilitate adaptive anonymization of data using statistical inference are provided. In one example, a system includes an anonymization component and a statistical learning component. The anonymization component applies an anonymization strategy to data associated with an electronic device. The statistical learning component modifies the anonymization strategy to generate an updated anonymization strategy for the data based on a machine learning process associated with a probabilistic model that represents the data.","['G06N7/005', 'G06F21/6254', 'G06N20/00', 'G06N7/01', 'G06N7/08', 'G06N99/005']"
US11055450B2,Industrial asset health model update,"Among other things, one or more techniques and/or systems are provided for updating a model for generating a health profile of an industrial asset based upon data pertaining to the industrial asset. The health profile comprises a maintenance plan(s), which respectively comprise one or more recommended maintenance actions to be performed with respect to the industrial asset during a prediction period. The model may be periodically and/or intermittently updated based upon a comparison between the recommended maintenance actions and actual events of the industrial asset during the prediction period. Moreover, in some embodiments, different models may be selected for generating future health profiles of the industrial asset based upon a comparison between the recommended maintenance actions and actual events of the industrial asset during the prediction period.","['G06F30/20', 'G06Q10/04', 'G06Q10/06311', 'G06Q10/067', 'G06Q10/20', 'G06Q50/06', 'Y02P90/80']"
CN111209383B,"Method and device for processing multi-turn dialogue, vehicle, and storage medium","The invention discloses a processing method and a processing device for multi-turn conversations, a vehicle and a storage medium. The processing method of the multi-turn dialog comprises the following steps: extracting a plurality of characteristics of the query text based on a multi-task joint training model, wherein the multi-task joint training model comprises a model obtained by performing multi-task joint fine tuning training based on a pre-training model; mapping the plurality of features to obtain a feature structure; the feature structure is used as input to a dialog state model to predict reply text. The multi-turn dialogue processing method provided by the embodiment of the invention adopts a multi-task joint training model to extract a plurality of characteristics of the query text, and then performs characteristic mapping and predicts the reply text. Because the query text is predicted by only adopting the multi-task joint training model, the use resources are reduced, and the framework is relatively simple and convenient to expand.","['G06F16/3329', 'G06F16/3343', 'G06F16/3344', 'G06F16/35', 'Y02T10/40']"
US20240265718A1,"Method of training text detection model, method of detecting text, and device","A method training a text detection model and a method of detecting a text. The training method includes: inputting a sample image into a text feature extraction sub-model of a text detection model to obtain a text feature of a text in the sample image, the sample image having a label indicating an actual position information and an actual category; inputting a predetermined text vector into a text encoding sub-model of the text detection model to obtain a text reference feature; inputting the text feature and the text reference feature into a decoding sub-model of the text detection model to obtain a text sequence vector; inputting the text sequence vector into an output sub-model of the text detection model to obtain a predicted position information and a predicted category; and training the text detection model based on the predicted and actual categories, the predicted and actual position information.","['G06F18/214', 'G06V30/19127', 'G06F18/00', 'G06F18/2411', 'G06V10/7715', 'G06V10/82', 'Y02T10/40']"
US20190122111A1,Adaptive Convolutional Neural Knowledge Graph Learning System Leveraging Entity Descriptions,"Systems and methods for predicting new relationships in the knowledge graph, including embedding a partial triplet including a head entity description and a relationship or a tail entity description to produce a separate vector for each of the head, relationship, and tail. The vectors for the head entity, relationship, and tail entity can be combined into a first matrix, and adaptive kernels generated from the entity descriptions can be applied to the matrix through convolutions to produce a second matrix having a different dimension from the first matrix. An activation function can be applied to the second matrix to obtain non-negative feature maps, and max-pooling can be used over the feature maps to get subsamples. A fixed length vector, Z, flattens the subsampling feature maps into a feature vector, and a linear mapping method is used to map the feature vectors into a prediction score.","['G06N3/08', 'G06N3/082', 'G06N3/042', 'G06N3/045', 'G06N3/048', 'G06N3/084', 'G06N5/022']"
US20230412620A1,System and methods for cybersecurity analysis using ueba and network topology data and trigger - based network remediation,"A system and method for network cybersecurity analysis that uses user and entity behavioral analysis combined with network topology information and trigger-based network remediation to provide improved cybersecurity. The system and method involve gathering network entity information, establishing baseline behaviors for each entity, and monitoring each entity for behavioral anomalies that might indicate cybersecurity concerns. Further, the system and method involve incorporating network topology information into the analysis by generating a model of the network, annotating the model with risk and criticality information for each entity in the model and with a vulnerability level between entities, and using the model to evaluate cybersecurity risks to the network. Triggers may be based on risks or anomalous behavior and associated with a remediation action executed on the network by a security mitigation engine. The system and method may also dynamically adjust monitoring characteristics based on trigger events.","['H04L41/12', 'H04L63/1416', 'H04L41/145', 'H04L63/1425', 'H04L63/1433', 'H04L63/20', 'G06N3/08', 'G06N7/01']"
US10979311B2,System and method for validating network configuration changes in a client environment,"System and method for validating distribution network configuration changes in a client environment are disclosed. The system residing on a client device detects network configuration changes made by a user. The disclosed system, working with a locally stored model of the network, validates the network configuration changes against one or more validation rules that are also stored locally in the client device. When validating the network configuration changes, the disclosed system can perform a number of network engineering calculations to detect invalid network configuration changes. The disclosed system then provides graphical feedback in real time to inform the user of any invalid network configuration changes.","['H04L41/22', 'H04L41/0813', 'H04L41/0866', 'H04L41/145', 'Y04S40/00']"
US20200293627A1,Method and apparatus for composite load calibration for a power system,"Briefly, embodiments are directed to a system, method, and article for generating a power system load model of a power system. Power grid disturbance data may be accessed. A power system simulation engine may be prepared, wherein the simulation engine may implement the power system model of the power system. A parameter subset A may be identified from a knowledge-based approach. The parameter subset B may be identified based on a special grid event type based on the power grid disturbance data. A final parameter subset may be selectively determined based on parameter subsets A and B using a decision-making approach. At least one parameter of the final parameter subset may be tuned. One or more parameters of the power system load model may be modified based on the tuning.",['G06F17/5009']
US12265538B2,Schema-adaptable data enrichment and retrieval,"A method includes obtaining a schema of a database and obtaining a field of a data table of the schema, the field including a field name and a field descriptor. The method includes accessing a first set of records structured in accordance with the schema and assigning a category to the field of the schema based on the field name and the field descriptor using a machine learning model. The method includes receiving an identifier via an application program interface and obtaining a set of query generation model parameters associated with the database or the schema. The method includes generating a plurality of queries based on the set of query generation model parameters, the identifier, and the category. The method includes retrieving a second set of records from the database based on the plurality of queries and storing the second set of records in association with the identifier.","['G06F16/25', 'G06F16/2455', 'G06F16/2379', 'G06N3/042', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N5/01', 'G06N5/022', 'G06N5/025', 'G06N20/00']"
CN111368079B,"Text classification method, model training method, device and storage medium","The application discloses a text classification method applied to the field of artificial intelligence, which comprises the following steps: acquiring a text to be classified; acquiring a hidden state corresponding to each word in the text to be classified through an encoder; acquiring a depth value corresponding to each word according to the hidden state corresponding to each word; generating word feature vectors corresponding to each word according to the depth value corresponding to each word and the word embedding corresponding to each word; based on word feature vectors corresponding to each word, obtaining a text coding result corresponding to the text to be classified through a text classification model; and obtaining a classification result of the text to be classified through the text classification model. The application also discloses a model training method. The method and the device can adaptively acquire the depth value corresponding to each word in the text, so that the text classification model encodes the corresponding word according to different depth values, and the word in the text can be fully calculated.",['G06F16/353']
US11163947B2,Methods and systems for multi-label classification of text data,"There are provided methods and systems for multi-label classification of a sentence. An example method includes obtaining the sentence and generating a first digital representation corresponding to the words of the sentence. The method also includes performing a first classification of the sentence using a classification engine receiving as input the first digital representation. The first classification generates a first set of probabilities each associated with one of the possible labels for the sentence. The classification engine may include a neural network. The method further includes generating an output probability for each given label of the possible labels, which output probability is generated based on a first probability associated with the given label. The first probability is from the first set of probabilities. Moreover, the method includes outputting the output probability for each of the possible labels.","['G06F16/35', 'G06N20/20', 'G06N3/04', 'G06N3/082', 'G06N5/003', 'G06N5/01', 'G06F40/20']"
US10719742B2,Image composites using a generative adversarial neural network,"The present disclosure relates to an image composite system that employs a generative adversarial network to generate realistic composite images. For example, in one or more embodiments, the image composite system trains a geometric prediction neural network using an adversarial discrimination neural network to learn warp parameters that provide correct geometric alignment of foreground objects with respect to a background image. Once trained, the determined warp parameters provide realistic geometric corrections to foreground objects such that the warped foreground objects appear to blend into background images naturally when composited together.","['G06K9/66', 'G06T11/00', 'G06N3/04', 'G06N3/045', 'G06N3/047', 'G06N3/084', 'G06N3/088', 'G06V10/82', 'G06V20/20', 'G06V30/19173']"
US20190251641A1,"Systems and methods for collecting, analyzing, billing, and reporting data from intelligent electronic devices","Systems and methods for collecting, analyzing, billing and reporting data from intelligent electronic devices are provided. The intelligent electronic devices include at least one sensor configured to measure at least one parameter of an electrical distribution system and output a signal indicative of the measured at least one parameter; at least one analog-to-digital converter configured to receive the output signals and convert the output signal to a digital signal; and at least one processing device configured to execute a plurality on instructions to implement a general purpose operating system for executing at least two applications, each application configured to implement a predetermined functionality based on the at least one parameter of the electrical distribution system, wherein each of the applications is independent of the other application. In one aspect, the intelligent electronic devices include a network discovery module configured to detect communication settings of a network.","['G06F16/972', 'G06Q50/06', 'G01D4/004', 'H04L67/06', 'H04W4/38', 'G01D4/002', 'G01R19/2513', 'G01R22/063', 'Y02B90/20', 'Y04S20/30']"
CN109983507B,Positioning based on large-scale CNN regression via two-dimensional map,A processing device includes computational logic for training a Convolutional Neural Network (CNN) to perform autonomous relocation of a serving robot or mobile device. An apparatus comprising: an image processor for processing visual data received via the sensor; and a general purpose graphics processing engine for performing camera pose estimation on the image data and generating a transformation matrix for transforming the position of the camera pose estimation to a position within a human-readable position map. The image and transformed locations are used to train the CNN to perform repositioning.,"['G01C21/206', 'G06F17/15', 'G06F17/16', 'G06F18/24143', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/08', 'G06N3/09', 'G06T17/00', 'G06T7/248', 'G06T7/55', 'G06T7/579', 'G06T7/74', 'G06T7/75', 'G06V10/764', 'G06V10/82', 'G06V20/10', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30244']"
US20230176840A1,Learned graph optimizations for compilers,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for compiler optimizations using a compiler optimization network. One of the methods includes receiving an input program, wherein the input program defines a graph of operation modules, wherein each node in the graph is a respective operation module, and each edge between nodes in the graph represents one operation module receiving the output generated by another operation module. The input program is processed by a compiler optimization network comprising a graph-embedding network that is configured to encode operation features and operation dependencies of the operation modules of the input program into a graph embedding representation and a policy network that is configured to generate an optimization action for each of one or more nodes encoded in the graph embedding representation. The compiler optimization network generates an output optimization plan comprising one or more optimization actions for the input program.","['G06F8/451', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06F8/433', 'G06F8/443']"
US20220377093A1,System and method for data compliance and prevention with threat detection and response,"A system and method to identify and prevent cybersecurity attacks on modern, highly-interconnected networks, to identify attacks before data loss occurs, using a combination of human level, device level, system level, and organizational level monitoring.","['H04L63/1425', 'H04L63/1408', 'G06F21/554', 'G06F21/577', 'H04L43/045', 'H04L43/08', 'H04L63/1433', 'H04L41/142', 'H04L41/16', 'H04L41/22']"
US10879727B1,Power source load control,"A method and apparatus for controlling the load presented to one or more power sources such as a power grid, backup power generator or solar panel is described. By selectively connecting, disconnecting, limiting and controlling various loads which are powered, the power grid service connection and power sources may be economically sized and operated while allowing reliability and convenience in selecting and powering loads. The control of the loads connected to the power sources is prioritized by various parameters including the power source operation parameters including load handling capability, type of load, load size, environmental factors, load usage during and subsequent to load connection, load priority and operator wishes. The control may also operate to facilitate transfer of power from one power source or load to another power source or load.","['H02J9/00', 'H04L12/2816', 'H02J3/007', 'H02J3/14', 'H02J7/007', 'H02J9/062', 'H02J1/00', 'H02J2207/20', 'H02J2207/40', 'H02J2300/22', 'H02J2310/12', 'H02J2310/14', 'H02J3/00', 'H04L2012/285', 'Y02B70/3225', 'Y04S20/222']"
EP4047935A1,"Point cloud data transmission device, point cloud data transmission method, point cloud data reception device, and point cloud data reception method","Disclosed herein is a point cloud data transmission method including encoding the point cloud data, encapsulating the point cloud data, and transmitting point cloud data. Disclosed herein is a point cloud data reception device including a receiver configured to receive the point cloud data, a decapsulator configured to decapsulate the point cloud data, and a decoder configured to decode the point cloud data.","['H04N19/597', 'H04N19/17', 'G06T17/05', 'G06T9/00', 'G06T9/001', 'H04N19/31', 'H04N19/70', 'H04N21/2343', 'H04N21/2362', 'H04N21/2365', 'H04N21/42201', 'H04N21/4345', 'H04N21/4347', 'H04N21/4402', 'H04N21/440245', 'H04N21/44218', 'H04N21/4728', 'H04N21/8153', 'H04N21/816', 'H04N21/85406', 'G06T2200/04', 'G06T2207/10028']"
WO2023115842A1,Data-driven offline and online integrated simulation system and method for power distribution network,"Provided are a data-driven offline and online integrated simulation system and method for a power distribution network. An adaptive topology visualization method for a radiation-type power distribution network comprises: S1, extracting topology data of a power distribution network; S2, determining a zero-level trunk on the basis of the topology data of the power distribution network, and on the basis of the zero-level trunk, performing view division on a display view of the topology data of the power distribution network; and S3, traversing all next-level trunks, and performing visualization. The network structure characteristics of a new power distribution network are taken into consideration, the overall topological structure of the network is improved by means of diversified layouts, and how to determine and distribute the specific positions of nodes and edges of a mapping geometric figure within a limited display space is determined by means of a topology layout. The structural features of a radiation-type power distribution network are fully used, and all layouts and positions are generated at one instance without adjustment during a subsequent topology process, such that the topology visualization speed is fast.",['H02J13/00001']
CN117151220B,Entity link and relationship based extraction industry knowledge base system and method,"The application discloses an industry knowledge base system and method based on entity link and relation extraction, relates to the technical field of knowledge base construction, which comprises the following steps: using an entity recognition model based on transfer learning, obtaining an entity contained in the text; the method comprises the steps of performing feature extraction and fusion on multi-modal information containing text features, image features and audio features by adopting a deep learning model, and outputting the fused multi-modal features of an entity; generating candidate entities for each input entity from a knowledge base by adopting a method based on character string matching and word vector matching, and selecting the candidate entity which is most matched with the context information for linking by using a joint inference model based on a knowledge graph; extracting the relation between the linked entities from the input text by adopting a method based on dependency syntactic analysis and semantic role labeling; and constructing an industry field knowledge graph. Aiming at the problem of low entity link accuracy in the prior art, the method and the device improve the entity link accuracy in the knowledge base construction process.","['G06N5/022', 'G06F16/901', 'G06F16/90344', 'G06F18/256', 'G06F40/211', 'G06F40/279', 'G06N3/0442', 'G06N3/0464', 'G06N3/08', 'G06N5/041', 'Y02D10/00']"
CN108983705B,Positive kinematics modeling and resolving method of multi-axis robot system based on axis invariants,"The invention provides a positive kinematics modeling and resolving method of a multi-axis system based on axis invariants. The method realizes the modeling and real-time resolving of internal compactness, real-time and function multiplexing and simplification layering and complete parameters, has the function of pseudo code and symbolic analysis, can be set into circuits and codes, and can be directly or indirectly executed partially or completely in a multi-axis machine system. In addition, the present invention also includes the analysis and verification system constructed on the principle, which is used for designing and verifying the multi-axis machine system.","['B25J19/007', 'G05B19/4142', 'B25J9/1664', 'B25J19/0037', 'B25J9/1605', 'B25J9/163', 'G05B2219/34013', 'G05B2219/41413']"
CN109564700B,Hierarchical Z-culling (HiZ) optimization for texture-dependent discard operations,"Methods and apparatus relating to techniques for providing hierarchical Z culling (HiZ) optimization for texture-dependent discard operations are described. In an embodiment, a processor performs one or more operations (such as HiZ or hierarchical stencil test) on depth data of an image tile in response to determining that a texture space boundary of the image tile is completely opaque. The processor performs the one or more operations whether or not a discard operation is enabled. Other embodiments are also disclosed and claimed.","['G06T15/405', 'G06T15/005', 'G06T15/04', 'G06T15/40', 'G06T2210/62']"
US10482113B2,Systems and methods to build and utilize a search infrastructure,"Methods and systems to build and utilize a search infrastructure are described. The system generates index information components in real-time based on a database that is time-stamped. The system updates index information at a plurality of query node servers based on the index information components. A query engine receives a search query from a client machine and identifies search results based on the query and the index information. The system communicates the search results, over the network, to the client machine.","['G06F16/316', 'G06F16/2228', 'G06F16/245', 'G06F16/24578', 'G06F16/319', 'G06F16/337', 'G06F16/93']"
CN105554059B,Logistics transportation Intellisense and position service system based on Beidou navigation technology,"The present invention relates to a kind of logistics transportation Intellisenses and position service system based on Beidou navigation technology, includingï¼šLocation-based service comprehensive platform, Intellisense and location-based service transmission network, vehicle intelligent terminal and vehicle intelligent Sensor Networkï¼›The location-based service comprehensive platform is under the support of the Intellisense and location-based service transmission network, monitoring and scheduling to access terminal and application service are provided, realized to total factor, overall process and comprehensive IT application during production run, intelligent managementï¼›The Intellisense and location-based service transmission network are for realizing the two-way communication between terminal and platformï¼›The vehicle intelligent terminal is original data processing center, has Big Dipper positioning, Beidou navigation, multimedia and record data storage functionï¼›The vehicle intelligent Sensor Network is using the vehicle intelligent terminal as the wired of core, the transmission of the on-vehicle information that wirelessly combines and sensing network.The present invention can realize the informatization strategy upgrading of carrier.","['H04L67/12', 'H04L67/52']"
US10776189B2,API query,A plurality of network application programming interfaces (APIs) are received. The plurality of network APIs are transformed then mapped into a unified data graph. A query in a distributed computing environment is performed using the unified data graph or performing discovery for at least one of the plurality of network APIs using the unified data graph.,"['G06F9/547', 'G06F16/24545', 'G06F16/2455', 'G06F16/256', 'G06F16/951']"
CN115618045B,"Visual question answering method, device and storage medium","The invention discloses a visual question answering method, a visual question answering device and a storage medium, wherein the method comprises the following steps: taking the picture and the corresponding problem text as input, and extracting picture characteristics and problem text characteristics through a multi-mode pre-training model; excavating implicit knowledge according to the picture characteristics and the problem text characteristics, and performing display modeling on the implicit knowledge to obtain implicit knowledge characteristics; external knowledge is retrieved from a knowledge base to obtain explicit knowledge characteristics; and fusing the implicit knowledge and the explicit knowledge to obtain fused knowledge characteristics, and performing answer reasoning according to the fused knowledge characteristics. The invention carries out explicit modeling on implicit knowledge hidden in the model and in the data set, can further mine and supplement the utilization of the pre-training model knowledge, and in addition, introduces external knowledge, can combine the supplemented common knowledge to assist the model to carry out further reasoning, thereby improving the reasoning accuracy. The invention can be widely applied to the field of visual question answering.","['G06F16/583', 'G06F16/532', 'G06F16/90332', 'G06N5/022', 'G06N5/027']"
US11757920B2,User and entity behavioral analysis with network topology enhancements,"A system and method for network cybersecurity analysis that uses user and entity behavioral analysis combined with network topology information to provide improved cybersecurity. The system and method involve gathering network entity information, establishing baseline behaviors for each entity, and monitoring each entity for behavioral anomalies that might indicate cybersecurity concerns. Further, the system and method involve incorporating network topology information into the analysis by generating a model of the network, annotating the model with risk and criticality information for each entity in the model and with a vulnerability level between entities, and using the model to evaluate cybersecurity risks to the network. Risks and vulnerabilities associated with user entities may be represented, in part or in whole, by the behavioral analyses and monitoring of those user entities.","['H04L63/1416', 'H04L41/12', 'H04L41/122', 'H04L41/145', 'H04L41/147', 'H04L41/149', 'H04L41/22', 'H04L41/40', 'H04L63/1425', 'H04L63/1433', 'H04L63/1466', 'H04L63/20', 'G06N20/00']"
US11018529B2,Wireless charger for underwater vehicles fed from a constant current distribution cable,"An apparatus for inductive power transfer (â€œIPTâ€) includes an active bridge section with input terminals that receive power from a constant current source, where the active bridge section operates at a fixed switching frequency, a primary resonant capacitor connected in series with an output terminal of the active bridge section, and a primary IPT coil connected in series with the primary resonant capacitor, where power is transferred wirelessly between the primary IPT coil and a secondary IPT coil, and the secondary IPT coil is connected in series with a secondary resonant capacitor, which is connected in series with an output rectifier section that receives power from the secondary IPT coil and comprising output terminals for connection to a load. The apparatus includes a controller that regulates output voltage to the load, where the controller regulates output voltage to the load by controlling switching of the active bridge section.","['B60L53/12', 'B60L53/30', 'B60L53/60', 'B63G8/00', 'H02J50/12', 'H02J7/025', 'H02M3/01', 'H02M3/33573', 'H02M3/33592', 'B60L2200/32', 'H02J2310/48', 'H02M1/0058', 'H02M3/285', 'H02M3/33576', 'Y02T10/70', 'Y02T10/7072', 'Y02T90/12', 'Y02T90/14']"
EP4138047A2,"Method of processing video, method of querying video, and method of training model","The present disclosure provides a method of processing a video, a method of querying a video, and a method of training a video processing model, which relate to a field of artificial intelligence, in particular to fields of computer vision, video understanding and deep learning technologies, and may be applied to smart city, intelligent transportation and other scenarios. A specific implementation solution of the method of processing the video includes: extracting, for a video to be processed, a plurality of video features under a plurality of receptive fields; extracting a local feature of the video to be processed according to a video feature under a target receptive field in the plurality of receptive fields; obtaining a global feature of the video to be processed according to a video feature under a largest receptive field in the plurality of receptive fields; and merging the local feature and the global feature to obtain a target feature of the video to be processed.","['G06V20/46', 'G06F18/253', 'G06F16/73', 'G06F16/732', 'G06F16/78', 'G06F16/783', 'G06F16/7867', 'G06F18/2155', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/08', 'G06V10/42', 'G06V10/44', 'G06V10/454', 'G06V10/50', 'G06V10/774', 'G06V10/80', 'G06V10/82', 'G06V10/86', 'G06V20/41', 'G06V20/48', 'G06N3/0499']"
US20250045644A1,Systems and methods for lightweight cloud-based machine learning model service,"A lightweight machine learning model (MLM) microservice is hosted in a cloud computing environment suitable for large-scale data processing. A client system can utilize the MLM service to run a MLM on a dataset in the cloud computing environment. The MLM can be already developed, trained, and tested using any appropriate ML libraries on the client side or the server side. However, no data schema is required to be provided from the client side. Further, neither the MLM nor the dataset needs to be persisted on the server side. When a request to run a MLM is received by the MLM service from a client system, a data schema is inferred from a dataset provided with the MLM. The MLM is run on the dataset utilizing the inferred data schema to generate a prediction which is then returned by the MLM service to the client system.","['G06N20/20', 'G06F16/211', 'G06N5/01', 'G06N5/04', 'H04L67/10', 'H04L67/133']"
CN111325095B,Intelligent detection method and system for equipment health state based on acoustic wave signals,"The invention belongs to the field of sound wave signal processing, and particularly relates to an intelligent detection method and system for equipment health state based on sound wave signals. When the device is used, a signal to be tested is collected, then the signal is input into a trained model for prediction, and a prediction result is displayed in real time. The method and the system not only strengthen the self-adaption capability and the judgment accuracy of the model, but also can display the result in real time and improve the efficiency of fault detection.","['G06F2218/08', 'G01H17/00', 'G06N3/045', 'G06N3/084', 'G06F2218/12']"
US12033436B2,Methods and apparatus for human pose estimation from images using dynamic multi-headed convolutional attention,An apparatus for 3D human pose estimation using dynamic multi-headed convolutional attention mechanism is presented. The apparatus contains two dynamic multi-headed convolutional attention mechanism with spatial attention and another with temporal attention that leverages the spatial attention mechanism to extract frame-wise inter-joint dependencies by analyzing sections of limbs that are related. The temporal attention mechanism extracts global inter-frame relationships by analyzing correlations between the temporal profile of joints. The temporal profile mechanism leads to a more diverse temporal attention map while achieving substantial parameter reduction.,"['G06V20/647', 'G06V40/23', 'G06T7/75', 'G06V10/806', 'G06V10/82', 'G06V40/103', 'G16H20/30', 'G16H30/40', 'G16H50/50', 'G06T2207/30196']"
EP4244770A1,Architecture for explainable reinforcement learning,An exemplary embodiment may provide an explainable reinforcement learning system. Explanations may be incorporated into an exemplary reinforcement learning agent/model or a corresponding environmental model. The explanations may be incorporated into an agent's state and/or action space. An explainable Bellman equation may implement an explainable state and explainable action as part of an explainable reward function. An explainable reinforcement learning induction method may implement a dataset to provide a white-box model which mimics a black-box reinforcement learning system. An explainable generative adversarial imitation learning model may implement an explainable generative adversarial network to train the occupancy measure of a policy and may generate multiple levels of explanations. Explainable reinforcement learning may be implemented on a quantum computing system using an embodiment of an explainable Bellman equation.,"['G06N20/00', 'G06N3/08', 'G09B9/00', 'G06N3/042', 'G06N3/045', 'G06N3/047', 'G06N7/01']"
CN110703057B,Power equipment partial discharge diagnosis method based on data enhancement and neural network,"The invention provides a power equipment partial discharge diagnosis method based on data enhancement and a neural network, aiming at partial discharge data detected when power equipment such as a transformer, a GIS (geographic information System), a switch cabinet and the like is defective, enriching sample amount and establishing a sample library by using a plurality of data enhancement methods, aiming at the characteristics of different power equipment types and defect types, adopting different neural networks for training, thereby obtaining a diagnosis algorithm with stronger generalization capability and stronger pertinence of different defect diagnoses of different power equipment, and improving the partial discharge diagnosis accuracy of the power equipment.",['G01R31/1227']
US10423688B1,Notifying entities of relevant events,"The present invention extends to methods, systems, and computer program products for notifying entities of relevant events. An event feed containing a plurality of events is received. Each event includes an event location, an event category, an event an event truthfulness, an event severity, and an event time. Entity notification preferences defining events relevant to an entity are accessed. Location and distance preferences collectively define an interest in events within a specified distance of one or more locations. The time preferences define that event notification occur at least within a specified time period of event detection. For an event in the event feed, characteristics of the event are compared to the entity notification preferences. It is determined that the event satisfies the entity notification preferences based on the comparisons. The entity (or another entity) is notified of the event in compliance with the time preferences.","['G06F16/9535', 'G06F16/9537']"
CN115964467B,Visual context-fused rich-semantic dialogue generation method,"The invention discloses a rich-semantic dialogue generating method for fusing visual situation, which is characterized in that a challenging audiovisual scene perception dataset is collected to train a model, a multi-step cross-modal attention mechanism is designed and realized on the basis of a transducer by an overall model, heterogeneous semantic association among different modes in space-time dimension is captured in fine granularity, then multi-modal characteristic representation is combined to form a space-time diagram structure and cross-modal learning reasoning is carried out by using a graph convolution network, and finally dialogue reply which accords with the current situation and has rich and accurate content is generated by decoding. According to the invention, multi-angle fine-granularity progressive feature interaction and inter-modal semantic association are captured through multi-modal data fusion and cross-modal interaction, so that visual-language cross-modal semantic alignment is realized, the semantic understanding and reasoning capacity of a model is improved, and finally, a reply with rich information and high quality is generated.",['Y02D10/00']
US20200389495A1,Secure policy-controlled processing and auditing on regulated data sets,"A system and method for secure policy-controlled processing and auditing on regulated data sets utilizing metadata and a plurality of analytics. The system and method combine the ability to restrict and control the transport and processing of data based on specified directives and provide rich auditable provenance to support evidential requirements. The system and method may additionally automatically optimize data routes and specify processing hardware based on data residency, sovereignty, or localization restrictions, furthermore, protect sensitive data from compromise by algorithmically generating digital tokens and employ sensors on all devices in the chain to provide a secure means of data transport.","['H04L63/20', 'H04L63/1425', 'G06F16/2477', 'G06F16/9024', 'G06F16/951', 'H04L63/1441']"
US11258258B2,Multi-input power conversion and energy storage,"Apparatuses, systems, and methods are presented for energy storage. A plurality of input connectors are configured to receive input power from one or more power sources. A plurality of input power converters are coupled to the input connectors, and are configured to convert the input power to direct current (DC) power for storage. A controller is configured to control power flow through the input power converters on a per-converter basis so that separate converters are separately controlled. One or more output power converters are configured to convert stored DC power to output power for use by one or more loads. The controller is configured to control power flow through the one or more output power converters. One or more output connectors are configured to transfer the output power to the one or more loads.","['H02J3/32', 'H02J3/38', 'H02J3/383', 'H02J3/388', 'H02J7/0021', 'H02J7/35', 'H02J2203/20', 'H02J3/386', 'Y02E10/56', 'Y02E10/76']"
US20220263860A1,Advanced cybersecurity threat hunting using behavioral and deep analytics,"A system for cyber threat hunting employing an advanced cyber decision platform comprising a time series data store, a directed computational graph module, an automated planning service module, and observation and state estimation module, wherein the state of a network is monitored and used to predict network resources that may be vulnerable to a future cyber threat and to produce a cyber-physical graph representing the vulnerable network resources, a human operator is provided with the cyber-physical graph to analyze the data contained therein to initiate an investigation of network resources, and the results of the threat investigation and their effects are analyzed to produce security recommendations.","['G06F11/3006', 'G06F11/3041', 'G06N20/00', 'G06N20/10', 'H04L63/1408', 'H04L63/1425', 'H04L63/1433', 'H04L63/1441', 'G06F11/362']"
US12051123B2,Utility resource asset management system,"The UTILITY RESOURCE ASSET MANAGEMENT SYSTEM APPARATUSES, METHODS AND SYSTEMS (â€œURAMSâ€) transform weather, terrain, and utility asset parameter data via URAMS components into damage predictions with confidence metrics, alerts, and asset allocation and response plans.","['G06Q50/06', 'G06Q10/04', 'G06Q10/06', 'G06Q10/0635', 'G06Q10/067']"
US11448671B2,Signature identification for power system events,"Briefly, embodiments are directed to a system, method, and article for identifying power system event signatures. Input measurement data may be received from one or more data sources relating to a power grid system. The input measurement data may comprise normal system operation measurement data and power system event measurement data. A processor may perform operations during an online application phase. During the online application phase, a feature matrix may be generated for the power system event measurement data and the at least one trained auto-associative model. The feature matrix for the power system event measurement data may be processed to determine power system event residuals. Also during the online application phase, the power system event signatures may be identified based on residual statistics for normal system operation measurement data residuals and on the power system event residuals.","['G01R19/2513', 'G01R19/2516', 'G01R31/086', 'G06N3/045', 'G06N3/08', 'G06N3/088', 'Y04S10/52']"
US10880362B2,Virtual electrical networks,"Systems, methods, and computer-readable media are provided for virtual electrical networks (VENs). An Energy Management System (EMS) appliance identifies each physical node of a plurality of physical nodes in an electrical grid as being an energy source node or an energy consumer node, where each node is electrically coupled to at least one other physical node. The EMS appliance generates a VEN to include a plurality of virtual nodes to represent the plurality of physical nodes and virtual couplings to couple the virtual nodes. The plurality of virtual nodes include virtual energy source nodes and virtual energy consumer nodes. The VEN is used to identify faulty virtual nodes and to identify electrical couplings to achieve load balancing for the conservation of energy resources. Instructions are generated to reconfigure the electrical grid according to remove faulty nodes and/or to achieve the load balancing. Other embodiments are disclosed and/or claimed.","['G06Q10/06', 'G06F9/5072', 'G06Q50/06', 'H02J3/005', 'H02J3/007', 'H02J9/061', 'H04L29/06', 'H04L63/0272', 'H04L67/10', 'H04L67/1004', 'H04L9/40', 'H02J2203/20', 'H02J2300/10', 'H02J3/28', 'H02J3/381', 'H04L67/12', 'Y02E60/00', 'Y04S40/18', 'Y04S40/20']"
EP3716574A1,Uniform resource locator classifier for malicious site detection,"Aspects of the disclosure relate to detecting and identifying malicious sites using machine learning. A computing platform may receive (505) a uniform resource locator (URL). The computing platform may parse and/or tokenize (510) the URL to reduce the URL into a plurality of components. The computing platform may identify (515) human-engineered features of the URL. The computing platform may compute a vector representation of the URL to identify (520) deep learned features of the URL. The computing platform may concatenate (525) the human-engineered features of the URL to the deep learned features of the URL, resulting in a concatenated vector representation. By inputting the concatenated vector representation of the URL to a URL classifier, the computing platform may compute (530) a phish classification score. In response to determining that the phish classification score exceeds a first phish classification threshold, the computing platform may cause a cybersecurity server to perform a first action (540, 545).","['H04L63/1483', 'G06F16/51', 'G06F16/9566', 'G06F18/217', 'G06F21/56', 'G06N20/00', 'G06N20/10', 'G06N3/08', 'H04L63/1408', 'H04L63/1416', 'H04L63/1441', 'G06F2221/2119', 'G06V2201/09']"
US12080084B2,Scene text detection method and system based on sequential deformation,"A method and a system for detecting a scene text may include extracting a first feature map for a scene image input based on a convolutional neural network, and delivering the first feature map to a sequential deformation module; obtaining sampled feature maps corresponding to sampling positions by performing iterative sampling for the first feature map, obtaining a second feature map by performing a concatenation operation in deep learning according to a channel dimension for the first feature map and the sampled feature maps; obtaining a third feature map by performing a feature aggregation operation for the second feature map in the channel dimension, and delivering the third feature map to the object detection baseline network; and performing text area candidate box extraction for the third feature map and obtaining a text area prediction result as a scene text detection result through regression fitting.","['G06V30/413', 'G06F18/214', 'G06F18/253', 'G06F30/27', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G06V10/225', 'G06V10/40', 'G06V10/44', 'G06V10/82', 'G06V20/63', 'G06T2207/20081', 'G06T2207/20084', 'G06V30/10']"
US11183843B1,Power source load control,"A method and apparatus for managing one or more loads powered by an alternating current power source in order to prevent and mitigate overloads and other abnormal conditions of the power source. The management includes monitoring the amount of power supplied to one or more or all loads by the power source and selectively connecting, disconnecting, limiting and controlling various loads which are powered thereby. The method and apparatus include conveying information by use of AC frequency to controllable loads and load control devices, the frequency relating to the amount of power being supplied by the power source, overloading and other power source conditions. The amount of power consumed by the loads is controllable by the loads and load control devices in response to the frequency of the AC power.","['H02J3/14', 'H02J3/0075', 'H02J1/10', 'H02J13/00004', 'H02J3/32', 'H02J3/322', 'H02J3/38', 'H02J9/06', 'H02J2310/14', 'H02J7/35', 'Y02B70/30', 'Y02B70/3225', 'Y02B90/20', 'Y04S20/12', 'Y04S20/222', 'Y04S20/242', 'Y04S20/248']"
US11691278B2,Hybrid computing achitectures with specialized processors to encode/decode latent representations for controlling dynamic mechanical systems,Provided is a robot that includes: a first sensor having a first output and configured to sense state of a robot or an environment of the robot; a first hardware machine-learning accelerator coupled to the first output of the first sensor and configured to transform information sensed by the first sensor into a first latent-space representation; a second sensor having a second output and configured to sense state of the robot or the environment of the robot; a second hardware machine-learning accelerator configured to transform information sensed by the second sensor into a second latent-space representation; and a processor configured to control the robot based on both the first latent-space representation and the second latent-space representation.,"['B25J9/163', 'B25J13/086', 'B25J9/161']"
US20200392178A1,Protein-targeted drug compound identification,"Methods and systems are provided for identifying drug compounds for targeting proteins in tissue cells. Such a method includes providing a neural network model which comprises an attention-based protein encoder and a molecular decoder. The protein encoder is pretrained in an autoencoder architecture to encode an input protein sequence into an output vector in a latent space representing proteins. The molecular decoder is pretrained in an autoencoder architecture to generate compound data, defining a compound molecule, from an input vector in a latent space representing molecules. The protein encoder and molecular decoder are coupled such that the input vector of the molecular decoder is dependent on the output vector of the protein encoder for an input protein sequence.","['C07K1/047', 'G16C20/50', 'G06N3/006', 'G06N3/045', 'G06N3/084', 'G06N3/088', 'G16B15/30', 'G16B5/00', 'G06N7/01', 'G16B40/00', 'G16C20/30', 'G16C20/70']"
US11392550B2,System and method for investigating large amounts of data,"A data analysis system is proposed for providing fine-grained low latency access to high volume input data from possibly multiple heterogeneous input data sources. The input data is parsed, optionally transformed, indexed, and stored in a horizontally-scalable key-value data repository where it may be accessed using low latency searches. The input data may be compressed into blocks before being stored to minimize storage requirements. The results of searches present input data in its original form. The input data may include access logs, call data records (CDRs), e-mail messages, etc. The system allows a data analyst to efficiently identify information of interest in a very large dynamic data set up to multiple petabytes in size. Once information of interest has been identified, that subset of the large data set can be imported into a dedicated or specialized data analysis system for an additional in-depth investigation and contextual analysis.","['G06F16/1744', 'G06F16/902', 'G06F11/2025', 'G06F16/10', 'G06F16/13', 'G06F16/148', 'G06F16/17', 'G06F16/2365', 'G06F16/24575', 'G06F16/248', 'G06F16/258', 'G06F16/35', 'G06F16/9535', 'G06F17/00']"
WO2025096974A1,Methods and systems for enhanced searching of conversation data and related analytics in a contact center,"A method in a contact center for generating insights from conversation data derived from interactions and storing the insights in an index. The method may include: determining an insight type; based on the insight type, determining inputs including a question prompt, answer prefix, and relevant portion of the conversation data; inputting the inputs into a LLM configured to receive the inputs and generate output text answering a question contained in the question prompt pursuant to an answer form suggested by the answer prefix given content contained in the relevant portion of the conversation data; generating the output text via operation of the LLM; transforming the output text of the first insight via a sentence transformer into vector embedding representative of a semantic meaning of the output text; and storing the computed vector embedding of the first insight in the index.","['G06F40/40', 'G06F16/33295', 'G06F16/951', 'G06F40/279', 'G06F40/35', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N5/00', 'G06Q10/10', 'G06Q30/01', 'G06Q30/015', 'G06Q30/0201', 'G06Q50/01']"
US10678225B2,Data analytic services for distributed industrial performance monitoring,"A data analytics service for performing data analytics functions within a distributed process control environment is provided. The service may provide a user interface for creating a first diagram, representing a data model. The first diagram may be configured, compiled, and evaluated using off-line (i.e., historical) data from a distributed process control system, which may include data stored in distributed data engines (DDEs). Following evaluation, the first diagram may be automatically converted into a second diagram that is bound to on-line (i.e., real-time) data sources within the process control environment, which may then be compiled and executed to generate performance or predictive analytics data for the process. The diagrams may comprise a plurality of configurable function blocks or modules, connected logically via wires conveying outputs or inputs of the blocks or modules.","['G05B19/41865', 'G05B19/41845', 'G05B19/4186', 'G05B19/41875', 'G05B2219/31455', 'G05B2219/32128', 'G05B2219/32339', 'Y02P90/02', 'Y02P90/14', 'Y02P90/18', 'Y02P90/22', 'Y02P90/26', 'Y02P90/80', 'Y02P90/86']"
US12267347B2,System and method for comprehensive data loss prevention and compliance management,"A system and method to identify and prevent cybersecurity attacks on modern, highly-interconnected networks, to identify attacks before data loss occurs, using a combination of human level, device level, system level, and organizational level monitoring.","['H04L63/1425', 'H04L43/045', 'H04L43/08', 'H04L63/1433', 'G06F21/577', 'H04L43/026']"
US20230067528A1,Multimodal domain embeddings via contrastive learning,"Systems and methods are provided for building and training machine learning models configured to generate in-domain embeddings and perform multimodal analysis inside the same domain. The models include a first encoder trained to receive input from one or more entities represented in a first modality and to encode the one or more entities in the first modality, such that the first encoder is configured to output a first set of embeddings. The models also include a second encoder trained to receive input from one or more entities represented in the second modality and to encode the one or more entities in the second modality, such that the second encoder is configured to output a second set of embeddings. The models also include a projection layer configured to project the first set of embeddings and the second set of embeddings to a shared contrastive space.","['G16C20/70', 'G16B15/00', 'G06F18/2155', 'G06K9/6259', 'G06N3/0455', 'G06N3/088', 'G06N3/0895', 'G16H70/40', 'G06N3/048', 'G16C20/30', 'G16C20/40']"
US11586880B2,System and method for multi-horizon time series forecasting with dynamic temporal context learning,"A system and a method for time series forecasting. The method includes: providing input feature vectors corresponding to a plurality of future time steps; performing bi-directional long-short term memory network (BiLSTM) on the input feature vectors to obtain hidden outputs corresponding to the plurality of future time steps; for each future time step: performing temporal convolution on the hidden outputs using a plurality of temporal scales to obtain context features at the plurality of temporal scales, and summating the context features at the plurality of temporal scales using a plurality of weights to obtain multi-scale context features; and converting the multi-scale context features to obtain the time series forecasting corresponding to the future time steps.","['G06Q10/04', 'G06F17/11', 'G06F17/18', 'G06N3/0418', 'G06N3/044', 'G06N3/045', 'G06N3/049', 'G06N3/084']"
US10310620B2,Type-agnostic RF signal representations,"This document describes techniques and devices for type-agnostic radio frequency (RF) signal representations. These techniques and devices enable use of multiple different types of radar systems and fields through type-agnostic RF signal representations. By so doing, recognition and application-layer analysis can be independent of various radar parameters that differ between different radar systems and fields.","['G06F3/017', 'G01S13/08', 'G01S13/58', 'G01S13/88', 'G01S7/292', 'G01S7/354', 'G01S7/415', 'G06F3/011']"
CN115205949B,Image generation method and related device,"The application discloses an image generation method and related equipment, and related embodiments can be applied to various scenes such as cloud technology, artificial intelligence, intelligent traffic, auxiliary driving and the like; the method comprises the steps of obtaining an original face image frame of a target object, audio driving information corresponding to a target face image frame to be generated and emotion guide information; extracting the spatial features of the original facial image frame to obtain the spatial features of the original facial image; performing feature interaction processing on the audio driving information and the emotion guidance information to obtain local facial posture features; and performing face reconstruction processing on the target object based on the original face space characteristics and the face local posture characteristics to generate a target face image frame. According to the method and the device, the audio driving information and the emotion guiding information can be utilized to capture the facial posture detail information of the target object part, so that the original facial image frame is subjected to facial adjustment, the corresponding target facial image frame is obtained, and the generation efficiency and accuracy of the target facial image frame are improved.","['G06T13/40', 'G06V40/171', 'G06N3/084', 'G06T13/205', 'G06V10/467', 'G06V10/761', 'G06V10/774', 'G06V10/806', 'G06V10/82', 'G06V40/172', 'G06V40/174', 'G06V2201/07', 'G10L25/63']"
CN112905868B,"Event extraction method, device, equipment and storage medium","The application discloses an event extraction method, an event extraction device, event extraction equipment and a storage medium, wherein the event extraction method comprises the following steps: acquiring a text to be processed; inputting the text to be processed into an event detection model for detection processing, and determining the event type in the text to be processed; constructing a first query corpus based on the event type; inputting the text to be processed and the first query corpus into a reading understanding model, and determining event elements in the text to be processed; and determining an event extraction result of the text to be processed based on the event type and the event element. According to the technical scheme, the event detection model and the reading understanding model are used, so that the event type and the event element in the text to be processed can be accurately determined, the method is suitable for event extraction in text with chapter level, the overall performance of event extraction is improved, and the event extraction efficiency is further improved.","['G06F16/953', 'G06F16/3329', 'G06F16/3344', 'G06F16/335', 'G06F16/35', 'G06F40/30']"
US20230073843A1,Data compatibility for text-enhanced visual retrieval,"An interaction module includes: a first text-image interaction module configured to generate a vector representation of a first text-image pair based on an encoded representation of a reference image and an encoded representation of a text modifier, the reference image and the text modifier received from a computing device. A second text-image interaction module is configured to generate a vector representation of a second text-image pair based on the encoded representation of the text modifier and an encoded representation of a candidate target image. A compatibility module is configured to compute, based on the vector representation of the first text-image pair and the vector representation of the second text-image pair, a compatibility score for a triplet including the reference image, the text modifier, and the candidate target image. A ranking module is configured to rank a set of candidate target images including the candidate target image by compatibility scores.","['G06V10/82', 'G06F16/5866', 'G06F16/532', 'G06F16/56', 'G06F40/30', 'G06V10/774']"
US20210173711A1,Integrated value chain risk-based profiling and optimization,"A system and method for gathering and analyzing the value chain relationships between legal entities, people, systems, and real and intangible assets using a temporospatial knowledge graph of the integrated value chain. The system provides the ability to layer private data from paid vendors with end-user owned and public records data to enable more comprehensive, contextualized and complete representations of the underlying value chain. Data analysis techniques, such as deep learning and machine learning, are performed on the knowledge graph and its underlying data set, in conjunction with simulation and modeling, to analyze the value chain, including generation of a risk profile for an entity's value chain and potential optimization options to remediate the identified risks.","['G06F9/5011', 'G06F16/9024', 'G06F21/577', 'G06N5/022', 'G06N5/025', 'G06Q10/067', 'G06Q30/0201', 'G06Q30/0205', 'G06Q40/04', 'H04L63/1433', 'G06N20/00']"
US11972869B2,Systems and methods for diagnosing a health condition based on patient time series data,"Disclosed systems, methods, and computer readable media can diagnose a health condition based on patient time series data. For example, a method for diagnosing a health condition based on patient time series data includes identifying a training set of health records comprising a first set of patient time series data, training a neural network using the training set of health records, and executing the trained neural network model to diagnose a health condition based on a second set of patient time series data. In further examples, the first set of patient time series data and the second set of patient time series data can each comprise electrocardiogram data and the health condition can comprise pulmonary hypertension.","['A61B5/7267', 'G16H50/20', 'A61B5/36', 'G06N3/08', 'G16H10/60', 'A61B5/02007', 'A61B5/0245']"
CN109359309B,"Translation method and device, and translation model training method and device","The application provides a translation method and device and a translation model training method and device, wherein the translation method comprises the following steps: obtaining a sentence to be translated; inputting the sentence to be translated into a translation model to obtain a translation sentence corresponding to the sentence to be translated; wherein the translation model is obtained by training in the following way: respectively taking a first sentence and a translated second sentence in the target corpus as a training sample and a training feature; inputting the training sample into a coding layer of the translation model to obtain at least one coding vector; inputting at least one coding vector to a weight layer of the translation model to obtain a weight vector; inputting the training features and the weight vector to a decoding layer of the translation model to obtain a decoding vector, and obtaining an error of the decoding vector according to the decoding vector; and adjusting the weight layer according to the error of the decoding vector, and continuing to train the translation model until a training stop condition is reached.","['G06F40/58', 'G06F40/44']"
US10840735B1,Power source load control,"A method and apparatus for managing one or more grid supplied and separately metered power services, backup power sources, transfer switches and related powered loads using load monitoring and control which allow selectively connecting, disconnecting, limiting and controlling various loads which are powered thereby. The method and apparatus include operating with a system with a dual revenue meters providing grid power, a backup power source and a dual transfer switch wherein the power capabilities of each are economically sized while allowing reliability and convenience in selecting and powering loads. The connections to power sources and control of the loads powered thereby take into account various parameters including cost of power, load handling capability, type of load, load size, environmental factors, load usage during and subsequent to load connection, load priority and operator wishes.","['H02J3/0075', 'H02J13/00004', 'H02J3/14', 'H02J3/32', 'H02J9/06', 'H02J2310/14', 'H02J7/35', 'Y02B70/30', 'Y02B70/3225', 'Y02B90/20', 'Y04S20/12', 'Y04S20/222', 'Y04S20/242', 'Y04S20/248']"
CN112732916B,A BERT-based multi-feature fusion fuzzy text classification system,"The invention relates to a BERT-based multi-feature fusion fuzzy text classification model, which comprises the following contents: preparing a fuzzy text classification original data set; building a BERT _ MFFM model, wherein the BERT _ MFFM model comprises a BERT model, a convolutional neural network, a two-way long and short memory network and a Self-authorization module, the input of the BERT model is fuzzy text, the output of the BERT model is respectively connected with the convolutional neural network, the two-way long and short memory network and the Self-authorization module, and the local characteristics, sentence semantic characteristics and syntactic structure characteristics of the fuzzy text are respectively extracted; the output of the BERT model is spliced with the output of the bidirectional long and short memory network at the same time, and then the optimal sentence semantic features are screened out by using the maximum pooling operation; and fusing the local features, the optimal sentence semantic features and the syntactic structure features by adopting a parallel splicing mode, and classifying fuzzy texts of a fusion result through a SoftMax function to complete the construction of the BERT _ MFFM model. The problem of incomplete feature acquisition is solved, so that the accuracy of classification is improved.","['G06F16/35', 'G06F40/211', 'G06F40/30', 'G06N3/043', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
US10943241B2,"System, method and computer program product for selecting internet-based advertising","Embodiments of a system method and computer program product for selecting an advertisement and presenting it to a user are described. Products and services offered by various merchants are read using a merchant specific catalog and stored in a common format. Categories for such products and services are normalized and virtual categories are created using various product attributes. Visual creatives, termed as ad-templates are created to control the visual and interactive aspects of the ad, including ad-size, color, as well as product attributes that are displayed in the ad. Ad-templates may be constrained to specific products or product categories. A learning algorithm uses an adaptive sampling process to sample various products, product categories and ad-templates independently for different learning units such as individual users, groups of users determined by some demographics, individual web pages and groups of web pages grouped using various similarity criteria. The performance of the ad is measured using various learning statistics, such as the click-through-rate, conversion rate, etc. The learning algorithm uses the learning statistics to optimize the return for the advertiser by favoring the products or categories that perform better on one or more specified criteria.","['G06Q30/02', 'G06Q30/0245', 'G06Q30/0247', 'G06Q30/0254', 'G06Q30/0274']"
CN111159416A,"Language task model training method and device, electronic equipment and storage medium","The invention provides a language task model training method, a language task model training device, electronic equipment and a storage medium; the method comprises the following steps: performing layered pre-training in the language model based on corpus samples of corresponding language tasks in a pre-training sample set; carrying out forward propagation on the corpus samples of the corresponding language tasks in the training sample set in the language task model; fixing the parameters of the language model, and performing back propagation in the language task model to update the parameters of the task model; and carrying out forward propagation and backward propagation on the corpus samples corresponding to the language tasks in the training sample set in the language task model so as to update the parameters of the language model and the task model. The invention can prevent the catastrophic forgetting phenomenon of the language model and simultaneously ensure that the language model and the task model can achieve the training effect according with the corresponding learning rate.","['G06F16/355', 'G06F16/3329', 'G06F16/353', 'G06F16/36', 'G06N3/084']"
CN111241237B,Intelligent question-answer data processing method and device based on operation and maintenance service,"The invention discloses an intelligent question-answer data processing method and device based on operation and maintenance service. The method comprises the following steps: obtaining corpus data to be processed of a target object; word segmentation is carried out on the corpus data to be processed to obtain word segmentation results, wherein the word segmentation results comprise at least two word segmentation data and corpus analysis results; inputting the word segmentation result into a classification model for service classification, wherein the classification model is obtained by performing machine learning training on a plurality of service sample data, and the service sample data carries corresponding service classification labeling information; when the classified result indicates operation and maintenance service, inputting the word segmentation result into a problem matching model matched with the operation and maintenance service, and determining a target problem matched with the word segmentation result from a candidate problem library by utilizing the problem matching model; and acquiring an answer associated with the target question, and taking the associated answer as a target answer to be returned.","['G06F16/3329', 'G06F16/3335', 'G06F16/35', 'G06N3/08', 'G06Q40/03', 'G06Q50/40']"
US10860599B2,Tool for creating and deploying configurable pipelines,"A computing system may provide an interface for creating a data processing pipeline through which the computing system may receive configuration information for a given pipeline that is configured to receive streaming messages from a given data source, process each of the streaming messages, and then output a processed version of at least a subset of the streaming messages to a given data sink. The given pipeline may comprise a chain of two or more operators, which may take the form of enrichers, routers, and/or transformers. The computing system may then use the received configuration information to create the given pipeline. In turn, the computing system may deploy the given pipeline for use in processing streaming messages received from the given data source.","['G06F16/254', 'G06F9/546', 'G06F16/258', 'H04L65/762', 'H04L65/765']"
US11714849B2,Image generation system and method,"Embodiments of this application provide an image generation system and method. In an exemplary manufacturing industry scenario, a style requirement of a product category in a manufacturing industry is automatically captured according to user behavior data and product description information associated with the product category. Based on these data, a style description text may be generated and converted to product images by using a text prediction-based image generation model. The product images are further screened by using an image-text matching model, to obtain a product image with high quality. This process covers from style description text mining to text-to-image prediction to image quality evaluation. It provides an automation product image generation capability for the manufacturing industry, shorten a cycle of designing and producing the product image in the manufacturing industry, and improve production efficiency of the product image.","['G06F16/535', 'G06F16/583', 'G06F16/58', 'G06F16/5866', 'G06F18/22', 'G06V10/7788']"
US11922569B2,Generating realistic point clouds,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating realistic full-scene point clouds. One of the methods includes obtaining an initial scene point cloud characterizing an initial scene in an environment; obtaining, for each of one or more objects, an object point cloud that characterizes the object; and processing a first input comprising the initial scene point cloud and the one or more object point clouds using a first neural network that is configured to process the first input to generate a final scene point cloud that characterizes a transformed scene that has the one or more objects added to the initial scene.","['G06T19/20', 'G06T17/00', 'G06N20/00', 'G06N3/045', 'G06N3/047', 'G06N3/084', 'G06V20/10', 'G06V20/56', 'G06T2210/56', 'G06V2201/12']"
US12225042B2,System and method for user and entity behavioral analysis using network topology information,"A system and method for network cybersecurity analysis that uses user and entity behavioral analysis combined with network topology information to provide improved cybersecurity. The system and method involve gathering network entity information, establishing baseline behaviors for each entity, and monitoring each entity for behavioral anomalies that might indicate cybersecurity concerns. Further, the system and method involve incorporating network topology information into the analysis by generating a model of the network, annotating the model with risk and criticality information for each entity in the model and with a vulnerability level between entities, and using the model to evaluate cybersecurity risks to the network. Risks and vulnerabilities associated with user entities may be represented, in part or in whole, by the behavioral analyses and monitoring of those user entities.","['H04L63/1433', 'H04L63/102', 'H04L63/1416', 'H04L63/1425', 'H04L63/20']"
TWI742476B,Rf plasma systems,"Various RF plasma systems are disclosed that do not require a matching network. In some embodiments, the RF plasma system includes an energy storage capacitor; a switching circuit coupled with the energy storage capacitor, the switching circuit producing a plurality of pulses with a pulse amplitude and a pulse frequency, the pulse amplitude being greater than 100 volts; a resonant circuit coupled with the switching circuit. In some embodiments, the resonant circuit includes: a transformer having a primary side and a secondary side; and at least one of a capacitor, an inductor, and a resistor. In some embodiments, the resonant circuit having a resonant frequency substantially equal to the pulse frequency, and the resonant circuit increases the pulse amplitude to a voltage greater than 2 kV.","['H01J37/32183', 'H01J37/321', 'H01J37/32128', 'H01J37/32431', 'H02M11/00', 'H03K17/56', 'H03K3/36', 'H03K3/57']"
US12367585B2,Utilizing machine learning models to generate refined depth maps with segmentation mask guidance,"The present disclosure relates to systems, non-transitory computer-readable media, and methods for utilizing machine learning models to generate refined depth maps of digital images utilizing digital segmentation masks. In particular, in one or more embodiments, the disclosed systems generate a depth map for a digital image utilizing a depth estimation machine learning model, determine a digital segmentation mask for the digital image, and generate a refined depth map from the depth map and the digital segmentation mask utilizing a depth refinement machine learning model. In some embodiments, the disclosed systems generate first and second intermediate depth maps using the digital segmentation mask and an inverse digital segmentation mask and merger the first and second intermediate depth maps to generate the refined depth map.","['G06T7/11', 'G06T7/215', 'G06T7/50', 'G06T2207/20081', 'G06T2207/20084']"
CN108696764B,"A method, system, device and medium for processing 360 video","The present invention provides motion vector/mode prediction, region of interest based transmission, metadata capture and format detection for 360 video. An embodiment of an electronic processing system may include a 2D frame corresponding to a projection of a 360 video space, and a component predictor to predict an encoded component of a first block of the 2D frame based on encoding information from neighboring blocks that are adjacent to the first block of the 2D frame only in the 360 video space, a prioritizer to prioritize transmission of a second block of the 2D frame based on an identified region of interest, and/or a format detector to detect a 360 video format of the 2D frame based on image content. A 360 video capture device may include a context annotator to annotate 360 video content with context information contemporaneous with the captured 360 video content. Other embodiments are disclosed and claimed.","['H04N19/593', 'H04N21/4122', 'G06T1/20', 'G06T11/003', 'H04N19/105', 'H04N19/136', 'H04N19/167', 'H04N19/176', 'H04N19/436', 'H04N19/46', 'H04N19/52', 'H04N19/597', 'H04N21/234', 'H04N21/44', 'H04N21/816']"
US10289757B2,System and methods for solar photovoltaic array engineering,"The present application concerns automated optimization, customization or production methods for the design of a solar photovoltaic array, involving one or more or all components in a photovoltaic array, in which the products include system designs, production drawings, permitting and construction drawings, layouts for the mechanical and electrical systems, bill of materials and financial return analyses of such a photovoltaic array.","['G06F30/13', 'G06F17/5004', 'F24S25/11', 'F24S25/12', 'F24S25/13', 'G06F17/11', 'H02S10/00', 'H02S20/10', 'H02S20/32', 'H02S99/00', 'F24S2201/00', 'Y02E10/47', 'Y02E10/50']"
US10430263B2,"Devices, systems and methods for validating and upgrading firmware in intelligent electronic devices","Apparatuses, systems, and method for validating and upgrading firmware in an intelligent electronic device (IED) are provided. In one aspect of the present disclosure, an IED is provided including at least one processor and at least one memory. The at least one memory includes at least a first firmware and a second firmware, where the second firmware is a version of the first firmware. The at least one processor determines if there is an error associated with the first firmware. If the processor determines there is no error associated with the first firmware, the processor executes first firmware. If the processor determines there is an error associated with the at least one firmware, the processor executes the second firmware.","['G06F11/0736', 'G06F11/0751', 'G06F11/0769', 'G06F11/0772', 'G06F11/0793', 'G06F11/1004', 'G06F11/1417', 'G06F11/1433', 'G06F11/1441', 'G06F11/1448', 'G06F8/654', 'H04L67/34', 'G01R19/2513']"
CN113039555B,"Method, system and storage medium for classifying actions in video clips","Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for classifying actions in video. One of the methods is as follows: obtaining a feature representation of the video clip; obtaining data specifying a plurality of candidate agent bounding boxes in a key video frame; and processing the feature representation through the action transformer neural network for each candidate agent bounding box.","['G06V20/46', 'G06V40/20', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/09', 'G06V10/25', 'G06V10/82', 'G06V20/40', 'G06V20/41']"
US10855696B2,Variable runtime transpilation,"In an approach, an apparatus comprises: one or more processors; a processor logic coupled to the one or more processors and configured to: intercept, from a client computer, a request directed to a server computer that identifies a purported user agent executing on the client computer; send, to the server computer, the request from the client computer; intercept, from the server computer, one or more original instructions to be executed by the purported user agent of the client computer; determine one or more features supported by the purported user agent that are not utilized by the one or more original instructions; transform the one or more original instructions into one or more revised instructions which, when executed by the purported user agent, cause the purported user agent to utilize the one or more features; send, to the client computer, the one or more revised instructions.","['H04L63/1466', 'H04L63/1416', 'H04L63/1433', 'H04L63/1441', 'H04L63/168', 'H04L67/02', 'H04L67/42', 'H04L67/565']"
CN109564695B,Apparatus and method for efficient 3D graphics pipeline,"A graphics processing apparatus and method are described. For example, one embodiment of a graphics processing apparatus includes: an input assembler of the graphics pipeline for determining a first set of triangles to be drawn based on parameters provided by the application; a depth buffer for storing depth data relating to the first set of triangles; a vertex shader to perform a position-only vertex shading operation on the first set of triangles in response to an indication that the graphics pipeline was initially operating in a depth-only mode; a culling and clipping module for reading depth values from the depth buffer to identify those triangles of the first set of triangles that are fully occluded by other objects in the current frame and for generating culling data usable to cull the occluded triangles, the culling and clipping module for associating the culling data with a replay token to be used to identify a subsequent rendering process through the graphics pipeline; upon detecting the replay token in the latter rendering process, the input assembler is operable to access the culling data associated with the replay token to remove culled triangles from the first set of triangles to generate a second set of triangles; the vertex shader is to perform a full vertex shading operation on the second set of triangles during the subsequent rendering process, the replay token being destroyed during or after the subsequent rendering process.","['G06T15/005', 'G06T15/04', 'G06T15/205', 'G06T15/30', 'G06T15/405', 'G06T15/80', 'G06T17/10', 'G06T17/20', 'G06T2210/12']"
CN109154990B,Finding convolutional layers in convolutional neural networks,"An embodiment provides a processor including logic to accelerate convolutional neural network processing, the processor comprising: first logic to apply a convolution layer to an image to generate a first convolution result; and second logic to apply a find convolution layer to the first convolution result to generate a second convolution result, the second convolution result associated with a location of the first convolution result within a global filter kernel.","['G06V10/82', 'G06N3/063', 'G06F16/36', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G06T7/73', 'G06V10/454', 'G06V20/10', 'G06V20/56', 'G06T2207/10024', 'G06T2207/20084', 'G06T2207/30244', 'G06T2207/30248']"
US9710525B2,Adaptive learning of effective troubleshooting patterns,The system may include a troubleshooting activity recorder configured to record troubleshooting sessions. Each troubleshooting session may include a sequence of queries and query results. The troubleshooting activity recorder may include a query transformer configured to transform the queries and the query results into transformed queries and transformed query results before recording the troubleshooting sessions. The troubleshooting activity recorder may be configured to record the transformed queries and the transformed query results as troubleshooting session information in a troubleshooting activity database. The system may include a troubleshooting pattern learning unit including a graph builder configured to generate a troubleshooting pattern graph having query nodes and links between the query nodes based on the troubleshooting session information.,"['G06F17/30554', 'G06F16/248', 'G06F11/0706', 'G06F11/079', 'G06Q10/06', 'G06Q30/016']"
US11233841B2,Systems and methods for configuration-based optimization by an intermediary,"This disclosure is directed to methods and systems for handling a request for a web page of a server. An intermediary between a client and a server may intercept a request from the client for a first web page of a plurality of web pages of the server. The intermediary may have a plurality of pre-determined configurations for the plurality of web pages. The intermediary may generate, responsive to the request, based on a first configuration pre-determined for the first web page, a fragment of the first web page. The fragment may comprise at least one object specified by the first configuration and cached from a prior copy of the first web page received by the intermediary, and (ii) at least one object not in the prior copy. The intermediary may transmit, prior to receiving a response from the server, the fragment to the client for rendering.","['H04L67/02', 'H04L67/28', 'H04L67/56', 'H04L67/2842', 'H04L67/568']"
US11431660B1,System and method for collaborative conversational AI,"A method for collaborative conversational artificial intelligence (CCAI). The invention discloses an architecture wherein members of the disclosed system participate in collaborative conversations with one or more AI and human â€œsubmindsâ€ connected via a forum, including conversing in natural language and facilitated by one or more â€œfacilitatorsâ€. CCAI Applications include the creation of widely extensible evolving modular polylogical groups that are capable of collaboration with sentient beings, collaborative control of devices, service worker interfaces, hybrid representations of sentient beings (including via â€œreconveyanceâ€ of conversation segments), in collaborations that may include, exclude or require human or AI participation.","['H04L51/02', 'G06F3/04847', 'G06F40/279', 'G06F40/30', 'G06N20/20', 'G06N3/006', 'G06N3/045', 'G06N3/0454', 'G10L15/22', 'G06N5/045', 'G10L2015/223']"
US20190325668A1,Submersible inspection system,"A submersible inspection system for inspection of liquid cooled electrical transformers having a wirelessly controlled submersible inspective device. A submersion depth of the submersible can be controlled using a ballast system. The system can also include an input/output selector to switch between camera images from the submersible. A heartbeat signal indicative of a health of the transmitted signal can be transmitted to the submersible, and redundant channel selection logic can facilitate switching to a channel that includes a current heartbeat. A plurality of status interrogation systems disposed on the submersible can capture data regarding inspection procedures performed on the transformer, and the submersible can include tools for repair procedures. Data transmitted from the submersible, and overlayed with input data from an operator, can facilitate real time inspection analysis. The system can also form a model of an internal in the transformer, as well as produce a three-dimensional field of view.","['G07C5/008', 'G05D1/0692', 'B08B9/023', 'G01N1/10', 'G01N1/22', 'G05D1/0022', 'G05D1/0038', 'G21C17/013', 'H01F27/12', 'G01N2001/021', 'G01N2001/1031', 'G01N21/88', 'G01N2201/0216', 'G01N2201/0218', 'G02B23/2492', 'Y02E30/30']"
US10698625B2,Data pipeline architecture for analytics processing stack,A data pipeline architecture is integrated with an analytics processing stack. The data pipeline architecture may receive incoming data streams from multiple diverse endpoint systems. The data pipeline architecture may include converter interface circuitry with multiple dynamic converters configured to convert the diverse incoming data stream into one or more interchange formats for processing by the analytics processing stack. The analytics processing stack may include multiple layers with insight processing layer circuitry above analysis layer circuitry. The analysis layer circuitry may control analytics models and rule application. The insight processing layer circuitry may monitor output from the analysis layer circuitry and generate insight adjustments responsive to rule changes and analytics model parameter changes produced at the analysis layer circuitry.,"['G06F3/0644', 'G06F9/5072', 'G06F3/0604', 'G06F3/067', 'G06F9/3869', 'G06F9/547', 'G06F13/382', 'G06F13/4068', 'G06F16/24568']"
US11580119B2,System and method for automatic persona generation using small text components,"Systems and methods for automated and explainable machine learning to generate seamlessly actionable insights by generating explainable personas directly from customer relationship management systems are disclosed. The personas are defined as a collection of segments, scored by likelihood to generate good opportunities, accompanied ranked profile attribute importance, with descriptive names and summaries, associated human and database readable queries which have been generated to optimally find cluster candidates in a broader data universe. Such a system would effectively and accurately model the composition of past clients, perform the categorization in an explainable way such that actions can be taken on the information to have predictable results. What is further required are the mean to categorize small text components, trained over dependent and independent model sets, to enable a cleaner and more explicit representation of information rich short-strings, in order to facilitate a more meaningful representation of the user profiles.","['G06F16/337', 'G06F16/24578', 'G06F16/242', 'G06F16/27', 'G06F16/287', 'G06F16/355', 'G06F21/602', 'G06N3/044', 'G06N3/0445', 'G06N7/01', 'G06Q30/0204', 'G06N3/045']"
US11475528B2,Platform for live issuance and management of cyber insurance policies,"A system for autonomous issuance and management of insurance policies for computer and information technology related risks, including but not limited to losses due to system availability, cloud computing failures, current and past data breaches, and data integrity issues. The system will use a variety of current risk information to assess the likelihood of operational interruption or loss due to both accidental issues and malicious activity. Based on these assessments, the system will be able to autonomously issue policies, adjust premium pricing, process claims, and seek re-insurance opportunities with a minimum of human input.","['G06Q40/08', 'G06F16/951', 'G06N20/00', 'G06N5/022', 'G06N5/045', 'G06N5/046', 'G06Q30/0202', 'G06Q30/0611', 'G06N3/006', 'G06N3/02', 'G06N7/005', 'G06N7/01']"
CN111125422B,"Image classification method, device, electronic equipment and storage medium","The disclosure relates to an image classification method, an image classification device, an electronic device and a storage medium, which relate to the technical field of computers and are used for solving the problem of lower accuracy of image classification technology in related technologies, and the method comprises the following steps: classifying the images in the data set to be identified, and determining class labels of the images in the data set to be identified; extracting text features of each image and text features of class labels of each image, wherein the text features of the images are used for representing states of objects in the images; determining the matching degree of each image and the corresponding category label according to the text characteristics of each image and the text characteristics of the category label of the corresponding image; and determining the target image corresponding to the category label from the images corresponding to the same category label according to the determined matching degree. After the image is classified, the image of the same class label is further screened according to the matching degree of the state of the object in the image and the class label of the image, so that the classification accuracy is improved.","['G06F16/65', 'G06F16/686', 'G06F18/241', 'G06F18/2411']"
CN110705301B,"Entity relationship extraction method and device, storage medium and electronic equipment","The present disclosure provides an entity relationship extraction method and apparatus, an electronic device, and a storage medium; relates to the technical field of natural language processing. The method comprises the following steps: acquiring a text of an entity relationship to be extracted, and processing the text through an entity and relationship combined extraction model to obtain a first entity relationship triple; determining words and labels corresponding to the words in the text, matching the words and the labels according to the relationship rule set, and determining a second entity relationship triple; constructing an entity pair according to the words and the labels, processing the text through a relation classification model, and determining a third entity relation triple aiming at the entity pair; and determining an entity relationship extraction result according to the first entity relationship triple, the second entity relationship triple and the third entity relationship triple. The method and the device can improve the accuracy of entity relationship extraction.","['G06F16/355', 'G06F16/367']"
US12342948B2,Merchandiser with on-product financial payment system,A merchandiser for storing a product for purchase by a customer is provided. The merchandiser includes: a housing that defines a cavity for storing the product; and a controller for facilitating a transaction for the product at the merchandiser. The controller includes one or more processors coupled to one or more memory devices. The one or more memory devices have instructions stored therein that are executable by the one or more processors to cause the controller to: receive an input to initiate the transaction for the product; unlock a door of the merchandiser to enable a removal of the product based on receiving the input; initiate a timer that defines an unlock time duration for the door; override the unlock time duration based on a received input; lock the door of the merchandiser based on the override; and charge the customer for the product of the transaction.,"['F25C5/24', 'A47F10/02', 'F25D11/04', 'F25D17/042', 'G06Q20/18', 'G06Q20/208', 'G07F11/62', 'G07F9/006', 'G07F9/105', 'F25D2317/0417', 'F25D2331/801']"
US11709989B1,Method and system for generating conversation summary,"Methods and systems for generating and using a conversation summary model. The method comprises receiving at least one training dataset. The at least one training dataset comprises data samples, each data sample comprising a text comprising text segments. The text is labelled with a conversation summary comprising any of the text segments which summarize the text. The at least one training dataset includes a dataset from a specific source. Using the at least one training dataset and the pre-trained model, the method further comprises generating the conversation summary model by fine-tuning the pre-trained model. The generated conversation summary model may be used to generate conversation summaries for chat conversations.","['G06F40/10', 'G06F16/313', 'G06F16/345', 'G06F16/38', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G06F40/216', 'G06F40/284', 'G06N20/00']"
CN111858944B,Entity aspect level emotion analysis method based on attention mechanism,"The invention relates to the field of emotion analysis of natural language processing, and discloses an entity aspect level emotion analysis method based on an attention mechanism, which improves the accuracy of aspect level emotion analysis. The method comprises the following steps: s1, crawling user comment data of an analysis object; s2, preprocessing the crawled user comment data; s3, extracting attribute words from the comment data by adopting an attribute word extraction method based on a historical perception attention mechanism; and S4, acquiring the aspect type and the emotion polarity corresponding to the attribute words by adopting an aspect level emotion analysis method based on BERT and a hierarchy attention mechanism.","['G06F16/353', 'G06F40/295', 'G06N3/044', 'G06N3/045', 'G06N3/048']"
CN110489639B,Content recommendation method and device,"The embodiment of the invention provides a content recommendation method and a content recommendation device, wherein the method comprises the following steps: the recent behavior of the user to be recommended, the long-term interest of the user to be recommended in the user image to be recommended and the user characteristics of the user to be recommended are combined to serve as the input of the trained interest representation model, and the vector embedded representation of the overall interest of the user to be recommended is output through the trained interest representation model. Since the long-term interest is an interest in a history period, the recent behavior is a recent behavior before the current time, the long-term interest is stable compared to the recent behavior, and the recent behavior becomes a part of the long-term interest as time advances. Therefore, the user interest can be more accurately and comprehensively described by considering the recent behaviors of the user to be recommended, combining the long-term interest of the user to be recommended in the user image to be recommended and the user characteristics of the user to be recommended, and then recommending the interested content for the user to be recommended based on the vector embedded representation of the whole user to be recommended.",['G06F16/9535']
US12039074B2,"Methods, personal data analysis system for sensitive personal information detection, linking and purposes of personal data usage prediction","Systems and methods for personal data classification, linkage and purpose of processing prediction are provided. The system for personal data classification includes an entity extraction module for extracting personal data from one or more data repositories in a computer network or cloud infrastructure, a linkage module coupled to the entity extraction module, a linkage module coupled to the entity extraction module and a processing prediction module. The entity extraction module performs entity recognition from the structured, semi-structured and unstructured records in the one or more data repositories. The linkage module uses graph-based methodology to link the personal data to one or more individuals. And the purpose prediction module includes a feature extraction module a purpose of processing prediction module, wherein the feature extraction module extracts both context features and record's features from records in the one or more data repositories, and the purpose of processing prediction module predicts a unique or multiple purpose of processing of the personal data.","['G06F21/6245', 'G06F16/148', 'G06F16/156', 'G06F16/164', 'G06F16/182', 'G06F18/2185', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N5/046', 'G06N7/01', 'G06V10/82', 'G06V30/1988', 'G06V30/274', 'G06V30/412', 'G06F18/2193']"
US10592282B2,Providing strong ordering in multi-stage streaming processing,"The technology disclosed relates to providing strong ordering in multi-stage processing of near real-time (NRT) data streams. In particular, it relates to maintaining current batch-stage information for a batch at a grid-scheduler in communication with a grid-coordinator that controls dispatch of batch-units to the physical threads for a batch-stage. This includes operating a computing grid, and queuing data from the NRT data streams as batches in pipelines for processing over multiple stages in the computing grid. Also included is determining, for a current batch-stage, batch-units pending dispatch, in response to receiving the current batch-stage information; identifying physical threads that processed batch-units for a previous batch-stage on which the current batch-stage depends and have registered pending tasks for the current batch-stage; and dispatching the batch-units for the current batch-stage to the identified physical threads subsequent to complete processing of the batch-units for the previous batch-stage.","['G06F9/4881', 'G06F2209/484', 'G06F2209/485']"
US11836650B2,Artificial intelligence engine for mixing and enhancing features from one or more trained pre-existing machine-learning models,"An AI engine having an architect module to create a number of nodes and how the nodes are connected in a graph of concept nodes that make up a resulting AI model. The architect module also creates a first concept node by wrapping an external entity of code into a software container with an interface configured to exchange information in a protocol of a software language used by the external entity of code. The architect module also creates a second concept node derived from its description in a scripted file coded in a pedagogical programming language, and connects the second concept node into the graph of nodes in the resulting AI model.","['G06Q10/00', 'G06F15/80', 'G06F16/2228', 'G06F16/951', 'G06F18/2148', 'G06F3/0482', 'G06F30/20', 'G06F30/27', 'G06F8/31', 'G06F8/311', 'G06F8/35', 'G06F8/36', 'G06F8/38', 'G06F9/451', 'G06F9/4881', 'G06N20/00', 'G06N3/006', 'G06N3/008', 'G06N3/04', 'G06N3/045', 'G06N3/08', 'G06N3/10', 'G06N3/105', 'G06N5/04', 'G06N7/01', 'G06V10/945', 'G06V10/96', 'H04L67/01', 'G06F3/03543', 'G06F30/333', 'G06F40/166', 'H04L67/02']"
US11621633B2,Transformerless stacked active bridge power converters and methods for operating the same,"Transformerless stacked active bridge (TSAB) direct current (DC)-to-DC power converters designed based on parent switched capacitor (SC) converter topologies. The TSAB DC-to-DC power converter includes the SC converter. The SC converter includes a plurality of switches and a plurality of capacitors. Each capacitor of the plurality of capacitors is electrically coupled to at least one of the plurality of switches. The plurality of capacitors includes tree capacitors, and link capacitors forming a loop with at least one of the tree capacitors. The TSAB DC-to-DC power converter includes at least one inductor electrically coupled in series to at least one of the link capacitors. The TSAB DC-to-DC power converters provide high efficiency bi-directional operation without requiring isolation transformers. The TSAB DC-to-DC power converters enable high power density in a wide variety of practical applications involving low, medium, or high power requirements, with comparably lower package sizes/weights and inductor component values.","['H02M3/07', 'H02M1/0048', 'H02M3/1586', 'H02M1/0058', 'Y02B70/10']"
US10878379B2,Processing events generated by internet of things (IoT),"The technology disclosed relates to processing events generated by Internet of Things (IoT) devices. In particular, it relates to storing a machine-readable declarative specification of stateful event processing of an automated multi-step progression of monitoring of Internet of Things (IoT) devices that generate events. It includes compiling into tangible memory, in response to the declarative specification, a state processing network that implements a multi-step progression of monitoring events generated by the IoT devices. The state processing network implements both the time based transition triggers and the event based transition triggers after being initiated by the IoT devices. Further, data structures, which record monitoring status of particular IoT devices, are compiled. Finally, the state processing network further selects an alternative action to trigger, and triggers the selected action using at least some data from a particular event being processed.","['G06Q10/10', 'G06F1/30', 'G06Q50/01']"
US12073326B2,Joint learning from explicit and inferred labels,"This document relates to training of machine learning models. One example method involves providing a machine learning model having a first classification layer, a second classification layer, and an encoder that feeds into the first classification layer and the second classification layer. The example method also involves obtaining first training examples having explicit labels and second training examples having inferred labels. The inferred labels are based at least on actions associated with the second training examples. The example method also involves training the machine learning model using the first training examples and the second training examples using a training objective that considers first training loss of the first classification layer for the explicit labels and second training loss of the second classification layer for the inferred labels. The method also involves outputting a trained machine learning model having the encoder and the first classification layer.","['G06N3/084', 'G06N3/08', 'G06N3/04', 'G06N3/045', 'G06V10/82', 'G06F16/176']"
US20210117586A1,"Method, apparatus, and storage medium for planning power distribution network","The disclosure provides a method for planning a power distribution network, an apparatus for planning a power distribution network, and a storage medium. The method includes: establishing a model for planning the power distribution network, the model including a target function and constraints, the target function for minimizing a cost of the power distribution network when branches, and nodes are installed into the power distribution network, the nodes including transformers and substations, the constraints including a power balance constraint of the power distribution network, a power constraint of the branches, a power constraint of the transformers, a radial operation constraint of the power distribution network, a fault constraint, a calculation constraint of indices of a reliability, a constraint of the indices of the reliability, and a logic constraint; and solving the model to determine whether the branches and the nodes are installed into the power distribution network.","['G06Q10/04', 'G06F30/13', 'G05B19/042', 'G06Q10/0631', 'G06Q10/0637', 'G06Q10/06393', 'G06Q50/06', 'H02J3/00', 'G05B2219/2639', 'H02J2203/10', 'H02J2203/20', 'Y02E40/70', 'Y02E60/00', 'Y04S10/50', 'Y04S40/20']"
CN104411535B,System and method for operating a hybrid vehicle system,"The invention provides a system and method for operating a hybrid power vehicle system body. The system comprises a switch control module that is configured to control operation of a first contactor and a second contactor in a vehicle system. The first and second contactors are configured to selectively connect front-end and direct-current (DC) buses, respectively, to an energy storage system of the vehicle system. The front-end bus is configured to receive electrical power from an external power source and provide the electrical power to a converter device. The converter device is configured to supply DC power to the DC bus. The switch control module is configured to close the second contactor when the vehicle system is operably coupled to the external power source so that the energy storage system is charged by the DC power. The switch control module is configured to close one of the first contactor or the second contactor when the vehicle system is operably decoupled to the external power source.","['B60L53/00', 'B60L3/0023', 'B60L50/53', 'B60L50/66', 'B60L58/19', 'B60L58/20', 'B60L58/22', 'B60L9/18', 'B60L9/24', 'B60W10/08', 'B60W10/30', 'B60W20/00', 'B60L1/00', 'B60L2200/26', 'B60L2240/547', 'Y02T10/70', 'Y02T10/7072', 'Y02T90/14', 'Y10S903/93']"
US12315283B2,"Methods, systems, articles of manufacture, and apparatus for decoding images","An example apparatus to decode an image comprises interface circuitry to receive an image of a purchase document, and processor circuitry to execute the machine readable instructions to extract text from the image of the purchase document, the image of the purchase document to memorialize a transaction that includes at least one product; determine a type of the purchase document to which the image corresponds; apply one of a first pipeline or a second pipeline to the image of the purchase document based on the type of the purchase document; obtain purchase facts corresponding to a respective one of the at least one product memorialized in the image of the purchase document; and map the obtained purchase facts against a products database to identify the at least one product memorialized in the image of the purchase document.","['G06Q30/0201', 'G06Q30/04', 'G06V10/82', 'G06V30/10', 'G06V30/19173', 'G06V30/412', 'G06V30/413', 'G06V30/414', 'G06V30/416', 'G06V30/42']"
CN113888744B,Image semantic segmentation method based on transform visual up-sampling module,"The invention discloses an image semantic segmentation method based on a transform visual up-sampling module. According to the invention, an up-sampling module for carrying out feature map based on a visual transducer module is introduced into an image semantic segmentation task, and part of feature map information stored originally is fused into the up-sampling process as up-sampling auxiliary information. Compared with the original traditional up-sampling module, the method avoids the operation of 0 supplementing in unknown information and simultaneously avoids chessboard effects caused by deconvolution and the like. The innovation process relying on windowed downsampling solves the embarrassment that the deep feature map pays attention to global semantic information and loses local detail information, brings more excellent detail information to the transform extraction of the local window, simultaneously solves the influence caused by insufficient computing power, provides possibility for the transform to solve the large-scale problem, and can improve the performance of image semantic segmentation.","['G06F18/24', 'G06F18/253', 'G06N3/045', 'G06N3/048', 'G06N3/08']"
CN112507715B,"Methods, devices, equipment and storage media for determining association relationships between entities","The application discloses a method, a device, equipment and a storage medium for determining association relations between entities, which are applied to the technical fields of natural language processing, knowledge maps and deep learning. The specific implementation scheme is as follows: acquiring target association information, wherein the target association information comprises a first word representing a first entity, a second word representing a second entity and a third word representing a target association relationship; determining first semantic features according to the target association relationship and similar text segments aiming at the target association information in preset text Duan Ku; determining a description text aiming at the target associated information according to the target associated information and a preset knowledge graph; determining a second semantic feature according to the target associated information and the descriptive text; and determining the confidence coefficient of the target association relationship between the first entity and the second entity according to the first semantic feature and the second semantic feature.","['G06F40/295', 'G06F16/367', 'G06F18/2415', 'G06F40/216', 'G06F40/289', 'G06F40/30', 'G06N3/045', 'G06N3/048', 'G06N3/08']"
US12106484B2,Three-dimensional medical image segmentation method and system based on short-term and long-term memory self-attention model,"The disclosure belongs to the field of image segmentation in medical image processing and discloses a three-dimensional medical image segmentation method and system based on short-term and long-term memory self-attention models, in which the method can segment a target area image in the medical image, which includes the following. (1) A training set sample is established. (2) Processing is performed on the original three-dimensional medical image to be segmented to obtain a sample to be segmented. (3) A three-dimensional medical image segmentation network based on short-term and long-term memory self-attention is established and trained. (4) The sample to be segmented is input to the network, and then a segmentation result of the target area in the sample to be segmented is output. By combining CNN and Transformer, a new model for accurate real-time segmentation of the target area (such as a tumor) in the three-dimensional medical image is obtained.","['G06V10/267', 'G06N3/08', 'G06T3/4046', 'G06T7/0012', 'G06T7/11', 'G06T7/143', 'G06V10/25', 'G06V10/764', 'G06V10/7715', 'G06V10/82', 'G16H30/40', 'G06T2200/04', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30096', 'Y02T10/40']"
US11900052B2,Automatic generation of transformations of formatted templates using deep learning modeling,"The present disclosure applies trained artificial intelligence (AI) processing adapted to automatically generating transformations of formatted templates. Pre-existing formatted templates (e.g., slide-based presentation templates) are leveraged by the trained AI processing to automatically generate a plurality of high-quality template transformations. In transforming a formatted template, the trained AI processing not only generates feature transformation of objects thereof but may also provide style transformations where attributes associated with a presentation theme may be modified for a formatted template or set of formatted templates. The trained AI processing is novel in that it is tailored for analysis of feature data of a specific type of formatted template. The trained AI processing converts a formatted template into a feature vector and utilizes conditioned generative modeling to generate one or more transformed templates using a representation of the feature data and feature data from one or more other formatted templates.","['G06F16/4393', 'G06F40/186', 'G06F40/103', 'G06N20/00', 'G06N3/02', 'G06N3/045', 'G06N3/047', 'G06N3/088', 'G06N5/04', 'G06V10/40', 'G06N3/042', 'G06N3/08', 'G06N5/047']"
US9190844B2,Systems and methods for reducing energy usage,"A system for detecting individual appliance energy loads from a building composite load profile includes an electric meter to capture building composite load profile; a detector coupled to the meter to detect transitions in the load profile to determine an appliance state machine for each appliance; a clusterizer to detect clusters of patterns in the load profile; and an analyzer coupled to the detector to receive the transitions and appliance state machines from the detector, the analyzer matching each transition to a predetermined appliance state machine to disaggregate the building composite load profile into individual appliance energy loads.","['H02J3/14', 'G06Q50/163', 'G01R21/007', 'G01W1/00', 'H02J3/004', 'G06F19/3418', 'G16H40/67', 'H02J2310/52', 'H02J2310/70', 'Y02B70/3225', 'Y04S20/222']"
US12019718B2,"Identity verification method and apparatus, computer device and storage medium","An identity authentication method is provided, including: acquiring a raw feature of a user; calling an identity authentication model to extract a primary attribute feature vector in the raw feature, the primary attribute feature vector being an unbiased feature representation for selectively decoupling mâˆ’1 domain discrepancy features in the raw feature, and m being an integer greater than 2; and performing unbiased identity authentication based on the primary attribute feature vector to obtain an identity authentication result.","['G06F21/31', 'G06F18/214', 'G06F21/32', 'G06N3/004', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06V40/168', 'G06V40/172', 'G06N3/047', 'G06N3/048', 'G06V40/1365', 'G06V40/197']"
US20210342549A1,"Method for training semantic analysis model, electronic device and storage medium","The disclosure provides a method for training a semantic analysis model, an electronic device and a storage medium. The method includes: obtaining a plurality of training data, in which each of the plurality of training data comprises a search word, information on at least one text obtained by searching the search word, and at least one associated word corresponding to the at least one text; constructing a graph model based on the training data, and determining target training data from the plurality of training data by using the graph model, the target training data comprising search word samples, information samples and associated word samples; and training a semantic analysis model based on the search word samples, the information samples, and the associated word samples.","['G06F40/30', 'G06F16/9024', 'G06F40/58', 'G06N3/045', 'G06N3/088', 'G06N7/01', 'G06N20/00']"
US10200248B1,Translating high-level configuration instructions to low-level device configuration,"In one example, a network management system (NMS) device manages a plurality of network devices including first and second network devices. Initially the first and second network devices are configured according to a first high-level configuration. The NMS is configured to determine a difference between the first high-level configuration and a second high-level configuration, apply a first transformation function, specific to the first network device, to the difference to generate a first low-level configuration change specific to the first device, apply a second transformation function, specific to the second network device, to the difference to generate a second low-level configuration change specific to the second device, configure the first device with the first low-level configuration change, and configure the second device with the second low-level configuration change.","['H04L41/0889', 'H04L41/0846', 'H04L41/0859', 'H04L41/0866', 'H04L41/0879', 'H04L41/22']"
US8898627B2,Systems and methods for applying rules to transform objects of an application,"The present application is directed towards systems and methods for applying one or more rules that transform objects of an application from a previous instance of the application. Customized functions, objects, databases, and code of the instance of the application may be analyzed by an application transformation tool to identify one or more objects that have been modified. The modified objects may be further analyzed to detect if they are permissibly alterable. The application transformation tool may determine whether each of the permissibly alterable objects reference a standard interface of the application. The application transformation tool may apply one or more rules that transform objects to those permissibly alterable objects that do not reference the standard interface of the application.",['G06F8/65']
CN109551085B,IGBT inversion multifunctional welding machine with liquid crystal display and multiple input voice prompt functions,"The invention relates to a liquid crystal display multi-input voice prompt IGBT inversion multifunctional welding machine, wherein the power supply is 220-240V or 110-120V; the inside of the welding machine is designed into a left side structure and a right side structure. One side is provided with a circuit board, a cooling fan and other parts; the other side is provided with parts such as a wire feeding mechanism; the circuit board is designed into three blocks; firstly, a liquid crystal display and operation control panel; secondly, switching control boards; the automatic identification, detection and conversion control of the input voltage can be realized; the main control board is an inversion main circuit mainly composed of input voltage detection, power-on buffering, rectification and filtering, IGBT inversion, main transformer, output rectification, filtering and the like; a switching power supply circuit; an inversion PWM and IGBT driving circuit; wire feeding and solenoid valve control circuit; output characteristic control and other circuits; the welding machine provided by the invention has multiple functions, and solves the problem of narrow application range of the welding machine with multiple power supplies and single function.","['B23K9/173', 'B23K9/133', 'B23K9/32', 'Y02P70/10']"
US11562521B2,Generating facial position data based on audio data,"A computer-implemented method for generating a machine-learned model to generate facial position data based on audio data comprising training a conditional variational autoencoder having an encoder and decoder. The training comprises receiving a set of training data items, each training data item comprising a facial position descriptor and an audio descriptor; processing one or more of the training data items using the encoder to obtain distribution parameters; sampling a latent vector from a latent space distribution based on the distribution parameters; processing the latent vector and the audio descriptor using the decoder to obtain a facial position output; calculating a loss value based at least in part on a comparison of the facial position output and the facial position descriptor of at least one of the one or more training data items; and updating parameters of the conditional variational autoencoder based at least in part on the calculated loss value.","['G06T13/205', 'G06N20/20', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/084', 'G06N3/088', 'G06T13/40']"
US11691650B2,Systems and methods for generating motion forecast data for a plurality of actors with respect to an autonomous vehicle,"A computing system can be configured to input data that describes sensor data into an object detection model and receive, as an output of the object detection model, object detection data describing features of the plurality of the actors relative to the autonomous vehicle. The computing system can generate an input sequence that describes the object detection data. The computing system can analyze the input sequence using an interaction model to produce, as an output of the interaction model, an attention embedding with respect to the plurality of actors. The computing system can be configured to input the attention embedding into a recurrent model and determine respective trajectories for the plurality of actors based on motion forecast data received as an output of the recurrent model.","['G06F16/909', 'B60W60/00272', 'B60W60/00274', 'G06F16/903', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/084']"
CN113190687B,"Knowledge graph determining method and device, computer equipment and storage medium","The embodiment of the invention discloses a method and a device for determining a knowledge graph, computer equipment and a storage medium. The method comprises the following steps: acquiring document content and determining a document format type corresponding to the document content; in the document content, inquiring text at least one target format position matched with the document format type, and determining document knowledge; and determining a knowledge graph corresponding to the document content according to the knowledge of each document. The embodiment of the invention can improve the accuracy and efficiency of extracting the document knowledge and establish the accurate knowledge map.",['G06F16/367']
US11995075B2,System and method for efficient transliteration of machine interpretable languages,"Aspects of the disclosure relate to transliteration of machine interpretable languages. A computing platform may receive a query formatted in a first format for execution on a first database. The computing platform may translate the query to a second format for execution on a second database by: 1) extracting non-essential portions of the query from the query, and replacing the non-essential portions of the query with pointers to create a query key; 2) storing, along with their corresponding pointers, the non-essential portions of the query as query parameters; 3) executing a lookup function on a query library to identify a translated query corresponding to the query key and including the corresponding pointers; and 4) updating the translated query to include the query parameters based on the corresponding pointers to create an output query. The computing platform may execute the output query on the second database.","['G06F16/2452', 'G06F16/214', 'G06F16/2425', 'G06F16/2433', 'G06F16/2448', 'G06F16/24526', 'G06F16/24534', 'G06F16/2465', 'G06F16/248', 'G06F16/252', 'G06F16/258', 'G06F40/205']"
CN106253680B,Operate the system and method with the power converter of series-multiple connection mode active-clamp,"The present invention relates to the system and method that operation has the power converter of series-multiple connection mode active-clamp.The system and method for the flyback converter for operations improvement are disclosed, wherein release model is returned to input power rather than back to output loading, while still realizing the operation of zero voltage switching (that is, ZVS).In some embodiments, improved converter can be at the shutdown moment of control switch by the energy transfer being stored in leakage inductance to damped capacitor.In addition, the energy of storage can also be retained in damped capacitor by improved converter embodiment when electric power is transmitted to load by secondary circuit.Once the energy being stored in transformer is delivered to load, improved converter embodiment can start the transfer of the release model that is stored in damped capacitor to armature winding.Finally, improved converter embodiment can intelligently control their active-clamp switch so that all leakage inductance energy return to input source.","['H02M1/083', 'H02M1/0058', 'H02M1/34', 'H02M3/01', 'H02M3/335', 'H02M3/33523', 'H02M3/33546', 'H02M3/33569', 'H02M3/33571', 'H02M1/0009', 'H02M1/342', 'Y02B70/10']"
WO2023030513A1,Internet of things system,"Provided in the present disclosure is a method for implementing a sensor on the basis of the Internet of Things, and further provided is a method for calibrating a sensor on the basis of deep learning and a system thereof. The method comprises the steps of: a sensor collecting historical data according to a time sequence; by means of a standard sensor, collecting a numerical value of at least a part of the corresponding historical data; feeding the historical data and the numerical value into a Transformer model; the Transformer model training the historical data and the numerical value to obtain an original model; by means of deep learning pruning or knowledge distillation, performing multi-level compression optimization on the original model, so as to obtain a model after multi-stage compression optimization; and, according to the original model or the model after multi-stage compression optimization, calibrating raw data subsequently collected by the sensor. The sensor calibration method and the system thereof in the present disclosure can be respectively deployed in a sensor terminal device, a base station, and a cloud server, thereby achieving application in multiple types of sensor device and multiple scenarios thereof, and the multi-level collaborative calibration also achieves simple and efficient verification of the calibration result.","['G01D21/02', 'G01N27/416', 'G06F18/214', 'G06N3/08', 'H04L41/145', 'H04L41/16', 'H04L67/12', 'H04W4/38']"
CN107708592B,Surgical system with tissue type based user adaptive techniques,"Various forms provided herein relate to systems and methods for tissue coagulation and dissection. A surgical instrument includes an end effector configured to seal and dissect tissue at a distal end of the surgical instrument, and a generator circuit configured to deliver energy to the end effector. A force sensor is in communication with the end effector and is configured to measure a force applied to tissue by the end effector. The energy delivered to the end effector is dynamic based on a determination of a type of tissue interacting with the end effector. The tissue type is determined based on a tissue friction coefficient calculated based on the measured force applied to tissue by the end effector, the ultrasonic motion of the end effector, and a rate of heat generated by the end effector.","['A61B18/1445', 'A61B17/320092', 'A61B2017/00017', 'A61B2017/00039', 'A61B2017/00075', 'A61B2017/00084', 'A61B2017/320094', 'A61B2017/320095', 'A61B2017/320097', 'A61B2018/00589', 'A61B2018/00601', 'A61B2018/0063', 'A61B2018/00642', 'A61B2018/00648', 'A61B2018/00702', 'A61B2018/00791', 'A61B2018/00875', 'A61B2018/0088', 'A61B2018/00994', 'A61B2090/064', 'A61B2560/0475', 'A61B2562/0261']"
CN106933206B,Source independent queries in distributed industrial systems,"Techniques are provided for requesting and providing process plant data using a standardized query that is source independent. The requesting device generates a standardized query to obtain data from one or more data sources (e.g., a relational database or a non-relational database). The query is in a standardized format that is not dependent on the data source, and the request can be generated as a JSON file. The standardized queries are not directly usable with any data source. Instead, upon receiving the standardized query, the data device generates one or more source-specific queries. The source-specific query obtains data using a syntax native to each data source. In some instances, the received data must be further processed to adjust for different sampling times or sampling rates, such as by interpolation. The resulting data from all data sources may be aggregated into a data frame before being returned to the requesting device.","['G06F16/24534', 'G05B19/4186', 'G06F16/2452', 'G06F16/256', 'G06F16/24528', 'G06F16/2471', 'G06F16/2477', 'G06F16/252', 'Y02P90/02', 'Y02P90/80']"
CN112925877B,A method and system for identifying associations between one person and multiple cases based on deep metric learning,"The invention relates to a one-person-multiple-case association identification method and system based on deep measurement learning. The method comprises the steps of obtaining a complaint shape to be subjected to multi-case association identification by one person, and obtaining case element information by utilizing a case element identification model which is trained in advance; inputting the case element information into a pre-trained case similarity measurement model, calculating the text semantic similarity of the case element information and the cases in the candidate case set by using a deep learning and measurement learning technology, further judging whether a situation of multiple cases of one person exists or not, and establishing case association according to the person. The system comprises a case element identification module, a case similarity measurement module and a case association identification module. The method for combining deep measurement learning with legal business rules realizes the association recognition of multiple persons in the whole process stage of court case establishment-trial and execution, provides technical support for the judicial resource overall planning of cross-region and cross-layer, and provides guarantee for court equity and efficient trial and execution of cases.","['G06F16/3334', 'G06F16/35', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06Q50/18', 'Y04S10/50']"
US12341990B2,"Method and apparatus for parametric, model-based, geometric frame partitioning for video coding",There are provided methods and apparatus for adaptive geometric partitioning for video encoding and decoding. An apparatus includes an encoder for encoding image data corresponding to pictures by adaptively partitioning at least portions of the pictures responsive to at least one parametric model. The at least one parametric model involves at least one of implicit and explicit formulation of at least one curve.,"['H04N19/57', 'H04N19/117', 'H04N19/119', 'H04N19/126', 'H04N19/13', 'H04N19/146', 'H04N19/156', 'H04N19/159', 'H04N19/176', 'H04N19/189', 'H04N19/44', 'H04N19/50', 'H04N19/507', 'H04N19/543', 'H04N19/61', 'H04N19/70']"
CN106933208B,Distributed industrial performance monitoring and analysis,"A technique is provided for providing early fault detection in a process plant using process control data generated by a control device. The technique determines a leading indicator of a condition (e.g., a fault, an anomaly, or a degradation in performance) within the process plant. The lead indicator may be determined using principal component analysis. A process signal indicative of the process variable corresponding to the lead indicator is then obtained and analyzed. A rolling Fast Fourier Transform (FFT) may be performed on the process signals to generate time series data with which to monitor the process plant. When the presence of a leading indicator is detected in the time series data, a warning or other prediction of the situation may be generated. Thus, process faults may be identified as leading predictors using fluctuations and anomalies.","['G05B23/024', 'G05B23/0218', 'G05B23/0275', 'G05B19/41845', 'G05B23/0267', 'G05B23/0286', 'G05B2219/31356', 'G05B2219/33273', 'Y02P90/02', 'Y02P90/80']"
CN111192682B,"Image exercise data processing method, system and storage medium","The application discloses an image training data processing method, an image training data processing system and a storage medium, wherein the image training data processing method comprises the following steps: responding to the image exercise request instruction, and determining data to be exercised; receiving an image report to be evaluated of a trainee aiming at an image to be exercised; acquiring a reference image report corresponding to an image to be exercised; based on the reference image report, a training quality of the image report to be evaluated is determined. The method for automatically evaluating the on-duty image training and training report quality is effective for the trainee, improves the accuracy, reliability and efficiency of the training quality evaluation result, improves the training effect of image training, and effectively improves the film reading level of the trainee.","['G16H50/20', 'G09B23/286', 'G09B5/02', 'A61B90/00', 'G06T7/0012', 'G16H15/00', 'G16H30/20', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10104', 'G06T2207/10116', 'G06T2207/10132', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30008', 'G06T2207/30016', 'G06T2207/30064', 'G06T2207/30081', 'G06T2207/30101']"
US10018613B2,Sensing system and method for analyzing a fluid at an industrial site,System includes a sensor operably coupled to a device body. The sensor includes a sensing region and at least one resonant inductor-capacitor-resistor (LCR) circuit. The sensing region is configured to be placed in operational contact with an industrial fluid. The at least one resonant LCR circuit is configured to generate an electrical stimulus that is applied to the industrial fluid via electrodes at the sensing region. The device body includes one or more processors configured to receive an electrical signal from the sensor that is representative of a resonant impedance spectral response of the sensing region in operational contact with the industrial fluid responsive to the electrical stimulus. The one or more processors are further configured to analyze the resonant impedance spectral response and determine both a water concentration in the industrial fluid and an aging level of the industrial fluid based on the resonant impedance spectral response.,"['G01N33/2888', 'G01N27/026']"
CN112803592B,Intelligent fault early warning method and system suitable for distributed power station,"The invention discloses an intelligent fault early warning method and system suitable for a distributed power station, which comprises the steps of collecting power generation data when a historical photovoltaic power station fails; establishing an expert database by utilizing the historical data, and performing classified storage according to different equipment data of the power station; establishing a fault tree analysis model according to the collected fault data to perform equipment fault probability analysis; collecting real-time data of the photovoltaic power station, and inputting the real-time data into the expert database for fault analysis and prediction according to an analysis result of the fault tree analysis model; and carrying out intelligent fault early warning according to the prediction condition. The workload of monitoring the stations is reduced, the unified monitoring and multilayer monitoring of various photovoltaic power stations of different types are realized, and the unattended operation mode with few persons is realized; a new fault early warning method is established, the accuracy and the speed of photovoltaic power station fault early warning are improved, and the stability, the reliability and the operation management efficiency of power station operation are improved.","['H02J13/00001', 'G06F18/24323', 'H02J13/00002', 'H02J3/381', 'H02J2203/20', 'H02J2300/22', 'Y02E40/70', 'Y04S10/123', 'Y04S10/50']"
US11323484B2,Privilege assurance of enterprise computer network environments,"A system and method for the prevention, mitigation, and detection of cyberattack attacks on computer networks by identifying weaknesses in directory access object allowances and providing professionals with centralized graph-centric tools to maintain and observe key security and performance insights into their security posture. The system uses an interrogation agent to collect Active Directory configuration parameters and activity information about a forest and the devices operating within. Cyber-physical graphs and histograms using persisted time-series data provides critical information, patterns, and alerts about configurations, attack vectors, and vulnerabilities which enable information technology and cybersecurity professionals greater leverage and control over their infrastructure.","['H04L63/20', 'H04L63/1433', 'G06F16/2477', 'G06F16/951', 'H04L63/1408', 'H04L63/1425', 'H04L63/1441', 'H04L67/02', 'H04L67/12', 'H04L67/306']"
US10984175B2,Systems and methods for dynamically modifying a requested web page from a server for presentation at a client,"In some aspects, the disclosure is directed to methods and systems for dynamically modifying a web page. An intermediary device may receive a request from a client for a web page of a server. The intermediary may transmit a first fragment of the web page to include in a modified web page for presentation at the client. The first fragment may correspond to a static portion of the web page. The intermediary may transmit a second fragment to the client to include in the modified web page, which may include a dynamic portion of the web page provided by the server in response to the request. The intermediary may incorporate code into at least one of the first and second fragments prior to transmission. The code may, upon execution as the corresponding fragment is presented to a user, dynamically perform an action to improve the user's experience.","['G06F40/14', 'G06F16/9577', 'G06F9/452']"
US20220058747A1,Risk quantification for insurance process management employing an advanced insurance management and decision platform,"A system for insurance process management employing an advanced insurance management platform has been developed. A high speed data retrieval and storage module retrieves insurance related data from a plurality of sources. A data analysis module determines an activeness metric for an object, such as a physical asset, in order to categorize risk and also receives a plurality of individual, entity, and object data to create one or more cognitive maps which may analyzed to determine a propensity for risk. The data analysis module generates a cognitive density metric based on the activeness metric and cognitive map. A predictive analytics module performs predictive analytics functions on normalized insurance related data and using the cognitive density metric. A predictive simulation module performs predictive simulation functions on normalized insurance related data. As a result, the system can produce various models to determine risk and loss associated with an insured physical asset.","['G06Q30/0202', 'G06F30/20', 'G06Q40/08']"
US20220128539A1,A sensor for transformer condition assessment,A sensor assembly for monitoring an internal condition of a transformer comprising:,"['G08C17/02', 'G01N33/2888', 'G01N33/2805', 'G01R31/62', 'H01F27/402', 'H04Q9/00', 'G01R31/1272', 'G08B21/187', 'G08C2200/00', 'H01F2027/404', 'H01F2027/406', 'H04L67/12', 'H04Q2209/40', 'H04W4/70', 'H04W4/80', 'H04W84/18', 'Y04S40/18']"
US11477245B2,Advanced detection of identity-based attacks to assure identity fidelity in information technology environments,"A system and method for the detection and mitigation of Kerberos golden ticket, silver ticket, and related identity-based cyberattacks by passively monitoring and analyzing Kerberos and authentication operations within the network. The system and method provide real-time detections of identity attacks using time-series data and data pipelines, and by transforming the stateless Kerberos protocol into stateful protocol. A packet capturing agent is deployed on the network where captured time-series Kerberos and related event and log information is processed in distributed computational graph (DCG) stages where declarative rules determine if an attack is being carried out and what type of attack it is.","['H04L63/20', 'H04L63/1433', 'G06F16/2477', 'G06F16/951', 'H04L63/0807', 'H04L63/1425', 'H04L63/1441', 'H04L67/02', 'H04L67/10', 'H04L67/12']"
CN111859987B,"Text processing method, training method and device for target task model","The invention provides a text processing method and a training method and device of a target task model, wherein the text processing method comprises the following steps: in response to receiving semantic information to be processed in a specified field, inputting the semantic information to be processed into a language model, and outputting enhanced text data associated with the semantic information to be processed; text enhancement is carried out on the text set in the appointed field based on the enhanced text data; the language model is obtained based on training of a preset first sample set; the training samples in the first set of samples include: sample semantic information in a specified domain, and sample text data that matches the sample semantic information. The method can realize text enhancement in the appointed field, can obtain text enhancement data in each professional field, and has a wider application range.","['G06F40/30', 'G06F40/289', 'G06F40/58', 'G06N3/044', 'G06N3/045', 'Y02D10/00']"
US9167664B2,Systems and methods for low-power lamp compatibility with a trailing-edge dimmer and an electronic transformer,"In accordance with these and other embodiments of the present disclosure, a system and method for providing compatibility between a load and a secondary winding of an electronic transformer driven by a trailing-edge dimmer may include predicting based on an electronic transformer secondary signal an estimated occurrence of a high-resistance state of the trailing-edge dimmer, wherein the high-resistance state occurs when the trailing-edge dimmer begins phase-cutting an alternating current voltage signal and operating the load in a high-current mode for a period of time immediately prior to the estimated occurrence of the high-resistance state.","['H05B45/315', 'H05B37/02', 'H05B33/0803', 'H05B33/0809', 'H05B33/0845', 'H05B45/10', 'H05B45/37', 'H05B47/17', 'Y02B20/30', 'Y02B20/383']"
US9825545B2,Switched-capacitor split drive transformer power conversion circuit,"A split drive transformer (SDT) and use of such a transformer in a power converter is described. The power converter includes a power and distributor circuit configured to receive one or more input signals and provides multiple signals to a first side of the SDT. The SDT receives the signals provided to the first side thereof and provides signals at a second side thereof to a power combiner and rectifier circuit which is configured to provide output signals to a load. In some embodiments, the SDT may be provided as a switched-capacitor (SC) SDT. In some embodiments, the power converter may optionally include a level selection circuit (LSC) on one or both of the distributor and combiner sides.","['H02M3/33546', 'H02M3/07', 'H02M3/158', 'H02M3/335', 'H02M3/33569', 'H02M3/33571', 'H02M3/33573', 'H02M3/33576', 'H02M7/12', 'H02M7/68', 'H02M1/0074', 'H02M1/0077', 'H02M2001/0074', 'H02M2001/0077']"
CN109661210B,Irreversible Electroporation Equipment,"Irreversible electroporation device (1, 2) comprising: a pulse forming device (10) configured to generate bipolar pulses; and an electrode (20) configured to receive the bipolar pulse from the pulse forming device (10) and adapted to apply the bipolar pulse to the biological tissue. Wherein the pulse width of the bipolar pulse and the electric field intensity generated in the biological tissue are set to be sufficient to generate irreversible electroporation on the cell membrane of the biological tissue. The bipolar pulse can effectively increase the uniformity of an ablation electric field, and the pulse width of the bipolar pulse and the electric field intensity generated in the biological tissue are enough to generate irreversible electroporation on cell membranes of the biological tissue, thereby reducing ablation dead zones.","['A61B18/12', 'A61B18/14', 'A61B2018/00613', 'A61B2018/00696', 'A61B2018/126']"
CN109791513B,Instruction and logic for detecting numerical accumulation errors,"A processor includes circuitry to decode at least one instruction and an execution unit. The decoded instruction may compute a floating point result. The execution unit includes circuitry to: the method includes executing instructions to determine a floating point result, calculating an amount of precision lost in a mantissa of the floating point result, comparing the amount of precision lost to a numerical accumulated error precision threshold, determining whether a numerical accumulated error occurred based on the comparison, and writing a value to a flag. The amount of precision lost corresponds to a number of bits lost in the mantissa of the floating-point result. The value to be written to the flag may be based on a determination that a numerical accumulation error has occurred. The flag may be used to signal that a numerical accumulation error has occurred.","['G06F7/49942', 'G06F9/3001', 'G06F7/483', 'G06F9/30014', 'G06F9/30101', 'G06F9/3016', 'G06F9/38', 'G06F9/3854', 'G06F9/3865']"
US10985558B1,Structural similarity based pilot protection method and system for renewable power transmission line,"The wide application of power electronic components in power systems with renewable energy sources has changed the fault characteristics of conventional power systems, resulting in the performance degradation of conventional protections. To solve these problems, a novel principle of pilot protection based on structural similarity and square error criteria is provided. The structural similarity criterion utilizes the difference of fault characteristics between renewable sources and synchronous generators to identify internal faults, and the square error criterion is used to solve abnormal calculation of the conventional similarity based protection. Compared with conventional differential protections, the disclosed protection shows excellent performance in speed and reliability during various faults.","['G05B19/042', 'H02J3/00125', 'G05B19/0428', 'H02H1/0007', 'H02H1/0092', 'H02H7/228', 'H02H7/263', 'H02J13/00002', 'G05B2219/2619', 'G05B2219/2639', 'H02J2300/28', 'H02J3/381', 'Y02E60/00', 'Y04S10/20']"
US11768831B2,Systems and methods for translating natural language queries into a constrained domain-specific language,"A natural language query to domain-specific language query (NLQ-to-DSLQ) translation system includes a language model and a domain-specific language (DSL) parser that constrains the output of the language model to a DSL, such as structured query language (SQL). At each decoding step, the language model generates a predicted next token for each of a set of potential translations of a NLQ. The DSL parser evaluates each of the potential translations at each decoding step based on a set of stored DSL rules, which define valid terminology, syntax, grammar, and/or other constraints of the DSL. The DSL parser may reject and remove from consideration partial potential translations that are invalid or receive a low parsing score, such that the language model only continues to generate new tokens at the next decoding step for partial potential translations that are determined to be valid and/or sufficiently high scoring.","['G06F16/24522', 'G06F16/243']"
CN110399454B,Text coding representation method based on transformer model and multiple reference systems,"The method comprises the steps of splicing a word vector and a segmentation character vector of a sentence to obtain a spliced word vector based on word vector and separator vector coding results of a contextualized text; mapping the spliced word vector according to at least two set semantic concepts to obtain at least two semantic concept vectors of the word vector, and enabling the semantic concept vectors of the word vector to express convergence when the absolute semantic concept number of the word vector is smaller than the set semantic concept total number, and finally leaving p dissimilar semantic concept vectors; selecting the most appropriate semantic concept vector of the word vector in the current context from the dissimilar semantic concept vectors through maximum pooling to serve as a semantic prediction result of the word vector in the current context; and obtaining a probability vector of the word vector, and determining the word probability under the semantic concept corresponding to the word vector according to the probability vector.","['G06F16/3344', 'G06N20/00']"
CN110597991B,"Text classification method and device, computer equipment and storage medium","The application relates to a text classification method, and relates to the technical field of natural language processing. The method comprises the following steps: generating a long text containing at least two texts to be classified; processing the long text through a self-attention submodel to obtain a fused word vector of each word in the long text, wherein the self-attention submodel is used for fusing the incidence relation among the words in the original word vector of each word; and processing the fused word vector of each word in the long text through the output sub-model to obtain the classification results of at least two texts to be classified. According to the scheme, word association relation fusion is carried out between different texts to be recognized in an artificial intelligence scene based on multi-text classification, and in the process of classifying through an output sub-model, text classification can be carried out by combining association relations between the texts to be classified, so that the information basis of text classification is expanded, and the accuracy of multi-text classification is improved.","['G06F16/3347', 'G06F16/35']"
US12099820B2,Training and using artificial intelligence (AI) / machine learning (ML) models to automatically supplement and/or complete code of robotic process automation workflows,"Training and using artificial intelligence (AI)/machine learning (ML) models to automatically supplement and/or complete code of RPA workflows is disclosed. A trained AI/ML model may intelligently and automatically predict and complete the next series of activities in RPA workflows (e.g., one, a few, many, the remainder of the workflow, etc.). Actions users take while creating workflows over a time period may be captured and stored. The AI/ML model may then be trained and used to match the stored actions with stored workflow sequences of actions in order to predict and complete the workflow. As more and more workflow sequences are captured and stored over time, the AI/ML model may be retrained to predict a larger number of sequences and/or to more accurately make predictions. Auto-completion may occur in real-time in some embodiments to save time and effort by the user.","['G06F8/36', 'G06F8/33', 'G06F8/34', 'G06N20/00', 'G06N3/04', 'G06N3/084', 'G06N3/006']"
CN110516253B,Chinese spoken language semantic understanding method and system,"The embodiment of the application provides a Chinese spoken language semantic understanding method. The method comprises the following steps: acquiring a generalized label-free text sequence training set, and sequentially carrying out forward prediction and reverse prediction on the training set, and training a word level and a word level bidirectional language model; receiving spoken voice audio input by a user, and performing sequence word segmentation, word sequence and word sequence; decoding the word sequence and the word sequence by using a word level and a word level bidirectional language model respectively to obtain hidden layer vectors of the word level and the word level; vector alignment is carried out on the word sequence and the hidden layer vector of the word sequence, so that the hidden layer vector of the spoken voice audio input by the semantic understanding model is obtained; and inputting the hidden layer vector of the spoken voice audio to a semantic understanding model to determine the semantic of the spoken voice audio. The embodiment of the application also provides a Chinese spoken language semantic understanding system. The embodiment of the application has good generalization capability, combines word sequences and word sequences, and improves the Chinese semantic understanding performance.",[]
US20190260204A1,"Devices, systems and methods for the collection of meter data in a common, globally accessible, group of servers, to provide simpler configuration, collection, viewing, and analysis of the meter data","The present disclosure is directed to a machine learning system for use with a power distribution system. The machine learning system includes a data library, a machine learning module, and an action module. The data library stores a plurality of data samples, where at least a portion of the data samples are associated with one or more intelligent electronic devices (IEDs). The machine learning module processes data samples from the data library using at least one machine learning algorithm and outputs at least one recommendation and/or prediction based on the data samples received. The action module receives the at least one recommendation and/or prediction and performs at least one action based on the recommendation and/or prediction. The at least one action includes outputting at least one communication signal and/or at least one control signal to at least one client or at least one of the one or more IEDs.","['G06Q30/0206', 'H02J3/14', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G06N3/126', 'H02J3/003', 'G05B2219/2642', 'G06N20/10', 'G06N20/20', 'G06N5/01', 'G06N7/01', 'G06Q50/06', 'H02J2003/003', 'H02J2310/12', 'Y02B70/3225', 'Y04S20/222', 'Y04S50/14']"
US12210829B2,"Entity recognition method, apparatus, electronic device and computer readable storage medium","An entity recognition method, apparatus, electronic device, and computer-readable storage medium are provided. The method includes: determining at least one entity boundary word corresponding to a text sequence; determining at least one entity candidate region in the text sequence based on the at least one entity boundary word; and performing entity recognition on the text sequence and identifying at least one entity in the text sequence based on the at least one entity candidate region.","['G06V30/416', 'G06F40/295', 'G06F16/35', 'G06F16/367', 'G06F40/205']"
EP4075797A1,"Point cloud data transmission device, point cloud data transmission method, point cloud data reception device, and point cloud data reception method","Disclosed herein is a point cloud data transmission method. The transmission method may include encoding point cloud data, and transmitting point cloud data. Disclosed herein is a point cloud data reception device. Disclosed herein is a point cloud data reception method. The reception method may include receiving point cloud data, decoding the point cloud data, and rendering the point cloud data.","['G06T9/00', 'G06T9/001', 'H04N13/194', 'H04N13/161', 'H04N13/172', 'H04N19/597', 'H04N19/70', 'H04N21/816', 'H04N21/85406', 'G06T2207/10028']"
US10696182B2,Integrated dual-output grid-to-vehicle (G2V) and vehicle-to-grid (V2G) onboard charger for plug-in electric vehicles,"An integrated and isolated onboard charger for plug-in electric vehicles, includes an ac-dc converter and a dual-output dc-dc resonant converter, for both HV traction batteries and LV loads. In addition, the integrated and isolated onboard charger may be configured as unidirectional or bidirectional, and is capable of delivering power from HV traction batteries to the grid for vehicle-to-grid (V2G) applications. To increase the power density of the converter, the dual-output DC-DC resonant converter may combine magnetic components of resonant networks into a single three-winding electromagnetically integrated transformer (EMIT). The resonant converter may be configured as a half-bridge topology with split capacitors as the resonant network components to further reduce the size of converter. The integrated charger may be configured for various operating modes, including grid to vehicle (G2V), vehicle to grid (V2G) and high voltage to low voltage, HV-to-LV (H2L) charging.","['B60L53/14', 'B60L53/22', 'B60L55/00', 'B60L58/20', 'H01F27/38', 'H01F3/14', 'H01F38/08', 'H02M1/4258', 'H02M3/33561', 'H02M3/33584', 'H02M3/33592', 'B60L2210/10', 'B60L2210/30', 'B60L2210/40', 'H02M1/0058', 'H02M1/007', 'H02M1/008', 'H02M1/4208', 'H02M2001/0058', 'H02M2001/007', 'H02M3/1586', 'Y02E60/00', 'Y02E60/721', 'Y02T10/70', 'Y02T10/7066', 'Y02T10/7072', 'Y02T10/72', 'Y02T10/92', 'Y02T90/14', 'Y04S10/126']"
WO2022105173A1,"Model distillation method and apparatus, and storage medium and device","Disclosed are a model distillation method and apparatus, and a storage medium and a device. The method comprises: acquiring training sample data which is used for training a preset student model; respectively recognizing the training sample data by using the preset student model and a preset teacher model, so as to obtain a teacher recognition result and a student recognition result of the training sample data; acquiring, from the teacher recognition result, a weight parameter which is used for adjusting a recognition result of the preset student model; and calculating a logarithm between the teacher recognition result and the student recognition result, performing a weighted operation on the logarithm by using the weight parameter, and adjusting the preset student model by taking a calculated numerical value as a loss value. By means of the present application, a student model can have the data processing capability of a teacher model, thereby improving the accuracy of the student model.","['G06N3/08', 'G06N5/02']"
US9575138B1,Method for small-signal stability assessment of power systems using source side and load side perturbations,A novel method for real-time small-signal stability analysis for power electronic-based components in a power system. The method may be used to monitor a power system in real-time by perturbing the source side of an electronic-based component of the power system by injecting a current of about 0.5 to 1 percent of a nominal current of the power system at the source side and perturbing the load side of the power electronic-based component by injecting a voltage of about 0.5 to about 1 percent of a nominal voltage of the power system at the load side and varying the voltage at the load side. Time-domain results of the perturbations may be transferred to frequency-domain results and the stability of the power system may be monitored by obtaining a Nyquist contour and employing Generalized Nyquist Criterion or unit circle criterion.,"['G01R19/2513', 'G01R31/40', 'G01R19/2506', 'G01R27/00', 'G01R27/02', 'G01R27/16']"
US12254413B2,Systems and methods for contrastive learning of visual representations,"Systems, methods, and computer program products for performing semi-supervised contrastive learning of visual representations are provided. For example, the present disclosure provides systems and methods that leverage particular data augmentation schemes and a learnable nonlinear transformation between the representation and the contrastive loss to provide improved visual representations. Further, the present disclosure also provides improvements for semi-supervised contrastive learning. For example, computer-implemented method may include performing semi-supervised contrastive learning based on a set of one or more unlabeled training data, generating an image classification model based on a portion of a plurality of layers in a projection head neural network used in performing the contrastive learning, performing fine-tuning of the image classification model based on a set of one or more labeled training data, and after performing the fine-tuning, distilling the image classification model to a student model comprising a relatively smaller number of parameters than the image classification model.","['G06N3/084', 'G06F18/2155', 'G06F18/2178', 'G06F18/241', 'G06N3/045', 'G06N3/08', 'G06V10/764', 'G06V10/7753', 'G06V10/7788', 'G06T2207/20081']"
US20240119217A1,Similar Information Determination and Text Generation for Auto-Annotation of Web Documents,"The present disclosure relates to methods, non-transitory computer readable mediums (CRMs), and systems for the determination of similar information and text generation for auto-annotation of web documents. This disclosure discusses the use of sentence embedding vectors for the determination of similar information. Furthermore, this disclosure discusses the use of natural language processing techniques and deep learning for the generation of text for use within web browser viewable auto-annotations of content within web documents.","['G06F40/106', 'G06F16/483', 'G06F16/5866', 'G06F16/81', 'G06F16/86', 'G06F16/94', 'G06F16/953', 'G06F16/9566', 'G06F16/957', 'G06F16/9577', 'G06F3/04817', 'G06F3/0483', 'G06F3/04847', 'G06F3/0489', 'G06F40/169', 'G06F40/30', 'G06Q30/0276', 'G06Q30/0277', 'G06F3/0482', 'G06F40/20', 'G06N20/00']"
US11927502B2,Simulating realistic test data from transformed real-world sensor data for autonomous machine applications,"In various examples, sensor data recorded in the real-world may be leveraged to generate transformed, additional, sensor data to test one or more functions of a vehicleâ€”such as a function of an AEB, CMW, LDW, ALC, or ACC system. Sensor data recorded by the sensors may be augmented, transformed, or otherwise updated to represent sensor data corresponding to state information defined by a simulation test profile for testing the vehicle function(s). Once a set of test data has been generated, the test data may be processed by a system of the vehicle to determine the efficacy of the system with respect to any number of test criteria. As a result, a test set including additional or alternative instances of sensor data may be generated from real-world recorded sensor data to test a vehicle in a variety of test scenariosâ€”including those that may be too dangerous to test in the real-world.","['G06F11/3684', 'B60W30/08', 'B60W30/12', 'B60W30/143', 'B60W50/04', 'B60W50/045', 'B60W60/0011', 'G01M17/007', 'G01M17/06', 'G01S13/86', 'G01S13/931', 'G01S7/417', 'G06F11/3698', 'G06V10/774', 'G06V20/56', 'G07C5/08', 'B60W2050/0028', 'G01S15/931', 'G01S17/894', 'G01S17/931', 'G06F11/3696', 'G06F2201/81']"
US11546380B2,System and method for creation and implementation of data processing workflows using a distributed computational graph,"A system and method for creating and implementing data processing workflows using a distributed computational graph comprising modules that represent various stages within a data processing workflow. Each module represents one or more data processing steps, with some of the modules representing data processing performed by a cloud-based service and containing code for interfacing with the application programming interface (API) of that cloud-based service. A series of modules and their interconnections specify the workflow. Data is processed according to the workflow by implementing the data processing step represented by each module, some of which may access cloud-based data processing services. The result is that users can create complex data processing workflows that utilize cloud-based services to process data without having to know how to access the cloud-based data processing services, or even know that they exist.","['H04L63/20', 'H04L63/1433', 'G06F16/2477', 'G06F16/951', 'G06F21/57', 'H04L63/1425', 'H04L63/1441', 'H04L67/02', 'H04L67/10', 'H04L67/12']"
US12086548B2,Event extraction from documents with co-reference,"Methods, systems, and computer-readable media for event extraction from documents with co-reference are disclosed. An event extraction service identifies one or more trigger groups in a document comprising text. An individual one of the trigger groups comprises one or more textual references to an occurrence of an event. The one or more trigger groups are associated with one or more semantic roles for entities. The event extraction service identifies one or more entity groups in the document. An individual one of the entity groups comprises one or more textual references to a real-world object. The event extraction service assigns one or more of the entity groups to one or more of the semantic roles. The event extraction service generates an output indicating the one or more trigger groups and one or more entity groups assigned to the semantic roles.","['G06F40/30', 'G06F40/295', 'G06N20/00', 'G06N3/045', 'G06N3/048', 'G06N5/02']"
CN110955970B,Photovoltaic power station layout method and device,"The embodiment of the invention discloses a photovoltaic power station layout method and device. The layout method of the photovoltaic power station comprises the following steps: acquiring a position point set of each equipped component in the photovoltaic square matrix, determining an initial population according to the position point set, and determining the position point of each equipped component by adopting a preset algorithm based on an initial race. According to the technical scheme, the technical problems that an existing photovoltaic power station layout method is low in efficiency and cannot effectively optimize a layout scheme according to layout requirements are solved, mathematical modeling is conducted on the photovoltaic power station layout optimization problem, self-adaptive algorithm optimizing is conducted on discrete position point variables of photovoltaic power station equipment components, global searching capability is improved, optimized equipment component position information is obtained, layout efficiency is improved, and layout cost is saved.",['Y04S10/50']
US10938927B2,Machine learning techniques for processing tag-based representations of sequential interaction events,Methods and systems are provided for processing tag-based event communications using machine learning. One or more event communications are received from a user device. The communication(s) include key-value pairs representing an ordered sequence of multiple interaction events of a set of predefined events. Each communication of the one or more event communications includes one generated via execution of tag code integrated with code of an app page or of a webpage. A representation of the ordered sequence is processed using a machine learning model to generate one or more profile estimation results that include an identification of a particular user profile from amongst a set of stored user profile. Profile data is transmitted to a client system that identifies the particular user profile or is from the particular user profile.,"['H04L67/22', 'G06N20/20', 'G06N20/00', 'G06N5/02', 'G06N7/005', 'G06N7/01', 'H04L67/306', 'H04L67/535', 'G06N20/10', 'G06N3/02', 'G06N5/003', 'G06N5/01', 'H04L67/02']"
CN105930861B,A kind of Diagnosis Method of Transformer Faults based on Adaboost algorithm,"The present invention relates to a kind of Diagnosis Method of Transformer Faults based on Adaboost algorithm, comprising: is trained by the training sample set to Weak Classifierï¼›Weak Classifier is integrated into the strong classifier of more high-class precision after circuit training and weighed value adjustingï¼›Using the test sample as the input of strong classifier, to obtain corresponding fault type.The present invention solves the problems, such as that strong classifier is difficult to obtain, in addition, operation of the present invention is simple, carrying out Classification and Identification to transformer fault mode has preferable practicability by integrating Weak Classifier.","['G06F18/2453', 'G06F18/23213']"
US11822603B2,Modeling higher-level metrics from graph data derived from already-collected but not yet connected data,"Systems and methods for modeling higher-level metrics from graph data derived from already-collected but not yet connected data are disclosed. A method includes extracting a first set of actor-related data, a second set of object-related data, and a third set of temporal data from the set of the already-collected but not yet connected data representative of a unit-level contribution to the target activity. The method further includes generating graph data for a graph using the set of the already-collected but not yet connected data, where each of the plurality of nodes of the graph corresponds to the actor or the object, and where an attribute associated with each of the plurality of edges of the graph corresponds to a measurement associated with the target activity. The method further includes modeling a relationship between graph attributes associated with the graph data and a higher-level metric associated with the target activity.","['G06F16/9024', 'G06F16/245', 'G06F16/2465', 'G06F16/285', 'G06F18/2323', 'G06N3/02', 'G06V20/41']"
WO2022247811A1,"Control method and system for energy storage charging and discharging of photovoltaic-energy storage-charging station, and device and medium","Disclosed in the present invention are a control method and system for energy storage charging and discharging of a photovoltaic-energy storage-charging station, and a device and a medium. The control method comprises: S1, collecting electricity consumption data of a user, obtaining the electricity consumption power of the user according to the collected electricity consumption data of the user, acquiring a correlation between photovoltaic electricity generation power and time, and querying and obtaining a correlation between an electricity price and time; S2, acquiring the maximum charging and discharging power of an energy storage battery; S3, setting the energy of the energy storage battery, and initial and end energy storage SOC values; S4, constructing a mixed-integer linear programming model for the charging and discharging power of the energy storage battery according to the electricity consumption power of the user, the photovoltaic electricity generation power and the electricity price, and setting several constraint conditions; and S5, calculating the charging and discharging power of the energy storage battery according to the mixed-integer linear programming model and the constraint conditions, and outputting same. In the present invention, the energy storage charging and discharging power of a photovoltaic-energy storage-charging station is controlled by means of a mixed-integer linear programming algorithm, such that the electricity consumption costs can be reduced.","['H02J3/008', 'B60L53/51', 'B60L53/53', 'B60L53/60', 'B60L53/63', 'B60L53/64', 'H02J3/32', 'H02J3/381', 'H02J7/0068', 'H02J7/35', 'H02J2203/10', 'H02J2203/20', 'H02J2300/24', 'Y02E10/56', 'Y02E70/30', 'Y02T10/70', 'Y02T10/7072', 'Y02T90/12']"
US20250272945A1,Data processing method and apparatus,"A data processing method is applied to image processing. The method includes: obtaining a first image feature corresponding to an image and a text feature corresponding to a text; obtaining a plurality of second embedding vectors through a neural network based on a plurality of preset first embedding vectors and the first image feature, where each second embedding vector corresponds to one candidate region of a target object, and each second embedding vector and the first image feature are used to be fused to obtain one corresponding second image feature; and determining, based on a similarity between the text feature and the plurality of second embedding vectors, a weight corresponding to each second embedding vector, where a plurality of weights are used to be fused with a plurality of second image features, to determine a prediction region corresponding to the target object.","['G06V20/70', 'G06V10/26', 'G06F16/33', 'G06F18/253', 'G06F40/126', 'G06F40/30', 'G06N3/0464', 'G06N3/048', 'G06N3/084', 'G06V10/25', 'G06V10/40', 'G06V10/806', 'G06V10/82', 'G06V30/148', 'G06V30/19', 'G06V30/41', 'G06V2201/07']"
WO2021180242A1,"Method and apparatus for detecting anomaly in diagnostic data, and computer device and storage medium","A method and apparatus for detecting an anomaly in diagnostic data, and a computer device and a storage medium, which belong to the field of intelligent medical treatment. The method comprises: respectively matching diagnostic data of a target patient with two rules, i.e. a preset medical treatment rule and a medical treatment mining rule, so as to obtain two types of candidate information, and by means of fusing the two types of candidate information, obtaining multi-dimensional third candidate information in which the medical treatment rule and the medical treatment mining rule are combined; using a disease recognition model to recognize the diagnostic data, so as to acquire fourth candidate information, such that the flexibility of recognition of the diagnostic data is improved and the recognition speed is fast; and determining suspected disease information of the target patient by means of combining the fourth candidate information and the third candidate information, so as to determine, according to the suspected disease information, whether the diagnostic data is anomalous, and thereby achieving the aim of quickly and effectively confirming a misdiagnosis.","['G06F18/214', 'G16H50/70', 'G16H70/60', 'Y02A90/10']"
US10437635B2,Throttling events in entity lifecycle management,"The disclosed declarative framework implements a machine for multi-step progression of interaction with an entity. The framework is usable for a broad range of applicationsâ€”providing a simple rule-based authoring tool for specifying elements and components of a complex state machine, including state definitions, state transition triggers, state transition conditions and state transition actions. Case-status states, a first filtering condition, and a count parameter that specifies a limit on a number of times within a time period in excess of which additional events with characteristics that match will be ignored or discarded are usable to determine whether to ignore or process an incoming eventâ€”throttling the rate at which certain actions occur. A workflow engine gets loaded with instructions derived from the states and event filtering conditions, for handling incoming machine-generated events. Once defined, the state machine is automatically generated and implemented based on the declarative input provided.","['G06F9/542', 'G06F11/30', 'G06F11/3006', 'G06F11/3013', 'G06F11/3072', 'G06F11/328', 'G06F9/485', 'G06F9/4887', 'G06F11/3438', 'G06F11/3452', 'G06F2201/86']"
US12399905B2,Context-sensitive linking of entities to private databases,"Methods, systems, and computer-readable media for context-sensitive linking of entities to private databases are disclosed. An entity linking service stores a plurality of representations of entities. Individual ones of the entities correspond to individual ones of a plurality of records in one or more private databases. The entity linking service determines a mention of an entity in a document. The entity linking service selects, from the plurality of records in the one or more private databases, a record corresponding to the entity. The record is selected based at least in part on the plurality of representations of the entities and based at least in part on a context of the mention of the entity in the document. The entity linking service generates output comprising a reference to the selected record in the one or more private databases.","['G06F16/254', 'G06F16/2358', 'G06F21/6218', 'G06F40/289', 'G06F40/30', 'H04L67/10']"
US11296961B2,Simplified entity lifecycle management,"The technology disclosed offers a declarative framework that implements a machine for multi-step progression of interaction with an entity. The declarative framework is usable over and over for a broad range of applications because it provides a simple rule-based authoring tool that can be used for specifying different elements and components of a complex state machine, including state definitions, state transition triggers, state transition conditions and state transition actions. Once defined, the state machine is automatically generated and implemented based on the declarative input provided by a non-technical user.","['H04L43/045', 'G06F3/0482', 'G06F3/04842', 'G06F8/00', 'G06F8/34', 'G06F8/38', 'G06F9/50', 'G06Q10/06', 'G06Q10/06316', 'G06Q10/30', 'G06Q50/01', 'G06T11/206', 'H04L43/0811', 'H04L43/0817', 'Y02W90/00']"
US20220201042A1,Ai-driven defensive penetration test analysis and recommendation system,A system and method for automated defensive penetration test analysis that predicts the evolution of new cybersecurity attack strategies and makes recommendations for cybersecurity improvements to networked systems based on a cost/benefit analysis. The system and method use captured system data to classify networked system based upon their susceptibility to privilege escalation attacks measured against the networked system's response to a penetration test. The system and method use machine learning algorithms to run simulated attack and defense strategies against a model of the networked system created using a directed graph. Recommendations are generated based on an analysis of the simulation results and system classifications against a variety of cost/benefit indicators.,"['H04L63/1425', 'G06F16/2477', 'G06F16/951', 'G06F21/577', 'H04L63/20', 'G06F2221/033', 'G06F2221/034', 'H04L63/1441']"
US20240161165A1,Cross-domain recommendation via contrastive learning of user behaviors in attentive sequence models,"The technology involves a personalized recommender system that can be used with an e-commerce platform. It employs a contrastive learning based cross-domain recommendation approach. The approach balances the learning of user behaviors within each domain, as well as user behaviors across multiple domains. To achieve robust user representations and to improve knowledge transfer between the source and target domains, multi-task intra-domain contrastive regularizations may be employed along with multiple branches of sequential attentive encoders in a model for cross-domain sequential recommendation. Different data augmentation approaches can be used to generate augmented data for contrastive learning. For instance, different data augmentation methods may be combined with recommendation optimization in a multi-task learning paradigm. An optimized sequence representation may be fine-tuned in a next-value prediction task for recommendation in a target domain.","['G06Q30/0631', 'G06N3/044', 'G06N3/045', 'G06N3/0455', 'G06N3/08', 'G06N3/084', 'G06N3/0895']"
US11503155B2,"Interactive voice-control method and apparatus, device and medium","The present disclosure discloses an interactive voice-control method and apparatus, a device and a medium. The method includes: obtaining a sound signal at a voice interaction device and recognized information that is recognized from the sound signal; determining an interaction confidence of the sound signal based at least on at least one of an acoustic feature representation of the sound signal and a semantic feature representation associated with the recognized information; determining a matching status between the recognized information and the sound signal; and providing the interaction confidence and the matching status for controlling a response of the voice interaction device to the sound signal.","['H04M3/4936', 'G06F40/30', 'G06F40/35', 'G10L15/02', 'G10L15/04', 'G10L15/063', 'G10L15/16', 'G10L15/1822', 'G10L15/22', 'G10L15/26', 'G10L15/28', 'G10L15/30', 'G10L2015/223', 'G10L2015/226', 'G10L25/78']"
US20250133121A1,Machine learning system and method for network security improvement,A system and method for automated cybersecurity defensive strategy analysis that predicts the evolution of new cybersecurity attack strategies and makes recommendations for cybersecurity improvements to networked systems based on a cost/benefit analysis. The system and method use machine learning algorithms to run simulated attack and defense strategies against a model of the networked system created using a directed graph. Recommendations are generated based on an analysis of the simulation results against a variety of cost/benefit indicators.,"['H04L63/1433', 'G06F16/2477', 'G06F16/951', 'H04L63/1425', 'H04L63/1441', 'H04L63/20', 'G06N3/006', 'G06N3/126']"
US20220318504A1,Interpreting text-based similarity,The disclosure herein describes a system for interpreting text-based similarity between a seed item and a recommended item selected by a pre-trained language model from a plurality of candidate items based on semantic similarities between the seed item and the recommended item. The system analyzes similarity scores and contextual paragraph representations representing text-based descriptions of the seed item and recommended item to generate gradient maps and word scores representing the text-based descriptions. A model for interpreting text-based similarity utilizes the calculated gradients and word scores to match words from the seed item description with words in the recommended item description having similar semantic meaning. The word-pairs having the highest weight are identified by the system as the word-pairs having the greatest influence over the selection of the recommended item from the candidate items by the original pre-trained language model.,"['G06F40/30', 'G06F16/9538', 'G06F40/284', 'G06F40/295', 'G06N3/045', 'G06N3/08']"
US11218510B2,Advanced cybersecurity threat mitigation using software supply chain analysis,"A system and method for comprehensive cybersecurity threat assessment of software applications based on the totality of vulnerabilities from all levels of the software supply chain. The system and method comprising analyzing the code and/or operation of a software application to determine components comprising the software, identifying the source of such components, determining vulnerabilities associated with those components, compiling a list of such components, creating a directed graph of relationships between the components and their sources, and evaluating the overall threat associated with the software application based its software supply chain vulnerabilities.","['H04L63/20', 'H04L63/1433', 'G06F16/2477', 'G06F16/951', 'G06F21/577', 'H04L63/1425', 'H04L63/1441']"
US11792229B2,AI-driven defensive cybersecurity strategy analysis and recommendation system,A system and method for automated cybersecurity defensive strategy analysis that predicts the evolution of new cybersecurity attack strategies and makes recommendations for cybersecurity improvements to networked systems based on a cost/benefit analysis. The system and method use machine learning algorithms to run simulated attack and defense strategies against a model of the networked system created using a directed graph. Recommendations are generated based on an analysis of the simulation results against a variety of cost/benefit indicators.,"['H04L63/20', 'G06F16/2477', 'G06F16/9024', 'G06F16/951', 'H04L63/1425', 'H04L63/1441']"
US20200293828A1,Techniques to train a neural network using transformations,"Apparatuses, systems, and techniques to perform training of neural networks using stacked transformed images. In at least one embodiment, a neural network is trained on stacked transformed images and trained neural network is provided to be used for processing images from an unseen domain distinct from a source domain, wherein stacked transformed images are transformed according to transformation aspects related to domain variations.","['G06T7/11', 'G06K9/6257', 'G06F18/2148', 'G06F18/217', 'G06K9/00208', 'G06K9/00979', 'G06K9/6262', 'G06N20/00', 'G06N3/04', 'G06N3/063', 'G06N3/08', 'G06V10/7715', 'G06V10/774', 'G06V10/95', 'G06V20/647', 'G06V30/19127', 'G06V30/19147', 'G06K2209/05', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30048', 'G06T2207/30081', 'G06V2201/03']"
US10770901B2,Systems and methods for optimally delivering electrical energy in a network,"A method for delivering an energy resource on a network includes receiving constraints, a first objective function, and a second objective function. The constraints represent electrical load capacities of elements of the network. The constraints and the first objective function are used to identify (i) a load matrix embodying a set of optimal electrical loads for each of the plurality of elements of the power grid and (ii) a corresponding first feature matrix. The load matrix and the first feature matrix optimize the first objective function and satisfy the constraints. If the load matrix and first feature matrix indicate a non-binding fully loaded (NBFL) constraint, the second objective function is used to identify a second feature matrix which optimizes both the first and second objective functions while satisfying the constraints. The second feature matrix assigns a non-zero value to at least one previously identified NBFL constraint.","['H02J3/381', 'H02J3/38', 'G05B13/041', 'H02J2203/20', 'Y02E40/70', 'Y02E60/00', 'Y04S10/50', 'Y04S40/20']"
US20240028841A1,"Speech translation method, device, and storage medium","Provided are a speech translation method, a device, and a storage medium. The method includes: extracting, through an encoder of an end-to-end speech translation model, the semantic feature of a to-be-processed speech; decoding, through a decoder of the end-to-end speech translation model, a source language text corresponding to the semantic feature from the semantic feature; decoding, through the decoder of the end-to-end speech translation model, the semantic feature according to the source language text to obtain a text sequence corresponding to the semantic feature; and splitting the text sequence to obtain a target language text corresponding to the to-be-processed speech.","['G06F40/58', 'G06F40/47', 'G06F18/2415', 'G06F40/30', 'G06N3/045', 'G06N3/08', 'G10L15/02', 'G10L15/063', 'G10L15/16', 'G10L15/1815', 'G10L15/26', 'G10L25/24', 'G10L2015/025']"
CN111738025B,"Artificial intelligence based translation method and device, electronic equipment and storage medium","The application discloses a translation method, a translation device, electronic equipment and a storage medium based on artificial intelligence; the method comprises the steps of obtaining a first language text to be translated; obtaining word vectors corresponding to each first language participle in a first language text through a first translation model, wherein the first translation model is obtained by training based on a plurality of pseudo parallel corpus pairs, each pseudo parallel corpus pair comprises a second language corpus and the first language corpus obtained by translating the second language corpus through a second translation model, and the second translation model is obtained by training based on a plurality of reference first language corpora; predicting a first probability that each first language segment translates to a corresponding candidate second language segment based on the word vector; determining a target second language participle corresponding to each first language participle based on the first probability; and fusing the target second language participles to obtain a translated second language text. According to the embodiment of the application, richer source information can be provided for training of the first translation model, and the translation quality is further improved.","['G06F40/58', 'G06F40/44']"
US11182433B1,Neural network-based semantic information retrieval,"A question and answer (Q&A) system is enhanced to support natural language queries into any document format regardless of where the underlying documents are stored. The Q&A system may be implemented â€œas-a-service,â€ e.g., a network-accessible information retrieval platform. Preferably, the techniques herein enable a user to quickly and reliably locate a document, page, chart, or data point that he or she is looking for across many different datasets. This provides for a unified view of all of the user's (or, more generally, an enterprise's) information assets (such as AdobeÂ® PDFs, MicrosoftÂ® Word documents, Microsoft Excel spreadsheets, Microsoft PowerPoint presentations, Google Docs, scanned materials, etc.), and to be able to deeply search all of these sources for the right document, page, sheet, chart, or even answer to a question.","['G06F40/30', 'G06F16/3329', 'G06F16/90332', 'G06K9/344', 'G06N3/04', 'G06N3/045', 'G06V30/153', 'G06K2209/01', 'G06N20/00', 'G06N5/01']"
CN112035743B,"Data recommendation method and device, computer equipment and storage medium","The embodiment of the application discloses a data recommendation method, a data recommendation device, computer equipment and a storage medium, wherein the data recommendation method can be applied to accurate recommendation and comprises the following steps: responding to a data recommendation request aiming at a target user in a target field, and acquiring a service object set which has an incidence relation with the target user in a plurality of fields; performing cross-domain cross coding processing on the multiple domains and the service object set to obtain target domain interest characteristics of a target user in a target domain; acquiring the characteristics of a plurality of to-be-recommended service objects in the target field; and acquiring target recommended service object characteristics matched with the target field interest characteristics from the plurality of service object characteristics to be recommended, and outputting a target recommended service object corresponding to the target recommended service object characteristics. By the method and the device, the accuracy of content recommendation can be improved.","['H04L67/535', 'G06F16/9035', 'G06F16/9535', 'H04L67/306']"
US11932135B2,Hybrid battery management system,"Provided is a device configured to determine a power capacity of a battery of a vehicle, predict a first set of values indicative of amounts of power to be stored during a time interval by the battery, the power being generated by a renewable energy generator carried by the vehicle, and predict a second set of values indicative of amounts of energy to be consumed from the battery during the time interval based on previous energy consumption by the vehicle. The device is also configured to determine a score based on the power capacity, the first set of values, and the second set of values. The system is also configured to determine whether the score satisfies a threshold and, in response to a determination that the score satisfies the threshold, activate an internal combustion engine to charge to the battery.","['B60L58/10', 'B60L1/00', 'B60L50/60', 'B60L53/24', 'B60L8/003', 'B60L8/006', 'B63B79/15', 'B63B79/30', 'B63B79/40', 'F02D29/06', 'F03D9/32', 'G01P5/00', 'G01R31/3647', 'G01W1/10', 'B60L2200/32', 'B60L2240/62', 'B60Y2200/92', 'B63B79/10', 'F05B2220/706', 'F05B2240/931', 'Y02E10/72', 'Y02T10/70', 'Y02T10/7072', 'Y02T70/10', 'Y02T90/14']"
US10914698B2,Sensing method and system,A system that includes a sensor for measuring a resonant impedance spectral response of an inductor-capacitor-resistor (LCR) resonator and correlating the measured response of one or more spectral parameters to one or more characteristics of the fluid. Such characteristics may be the age or health of the fluid and/or the identification of and concentration of components in the fluid.,"['G01N27/126', 'G01N27/026', 'G01N33/1893', 'G01N33/2888']"
US12374333B2,VAS toggle based on device orientation,"As noted above, example techniques relate to toggling a cloud-based VAS between enabled and disabled modes. An example implementation involves a NMD detecting that the housing is in a first orientation and enabling a first mode. Enabling the first mode includes disabling voice input processing via a cloud-based VAS and enabling local voice input processing. In the first mode, the NMD captures sound data associated with a first voice input and detects, via a local natural language unit, that the first voice input comprises sound data matching one or more keywords. The NMD determines an intent of the first voice input and performs a first command according to the determined intent. The NMD may detect that the housing is in a second orientation and enables the second mode. Enabling the second mode includes enabling voice input processing via the cloud-based VAS.","['G10L15/22', 'G06F3/0482', 'G06F3/04842', 'G06F3/167', 'G10L15/1815', 'G10L15/1822', 'G10L15/26', 'G10L17/22']"
CN116662582B,Specific domain business knowledge retrieval method and retrieval device based on natural language,"The application discloses a specific field business knowledge retrieval method and a retrieval device based on natural language, comprising the following steps: constructing a pre-training language model; carrying out feature representation on the business knowledge data to obtain a database formed by feature vectors; constructing a language understanding model; inputting natural sentences and obtaining query vectors of the search questions through a language understanding model; calculating the similarity between the query vector and the feature vector in the database; and returning the business knowledge corresponding to the first k feature vectors; the application can better understand the query intention of the user by constructing the pre-training language model and the language understanding model, thereby more accurately matching and retrieving the related business knowledge. Meanwhile, by calculating the similarity between the query vector and the feature vector in the database, the service knowledge most relevant to the query can be found more quickly, and the retrieval efficiency is greatly improved.","['G06F16/38', 'G06F16/3329', 'G06F16/3344', 'G06N3/0442', 'G06N3/0455', 'G06N3/08', 'G06N5/022', 'Y02D10/00']"
US12039766B2,"Image processing method, apparatus, and computer product for image segmentation using unseen class obtaining model","The present disclosure provides an image processing method, apparatus, device, and computer-readable storage medium. The method includes: obtaining an image dataset, the image dataset including an image and an accompanying text related to an unseen class in the image; and generating a probability and/or distribution of the unseen class using an unseen class obtaining model, the probability and/or distribution of the unseen class including a probability that each pixel in the image is from the unseen class, a probability that the unseen class is present in the image, and a regional probability after the image is subdivided into a plurality of regions.","['G06V20/70', 'G06F18/2415', 'G06V10/26', 'G06N3/045', 'G06N3/0455', 'G06N3/047', 'G06N3/08', 'G06N3/09', 'G06V10/267', 'G06V10/44', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06V10/84', 'G06V20/41', 'G06V30/19147', 'G06V30/274', 'G06N3/048', 'G06V2201/10']"
US12400296B2,Image super-resolution,"There is provided a solution for image processing. In this solution, first and second information is determined based on texture features of an input image and a reference image. The first information at least indicates for a first pixel block in the input image a second pixel block in the reference image most relevant to the first pixel block in terms of the texture features, and the second information at least indicates a relevance of the first pixel block to the second pixel block. A transferred feature map with a target resolution is determined based on the first information and the reference image. The input image is transformed into an output image with the target resolution based on the transferred feature map and the second information. The output image reflects a texture feature of the reference image.","['G06T3/4053', 'G06T3/4046', 'G06T7/40', 'G06N3/045', 'G06N3/08', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084']"
US9839103B2,Method and apparatus for power extraction in a pre-existing AC wiring infrastructure,"A method and apparatus for extracting power in a switch location, and in a load location for use in a pre-existing infrastructure of switching AC power to a lamp (or other load) via a two terminal switch device. The switch is replaced with a module including a first controlled switch (such as a triac or relay) and a first impedance (such as a capacitor) connected in parallel, and another module including a second controlled switch (such as a triac or relay) and a second impedance (such as a capacitor) connected in parallel, is installed at the load location. In an â€˜offâ€™ state where the two controlled switches are in â€˜openâ€™ state, current is flowing via the impedances, but not through the load, so that power extractor circuits in the modules, connected in series to the impedances, extract low power for DC powering logic and other loads in the modules.","['H05B37/0272', 'A01G31/02', 'A01G7/045', 'B01D61/147', 'C02F1/42', 'H02J3/14', 'H05B37/0227', 'H05B39/086', 'H05B39/088', 'H05B47/115', 'H05B47/19', 'H05B47/1965', 'A01G25/16', 'A01G27/005', 'A01G31/00', 'A01G31/065', 'A01G7/04', 'A01G9/02', 'F21V23/04', 'G05B15/02', 'H02M1/0006', 'H02M2001/0006', 'H04W84/12', 'Y02B20/40', 'Y02B70/30', 'Y02B70/3225', 'Y04S20/222', 'Y04S20/246']"
US11264140B1,System and method for automated pharmaceutical research utilizing context workspaces,"A system and method for an automated pharmaceutical research utilizing contextual workspaces comprising a workspace drive engine, a data analysis engine, one or more machine and deep learning modules, a knowledge graph, and a workspace interface, which can create a virtual research workspace where data files containing biochemical data related to current research can be uploaded, which automatically processes and analyzes the uploaded data file to autonomously extract a plurality of information related to the uploaded data file, which performs various similarity searches on the uploaded data, and which formats and displays all the extracted information in the workspace, such that the workspace may provide a deeper contextualized view of the uploaded biochemical data.","['G16H20/10', 'G06F16/24575', 'G16H50/20', 'G16H50/50', 'G16H50/70', 'G16H70/40']"
CN115171838B,Training method of medical report generation model based on cross-modal fusion,"The embodiment of the invention provides a training method of a medical report generation model based on cross-modal fusion, belonging to the technical field of data processing, and specifically comprising the following steps: obtaining a first image characteristic; associating the fine-grained abnormal region to obtain a second image feature, and extracting coarse-grained disease semantic information to obtain a third image feature; inputting the second image characteristic and the third image characteristic into a cross attention module for modeling multi-scale association to obtain a fourth image characteristic; inputting the fourth image characteristic and the first text characteristic of the t-1 round into a text generation module to obtain a modal invariance characteristic; obtaining a medical text prediction result of the t round according to the modal invariance characteristics until the prediction of each round of the text sequence is completed, and calculating text generation loss by combining text labels corresponding to the source images; and performing iterative training according to the text generation loss to obtain a trained medical report generation model. By the scheme of the invention, cross-mode recognition is realized, and the model recognition precision and robustness are improved.","['G16H15/00', 'G06F40/126', 'G06F40/30', 'G06T7/0012', 'G06V10/454', 'G06V10/811', 'G06V10/82', 'G06T2207/10116', 'Y02A90/10']"
US11404145B2,Medical machine time-series event data processor,"Systems, apparatus, instructions, and methods for medical machine time-series event data processing are disclosed. An example time series event data processing apparatus includes memory storing instructions and one-dimensional time series healthcare-related data; and at least one processor. The example at least one processor is to: execute artificial intelligence model(s) trained on aggregated time series data to at least one of a) predict a future medical machine event, b) detect a medical machine event, or c) classify the medical machine event using the one-dimensional time series healthcare-related data; when the artificial intelligence model(s) are executed to predict the future medical machine event, output an alert related to the predicted future medical machine event to trigger a next action; and when the artificial intelligence model(s) are executed to detect and/or classify the medical machine event, label the medical machine event and output the labeled event to trigger the next action.","['G16H10/00', 'G16H50/20', 'A61B5/7267', 'G06F9/451', 'G06N20/00', 'G06N20/20', 'G06N3/044', 'G06N3/0445', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/049', 'G06N3/08', 'G06N3/084', 'G06N3/088', 'G06N3/094', 'G16H10/60', 'G16H15/00', 'G16H40/67', 'G16H50/30', 'G16H50/50', 'A61B5/7275', 'G06T2207/20081', 'G06T2207/20084', 'G16H30/00', 'G16H70/20']"
US12120838B2,Semantic labeling analysis,"Tools and techniques are described to create an interface that can translate a device language into an internal language, and describe the device to the controller in terms of actors and quanta such that when a device is attached to a controller, the controller can understand what the device does and why it does it. This internal language can then be translated back to a natural language, such as English. This allows the controller to track errors, determine what upstream or downstream device and action of the device caused the error, and to track many different facts of the system that allow for detailed reports.","['H05K7/1465', 'F24F11/32', 'F24F11/49', 'F24F11/63', 'F24F11/88', 'G01R31/55', 'G05B13/0265', 'G05B15/02', 'G05B19/048', 'G05B23/0216', 'G05B23/0264', 'G05B23/0272', 'G06F1/3209', 'G06F1/3246', 'G06F3/04186', 'G06F3/0482', 'G06F3/04847', 'G06F3/147', 'G06F30/12', 'G06F30/13', 'G06F30/18', 'G06F8/436', 'G06F8/51', 'G06F8/53', 'G06F8/74', 'G06F9/4418', 'G06Q30/0283', 'H02J3/00', 'H04B3/46', 'H04L43/50', 'H04L67/12', 'H04L67/125', 'H04L67/75', 'H04M3/305', 'H04W4/80', 'H04W84/00', 'H05K7/1468', 'H05K7/1477', 'H05K7/1481', 'G06F2111/04', 'G06F2111/16', 'G06F2113/04', 'G06F2113/16', 'G06F2115/12', 'G06F30/392', 'H02J2310/12']"
US12288012B2,System and method for evaluating models for predictive failure of renewable energy assets,"An example method comprises receiving historical sensor data from sensors of components of wind turbines, training a set of models to predict faults for each component using the historical sensor data, each model of a set having different observation time windows and lead time windows, evaluating each model of a set using standardized metrics, comparing evaluations of each model of a set to select a model with preferred lead time and accuracy, receive current sensor data from the sensors of the components, apply the selected model(s) to the current sensor data to generate a component failure prediction, compare the component failure prediction to a threshold, and generate an alert and report based on the comparison to the threshold.","['H02S50/00', 'G01R31/343', 'G01R31/42', 'G01R31/62', 'G06F17/16', 'G06F30/20', 'G01R31/2846', 'G06F2113/06', 'Y02E10/50']"
CN118280562B,Knowledge graph-driven medical big model diagnosis method,"The invention relates to the technical field of medical information, in particular to a knowledge graph driven medical large model diagnosis method, which comprises the following steps: s1: constructing a medical knowledge graph; s2: extracting information of the illness state, and mapping the information to an entity and a relation corresponding to the medical knowledge graph in the step S1; s3: constructing a multi-dimensional patient representation; s4: combining the medical knowledge graph of the S1 and the multidimensional patient portrait constructed in the S3, and training and optimizing a medical large model; s5: generating personalized diagnosis and treatment suggestions aiming at specific situations of patients; s6: continuously optimizing the medical knowledge graph of the S1 and the medical large model of the S4. According to the invention, by combining the medical knowledge graph and the advanced machine learning technology, the accuracy of medical diagnosis and individuation of treatment advice are realized, and meanwhile, a continuous learning mechanism is introduced to dynamically optimize the diagnosis and treatment scheme, so that the medical service quality and efficiency are remarkably improved, and the progress of medical research and practice is promoted.","['G16H50/20', 'G06F18/23', 'G06F40/216', 'G06F40/295', 'G06F40/30', 'G06N3/042', 'G06N3/08', 'G06N5/022', 'G16H50/70']"
US12205723B2,"Pathological diagnosis assisting method using AI, and assisting device","Diagnosis is assisted by acquiring microscopical observation image data while specifying the position, classifying the image data into histological types with the use of AI, and reconstructing the classification result in a whole lesion. There is provided a pathological diagnosis assisting method that can provide an assistance technology which performs a pathological diagnosis efficiently with satisfactory accuracy by HE staining which is usually used by pathologists. Furthermore, there are provided a pathological diagnosis assisting system, a pathological diagnosis assisting program, and a pre-trained model.","['G16H50/20', 'G06T7/00', 'G06T7/0012', 'G01N33/483', 'G01N33/574', 'G02B21/365', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N3/096', 'G06N5/02', 'G06N5/04', 'G06V10/267', 'G06V10/764', 'G06V10/7715', 'G06V10/774', 'G06V10/82', 'G06V20/693', 'G06V20/698', 'G16H10/40', 'G16H30/40', 'G06N3/0464', 'G06N3/0475', 'G06T2207/10056', 'G06T2207/20021', 'G06T2207/20084', 'G06T2207/30024']"
CN117574976B,Large language model software and hardware collaborative quantization acceleration calculation method and system,"The invention discloses a large language model software and hardware collaborative quantization acceleration calculation method and a large language model software and hardware collaborative quantization acceleration calculation system, which adopt a channel as granularity to process outliers in a large language model, store the whole channel with a large number of outliers with high precision, realize the large language model software and hardware collaborative reasoning acceleration through a self-adaptive quantization process, a compiling optimization process and a hardware realization process, ensure the flexibility and regularity of data coding, and are easy to realize and deploy on a system and hardware; the reconfigurable accelerator system includes: the system comprises an operation module, an accumulation module, a decoding module, a control module, an on-chip cache and a main memory. By adopting the method, the precision of the quantized model can be effectively maintained, and the efficient reasoning calculation of hardware can be realized.","['G06N3/063', 'G06F16/334', 'G06F16/35', 'G06F18/2433', 'G06N3/042', 'G06N3/0455', 'G06N3/048', 'G06N3/08', 'G06N5/04', 'Y02D10/00']"
CN112417134B,Abstract automatic generation system and method based on speech and text deep fusion features,"The system comprises a preprocessing and voice corresponding module, an encoder module, a decoder feature fusion module and a loss function module. The preprocessing and voice corresponding module comprises text acquisition and voice corresponding. The decoder feature fusion module comprises intermediate abstract generation, sound feature fusion and modified abstract generation. The loss function module comprises an intermediate summary loss function and an evaluation function of the corrected summary. For user voice data, obtaining a text corresponding to voice through the text, and obtaining voice characteristics corresponding to characters through voice correspondence; text data is subjected to a pre-training xlnet encoder to obtain vector representation of the text; the text vector and the voice feature are subjected to voice feature fusion of a decoder and learning after the intermediate abstract is generated to obtain an intermediate abstract; and (4) the intermediate abstract is encoded by xlnet again to obtain further understanding of the text, and finally the intermediate abstract is corrected to generate the final abstract through learning.","['G06F16/345', 'G06F16/3343', 'G06F16/3344', 'G06F18/253', 'G06F40/126', 'G06F40/232', 'G06F40/284', 'G10L15/26']"
US20220383206A1,Task Augmentation and Self-Training for Improved Few-Shot Learning,"Systems and methods can leverage task-specific unlabeled data to improve downstream performance in data-constrained scenarios. Given a target task, a first technique proposed herein, which can be referred to as task augmentation, uses unlabeled text from the target domain to synthesize a large amount of in-domain training data for an auxiliary task A second technique provides a self-training algorithm, where a model learns to improve itself using its predictions on unlabeled examples.","['G06N20/20', 'G06N3/096', 'G06F18/214', 'G06F18/2113', 'G06F18/2155', 'G06F18/24', 'G06K9/623', 'G06K9/6259', 'G06N20/00', 'G06N3/0895', 'G06N5/04', 'G06N3/0455', 'G06N3/084', 'G06V10/82']"
US8489982B2,Reusable data markup language,"Methods and systems provide a computer markup language, referred to as Reusable Data Markup Language (â€œRDMLâ€), and a data viewer for retrieving, manipulating and viewing documents and files in the RDML format that may be stored locally or over a network (e.g., the Internet). Generally, RDML permits the browsing and manipulation of numbers, as opposed to text and images like in HTML, and does so by including attributes describing the meaning of the numbers to be attached to the numbers. Documents compliant with the markup language encapsulate machine-readable documentation with numbers and data, and permit the data viewer to act as a combination web browser and spreadsheet to automatically read, interpret and manipulate the numbers and data.","['G06F16/9558', 'G06F16/94', 'G06F40/18']"
US9362746B2,Communication network topology management based on an associated electric grid topology,"In one embodiment, a computer determines a grid topology of an electric grid based on one or more electric grid configuration description files, and also determines a network topology of a computer network used to provide communication to grid devices of the electric grid. By assessing whether the network topology is configured to meet one or more communication requirements of the grid topology (e.g., initially and/or through monitoring for grid changes), the computer may trigger a corrective action (e.g., alarm or reconfiguration) in response to the network topology not being configured to meet the one or more communication requirements of the grid topology.","['H02J3/00', 'H02J2003/007', 'H02J2203/20', 'Y02E40/70', 'Y02E60/00', 'Y02E60/728', 'Y02E60/76', 'Y04S10/00', 'Y04S10/22', 'Y04S10/265', 'Y04S40/20', 'Y04S40/22']"
US9223827B2,"Database query language transformation method, transformation apparatus and database query system","A method, device and system for transforming a concept-based query into SQL query statements, that includes transforming inputted concept-based queries into logic rules, checking validity of the logic rules, optimizing the valid logic rules, and translating the logic rules into SQL query statements in accordance with the optimization result.","['G06F16/2452', 'G06F17/30427', 'Y10S707/99934']"
US8611505B2,Method and system of generating reference variations for directory assistance data,"Methods and systems of performing user input recognition are disclosed. A digital directory comprising listings is accessed. Metadata information is associated with individual listings describing the individual listings. The metadata information is modified to generate transformed metadata information. Therefore, the transformed metadata information is generated as a function of context information relating to a typical user interaction with the listings. Information is generated for aiding in an automated user input recognition process based on the transformed metadata information.","['H04M3/4931', 'G10L15/26', 'H04M2201/40']"
EP1762930B1,Process model transformation for event-based coordination of composite applications,"The present description refers in particular to a computer-implemented method, system, and computer program product for providing and executing event-based coordination of process-oriented composite applications. The computer-implemented method for providing and executing event-based coordination of process-oriented composite applications may comprise: determining a selected task from among a plurality of tasks of a process model; associating a first event-based application with the selected task; determining at least one input set which includes a set of object templates and boolean conditions and which is associated with activating the first event-based application, the input set associated with at least one object generated either when an instance of the process model is created and/or in response to completion of at least one preceding task of the selected task within the process model, wherein the input set specifies at least one path to the selected task from a respective preceding task of the process model; defining a second event-based application as operable to receive the at least one object and evaluate the input set in order to determine whether to output a task-enabling object for enabling the first event-based application; and deploying the first event-based application and the second event-based application into a middleware in which the at least one object and the task-enabling object are written and/or read by the first event-based application and/or the second event-based application, wherein the task-enabling object triggers the first event-based application to perform the at least one task in association with at least one external application.","['G06F9/5038', 'G06F8/20', 'G06F9/541']"
US7886269B2,XML application framework,"An extensible markup language (XML) application framework (XAF) may be provided. XAF applications are data driven such that all operations with a computer system are data focused. In addition, the components used in the XAF application are instantiated and connected according to how data is displayed and what data type is used. Applications within XAF comprise a user interface (UI) connector, an action module, and a data connector. UI connectors receive UI events and connect the UI event to an action module. The action module generates a standard format action from the UI event and sends it to the data connector. The data connector translates the standard format action into a data-specific action that changes data in a data store. A data connector then sends a standard format data representation corresponding to the changed data back to the UI connector to provide the changed data to the UI.","['G06F9/542', 'G06F8/00', 'G06F9/451', 'G06F2209/543', 'G06F2209/545', 'G06F9/44505']"
US7769769B2,Methods and transformations for transforming metadata model,Transformations are provided for transforming a metadata model that contains model objects. The metadata model has a multiple layers including a lower layer containing one or more lower abstraction model objects having a lower abstraction level and a higher layer containing one or more higher abstraction model objects having a higher abstraction level. The transformations transform the model objects from the lower layer to the higher layer.,"['G06Q10/10', 'G06F16/25', 'Y10S707/99934', 'Y10S707/99944']"
US8457996B2,Model-based business continuity management,"A business process model (BPM) handler may determine a business process model including tasks arranged according to a directed graph, at least some of the tasks associated with requirements for executing the tasks. An information technology topology model (ITTM) handler may determine an information technology topology model with connected resources used to perform at least some of the tasks. A behavior model generator may determine behaviors of the resources from a behavior information library, and may generate a behavior model in which the tasks and their respective requirements are connected to the resources and to their respective behaviors. A continuity analyzer may thus provide a continuity analysis, based on the behavior model.",['G06Q10/00']
US8365155B2,Software analysis framework,"Presently described is a decompilation method of operation and system for parsing executable code, identifying and recursively modeling data flows, identifying and recursively modeling control flow, and iteratively refining these models to provide a complete model at the nanocode level. The nanocode decompiler may be used to determine if flaws, security vulnerabilities, or general quality issues exist in the code. The nanocode decompiler outputs in a standardized, human-readable intermediate representation (IR) designed for automated or scripted analysis and reporting. Reports may take the form of a computer annotated and/or partially human annotated nanocode listing in the above-described IR. Annotations may include plain English statements regarding flaws and pointers to badly constructed data structures, unchecked buffers, malicious embedded code or â€œtrap doors,â€ and the like. Annotations may be generated through a scripted analysis process or by means of an expert-enhanced, quasi-autonomous system.","['G06F8/53', 'G06F8/427']"
US8290806B2,Method and system for estimating financial benefits of packaged application service projects,"A system for calculating financial benefit estimations and generating reports for multi-dimensional project plans for implementing packaged software applications, the system includes: a view layer configured to act as a user interface for user inputs and system outputs; a model and control layer configured to implement rules based on a series of estimation and implementation models, and to perform calculations to determine financial benefits of implementing multi-dimensional project plans; an estimation knowledge base layer configured to hold and derive the series of estimation and implementation models; and wherein the system for generating financial benefit estimations and reports for the implementation of packaged software applications is carried out over networks comprising: the Internet, intranets, local area networks (LAN), and wireless local area networks (WLAN).","['G06Q30/02', 'G06Q10/063118', 'G06Q10/06313', 'G06Q10/0635', 'G06Q10/0637', 'G06Q10/06375', 'G06Q10/06393', 'G06Q30/0206']"
CA2749770C,Optimization of microgrid energy use and distribution,"An energy distribution may include a server and one or more databases. The system may communicate with an energy provider to receive energy provider data, at least one information collector to receive information collector data such as individualized energy usage data, customer preferences, and customer or location characteristics, and the one or more databases for receiving data for optimization. The system may calculate a cost of service or avoided cost using at least one of the individualized energy usage data and a system generation cost at a nearest bus. The system may also forecast individualized demand by end-use, individualized demand by location, energy prices, or energy costs. The system may optimize energy distribution, energy use, cost of service, or avoided cost using the forecasted individualized demand by end-use, the forecasted individualized demand by location, the forecasted energy prices, and the forecasted energy costs.","['G06Q30/06', 'G06Q10/06315', 'G06Q30/0283', 'G06Q50/06', 'H02J13/00022', 'H02J13/00034', 'H02J3/003', 'H02J3/008', 'H02J3/14', 'H02J3/322', 'H02J7/35', 'H02J2310/10', 'H02J2310/64', 'Y02B70/3225', 'Y02B90/20', 'Y02E40/70', 'Y02E70/30', 'Y04S10/123', 'Y04S20/12', 'Y04S20/222', 'Y04S40/126', 'Y04S50/10', 'Y04S50/14']"
US20250156487A1,Providing a user-tailored answer to a query from a ue operating on a wireless telecommunication network,"The system obtains network information by creating an index of network websites and obtains UE contextual information including: cell tower ID serving the UE, technical capability of the UE, UE performance information, updates to the UE, UE browsing history, and site deployment information. The system obtains from the UE a natural language query and provides the query, the network information, and the UE contextual information to an AI. The system obtains from the AI an answer to the natural language query, where the answer is a summary of a relevant website. Based on the UE contextual information, the system determines whether a user of the UE is technologically savvy or not technologically savvy. If the user is technologically savvy, the system presents the answer and a link to the relevant website; otherwise, the system offers to connect the user to an operator of the network.","['H04M3/5238', 'G06F16/9535', 'G06F16/9538', 'G06N3/0455', 'G06Q30/0267', 'H04M3/5191']"
KR20240004054A,Marketing Phrase Generation Method Using Language Model And Apparatus Therefor,"The present invention relates to a marketing phrase generation method using a language model and a device therefor. The present embodiment provides the marketing phrase generation method that receives keywords corresponding to a target to be advertised and a context (weather, season, etc.) of an advertisement time to generate a plurality of marketing phrases by using a light language model, and uses reinforcement learning based on a language model based on user feedback on the plurality of generated phrases and the device thereof.","['G06Q30/0241', 'G06F16/903', 'G06F40/186', 'G06F40/289', 'G06F40/56', 'G06N20/00', 'G06N3/08']"
US11847424B1,Natural language generation,"Devices and techniques are generally described for data-to-text generation. In various examples, a first machine learned model may receive first data including a structured representation of linguistic data. In various examples, the first machine learned model may generate first output data comprising a first natural language representation of the first data. In at least some examples, a second machine learning model may determine second data indicating that the first natural language representation is a semantically accurate representation of the first data. In some examples, the first output data may be selected for output based at least in part on the second data.","['G06F40/30', 'G06F40/56', 'G06F40/51', 'G06N20/00', 'G06N3/0895', 'G06N3/092']"
US8990127B2,Method and system for ontology-driven querying and programming of sensors,"Described embodiments relate to a method of ontology-driven querying or programming of at least one sensor. The method comprises generating at a query origin a query or command for execution in relation to the at least one sensor, transmitting the query or command to an ontology transformer over a first network, classifying the query or command according to a domain ontology and one or more predetermined capabilities of the at least one sensor, generating a transformed query or program based on the classified query or command using one or more code fragments stored in a memory accessible to the ontology transformer, transmitting the transformed query or program to at least one sensor node in communication with the at least one sensor for execution of the transformed query or program by the at least one sensor node in relation to the at least one sensor, receiving at the ontology transformer from the at least one sensor node at least one result of the query or program, and returning the at least one result.","['G06F17/30734', 'G06F16/367', 'G01D21/00', 'G06F16/9032', 'G06F16/90335', 'G06F17/30967', 'G06F17/30979', 'H04Q9/00']"
US20210350915A1,Universal physician ranking system based on an integrative model of physician expertise,"Systems and methods for measuring physician expertise are disclosed. Furthermore, systems and methods for ranking physicians based on said measure of expertise are disclosed. The systems and methods may comprise (a) capturing and mining large-scale, up-to-date medical and clinical knowledge sources (e.g., biomedical corpora, clinical guidelines, clinical trials, professional physician data); (b) building models of medical conditions that link each condition to relevant concepts (e.g., specialties, medical procedures, drug regimens) and relevant clinical research (e.g., published articles, clinical trials); (c) transforming biomedical concepts (e.g., conditions, procedures, drugs) extracted from text in natural language into terms and codes of biomedical ontologies; (d) enriching mission-critical biomedical ontologies and creating mappings between them; (e) matching physician data against a medical condition model in order to rank physicians according to their relevance to a given condition.","['G06F40/30', 'G06N3/042', 'G06N3/0427', 'G16H40/20', 'G16H50/20', 'G16H50/70', 'G16H70/20', 'G16H70/60', 'G06F40/216', 'G06F40/284', 'G06N3/045', 'G06N3/088', 'G06N5/02']"
US8121350B2,"Apparatus, method and computer program for determining a position on the basis of a camera image from a camera","An apparatus for determining a position on the basis of a camera image from a camera includes a Hough transformer, a positional description establisher and a database comparator. The Hough transformer is formed to identify circular arcs or elliptical arcs in the camera image or in a preprocessed version of the camera image derived therefrom, and to identify a plurality of straight stretches passing in various directions through the camera image or through the preprocessed version. The positional description establisher is formed to obtain a positional description describing the identified circular arcs or elliptical arcs and the identified straight stretches by parameters, on the basis of the identified circular arcs or elliptical arcs and on the identified straight stretches. The database comparator further is formed to compare the positional description with a plurality of comparative positional descriptions and to obtain information on a position as a result of the comparison.","['G06V10/48', 'G06T7/13', 'G06T7/73', 'G06V20/58', 'G06V20/588', 'G06T2207/20061', 'G06T2207/20164', 'G06T2207/30256']"
CN101706773B,Method for realizing fast and automatic modeling of transformer substation IEC 61850 by adopting XML information recombination,"The invention provides a method for realizing fast and automatic modeling of a transformer substation IEC 61850 by adopting XML information recombination, which is applied to IED modeling of space layers in a digital transformer substation and a traditional transformer substation by adopting IEC 61850 communication. The method comprises the following steps of: recombining the IED information by adopting XML, and providing an IED information standardization interface; editing and setting the standardized XML files; loading an embedded minimum model which accords to the IEC61850 standard; appointing an sAddr information format in advance, automatically adding sAddr information and leading out an ICD model file; analyzing the led-out ICD model file by the IED, and automatically extracting the sAddr information, and automatically mapping model leafage nodes to a real-time database according to the sAddr information; and enabling the information of the real-time database to normally communicate with station control layer equipment by the IED through adopting an IEC61850 protocol. The model collocation can be set by engineering personnel according to actual engineering. Therefore, the invention greatly shortens the time for generating the IEC61850 model, improves the efficiency of production and regulation and is convenient for 61850 engineered actualization and generalization.",[]
US9529057B2,Apparatus and method for out-of-step protection using the analysis of trajectories of electrical measurements in state plane,"Various embodiments are described herein for a device and method for predicting an out-of-step or transient instability condition in a power system due to a disturbance, such as a fault, where the power system comprises a power generator. The method comprises determining values for a state plane trajectory for the power generator during the disturbance based on using an internal voltage angle of the power generator or an equivalent power generator and a during fault rotor speed of the power generator or equivalent power generator as state variables, determining time scale values; determining values for a critical state plane trajectory for the power generator after clearing the fault, determining values for a state plane trajectory for the power generator after clearing the fault, determining a critical clearing angle and a critical clearing time, and predicting that the power generator will be stable or unstable by comparing the time to clear the fault with the critical clearing time.","['G01R31/40', 'G01R31/343', 'G06F11/30', 'H02H1/0092', 'G01D21/00', 'G01R31/00', 'G01R31/42', 'G05B9/02', 'G06F17/40', 'G06F19/00', 'G16Z99/00', 'H01H83/20']"
US12361740B2,Domain-specific processing and information management using machine learning and artificial intelligence models,"Systems and techniques are provided for automatically analyzing and processing domain-specific image artifacts and document images. A process can include obtaining a plurality of document images comprising visual representations of structured text. An OCR-free machine learning model can be trained to automatically extract text data values from different types or classes of document image, based on using a corresponding region of interest (ROI) template corresponding to the structure of the document image type for at least initial rounds of annotations and training. The extracted information included in an inference prediction of the trained OCR-free machine learning model can be reviewed and validated or corrected correspondingly before being written to a database for use by one or more downstream analytical tasks.","['G06V30/414', 'G06V30/412', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06V10/22', 'G06V10/82', 'G06V30/1448', 'G06V30/147', 'G06V30/191', 'G06V30/19147', 'G06V30/19167', 'G06V30/19173', 'G06F2218/08', 'G06T2207/20081']"
US9092593B2,Systems and methods for intuitive modeling of complex networks in a digital environment,"A system for modeling a topology of an electrical power system may include a memory device to maintain a component database, a component control engine, and a power system topology modeling engine. A display device can be configured for displaying the topology of the electrical power system. An input device can be operative to select one of a plurality of power system components stored in the component database as a selected component, position the selected component within a framework, and interface the selected component with other selected components within the framework. A processor can be operative to execute instructions within the component control engine to control the position of the selected components within the framework and execute instructions within the power system topology modeling engine to render the topology of the electrical system after the selected components have been positioned.","['G06F17/509', 'G06F30/18', 'G06F2111/02', 'G06F2119/06', 'G06F2217/04', 'G06F2217/78']"
US8788577B2,Method and system for automated analysis and transformation of web pages,"A method and system for modifying web pages, including dynamic web pages, based on automated analysis wherein web pages are transformed based on transformation instructions in nearly real-time, and wherein analysis is performed and transformation instructions based on the analysis are prepared prior to a request for the web page. The system has two primary components, an analyzer which asynchronously and repeatedly analyzes web pages creating and updating transformation instructions relating to the web pages, and a transformer which intercepts traffic to a web server in response to a request for the web page, receives the returned web pages, and transforms them based on stored transformation instructions.","['G06F17/30905', 'G06F40/143', 'G06F16/25', 'G06F16/9577', 'G06F40/151', 'H04L43/04', 'H04L67/02']"
US8825869B2,Method and system for deploying content to wireless devices,"A method and system for deploying content to devices is provided. The method and system can be used to deploy markup content to web browser applications on wireless devices. The wireless devices may included, for example, WAP phones, HDML phones, Palm VII, Palm V, etc. The method and system can also be used in conjunction with wired systems. A translator receives data, in a variety of formats, from the wireless devices. The translator then translates the data into a standardized format. A content provider interface receives the data in the standardized format and provides content data, also in the standardized format. The content data in the standardized format is provided to a transformer. The transformer transforms the content data into the pre-selected format for the particular wireless device. The content data can then be utilized and manipulated by the wireless device.","['G06F40/154', 'H04L67/02', 'H04L67/56', 'H04L67/565', 'H04L67/53']"
US20230050655A1,Dialog agents with two-sided modeling,"A central learning model is deployed as a user model and as an assistant model. Sensitive information utterances from a corpus of previously stored conversation language corresponding to user queries and chat agent responses thereto are used to train the user model to become an updated user model and to train the assistant model to become an updated assistant model, respectively. The user model provides user contexts corresponding to user queries to the assistant model and the assistant model provides assistant contexts corresponding to chat agent responses to the user model. During training, the user model does not provide plain-text queries to the assistant model and the assistant model does not provide plain-text responses to the user model. The updated assistant model may facilitate a federated training process produce an updated central model. An updated central model may be used to provide real-time chat agent responses to live user queries.","['G06N3/08', 'G06F40/35', 'G06F16/3329', 'G06N3/044', 'G06N3/0455', 'G06N3/084', 'G06N3/098']"
US12056592B2,"System, method, and computer program for transformer neural networks","A system and method include one or more processing devices to implement a sequence of transformer neural networks, first and second sequence-to-sequence layers that each comprises a sequence of nodes, and an output layer to provide the first set and second set of score vectors to a downstream application of a natural language processing (NLP) task.","['G06F40/284', 'G06F18/2113', 'G06F18/2155', 'G06F40/194', 'G06F40/216', 'G06F40/30', 'G06N3/045', 'G06N3/08', 'G06T1/20', 'G06F16/35', 'G06N3/044']"
US11681954B2,Parallel decoding using transformer models,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for performing parallel generation of output from an autoregressive sequence to sequence model. In one aspect, a blockwise parallel decoding method takes advantage of the fact that some architectures can score sequences in sublinear time. By generating predictions for multiple time steps at once then backing off to a longest prefix validated by the scoring model, the methods can substantially improve the speed of greedy decoding without compromising performance.","['G06F40/47', 'G06N20/20', 'G06F18/2185', 'G06F18/22', 'G06F18/2413', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N7/00', 'G06V10/764', 'G06V10/82']"
US8782182B2,Generating device-specific configurations,"An approach to generating device-specific configurations is described. In one approach, a method of generating a device-specific configuration for a target device is described. The method involves receiving a configuration parameter, and receiving command syntax information. A state description is generated from the configuration parameter, with reference to a configuration library. Device information is retrieved from the target device, and the device-specific configuration is generated with reference to the command syntax information, the device information, the state description, and a command library.","['H04L41/0853', 'H04L41/0813', 'H04L41/0266']"
EP1760588B1,Event-based coordination of process-oriented composite applications,"A process model (104) specified using, for example, UML activity diagrams can be translated into an event-based model that can be executed on top of a coordination middleware (116). For example, a process model (104) may be encoded as a collection of coordinating objects (126, 128) that interact with each other through a coordination middleware including a shared memory space (124). This approach is suitable for undertaking post-deployment adaptation of process-oriented composite applications. In particular, new control dependencies can be encoded by dropping new (or enabling existing) coordinating objects (126, 128) into the space and/or disabling existing ones.","['G06F8/10', 'G06F8/35', 'G06F9/542', 'G06Q10/06']"
US8200505B2,System and method for creating and rendering DICOM structured clinical reporting via the internet,"Methods for implementing a structured clinical reporting system employing a DICOM Structured Reporting (SR) software database engine that maps clinical report data into a clinical structured reporting data format are disclosed. A workstation is used to create, sign, render, or transmit a structured clinical report. A conversion engine performs image recognition on the structured clinical report, which includes identifying data therein, segmenting the report, using a library to perform value object extraction on the segments, and converting the value objects to a DICOM format. A PACS server is usable to communicate with the conversion engine, workstation, and database. Various software plug-ins are usable, such that different methods of acquiring user input may be implemented.","['G06Q10/00', 'G06F40/143', 'G06F40/154', 'G16H15/00', 'G16H30/20', 'G16H30/40', 'G06F16/245']"
EP2344980B1,"Device, method and computer program for detecting a gesture in an image, and said device, method and computer program for controlling a device","An apparatus for recognizing gestures in a picture includes a Hough transformer configured to identify elements in the picture or in a pre-processed version of the picture as identified gesture elements and to obtain information about the identified gesture elements. The apparatus further includes a gesture description creator configured to obtain a gesture description while using the information about the identified gesture elements. Moreover, the apparatus includes a gesture classifier configured to compare the gesture description to a plurality of comparative gesture descriptions having gesture codes associated with them. The gesture classifier is configured to provide, as the result of the comparison, a gesture code of a recognized gesture.","['G06V40/107', 'G06V10/48']"
CN112651448A,Multi-modal emotion analysis method for social platform expression package,"The invention provides a multi-modal emotion analysis method for an expression package of a social platform, which comprises the following steps of: s1: crawling the expression bag picture from the social platform by using a crawler tool, performing emotion marking on the expression bag picture, and performing pretreatment; s2: semantic information of the emotion bag pictures crawled in the step S1 is obtained, and text information feature vector representation corresponding to each emotion bag picture is obtained; s3: acquiring the visual features of the emotion bag pictures crawled in the step S1 to obtain the visual feature vector representation corresponding to each emotion bag picture; s4: the text information feature vector representation and the visual feature vector representation are subjected to multi-mode fusion to obtain multi-mode fusion feature vector representation; s5: the multi-mode fusion feature vector represents emotion recognition results obtained through a classifier, and the emotion recognition result with the highest confidence coefficient is selected as predicted emotion.","['G06F18/251', 'G06F16/951', 'G06F18/253', 'G06F18/254', 'G06N3/045', 'G06N3/08', 'G06V20/62', 'Y02D10/00']"
CN110427627B,Task processing method and device based on semantic representation model,"The application discloses a task processing method and device based on a semantic representation model, and relates to the field of NLP. The specific implementation scheme is as follows: by acquiring a task processing request containing a task to be processed. And acquiring the data to be represented related to the task to be processed according to the task processing request. The target semantic representation model is a general semantic representation model, the learned task is a self-defined pre-training task supporting the hierarchy of vocabulary, grammar, semantics and the like based on using massive non-labeled training corpus, and the pre-configured semantic representation model is obtained by adjusting and training the target semantic representation model according to the application data of the same type as the task to be processed, so that the pre-configured semantic representation model is adopted to carry out semantic analysis on the data to be represented, semantic representation of the data to be represented is obtained, the method is applicable to various specific NLP tasks, and the applicability and the usability of the model are improved.",['Y02D10/00']
US8117459B2,Personal identification information schemas,"A digital identity system includes a principal including an identity selector programmed to receive a security policy from a relying party, review a plurality of digital identities associated with the principal, and request one or more claims related to an identity of the principal from an identity provider. The principal is further programmed to receive one or more security tokens including the claims from the identity provider, and to forward the security tokens to the relying party.","['G06F21/33', 'G06F2221/2115']"
CN117058750A,Method for segmenting Transformer sign language based on human skeleton input,"A method for segmenting a transducer sign language based on human skeleton input adopts a SkeF carrier model based on deep learning as an automatic segmentation tool for identifying and positioning each gesture in continuous sign language, and can replace part of staff with annotation technology to mark each gesture in a continuous sign language data set; the model adopts ST-GCN as a feature extractor, models an input skeleton coordinate sequence as space-time features with space layers among human joints, and utilizes a layering mode of an Encoder in an ASFormer to pay attention to local information in the space-time features firstly, and then gradually expands a receptive field to acquire global information. The method is used for establishing and perfecting the sign language corpus and further enriching the content of the sign language corpus.","['G06V40/28', 'G06N3/0464', 'G06N3/0985', 'G06V10/26', 'G06V10/7715', 'G06V10/82', 'G06V20/70']"
US8104074B2,Identity providers in digital identity system,"A digital identity system includes a principal including an identity selector programmed to receive a security policy from a relying party, review a plurality of digital identities associated with the principal, and request one or more claims related to an identity of the principal from an identity provider. The principal is further programmed to receive one or more security tokens including the claims from the identity provider, and to forward the security tokens to the relying party.","['G06F21/33', 'G06F2221/2115']"
US7706992B2,"System and method for signal decomposition, analysis and reconstruction","The present invention provides a system and method for representing quasi-periodic (â€œqpâ€) waveforms comprising, representing a plurality of limited decompositions of the qp waveform, wherein each decomposition includes a first and second amplitude value and at least one time value. In some embodiments, each of the decompositions is phase adjusted such that the arithmetic sum of the plurality of limited decompositions reconstructs the qp waveform. These decompositions are stored into a data structure having a plurality of attributes. Optionally, these attributes are used to reconstruct the qp waveform, or patterns or features of the qp wave can be determined by using various pattern-recognition techniques. Some embodiments provide a system that uses software, embedded hardware or firmware to carry out the above-described method. Some embodiments use a computer-readable medium to store the data structure and/or instructions to execute the method.","['H03H17/0201', 'G06F18/00', 'A61B5/0006', 'A61B5/339', 'A61B5/347', 'G06F2218/08']"
CN102725933B,Dynamic distributed grid control system,"Disclose the system of dynamic management and control distributed energy in transmission/distrbution network.Use the multiple regions in Region control module autonomous management transmission/distrbution network.The management of each Region control module supervision transmission/distrbution network and control, and be associated with multiple Local Control Module further, Local Control Module connects distributed energy in region.Enterprise control module supervision and analyze electrical production and power consumption, once determine that the power consumption in region is not mated with power capacity, so this enterprise's control module dynamically assignment profile formula energy again in whole electrical network, keeping system balance.Local Control Module, Region control module and enterprise control module supervision and analyze the poower flow of key node in network, when system parameters has the risk hindering safety, stable or operational threshold, takes compensation behavior.","['H02J13/00017', 'G05B13/02', 'G05B19/0421', 'G05F1/66', 'H02J13/00', 'H02J13/00002', 'H02J13/00034', 'H02J3/00', 'H02J3/381', 'G06Q30/0202', 'H02J13/00001', 'H02J2300/10', 'H02J3/06', 'H02J3/1828', 'Y02B70/30', 'Y02E40/30', 'Y02E40/70', 'Y02E60/00', 'Y02P80/15', 'Y04S10/12', 'Y04S10/22', 'Y04S10/30', 'Y04S10/40', 'Y04S20/20', 'Y04S40/124', 'Y04S50/10', 'Y04S50/14']"
US11373120B2,Attention mechanism for natural language processing,"A method may include applying a machine learning model, such as a bidirectional encoder representations from transformers model, trained to generate a representation of a word sequence including a reference word, a first candidate noun, and a second candidate noun. The representation may include a first attention map and a second attention map. The first attention map may include attention values indicative of a strength of various linguistic relationships between the reference word and the first candidate noun. The second attention map may include attention values indicative of a strength of various linguistic relationships between the reference word and the second candidate noun. A natural language processing task, such as determining whether the reference word refers to the first candidate noun or the second candidate noun, may be performed based on the first attention map and the second attention map. Related methods and articles of manufacture are also disclosed.","['G06F40/284', 'G06F17/16', 'G06F18/2155', 'G06F40/205', 'G06F40/30', 'G06K9/6259', 'G06N20/00', 'G06N3/045', 'G06N3/088']"
US8229722B2,"Electrical power system modeling, design, analysis, and reporting via a client-server application framework","A system for intelligent web-based monitoring and management of an electrical system is provided. The system is configured to acquire real-time data output from the electrical system, and to transmit a user interface to a client terminal which is configured to display the user interface. In an embodiment, the system is configured to store a virtual system model of the electrical system. The system is configured to generate a predicted data output for the electrical system utilizing the virtual system model of the electrical system, monitor the real-time data output and the predicted data output of the electrical system, and initiate a calibration and synchronization operation to update the virtual system model when a difference between the real-time data output and the predicted data output exceeds a threshold.",['G05B17/02']
US12314673B2,Generative language model for few-shot aspect-based sentiment analysis,"Sentiment analysis is a task in natural language processing. The embodiments are directed to using a generative language model to extract an aspect term, aspect category and their corresponding polarities. The generative language model may be trained as a single, joint, and multi-task model. The single-task generative language model determines a term polarity from the aspect term in the sentence or a category polarity from an aspect category in the sentence. The joint-task generative language model determines both the aspect term and the term polarity or the aspect category and the category polarity. The multi-task generative language model determines the aspect term, term polarity, aspect category and category polarity of the sentence.","['G06F40/30', 'G06F40/284', 'G06N3/04', 'G06N3/08', 'G06N3/088', 'G06N3/096', 'G06F40/216']"
US20210042606A1,Deep neural network with equilibrium solver,"Some embodiments are directed to a neural network comprising an iterative function (z[i+1]=Æ’(z[i], Î¸, c(Î»)). Such an iterative function is known in the field of machine learning to be representable by a stack of layers which have mutually shared weights. According to some embodiments the stack of layers may during training be replaced by the use of a numerical root-finding algorithm to find an equilibrium of the iterative function in which a further execution of the iterative function would not substantially further change the output of the iterative function. Effectively, the stack of layers may be replaced by a numerical equilibrium solver. The use of the numerical root-finding algorithm is demonstrated to greatly reduce the memory footprint during training while achieving similar accuracy as state-of-the-art prior art models.","['G06N3/0445', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G06N5/041']"
US8155943B2,"Systems and methods for automatically converting CAD drawing files into intelligent objects with database connectivity for the design, analysis, and simulation of electrical power systems","A computer system for converting a computer aided design drawing file of an electrical power system into one or more component objects for power analytic analysis and simulation, is disclosed. The computer system can include a processor, a memory, a display device, and an input device. The memory device can be coupled to the processor and configured to maintain a component classification database, an import engine, computer aided design drawing parser, and a symbol classification engine. The display device can be coupled to the processor and configured for displaying the computer aided design drawing file of the electrical power system. The processor can be operative to execute instructions within the import engine to control the import of the computer aided design drawing file to the computer, execute instructions within the computer aided design drawing parser to control the parsing of the computer aided design drawing file into at least one component object containing a component symbol and execute instructions within the symbol classification engine to control the assigning of a component classification to the component symbol.","['G06F30/20', 'G06F2111/02', 'G06F2119/06']"
US8131401B2,Real-time stability indexing for intelligent energy monitoring and management of electrical power network system,"A system and method for intelligent monitoring and management of an electrical system is disclosed. The system includes a data acquisition component, a power analytics server and a client terminal. The data acquisition component acquires real-time data output from the electrical system. The power analytics server is comprised of a real-time electrical system security index engine that calculates real-time system security index values from stability indices data generated from a virtual system model of the electrical system. The client terminal displays the system security index values to assess the security and stability of the electrical system.",['G05B23/0235']
CN111460800B,"Event generation method, device, terminal equipment and storage medium","The embodiment of the application is suitable for the technical field of information and provides an event generation method, an event generation device, terminal equipment and a storage medium, wherein the method comprises the following steps: acquiring a text title to be processed, wherein the text title comprises a plurality of title characters; converting each title character into an input vector; determining a target input vector corresponding to the text title according to the input vector of each title character obtained after conversion; encoding and decoding the target input vector, and outputting candidate characters of each character bit in an event to be generated and probability values of the candidate characters; and generating an event according to the candidate character of each character bit and the probability value of the candidate character. The event generated by the method has strong generalization, can completely represent text content, has better grammar consistency and has higher readability.","['G06N3/044', 'G06N3/045', 'Y02D10/00']"
CN110059320A,"Entity relation extraction method, apparatus, computer equipment and storage medium","This application involves a kind of entity relation extraction method, apparatus, computer equipment and storage mediums.The described method includes: obtaining text, the entity in the text is identifiedï¼›The entity includes first instance and second instanceï¼›The entity is marked in the textï¼›The text after label is encoded using pre-training language model, obtains first instance coding vector and second instance coding vectorï¼›Entity is generated to label vector using the first instance coding vector and the second instance coding vectorï¼›Classify to the entity to label vector, obtains the relationship classification between the first instance and second instance.Effective entity relation extraction directly can be carried out using pre-training language model using this method.","['G06F16/355', 'G06F16/367', 'G06F40/295']"
US8775934B2,"Systems and methods for creation of a schematic user interface for monitoring and predicting the real-time health, reliability and performance of an electrical power system","A system for automatically generating a schematic user interface of an electrical system is disclosed. The system includes a data acquisition component, a power analytics server and a client terminal. The data acquisition component acquires real-time data output from the electrical system. The power analytics server is comprised of a virtual system modeling engine, an analytics engine, a machine learning engine and a schematic user interface creator engine. The virtual system modeling engine generates predicted data output for the electrical system. The analytics engine monitors real-time data output and predicted data output of the electrical system. The machine learning engine stores and processes patterns observed from the real-time data output and the predicted data output to forecast an aspect of the electrical system. The schematic user interface creator engine is configured to create a schematic user interface that is representative of the virtual system model and link the schematic user interface to the data acquisition component.","['G06F17/5009', 'G06F30/20', 'G06F8/38']"
US12387720B2,Neural sentence generator for virtual assistants,"Methods and systems for automatically generating sample phrases or sentences that a user can say to invoke a set of defined actions performed by a virtual assistant are disclosed. By enabling finetuned general-purpose natural language models, the system can generate potential and accurate utterance sentences based on extracted keywords or the input utterance sentence. Furthermore, domain-specific datasets can be used to train the pre-trained, general-purpose natural language models via unsupervised learning. These generated sentences can improve the efficiency of configuring a virtual assistant. The system can further optimize the effectiveness of a virtual assistant in understanding the user, which can enhance the user experience of communicating with it.","['G10L15/1822', 'G06F40/284', 'G06F40/30', 'G06F40/35', 'G10L15/02', 'G10L15/063', 'G10L15/22', 'G06F40/56']"
US8086597B2,Between matching,"A query of at least one mark-up language document has a path expression comprising a conjunction, a first filter and a second filter. The first filter has a first probe. The second filter has a second probe. The first and second filters form a between filter having start and stop values specified by the first and second probes. A plan to process the query is generated based on, at least in part, a range defined by the start and stop values. An index of mark-up language documents is defined by another path expression; the index comprises values of mark-up language documents that satisfy the other path expression; the values are key values of the index. The plan is to perform a single scan of the key values from the start value to the stop value to identify at least one key value that satisfies the between filter.",['G06F16/8373']
CN117549310A,"General system of intelligent robot with body, construction method and use method","The invention discloses a general system, a construction method and a use method of an intelligent robot with a body, wherein the system comprises an information self-organizing core neural network and body equipment, wherein the information self-organizing core neural network is configured on the robot and comprises a large language model, a memory model, a multi-mode perception model and a motion control model, and the body equipment comprises a driving system, a mechanical system, a sensor system and the like. The invention integrates models with different characteristics, can fully sense information such as user data, objects, barriers and the like in the environment, formulates an execution strategy for controlling the movement and operation of the robot, trains, learns and adjusts in real time based on the simulation environment and the real environment sensing feedback information, improves the man-machine interaction experience and the environment self-adaption capability of the robot in the real environment, can be suitable for autonomous mobile robots and operation robots of different types, and has strong universality.","['B25J9/163', 'B25J9/161', 'B25J9/1664', 'Y02P90/02']"
US8055606B2,Method and system for self-calibrating project estimation models for packaged software applications,"An estimation system for deriving multi-dimensional project plans for implementing packaged software applications with self-calibration and refinement of project estimation models, the system includes: a view layer configured to act as a user interface for user inputs and system outputs; a model and control layer configured to implement rules based on a series of estimation and implementation models, and to perform self-calibration and refinement of project estimation models for multi-dimensional project plans; an estimation knowledge base layer configured to hold and derive the series of estimation and implementation models; and wherein the system for self-calibration and refinement of project estimation models for multi-dimensional project plans for implementing packaged software applications is carried out over networks comprising: the Internet, intranets, local area networks (LAN), and wireless local area networks (WLAN).","['G06Q10/06', 'G06Q10/0631', 'G06Q10/06311', 'G06Q10/06312', 'G06Q10/06313', 'G06Q10/06314', 'G06Q10/1093', 'G06Q10/1097']"
RU2583703C2,Malicious attack detection and analysis,"FIELD: physics, computer engineering.","['G06F21/55', 'H04L63/1433', 'H04L63/20', 'H02J3/00', 'Y02E40/70', 'Y02E60/00', 'Y04S10/00', 'Y04S10/22', 'Y04S40/20']"
US7391126B2,Systems and methods for an integrated electrical sub-system powered by wind energy,"Various embodiments relate to systems and methods related to an integrated electrically-powered sub-system and wind power system including a wind power source, an electrically-powered sub-system coupled to and at least partially powered by the wind power source, the electrically-powered sub-system being coupled to the wind power source through power converters, and a supervisory controller coupled to the wind power source and the electrically-powered sub-system to monitor and manage the integrated electrically-powered sub-system and wind power system.","['F03D7/00', 'F03D9/11', 'F03D9/19', 'F03D9/255', 'F03D9/257', 'F03D9/28', 'H02J3/381', 'F05B2220/62', 'F05B2260/80', 'H02J2300/10', 'H02J2300/28', 'Y02A20/141', 'Y02E10/72', 'Y02E10/76', 'Y02E70/30']"
US20210375269A1,Systems and methods for domain adaptation in dialog act tagging,"Embodiments described herein utilize pre-trained masked language models as the backbone for dialogue act tagging and provide cross-domain generalization of the resulting dialogue acting taggers. For example, a pre-trained MASK token of BERT model may be used as a controllable mechanism for augmenting text input, e.g., generating tags for an input of unlabeled dialogue history. The pre-trained MASK model can be trained with semi-supervised learning, e.g., using multiple objectives from supervised tagging loss, masked tagging loss, masked language model loss, and/or a disagreement loss.","['G10L15/183', 'G06F40/35', 'G06F17/18', 'G06N20/00', 'G06N3/045', 'G06N3/084', 'G10L15/063']"
US8176470B2,Collaborative derivation of an interface and partial implementation of programming code,"A method, system and computer program product provide an implementation of software. A control flow of a software component is constructed based on a specification model. In various embodiments, the specification model comprises at least one input and at least one requirement referencing the at least one input. At least a partial implementation of the software component is generated based on the control flow and the at least one input and the at least one requirement of the specification model. In some embodiments, the specification model further comprises at least one output, and the at least a partial implementation of the software component is also based on the at least one output.",['G06F8/10']
CN114722838A,Dialogue emotion recognition method based on common sense perception and hierarchical multi-task learning,"The invention discloses a conversation emotion recognition method based on common sense perception and hierarchical multitask learning, which is characterized in that word-level and utterance-level hierarchical codes are constructed by utilizing a pre-training language model RoBERTA and a Transformer architecture based on an attention mechanism, and each input sentence in a conversation is coded into embedded expression in a vector form; and (3) constructing a hierarchical relationship among the model tasks by adopting hierarchical nodes under a hierarchical structure multi-task learning framework so as to simultaneously complete three tasks related to conversational understanding: and obtaining a conversation behavior category prediction label corresponding to the statement and an emotion prediction label of the current statement, and carrying out emotion inference on the two prediction results to obtain an emotion prediction label of the next statement. Compared with the prior art, the method can obviously improve the emotion recognition performance, and has good conversation behavior classification capability and certain emotion inference capability.","['G06F40/35', 'G06F16/3329', 'G06F16/353', 'G06F18/2431', 'G06N3/044', 'G06N3/08', 'G06N5/04']"
CN116075891A,Speech analysis for monitoring or diagnosing health conditions,"The present invention relates to a computer-implemented method of training a machine learning model for performing a voice analysis for monitoring or diagnosing a health condition. The method uses training data comprising audio speech data and comprises obtaining one or more linguistic representations, each linguistic representation encoding a sub-word, word or sequence of words of the audio speech data. Obtaining one or more audio representations, each audio representation encoding audio content of a segment of audio speech data; combining the language representation and the audio representation into an input sequence, comprising: a linguistic representation of a sequence of one or more words or subwords of audio speech data; and an audio representation of segments of audio speech data, wherein the segments together comprise a sequence of one or more words or subwords. The method also includes training a machine learning model using unsupervised learning to map the input sequence to a target output to learn a combined audio language representation of the audio speech data for use in speech analysis for monitoring or diagnosing health conditions.","['G10L25/66', 'A61B5/4803', 'A61B5/7264', 'A61B5/7267', 'G10L25/30']"
CN113160854A,"Voice interaction system, related method, device and equipment","The application discloses a voice interaction system, a related method, a device and equipment. The system collects user voice data through the intelligent sound box; if the user voice silence duration is greater than a first duration threshold, sending a sentence end detection request aiming at the voice data to a server; the server determines a sub-network through a first acoustic feature included in the sentence end detection model, and determines acoustic feature information of subsequent voice according to the acoustic feature information of the voice data; determining a sub-network through semantic features included in the sentence end detection model, and determining semantic feature information of subsequent voice according to a text sequence of voice data; determining whether the voice data comprises sentence end information or not according to the acoustic characteristic information and the semantic characteristic information of the subsequent voice through a sentence end prediction sub-network included in the sentence end detection model; and if sentence tail information is detected, the intelligent sound box closes the microphone. By adopting the processing mode, the accuracy of the intelligent sound box at the microphone closing time can be effectively improved.","['G10L25/87', 'G10L15/02', 'G10L15/063', 'G10L15/16', 'G10L15/1822', 'G10L15/22', 'G10L15/30', 'G10L21/0208', 'G10L25/30', 'G10L2015/223']"
US7971180B2,Method and system for evaluating multi-dimensional project plans for implementing packaged software applications,"A method for scoring and ranking multi-dimensional project plans for implementing packaged software applications, the method includes: determining one or more dimensions for a project plan; assigning one or more attributes to each of the one or more dimensions; assigning one or more utility functions to each of the one or more attributes; assigning one or more weights to each of the one or more attributes; assigning one or more ordering factors to one or more combinations and permutations of the dimensions; calculating a series of scores for the one or more combinations and permutations of the dimensions; ranking the one or more combinations and permutations of the dimensions based on the calculated series of scores; and wherein each of the one or more combinations and permutations of the dimensions represents a different individual multi-dimensional project plan.","['G06Q10/06', 'G06F8/20', 'G06Q10/06315']"
US8006223B2,Method and system for estimating project plans for packaged software applications,"A system for estimating and generating project plans for implementing packaged software applications, the system includes: a view layer configured to act as a user interface for user inputs and system outputs; a model and control layer configured to implement rules based on a series of estimation and implementation models, and to perform calculations of project plan details and project plan schedules; an estimation knowledge base layer configured to hold and derive the series of estimation and implementation models; and wherein the system for estimating and generating multi-dimensional project plans for implementing packaged software applications is carried out over networks comprising: the Internet, intranets, local area networks (LAN), and wireless local area networks (WLAN).","['G06F8/20', 'G06Q10/06', 'G06Q10/06312', 'G06Q10/06313', 'G06Q10/0635', 'G06Q10/06375', 'G06Q30/0201']"
CN111695355A,"Address text recognition method, device, medium and electronic equipment","The disclosure relates to the technical field of computers, and discloses an address text recognition method, an address text recognition device, an address text recognition medium and electronic equipment. The method comprises the following steps: acquiring an address text to be identified; performing word segmentation on the address text to be recognized to obtain a character string sequence comprising at least one character string; detecting at least one characteristic of the character string sequence according to a pre-trained address language model; and when at least one characteristic of the character string sequence has an error, correcting the at least one characteristic with the error through the address language model so as to identify correct address text. Under the method, the address semantic information contained in the address text can be fully excavated through the pre-trained address language model, the text address can be corrected and complemented better, and the accuracy of identifying the address text is improved.","['G06F40/295', 'G06N3/045', 'G06N3/08']"
WO2025024326A2,Generative artificial intelligence (ai) for digital workflows,"An artificial intelligence (Al) assisted generative digital task fulfillment process within digital model platforms is provided. Disclosed are methods and systems for carrying out digital tasks through generative Al, including tasks related to the streamlined design, validation, verification, certification, assembly, operations, and maintenance processes of complex systems. Hie method includes receiving access to a context Al model trained on Internet-scale data, receiving a user prompt indicating the digital task, and generating contextual data based on the user prompt using the context Al model, where the contextual data identifies a syntax Al model. The method includes training the syntax Al model to generate a template script having a placeholder variable for a parameter related to the digital task. The method also includes using a parameter substitution process to generate the orchestration script by substituting the variable with a parameter value.","['G06N20/00', 'G06N3/0475']"
CN112151030A,Multi-mode-based complex scene voice recognition method and device,"The invention discloses a complex scene voice recognition method based on multiple modes, which comprises the following steps: if the collected lip images of the user are monitored to change, audio signals, lip image signals and face electromyographic signals corresponding to voice input are collected synchronously, multi-source data characteristics of the signals in space and time domains are determined, a voice recognition model is used for coding and modeling the multi-source data characteristics, common information of different modal expression contents is obtained, multi-modal voice information is obtained, and a language model is used for synthesizing a text. The invention also discloses a multi-mode-based complex scene voice recognition device, which comprises a data acquisition module, a feature extraction module, a coding and decoding module, a text synthesis module and an interaction module. The invention realizes efficient, accurate and robust voice recognition under complex scene environments with damaged vocal cords, high noise, high sealing, high privacy requirements and the like, and provides a more reliable voice interaction technology and system for complex human-computer interaction scenes.","['G10L15/22', 'G10L15/06', 'G10L15/063', 'G10L15/16', 'G10L15/25', 'G10L25/24', 'G10L25/30', 'G10L25/45']"
US20240126997A1,Conversational Interface for Content Creation and Editing using Large Language Models,"Example embodiments of the present disclosure provide for an example method that includes obtaining via a conversational campaign assistant interface, by a custom language model, natural language input. The method includes generating, by the custom language model, an output comprising a predicted user intent. The method includes determining actions to perform and determining a natural language response. The method includes transmitting, to an action component, the action data structure comprising executable instructions that cause the action component to automatically perform operations associated with completing the action. The method includes transmitting to the conversation campaign assistant interface, the response data structure comprising the natural language response to be provided for display to a user via the conversational campaign assistant interface. The method includes obtaining user input indicative of a validation of the action data structure or the response data structure and updating the custom language model based on the user input.","['G06Q30/0276', 'G06F40/166', 'G06F40/205', 'G06F40/35', 'G06F40/40', 'G06N20/00', 'G06N3/045', 'G06N3/084', 'G06N3/096', 'G06Q30/0242', 'G06Q30/0275', 'G06Q30/08']"
US8997182B2,"Legacy device registering method, data transferring method and legacy device authenticating method","A method of registering a legacy device, a method of transferring data, and a method of authenticating a legacy device are provided. The method of registering a legacy device by using a virtual client, which allows the legacy device to access a domain, includes: receiving unique information on the legacy device from the legacy device which requests the domain to register the legacy device; searching a registrable legacy device list including the unique information on the legacy device which can be registered in the domain for the unique information on the legacy device; and requesting a domain manager, which manages the domain, to register the legacy device, when the unique information on the legacy device is included in the registrable legacy device list, and not allowing the legacy device to be registered in the domain when the unique information on the legacy device is not included in the registrable legacy device list.","['H04L29/08072', 'H04L63/0428', 'G06F21/10', 'G06F21/1012', 'H04L63/0823', 'H04L63/10', 'H04L63/166', 'H04L69/329', 'G06F2221/0706', 'H04L2463/101', 'Y10S707/99931', 'Y10S707/99932', 'Y10S707/99933', 'Y10S707/99934', 'Y10S707/99935', 'Y10S707/99936', 'Y10S707/99937', 'Y10S707/99938', 'Y10S707/99939', 'Y10S707/9994', 'Y10S707/99941']"
US8587238B2,System for operating DC motors and power converters,"A system is disclosed for driving a DC motor (15) under conditions of a controlled average current. An inductive element may be arranged for connection in series with the DC motor. A switch (14) is preferably coupled to the inductive element for connecting and disconnecting a terminal of the inductive element from the voltage source. A diode may be arranged for connection in parallel with a combination of the inductive element and the DC motor arranged in series, with appropriate polarity so that current circulating through the inductive element circulates through the diode when the switch disconnects the terminal from the voltage source. A capacitor is arranged for connection in parallel with the motor, for limiting a resulting voltage over the motor or for storing charge depending on the embodiment of the invention. A device for measuring a current through the motor is provided, and a device (13) for controlling operation of the switch dependent upon the measured current in the motor is also provided. An airflow apparatus is also disclosed.","['H02P7/29', 'H02M3/156', 'H02M3/157', 'H02M3/1588', 'H02M3/33507', 'H02M3/3372', 'H02P6/085', 'H02M1/0012', 'H02M1/009', 'H02M3/1555', 'H02P2209/07', 'Y02B70/10']"
US9880982B2,System and method for rendering presentation pages based on locality,A system renders presentation pages such as in a wireless communications system. A server includes an extensible stylesheet transformation (XSLT) module. A storage medium contains at least one of images and text. The XSLT module is operative for calling an XSL extension and rendering the at least one of images and text into a presentation page based on a language requirement at a foreign locale.,"['G06F17/2247', 'G06Q30/02', 'G06F17/227', 'G06F40/143', 'G06F40/154', 'G06F9/4448', 'G06F9/454']"
CN101944204B,Geographic information system applied to power system,"The embodiment of the invention provides a geographic information system applied to a power system and a method for improving display efficiency thereof. The system comprises a data unit, an interface engine unit, a power grid data model unit, a system component unit, a map layer management component and an application component unit, wherein the data unit stores two-dimensional geospatial data and attribute information of a power grid equipment object; the interface engine unit provides an interface display control and a message mechanism and is interacted with a user; the power grid data model unit is used for accessing the data unit to establish and maintain a power grid equipment model and performing network analysis on the power grid equipment model; the system component unit is usedfor calling services of the interface engine unit and the power grid data model unit and implementing display of a map and operation of the map, wherein the services comprise map display and component operation; the map layer management component is used for providing a management tool to support the display of the map; and the application component unit comprises a power grid modeling component and is used for calling the service of the power grid data model unit through the system component unit to perform modeling on power grid equipment. The system realizes function customization and flexible expansion.",[]
US8073857B2,Semantics-based data transformation over a wire in mashups,A method of sharing data between wired properties associated with disparate data types includes receiving a first specification for dividing data items having a first data type published by an output property of a first service task into a first set of sub-units and a second specification for dividing data items having a second data type consumed an input property of a second service task into a second set of sub-units; receiving transformation logic for transforming data items divided into the first set of sub-units to data items divided into the second set of sub-units; dividing a first data item published by the output property into the first set of sub-units; applying the transformation logic to transform the divided first item to a second data item divided into the second set of sub-units; and joining the divided second data item. The service tasks are registered with an aggregation application that includes a wire between the properties.,['G06F8/10']
CN117273150A,Visual large language model method based on few sample learning,"The invention discloses a visual large language model method based on less sample learning, which aims at identifying, predicting and generating text based on context correlation by a Large Language Model (LLM), creates value for different business units and functional departments in a mode of combining language, sound and images in financial institutions such as bank securities insurance, and trains on a large-scale multi-modal network corpus containing any staggered text and image, wherein the key points of the invention are as follows: (i) bridge powerful pre-trained pure vision and pure language models, (ii) process arbitrary interlaced sequences of vision and text data, and (iii) seamlessly capture images or videos as input, analyze customer feedback opinion creation or improve customer chat robots based on customer profiles, historical data, and important topics summarized and extracted from online text and images, provide customers with queries of all business types, and enable automatic interactive facilitation of document processing, recognition, and summarization.","['G06N5/041', 'G06N3/0442', 'G06N3/0455', 'G06Q30/015', 'G06V10/82']"
CN102812614B,Generating equipment,"A kind of equipment comprises: can by engine drive for generation the one AC electric current alternating current generator, with alternative electric generation electrical communication for produces DC electric current rectifier, and rectifier telecommunication for generation the 2nd AC electric current current transformer and the energy storage device of alternating current generator, rectifier and/or current transformer can be electrically coupled to, wherein the 2nd AC electric current has acceptable frequency and/or voltage, and current transformer response the 2nd AC signal of telecommunication and one or more electric loading telecommunication.","['H02J7/1415', 'B60K6/46', 'B60L1/02', 'B60L50/10', 'B60L50/61', 'B63H21/17', 'H02J4/00', 'H02P9/307', 'H02P9/48', 'B60K5/08', 'B60L2200/26', 'B60L2200/32', 'B63H21/22', 'B63H23/24', 'H02J2310/42', 'Y02T10/62', 'Y02T10/64', 'Y02T10/70', 'Y02T10/7072', 'Y02T70/50', 'Y02T90/16', 'Y02T90/40']"
US8352397B2,Dependency graph in data-driven model,"The inference of a dependency graph that represents a graph of solves that leads from input model parameter(s) to output model parameters using analytics. In one embodiment, the dependency graph is part of visually driven analytics in which the output model parameter(s) are used to formulate data-drive scenes. As the identity of the input and/or output model parameter(s) change, or as the analytics themselves change, the dependency graph may also change. This might trigger a resolve of the analytics. In one embodiment, the intermediate parameters involved in the dependency graph may be viewed and evaluated by the user.","['G06N20/00', 'G06N5/04']"
CN119027960A,A multimodal large language model for fine-grained visual perception of remote sensing images,"The invention provides a multi-mode large language model for realizing fine-grained visual perception of a remote sensing image, which comprises the following steps: the shared visual coding module comprises two functional complementary visual encoders, and the two functional complementary visual encoders respectively receive images with different resolutions after downsampling for coding and are connected according to channel dimensions to obtain an integrated multi-scale visual characteristic diagram. Meanwhile, respectively inputting the visual prompts into two visual encoders with complementary functions to obtain coded visual prompts; the modality alignment mapping layer is used for converting the multi-scale feature map and the coded visual prompts into language semantic space to obtain mapped visual and visual prompts; the text word segmentation device module is used for converting a text instruction into a vector for embedding; and the large language model decoder is used for receiving the mapping image token, the visual prompt token and the text instruction embedding and generating a model response sequence. The invention provides a simple and feasible method for researching and applying the fine-granularity multi-mode large language model in the remote sensing field.","['G06V30/19147', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06V10/82']"
US12050867B2,"Language model based writing aid method, device and system","The present disclosure provides a language model based writing aid method, apparatus and system. The method includes: a server acquiring original text, where the original text may be writing text already generated and/or user input text; the server inputting the original text into a language model to generate a preset number of pieces of writing text, where the writing text and the original text have a correlation; the server sending the preset number of pieces of writing text to a frontend interface. The method of the present disclosure enables a computer to aide a user in text creating so that intelligence for writing aid is improved.","['G06F40/56', 'G06F40/253', 'G06F40/166', 'G06F40/216', 'G06F40/289', 'G06F40/30', 'G06F40/42', 'G06F40/44']"
CN118673394A,"Large language model sparsification method and device, electronic equipment and storage medium","The embodiment of the invention provides a large language model sparsification method, a device, electronic equipment and a storage medium, and relates to the technical field of artificial intelligence. According to the method, different predictors are configured for different network layers, the predictors predict based on different input features, the difference of different input features and different network layer feature distribution is fully considered, and proper sparsity can be flexibly selected according to the input features and the network features. For the input with high sparsity degree, the processing efficiency of the large language model is improved, and for the input with low sparsity degree, the accuracy of the large language model is ensured, so that the balance effect between the processing efficiency and the accuracy of the large language model is achieved.","['G06F18/2411', 'G06F18/2136', 'G06F18/214', 'G06F18/29']"
WO2024186177A1,"Method for pre-training vision-language transformer, and artificial intelligence system comprising vision-language transformer pre-trained through same method","The present invention relates to a method and a system for promptly training a simplified vision-language transformer, in which large uncurated datasets are augmented through image enlargement and/or masking, etc. and vision-language transformers are pre-trained by reflecting, through a knowledge distillation framework, misaligned information between an augmented image and text upon the augmentation, thereby reducing data processing overhead.","['G06V10/778', 'G06N3/0455', 'G06N3/0985', 'G06V10/46', 'G06V10/469', 'G06V10/80', 'G06V10/803', 'G06V10/96', 'G06V30/182', 'G06V30/1823']"
CN117332247B,Big data transaction and quality assessment method and system using big language model as medium,"The invention discloses a big data transaction and quality assessment method and a system using a big language model as a medium, which relate to the technical field of data transaction and assessment and comprise the following steps: each company establishes an instruction fine tuning data set by using private data, and independently trains a large language model according to the instruction fine tuning data set; integrating the large language model trained by each company into a complete industry large language model based on a mixed expert system; the direct transaction of the data is replaced by the industry large language model, and the quality evaluation is carried out on the effect of the industry large language model; compared with the federal learning scheme, the method does not need frequent model parameter updating communication among the participants, so that the performance is higher, and the safety is stronger; the invention generates the answer by using the large language model, so that the evaluation method has higher universality, is suitable for diversified large data scenes, reduces the threshold of non-professional persons for evaluating the data quality, and enables more people to accurately evaluate the data quality.","['G06F18/217', 'G06F18/214', 'G06N3/0442', 'G06N3/0455', 'G06N3/0499', 'G06N3/082', 'G06N3/084']"
CN117474010A,Construction method of power transmission and transformation equipment defect corpus oriented to power grid language model,"The invention provides a power transmission and transformation equipment defect corpus construction method oriented to a power grid language model, and belongs to the technical field of power data processing. The invention divides data into structured, semi-structured and unstructured data, designs the corpus structure into a relational database comprising a text table, an entity table and a relational table, and creatively provides a construction scheme of a defect corpus of rich information power transmission and transformation equipment facing to a power grid language model: in the scheme, after the excellent language model in the natural language processing field is adapted to the power grid field, a plurality of power grid language models are designed, and compared with a traditional corpus only containing text corpus, the corpus designed and constructed by the scheme comprises entity and relation information in the text corpus, and the traditional corpus is subjected to information enrichment; in addition, the invention provides an application scheme of generating the data set by the corpus for the power grid language model, and creatively realizes the ecological closed-loop logic of continuous self-optimization of the corpus along with the scale expansion.","['G06F40/30', 'G06F16/2282', 'G06F16/367', 'G06F40/177', 'G06F40/295', 'Y04S10/50']"
EP4287143A1,Entity extraction with encoder decoder machine learning model,A method includes executing an encoder machine learning model on multiple token values contained in a document to create an encoder hidden state vector. A decoder machine learning model executing on the encoder hidden state vector generates raw text comprising an entity value and an entity label for each of multiple entities. The method further includes generating a structural representation of the entities directly from the raw text and outputting the structural representation of the entities of the document.,"['G06V30/19167', 'G06V30/19173', 'G06V10/82', 'G06V30/413']"
CN118520932B,"Visual language model training method, device, medium and computer program product","The invention discloses a visual language model training method, equipment, medium and computer program product, relating to the technical field of artificial intelligence, wherein the method comprises the following steps: the method comprises the steps of obtaining learner-prompting words of images of different categories by using a learner-prompting device, and encoding the learner-prompting words of different categories to obtain a first text code; acquiring a second text code after fusion of preset prompt words of images of different categories with external knowledge; calculating a first loss value based on the first text code and the second text code corresponding to the same category; extracting visual codes of images of different categories on a visual level by using a visual extractor, and calculating a second loss value based on the first text codes and the visual codes corresponding to the same category; and calculating an overall loss value based on the sum of the first loss value and the second loss value, and training the learnable prompter and the visual extractor based on the overall loss value to obtain a trained visual language model. The invention improves the generalization capability of the visual language model.","['G06N3/096', 'G06F16/55', 'G06F16/5846', 'G06F40/30', 'G06N3/042', 'G06N3/0455', 'G06N3/0464', 'G06N5/022', 'G06N5/041', 'G06V10/82', 'G06V20/70', 'G06N3/0442']"
RU2488878C2,Electrical efficiency measurement for data processing centres,FIELD: electrical engineering.,"['G06F1/26', 'G06F30/13', 'G06F30/20', 'H05K7/20836', 'G06F2119/06']"
EP4567736A1,Data processing method and apparatus,"A data processing method is applied to processing of an image including a text, relates to the field of artificial intelligence, and includes: obtaining a first feature representation and a second feature representation, where the second feature representation is a text feature of a first text, and the first text is text content included in an image; and obtaining a third feature representation based on the first feature representation and the second feature representation by using a target encoder, where the third feature representation is used to execute a downstream task, and a similarity between an execution result and a corresponding label and a similarity between the first feature representation and the second feature representation are used to update an image encoder. In this application, a dual-tower manner may be first used to improve a capability of alignment between an image and a text, and then a single-tower structure is used to further enhance an interactive learning capability for a feature.","['G06T11/60', 'G06V30/19093', 'G06T9/00', 'G06V10/44', 'G06V10/74', 'G06V10/761', 'G06V10/82', 'G06V30/153', 'G06V30/18086', 'G06T2207/20084']"
US8756047B2,Method of artificial nueral network loadflow computation for electrical power system,"ANN Loadflow Computation Method (LCM) is invented involving input vector composed of net nodal injected real power (P), reactive power (Q), and diagonal elements of conductance and susceptance matrices multiplied by squared initial guess node voltage magnitude (V) or sum of its squared real and imaginary components. Training, and testing and validating input data sets/vectors are generated by applying uniform and non-uniform scaling factors applied to base case loads P and Q at PQ-nodes, and resistance and reactance of network branches. These scale factors are increased until Loadflow Solution by a conventional LCM diverges due to V or angle (Î¸) instability. V and Î¸ values in the solution just before divergence are respective stability limits, and corresponding values in Loadflow Solution provide direct measure of respective stability margins. Suresh's diakoptics based feature selection technique is presented for calculating one node variable with one neuron in each of ANNs output layers.","['G06N3/08', 'H02J3/06', 'H02J2203/20']"
US8446911B2,System and method for managing communication for component applications,"An application gateway server is provided for managing communication between an application executing in a runtime environment on a device and at least one backend server. The application gateway server comprises a message listener, a connector subsystem, and a messaging subsystem. The message listener receives messages from the component applications. The connector subsystem comprises a plurality of connectors, each of the plurality of connectors for communicating with one or more associated backend servers. The messaging subsystem comprises a message broker for processing messages received from the message listener and transmitting them to an associated one of the plurality of connectors and a communication mapping for identifying which of the plurality of connectors is to be used for each message in accordance with an origin of the message.","['G06F9/546', 'H04L67/04', 'H04L67/10', 'H04L67/55', 'H04L67/565', 'H04L67/567', 'H04L12/1859', 'H04L51/58', 'H04L63/0428', 'H04L67/02', 'H04L67/51']"
CN116451697A,Text named entity identification method for power transmission and transformation line defects based on pointer network,"The invention provides a text named entity identification method for a power transmission and transformation line defect based on a pointer network, which comprises the following steps: s1, dividing a defect text of a power transmission and transformation line into an original training set D and an original testing set T; s2, obtaining a feature vector representation corresponding to each character of the training set through an Embedding layer and a bidirectional transducer layer of the BERT-WWM pre-training language model, and generating word vectors corresponding to the training set; s3, obtaining the head and the tail of the entity through the Span decoding layer, and splicing the head and the tail of the entity to obtain a text entity of the defect of the power transmission and transformation line; s4, inputting Span representation of the text entity of the power transmission and transformation line defect into the Softmax layer in parallel to obtain a final named entity and a corresponding entity category. The method can identify the professional named entity in the electric power field from the electric power defect text, realize the structural management of the electric power transmission and transformation line defect text, help first-line staff to manage the electric power transmission and transformation line defect text more accurately and efficiently, process defect equipment in time and ensure safe and reliable efficient operation of a power grid system.","['G06F40/295', 'G06F18/2415', 'G06F40/211', 'G06F40/289', 'G06F40/30', 'Y04S10/50']"
US8140371B2,Providing computing service to users in a heterogeneous distributed computing environment,"For providing a service-centric approach to allocating computing service power to users in a heterogeneous distributed computing environment such as a grid, a grid service broker comprises a central service broker and distributed service brokers. Service power is measured in resource-independent service units defined and used by the service broker. The central service broker uses meta-scheduling for distributing service requests to the distributed service brokers, based on projected service units needed to process the service requests.","['H04L67/565', 'G06Q10/0631', 'G06Q10/06313', 'G06Q10/06315']"
US12182555B2,Adapting existing source code snippets to new contexts,"Implementations are described herein for adapting existing source code snippets to new contexts. In various implementations, a command may be detected to incorporate an existing source code snippet into destination source code. An embedding may be generated based on the existing source code snippet, e.g., by processing the existing source code snippet using an encoder. The destination source code may be processed to identify one or more decoder constraints. Subject to the one or more decoder constraints, the embedding may be processed using a decoder to generate a new version of the existing source code snippet that is adapted to the destination source code.","['G06F8/447', 'G06F16/00', 'G06F8/30', 'G06F8/33', 'G06F8/34', 'G06F8/36', 'G06F8/51', 'G06F8/71', 'G06N3/044', 'G06N3/084', 'G06F16/2425', 'G06N3/045', 'G06N3/088']"
US20250060944A1,Automated data extraction pipeline for large language model training,"An automated data extraction pipeline for large language model (LLM) training may include extracting a set of code segments from a set of natural language question-answer (Q&A) combinations that each include a provided input, a provided output, and a provided code segment formatted to transform the provided input into the provided output. The data extraction pipeline may then generate a predicted output from a question portion of a first natural language Q&A combination using a first LLM. A first extracted code segment from the extracted set of code segments may then be executed to generate a first actual output of the first extracted code segment. One or more data samples may then be generated for training a second LLM based on a comparison of the first actual output to the predicted output. The second LLM may then be trained using the one or more data samples.","['G06F8/33', 'G06F40/30', 'G06F40/35', 'G06F40/55']"
RU2427871C2,Inspection of intelligent electronic device configuration,"FIELD: information technologies. ^ SUBSTANCE: method relates to inspection of the first intelligent electronic device (IED) 21 configuration, which represents a part of substation automatics subsystem (SA), and which was initially configured to perform functions of measurement, protection and/or management in compliance with specification of substation configuration. Information on configuration is first read from an internal server of the first IED and converted in compliance with the identified data model. The related information is then read, for instance, from the second IED, which was first configured to perform the same functions as the first IED, or from a file of substation configuration description (SCD), and is transformed in a similar manner. Transformed data is then compared for identification of differences and discrepancies and to resolve errors in process of IED configuration. ^ EFFECT: provision of functional compatibility of all substation devices. ^ 5 cl, 2 dwg","['H04L41/0869', 'H02J13/00028', 'H02J13/00034', 'Y02E60/00', 'Y04S10/16', 'Y04S10/18', 'Y04S40/00']"
US7966087B2,"Method, system and medium for controlling manufacture process having multivariate input parameters","A method, system, and medium of modeling and/or for controlling a manufacturing process is disclosed. In particular, a method according to embodiments of the present invention includes calculating a set of predicted output values, and obtaining a prediction model based on a set of input parameters, the set of predicted output values, and empirical output values. Each input parameter causes a change in at least two outputs. The method also includes optimizing the prediction model by minimizing differences between the set of predicted output values and the empirical output values, and adjusting the set of input parameters to obtain a set of desired output values to control the manufacturing apparatus. Obtaining the prediction model includes transforming the set of input parameters into transformed input values using a transformation function of multiple coefficient values, and calculating the predicted output values using the transformed input values.","['G05B11/32', 'G05B13/042', 'G05B13/048', 'Y02P90/02']"
CN112632972A,Method for rapidly extracting fault information in power grid equipment fault report,"The invention relates to a method for quickly extracting fault information in a fault report of power grid equipment, which solves the problem that the existing model is insufficient in field entity recognition capability. The model solves the problem of insufficient acquisition of BilSTM context information by using a bidirectional encoder BERT model based on a Transformer, improves the identification precision of the model on the basis of acquiring global context information, and enhances the field word information acquisition capability by using a pre-training model BERT based on transfer learning. On the basis, local fine tuning training facing to the power grid field is carried out, domain-facing retraining is carried out on part of transform layers of the BERT, and under the condition that Chinese syntax grammar information contained in an original model is kept, a universal model which is originally not suitable for the power grid field can obtain a better result on a power grid fault report text.","['G06F40/253', 'G06F40/295', 'G06N3/044', 'G06N3/049']"
US12243518B2,Data augmentation for intent classification,"The present disclosure relates to a data augmentation system and method that uses a large pre-trained encoder language model to generate new, useful intent samples from existing intent samples without fine-tuning. In certain embodiments, for a given class (intent), a limited number of sample utterances of a seed intent classification dataset may be concatenated and provided as input to the encoder language model, which may generate new sample utterances for the given class (intent). Additionally, when the augmented dataset is used to fine-tune an encoder language model of an intent classifier, this technique improves the performance of the intent classifier.","['G06F40/30', 'G10L15/1815', 'G10L15/063', 'G10L2015/0631']"
US8527641B2,Systems and methods for applying transformations to IP addresses obtained by domain name service (DNS),"Described herein are systems and methods for improving networked communication systems by transforming IP addresses. In particular, an intermediary device disposed in a network between a plurality of clients and a plurality of servers can receive a request for a service offered at a specified domain name. The appliance can also receive a DNS-resolved primary address for a server associated with the domain name, and transform the primary address to a secondary address for the server. The address transformation can be done by the intermediary to prevent service interruption between a client and server due, for example, to server maintenance.","['H04L61/2503', 'H04L45/74', 'H04L61/4511', 'H04L2101/677']"
US7954107B2,Method and system for integrating the existing web-based system,"The present invention provides a service-oriented system comprises: a service inlet for receiving a service request containing an input data set submitted by a user; a service proxy interface formed by integrating an existing web-based application, for transforming the input data set in the service request submitted by the user into input parameters recognizable by the requested service and loading a model definition document corresponding to the service according to the service request submitted by the user, the model definition document being formed by integrating the existing web-based application; and an execution engine for interacting with an external resource according to the input parameters transformed by the service proxy interface and returning the information obtained from the external resource to the service proxy interface.",['H04L67/02']
US12010076B1,Increasing security and reducing technical confusion through conversational browser,"Systems and method for providing an application chatbot that provides a conversational interface that receives natural language input from an application user, interprets the user's intent, and uses application-related context for generating and providing a contextually accurate response in a conversation with the user. In some examples, the application chatbot determines an action to perform corresponding to the response and provides an option to perform the action in the conversational user interface. A selection of the option causes the action to be performed.","['H04L51/02', 'H04L51/046', 'H04L51/18', 'H04L63/20']"
US20230386456A1,Method for obtaining de-identified data representations of speech for speech analysis,"The invention relates to a computer-implemented method of obtaining de-identified representations of audio speech data for use in a speech analysis task, the method comprising: pre-processing the audio speech data to remove timbral information; encoding sections of the pre-processed audio speech data into audio representations by inputting sections of the pre-processed audio data into a prosody encoder, the prosody encoder comprising a machine learning model trained using self-supervised learning to map sections of the pre-processed audio data to corresponding audio representations. The combination of removing timbral information during pre-processing and encoding segments of pre-processed audio data using an encoder trained using self-supervised learning results in the provision of strong prosodic representations which are substantially de-identified from the speaker.","['G10L25/30', 'G10L15/1807', 'G06N20/00', 'G06N3/045', 'G06N3/082', 'G10L15/05', 'G10L15/063', 'G10L15/16', 'G10L19/032', 'G10L25/45', 'G10L25/48', 'G10L25/66', 'A61B5/4803', 'G10L21/013']"
US8874434B2,Method and apparatus for full natural language parsing,"The method and apparatus for discriminative natural language parsing, uses a deep convolutional neural network adapted for text and a structured tag inference in a graph. In the method and apparatus, a trained recursive convolutional graph transformer network, formed by the deep convolutional neural network and the graph, predicts â€œlevelsâ€ of a parse tree based on predictions of previous levels.","['G06F17/271', 'G06F40/211', 'G06F17/2775', 'G06F40/289']"
EP4336378A1,Data processing method and related device,"This application relates to the field of artificial intelligence, and discloses a data processing method. The method includes: obtaining a transformer model including a target network layer and a target module; and processing to-be-processed data by using the transformer model, to obtain a data processing result. The target module is configured to: perform a target operation on a feature map output at the target network layer, to obtain an operation result, and fuse the operation result and the feature map output, to obtain an updated feature map output. In this application, the target module is inserted into the transformer model, and the operation result generated by the target module and an input are fused, so that information carried in a feature map output by the target network layer of the transformer model is increased. In addition, data processing accuracy of the model is improved while a quantity of parameters of the target module and computing power overheads required during an operation are small, that is, the quantity of parameters of the model and the computing power overheads are reduced.","['G06N3/048', 'G06F16/3329', 'G06N3/045', 'G06F16/3344', 'G06N3/082', 'G06N5/04']"
CN113656563B,A neural network search method and related equipment,"The application relates to the field of artificial intelligence, and discloses a neural network searching method and a related device, wherein the neural network searching method comprises the following steps: when the model search is carried out, the attention head in the transducer layer is constructed by sampling a plurality of candidate operators, so that a plurality of candidate neural networks are constructed, and performance comparison is carried out on the plurality of candidate neural networks to select a target neural network with higher performance. The application combines model search to construct a transducer model, can generate a novel attention structure with better performance than the original self-attention mechanism, and has obvious effect improvement on a wide downstream task.","['G06F16/3329', 'G06F16/3344', 'G06F16/35', 'G06F40/295', 'G06F40/30', 'G06N20/00', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/063', 'G06N3/08', 'G06N3/084', 'G06N3/0985']"
RU2402784C2,System level testing for substation automation systems,FIELD: physics.,"['G01R31/327', 'H04L43/0817', 'H04L43/50', 'Y04S40/00']"
CN116932708A,Open domain natural language reasoning question answering system and method driven by large language model,"The invention provides a large language model driven open domain natural language reasoning question-answering system and method, a question rewrite module rewrites user questions into rewritten questions; the central computing and managing module manages computing and knowledge resources of the large language model, and outputs the computing and knowledge resources of the large language model required by the rewrite problem and the problem core engine module to one or more sub-question-answering modules in the question core engine module according to the type of the rewrite problem; the question-answering core engine module obtains one or more candidate answers of the rewritten questions and explanatory information of the candidate answers according to calculation and knowledge resource reasoning of the rewritten questions and the large language model; the aggregation reasoning module aggregates and reasoning according to one or more candidate answers of the rewritten questions and the interpretable explanatory information of the candidate answers to obtain a final answer of the rewritten questions and the interpretable explanatory information of the final answer, and supports the questions by adopting a large language model, so that the types of the questions are comprehensive, easy to expand, interpretable and strong in universality.","['G06F16/3329', 'G06F16/3344', 'G06F16/338', 'G06F16/34', 'G06N5/04', 'Y02D10/00']"
US12255749B2,Meeting insights with large language models,"In accordance with examples of the present disclosure, a collaborative platform provides a digital collaboration assistant that continuously monitors and analyzes shared meeting contents (e.g., voice, text chat messages, shared links and documents, presentation materials, and the like) by participants during a collaborative meeting in near real-time, periodically updates a structure summary log of the meeting contents that are deemed important during the collaborative meeting, and interacts with the participants throughout the collaborative meeting in near real-time, for example, to answer questions or provide additional information.","['H04L12/1822', 'H04L12/1831']"
US12086716B1,"Method for constructing multimodality-based medical large model, and related device thereof","A method for constructing a multimodality-based medical large model, and a related device thereof are provided. The medical large model includes a multimodal transformer T, a prompt manager M, a dialogue engine L, a task controller H, and a multimodal foundation (MMF) that includes at least one medical foundation model (MFM). Five stages, namely modal analysis, model allocation, downstream task result feedback, modal transformation normalization, and response generation are designed.","['G16H50/70', 'G06N3/08', 'G16H50/20', 'Y02D10/00']"
CN116097248A,System and method for controllable text summarization,"The embodiments described herein provide a flexible and controllable summary system that allows a user to control the generation of a summary without requiring manual editing or writing of the summary, e.g., without requiring the user to physically add or delete certain information to various extents. In particular, the summary system performs a controllable summary through keyword manipulation. The neural network model learns to generate an overview conditioned on keywords and source documents so that a user can interact with the neural network model through a keyword interface at the time of testing, potentially enabling multi-factor control.","['G06F16/345', 'G06F40/284', 'G06N7/01', 'G06F3/0483']"
US20230315999A1,Systems and methods for intent discovery,"Systems and method are disclosed for processing unrecognized user queries. A received user query is classified via a first machine learning model. A first classification determination is made for the user query. In response to the first classification determination, features of the user query are identified via a second machine learning model. The user query is grouped into a cluster based on the features of the user query. Information about the cluster is displayed for prompting a user action. The user action may include identification of an intent for the user query.","['G06F40/279', 'G06F40/35', 'G06F16/3325', 'G06F16/3329', 'G06F16/35', 'G06F40/30', 'G06F40/40', 'G06N20/20', 'G06N3/045', 'G06N3/084', 'G06N3/096', 'G06N3/088', 'G06N3/09']"
US8886686B2,Making and using abstract XML representations of data dictionary metadata,"An XML representation of the metadata for objects in a database system. The XML representation is called SXML. The SXML representations of two objects that belong to the same class of objects may be compared to produce a difference representation that shows the differences between the objects. Database commands that will create or alter objects may be produced from the difference representation, as well as an XML representation of the database commands. In addition to being comparable, SXML representations have the characteristics that a given property of the metadata is expressed in only one way in the representation, that SXML does not describe properties that belong only to particular instances of the object described by the metadata, that SXML describes only properties that the user can specify when creating an object, and that SXML is easy for humans to read, understand, and write.","['G06F16/83', 'G06F17/30923', 'G06F16/288', 'G06F17/30604']"
US8667482B2,Automated application modeling for application virtualization,"Automated application modeling for application virtualization (auto-modeling) may be incorporated into an application installer and/or other suitable component of a computer operating system. Auto-modeling may be performed by an auto-modeling agent. The auto-modeling agent may employ one or more of multiple auto-modeling strategies. The auto-modeling agent may assess one or more of a particular application, application installation package and/or application environment in order to determine a suitable auto-modeling strategy. Auto-modeling strategies may include active auto-modeling and passive auto-modeling. Active auto-modeling strategies may require at least partial installation and/or execution of the application to be modeled, whereas passive auto-modeling may generate corresponding auto-modeling data independent of application installation and/or execution, for example, by obtaining suitable data from a corresponding application installation package.",['G06F8/61']
CN112100388A,Method for analyzing emotional polarity of long text news public sentiment,"The invention discloses an analysis method of emotion polarity of long text news public sentiment, which comprises the following steps: s1, collecting text data as a training sample, S2 cleaning the data of the training sample collected in the step S1, and processing special characters in the cleaned data of the training sample to obtain a data set; s3, segmenting the data set, and segmenting the training samples into a training set and a test set according to the proportion; s4 building a deep learning network based on the representation model and loading pre-training parameters; s5, constructing a long text emotion polarity analysis network model; s6, modifying the structure of the training sample data; and (8) training the model S7, adopting a method of hierarchical sampling and K-fold cross validation, ensuring that the sample proportion in the sample data set of each fold is consistent with the proportion of the original data during the hierarchical sampling, storing the result of each fold in the model with the highest score in the validation set, testing the test set by integrating the K-fold model, and taking the average probability as the test result of the model.","['G06F16/35', 'G06F40/284', 'G06F40/30', 'G06N3/045', 'G06N3/08']"
US20230386238A1,"Data processing method and apparatus, computer device, and storage medium","This application discloses a data processing method and apparatus, a computer device, and a non-transitory computer-readable storage medium in the technical field of computers. This application, for textual data and picture data of an article, extracts a textual feature and a picture feature, respectively, and predicts an article classification to which the article belongs using a cross-modal interaction feature between the textual feature and picture feature. At the same time, this application considers the contribution degree of each of a textual modality and a picture modality to the article classification, rather than determining from a textual perspective only. In addition, the extracted cross-modal interaction feature is not a simple concatenation of the textual feature and the picture feature, which can reflect richer and deeper inter-modal interaction information, and greatly improve the identification accuracy of the article classification. Furthermore, it can improve the discovering accuracy of high-quality articles in the scene of identifying high-quality articles.","['G06V30/1918', 'G06F16/383', 'G06F16/583', 'G06F40/126', 'G06F40/216', 'G06F40/258', 'G06F40/284', 'G06F40/289', 'G06F40/30', 'G06N20/00', 'G06N3/04', 'G06N3/0464', 'G06N3/0499', 'G06N3/08', 'G06T7/50', 'G06V10/82', 'G06V30/1801', 'G06V30/18133', 'G06V30/19173', 'G06V30/20', 'G06V30/412', 'G06V30/413', 'G06T2207/30176', 'G06V10/806']"
CN115952263A,A Question Answering Method Fused with Machine Reading Comprehension,"The invention discloses a question-answering method fusing machine reading understanding, which comprises the steps of 1) data preparation; 2) Processing a natural language; 3) A named entity recognition model; 4) Deploying a TorchSever model; 5) Preprocessing an answer; 6) Predicting by a BERT model; 7) The answer is displayed. The method can improve the accuracy of question answering in front-end and back-end data interaction.",['Y02D10/00']
CN114298158A,A Multimodal Pre-training Method Based on Linear Combination of Graphics and Text,"A multi-mode pre-training method based on image-text linear combination belongs to the technical field of image-text multi-mode retrieval, and comprises the following steps: s1: respectively extracting the features of the text and the image; s2: establishing a relation between two modes of a text mode and an image mode in an interaction layer; s2.1: jointly inputting the feature vectors of the visual mode and the language mode obtained in the S1 into an interaction layer of the multi-mode pre-training model; s2.2: the attention mechanism in the Transformer is utilized to enable the two modes to be mutually connected; s3: using the image-text matching or shielding language model as a pre-training target, and training the model to be available; s4: and taking a specific application scene and a downstream task as training targets, carrying out fine tuning training on the pre-training model, and optimizing the performance of the model under the scene. The training method solves the bottleneck problem of model operation time and the performance problem of the improved pre-training model after fine adjustment, and has important scientific significance and practical application value.",[]
CN110647619A,A Common Sense Question Answering Method Based on Question Generation and Convolutional Neural Networks,"The invention provides a common sense question-answering method based on question generation and a convolutional neural network, which encodes content-questions into vector sequences through a BERT language model, transmits the vector sequences into a question generation module and then into a shared BERT language model, transmits triples consisting of the content-questions-answers into an answer selection module through the BERT language model and an output content-question-answer encoding sequence, classifies the triples through the convolutional neural network, and finally selects optimal options as candidate answers selected by the model according to scores obtained by the model.","['G06F16/3329', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
CN118627617A,A document-level event causal relationship identification method based on graph reinforcement learning,"The invention discloses a method for identifying causal relationship of a document level event based on graph reinforcement learning, which comprises the following steps: acquiring text data containing an event; generating a priori causal graph by using a large language model as an external knowledge base and an reasoner, and acquiring an initial graph structure; obtaining a contextualized event representation and calculating the event pair correlation degree by using a long text pre-training language model, and providing priori knowledge; introducing an attention mask and a self-attention initialization mechanism, generating a contextualized representation of the event, and integrating the prior causal graph and the contextualized event correlation into a transducer model to guide and control training of the model; training and optimizing a model using graph reconstruction loss and attention profile offset regularization loss; causal links between events extracted in the output gene database. The invention introduces a self-attention initialization mechanism based on event correlation and limits the attention of a transducer to a reasonable distribution space.","['G06N5/04', 'G06F16/3344', 'G06N3/0455', 'G06N3/08', 'G16B50/00']"
US11687733B2,Contrastive self-supervised machine learning for commonsense reasoning,"In an example embodiment, a self-supervised learning task is used for training commonsense-aware representations in a minimally supervised fashion and a pair level mutual-exclusive loss is used to enforce commonsense knowledge during representation learning. This helps to exploit the mutual-exclusive nature of the training samples of commonsense reasoning corpora. Given two pieces of input where the only difference between them are trigger pieces of data, it may be postulated that the pairwise pronoun disambiguation is mutually exclusive. This idea is formulated using a contrastive loss and then this is used to update the language model.","['G06N20/00', 'G06F40/40', 'G06F18/2415', 'G06F40/211', 'G06F40/284', 'G06N3/045', 'G06N3/08', 'G06N3/088', 'G06N5/04', 'G06N3/042']"
US11521639B1,Speech sentiment analysis using a speech sentiment classifier pretrained with pseudo sentiment labels,"The present disclosure describes a system, method, and computer program for predicting sentiment labels for audio speech utterances using an audio speech sentiment classifier pretrained with pseudo sentiment labels. A speech sentiment classifier for audio speech (â€œa speech sentiment classifierâ€) is pretrained in an unsupervised manner by leveraging a pseudo labeler previously trained to predict sentiments for text. Specifically, a text-trained pseudo labeler is used to autogenerate pseudo sentiment labels for the audio speech utterances using transcriptions of the utterances, and the speech sentiment classifier is trained to predict the pseudo sentiment labels given corresponding embeddings of the audio speech utterances. The speech sentiment classifier is then subsequently fine tuned using a sentiment-annotated dataset of audio speech utterances, which may be significantly smaller than the unannotated dataset used in the unsupervised pretraining phase.","['G10L25/30', 'G10L25/63', 'G06N3/0442', 'G06N3/088', 'G06N3/0895', 'G06N3/096', 'G10L19/02', 'G10L21/10', 'G06F40/30', 'G10L15/26']"
CN119398106B,Fault processing equipment for electric power distribution network,"The invention relates to a fault processing device of an electric power distribution network, which is applied to the technical field of power distribution and aims at improving the fault prediction, diagnosis and processing capacity of an electric power system, and the device comprises a pre-training module, a neural network architecture module, a data acquisition layer, an edge calculation layer, a core processing layer, a context learning module, an reasoning processing module, an output and interaction layer and a continuous learning and improvement layer; the method uses reinforcement learning to optimize fault response strategies, realizes the interpretability of the model and improves the decision transparency.","['H02J13/00002', 'G01R31/086', 'G01R31/088', 'G06F18/10', 'G06F18/21', 'G06F18/214', 'G06F18/24', 'G06N3/0455', 'G06N3/0499', 'G06N3/084', 'G06N3/096', 'G06N3/098', 'G06N5/022', 'G06Q10/20', 'G06Q50/06', 'H02J13/00001', 'H02J13/00006', 'H02J13/00017', 'H02J13/00022', 'H02J13/00028', 'H02J13/00032', 'Y04S10/50']"
US20240256965A1,Instruction Fine-Tuning Machine-Learned Models Using Intermediate Reasoning Steps,"An example method for training a machine-learned sequence processing model includes obtaining a plurality of training examples for training the machine-learned sequence processing model. For each respective training example of the plurality of training examples, the example method includes: obtaining a respective query associated with the respective training example; inputting the respective query to the machine-learned sequence processing model; obtaining, from the machine-learned sequence processing model a response to the respective query and a trace of intermediate states from the respective query to the response; evaluating the response using a ground truth response associated with the respective training example; evaluating the trace using a ground truth trace associated with the respective training example; and updating one or more parameters of the machine-learned sequence processing model based on the evaluation of the response and based on the evaluation of the trace.","['G06N20/00', 'G06F16/33']"
CN101212136B,Method and device for operating relay device,"The present invention relates to a relay device and a corresponding method. A method of operating a relay device, the method comprising the steps of: computing positive-sequence first and second current-phasor for the first and second main circuit-breakers and computing currents from phasors, and the bus-tie-breaker; computing positive-sequence voltage for the bus-tie-breaker; triggering a method when at least one fault current at the bus-tie-breaker exceeds a threshold, without the presence of a fault on one of the feeders; determining whether the fault is internal or external, wherein the fault is internal if the positive-sequence current of the first and second main circuit breakers is in phase, and wherein if the positive-sequence current through the first main circuit breaker is not in-phase with the positive-sequence current through the second main circuit breaker, the fault is external to the bus; determining whether the fault is on a first or second side of the tie circuit; and locating the fault.","['H02H3/04', 'H02H7/30', 'H02H3/26', 'H02H3/387', 'H02H3/28']"
CN101706816B,Method for detecting digitized transformer station model,"The invention discloses a method for detecting a digitized transformer station model. In recent years, IEC61850 and digitized transformer stations are developed quickly; however, great difference exists due to different producers and different device models, which brings great difficulties and hidden troubles to debugging and running; therefore, a detection means and a method which are simple, practical, effective and comprehensive are needed. In accordance with an IEC61850 standard, the method for detecting the digitized transformer station model supports a standardized detection aiming at the domestic 'Engineering Implementation Technical Specifications of IEC61850 International Standard', and is widely applied to the model detection of digitalized and conventional IEC61850 transformer station comprehensive automatic systems, wherein the method comprises the following steps of: detecting whether an XML format is right and performing Schema verification; detecting basic unit data, namely detecting the information such as logical nodes, common data, data objects, attributes and the like; resolving and detecting a model GOOSE and four-remote information; comparing offline and online models; and detecting SCD file virtual terminals.",[]
CN119526422A,A method for interactive operation control of deformable objects based on a visual-touch-language-action multimodal model,"The invention relates to a deformable object interactive operation control method based on a visual touch-language-action multi-modal model, which comprises the steps of carrying out image, touch and language data coding on a deformable object to obtain visual, touch and language characteristics, carrying out cross-modal characteristic alignment processing on the visual characteristics, the touch characteristics and the language characteristics to obtain multi-modal fusion characteristics, inputting the multi-modal fusion characteristics into a large model for environmental understanding, carrying out action planning and execution in an 'thinking-decision' planning mode iteration, and repeatedly executing the steps until the current deformable object interactive operation task is completed. Compared with the prior art, the method and the device have the advantages that the multi-mode characteristic alignment capability, the action planning precision and the task suitability are improved, the efficient identification and interaction of the robot on the deformable object can be realized, the deformation and the state change of the object can be effectively processed particularly in a complex environment, the operation strategy can be dynamically adjusted, and the more intelligent and accurate deformable object operation can be realized.","['B25J9/1602', 'B25J9/1605', 'B25J9/1664', 'B25J9/1697']"
US11429352B2,Building pre-trained contextual embeddings for programming languages using specialized vocabulary,"A method, a computer system, and a computer program product for building pre-trained contextual embeddings is provided. Embodiments of the present invention may include collecting programming code. Embodiments of the present invention may include loading and preparing the programming code using a specialized programming language keywords-based vocabulary. Embodiments of the present invention may include creating contextual embeddings for the programming code. Embodiments of the present invention may include storing the contextual embeddings.","['G06F8/31', 'G06N3/088', 'G06F40/40', 'G06F8/40', 'G06F9/445', 'G06N20/00', 'G06N3/044', 'G06N3/045']"
CN113345418B,Multilingual model training method based on cross-language self-training,"The invention provides a multi-language model training method based on cross-language self-training, which comprises the steps of firstly training an acoustic phoneme classifier on marked data of a certain high-resource language as a target network, and then training a main network to approximate the representation of the acoustic phoneme classifier on multi-languages; the method specifically comprises the following steps: acquiring a target network; training a main network; and migrating the trained main network to a target language automatic speech recognition model.","['G10L15/005', 'G10L15/02', 'G10L15/063', 'G10L15/16', 'G10L2015/025', 'G10L2015/0631']"
US12334068B1,Detecting corrupted speech in voice-based computer interfaces,Approaches are generally described for corrupted speech detection in voice-based computer interfaces. First input data including first audio data representing a user utterance may be received. First data representing the first audio data may be generated using a first encoder. First text data representing a transcription of the user utterance may be generated. Second data representing the first text data may be generated using a second encoder different from the first encoder. Third data may be generated by combining the first data and the second data. The third data may be sent to a classifier network trained to predict a relevant corruption state for speech processing inputs. The classifier network may determine that the first input data corresponds to a first corruption state.,"['G06F40/279', 'G06F40/30', 'G10L15/18', 'G10L15/22', 'G10L15/26', 'G10L25/30', 'G10L2015/225']"
CN112364150A,Intelligent question and answer method and system combining retrieval and generation,"The invention discloses an intelligent question and answer method combining retrieval and generation, which comprises the following steps: constructing a preset domain knowledge base module; constructing a search question-answering module; constructing a generating question-answering module; inputting the user questions into a search-type question-answering module and a generation-type question-answering module respectively to obtain two corresponding output results; and judging the two output results, selecting one of the two output results as a final answer by adopting a preset answer selection strategy, and expanding the preset domain knowledge base according to the selection result. The invention can comprehensively utilize two strategies of a search-type question answering and a generation-type question answering, ensure the accuracy of intelligent question answering in the field by utilizing the search-type question answering module, expand the field knowledge base by utilizing the generation-type question answering module and relieve the problem of data shortage, thereby improving the answering capability of the intelligent question answering system.","['G06F16/3329', 'G06F16/31', 'G06F16/3334', 'G06F16/3347']"
US12045568B1,Span pointer networks for non-autoregressive task-oriented semantic parsing for assistant systems,"In one embodiment, a method includes receiving a user input comprising input tokens from a client system, parsing the user input to determine ontology tokens and utterance tokens corresponding to the input tokens, decoding the ontology tokens and the utterance tokens to generate a span-based frame representation comprising intents, slots, and a span, wherein the ontology tokens are decoded into the intents and slots, and wherein the utterance tokens are decoded to determine the span comprising one or more tokens of the input tokens, wherein the span comprises a first index endpoint associated with a first token of the one or more tokens and a second index endpoint associated with a second token of the one or more tokens, and executing, responsive to the user input, one or more tasks based on the span-based frame representation.","['G06F40/284', 'G06F40/205', 'G06F40/30', 'G10L15/1815', 'G10L15/1822', 'G10L15/22', 'G10L15/30', 'G10L2015/223']"
CN113297841A,Neural machine translation method based on pre-training double-word vectors,"The invention discloses a neural machine translation method based on pre-training dual-language word vectors, which comprises the steps of splicing a source language-target language of parallel linguistic data marked and aligned as input of an XLM (cross-linked list model) for pre-training; training: taking a bilingual word vector matrix obtained by pre-training to initialize a translation model; inputting a source language into an encoder, inputting a vector representation of the source language encoding and a corresponding target language into a decoder to output a prediction sequence, comparing the prediction sequence with a corresponding target sequence, calculating a loss value, and inputting the loss value into an optimizer to optimize translation model parameters; and (3) prediction: in a certain time step, the source language is input into an optimized encoder, the encoder outputs corresponding vector representation, the vector representation and the target language words translated in the last time step are input into a decoder, the decoder outputs the target words in the time step, and the target words translated in different time steps are spliced according to the time sequence to obtain a source language translation result. The method improves the machine translation effect of low-resource languages.","['G06F40/58', 'G06F40/216', 'G06F40/284', 'G06N3/045', 'G06N3/047', 'G06N3/048', 'G06N3/08']"
CN114492382A,"Character extraction method, text reading method, dialog text generation method, device, equipment and storage medium","The invention relates to a character extraction method, a text reading method, an interactive text generation method, a device, equipment and a storage medium, wherein the character extraction method comprises the following steps: receiving a target text, wherein the target text comprises one or more sentences of utterances; inputting the target text into a pre-trained language model, and determining a speaker corresponding to an utterance in the target text by using the language model; wherein the determining, using the language model, an uttered character corresponding to the utterance in the target text comprises: inferring a speaker corresponding to an utterance in the target text using a context of a sentence in text input to a language model; wherein the process of training the language model comprises: a speaker corresponding to an utterance in a training text input to a language model is inferred using a context of a sentence in the training text. By using the character extraction method, the speaker corresponding to the utterance in the text can be accurately extracted and recognized.","['G06F40/211', 'G06F40/216', 'G06F40/284', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
US20250036877A1,Systems and methods for automated data procurement,"The present disclosure provides a system for generating customized messages to a prospect. The system comprises: a data input module configured to receive prospect data, wherein the prospect data comprises data related to a prospect and data gathered from a plurality of sources; an input data processing module configured to select a subset of the data to create an input data; a first model trained to generate one or more sentences based on the input data; and a second model trained to determine whether the one or more sentences are unacceptable, wherein upon determining the one or more sentences are unacceptable, directing the input data processing module to select another subset of the data to create a new input data.","['G06N3/0475', 'G06F40/289', 'G06F40/56', 'G06N20/00', 'G06N3/0455', 'G06N3/08', 'G06V10/82', 'G06V30/10', 'G06F40/253', 'G06N3/044', 'G06N3/0464']"
WO2021121198A1,"Semantic similarity-based entity relation extraction method and apparatus, device and medium","A semantic similarity-based entity relation extraction method and apparatus, a device and a medium, relating to the field of artificial intelligence. The method comprises: obtaining annotated corpora and unannotated corpora, and storing each annotated corpus in a seed set (S201); for each annotated corpus in the seed set, on the basis of a preset feature construction mode, constructing features for each annotated corpus, and obtaining relation features of the annotated corpora (S202); inputting the unannotated corpora, the annotated corpora, and the relation features of the annotated corpora into a preset similarity evaluation model (S203); and on the basis of the preset similarity evaluation model and the relation features, evaluating the unannotated corpora, obtaining an evaluation result, then determining an entity relation of the unannotated corpora on the basis of the evaluation result (S204). By means of a semi-supervised method, rapid entity relation extraction may be performed in respect of the unannotated corpora, improving entity relation extraction accuracy and efficiency.","['G06F40/30', 'G06F18/22', 'G06F18/24', 'G06F40/211', 'G06F40/284', 'G06F40/295', 'G06N3/045', 'G06N3/08']"
US9194899B2,Utility network and revenue assurance,"A system and method for detecting anomalies in the measurement and distribution of utilities is disclosed. Utility metering data obtained at a utility meter is received through a communications network. A utility consumption associated with an entity is then measured based on the utility metering data. The utility consumption can then be monitored for anomalies based on entity profile characteristics associated with the entity. The utility analytics system and method can be applied to the electrical utility industry, but also applicable to gas and water distribution, and other utilities.","['G01R22/10', 'G01D4/004', 'G01R22/066', 'G08B21/0484', 'G01R22/063', 'Y02B90/20', 'Y02B90/242', 'Y02B90/246', 'Y04S20/30', 'Y04S20/322', 'Y04S20/42']"
US11556709B2,Text autocomplete using punctuation marks,"A dataset comprising text-based messages can be accessed. Tokens for words and punctuation marks contained in the text-based messages can be generated. Each token corresponds to one word or one punctuation mark. A vector representation for each of a plurality of the tokens can be generated using natural language processing. A sequence of tokens corresponding to the text-based message can be generated for each of a plurality of the text-based messages in the dataset. Ones of the tokens that represent punctuation marks can be identified. An artificial neural network can be trained to predict use of the punctuation marks in sentence structures. The training uses the generated sequence of tokens and the vector representations for the tokens, in the sequence of tokens, that represent the punctuation marks.","['G06F40/274', 'G06F40/284', 'G06F17/16', 'G06F40/205', 'G06F40/253', 'G06F40/279', 'G06N20/00', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/048']"
CN117197569A,"Image auditing method, image auditing model training method, device and equipment","The application relates to an image auditing method, an image auditing model training method, an image auditing device, computer equipment, a storage medium and a computer program product, and relates to computer vision and natural language processing technology. The image auditing method comprises the following steps: acquiring an image to be inspected and an inspection condition set comprising the respective tag matching conditions of a plurality of candidate tags; based on image elements contained in the image to be checked, carrying out semantic analysis on the image to be checked to obtain semantic information of the image to be checked; determining target conditions matched with the semantic information from the tag matching conditions; according to the semantic information, carrying out context coding processing on the text representing the target condition to obtain a matching reason of the image to be checked and the target condition; and determining an auditing result of the image to be audited based on the target label corresponding to the target condition in each candidate label and the matching reason. By adopting the method, the working efficiency of the image auditing and processing process can be improved.",[]
WO2024045444A1,"Processing method and apparatus for visual question answering task, and device and non-volatile readable storage medium","The present application relates to the technical field of image processing. Disclosed are a processing method and apparatus for a visual question answering task, and a device and a non-volatile readable storage medium. The processing method comprises: performing feature fusion processing on an image to be analyzed and first text, so as to obtain fused features, wherein the fused features contain coordinate information of detection boxes; according to the correlation between said image and the first text, screening the fused features to obtain a target detection box, which meets a correlation requirement; and inputting coordinate information, a classification category and a semantic feature, which correspond to the target detection box, into a trained visual question answering model, so as to obtain second text, which matches the first text, wherein the first text has a logical correlation with the second text. Feature fusion processing is performed on an image to be analyzed and first text, such that a comprehensive analysis of said image and the first text can be realized. Detection boxes are pruned on the basis of correlation, thereby effectively reducing the interference caused by invalid detection boxes, reducing the amount of computation of a visual question answering model, and improving the performance of a visual question answering task.","['G06F16/3329', 'G06F16/335', 'G06F16/35', 'G06F16/5846', 'G06F40/126', 'G06F40/30', 'G06V30/19007', 'G06V30/19147', 'G06V30/19173', 'G06V30/1918', 'G06V30/41']"
US8730778B2,Data storage tape analytics method and system,"A system for monitoring and analyzing operation of a tape infrastructure. The system includes a plurality of real and virtual tape libraries each including a plurality of tape drives and tape media. The system includes a server linked to the tape library. On the server, a storage tape analytics application is provided that functions to extract raw data sets from the tape library corresponding to operations of the tape library. The data pertains to use of the drives and media during data storage operations and data about the activities of the real or virtual tape libraries. The analytics application stores the extracted data sets in an analytics database. The analytics application provides a user interface for viewing data in the analytics database, e.g., a GUI operable by monitoring personnel to view and interact with the gathered and processed tape operations monitoring data to provide historical and user-driven analysis of data for an entire data center.","['G11B27/36', 'G11B27/002', 'G11B27/105', 'G11B27/34', 'G11B2220/90']"
WO2024187031A2,Systems and methods for dynamic-backbone protein-ligand structure prediction with multiscale generative diffusion models,"In some aspects, the present disclosure provides a method for generating a geometrical structure of a binding complex formed between a protein and a ligand. In some embodiments, the method comprises sampling an initial geometrical structure of the binding complex from a geometry prior. In some embodiments, the method comprises denoising, using a machine-learned stochastic differential equation (SDE), the initial geometrical structure to generate the geometrical structure of the binding complex.","['G16B40/20', 'G16B15/30']"
US20240202452A1,Prompt generation simulating fine-tuning for a machine learning model,"Aspects of the present disclosure relate to systems and methods for generating one or more prompts based on an input and the semantic context associated with the input. In examples, the prompts may be provided as input to one or more general ML models to provide a semantic context around the input and/or output of the model. The prompt simulates training and fine-tuned specialization of the general ML model without the need to use a fine-tuning process to actually train the general ML model into a fine-tuned state. Additionally, the model output may be evaluated for responsiveness to the input prior to being returned to the user. An advantage of the present disclosure is that it allows a general ML model to be applied to a plurality of applications without the need for expensive and time-consuming training to fine-tune the ML model.","['G06N3/0475', 'G06F40/30', 'G06N3/045']"
US20240176958A1,Prompting language models with workflow plans,A pre-trained natural language processing machine learning model is received. The pre-trained natural language processing machine learning model is tuned. A workflow to be implemented by a chatbot to complete a task-oriented dialog is received. A representation of one or more plan steps based on a current progression state of the workflow is dynamically determined. At least the representation of the one or more plan steps is provided as an input to the pre-trained natural language processing machine learning model to guide the pre-trained natural language processing machine learning model. An output action and/or an utterance for the task-oriented dialog is received from the pre-trained natural language processing machine learning model.,"['G06N3/0455', 'G06F40/35', 'G06F40/40', 'G06N3/006', 'G06N3/044', 'G06N3/045', 'G06N3/08']"
US12254420B1,Point-of-interest recommendation method based on temporal knowledge graph,"The disclosure discloses a point-of-interest recommendation method based on a temporal knowledge graph. Based on the historical behavior trajectory of the user and multimodal information, the disclosure constructs a dynamic temporal knowledge graph and a static group knowledge graph, which are used to learn interest preferences of the user that change over time and stable features that do not change over time, respectively. At the same time, the disclosure uses deep learning methods to build a point-of-interest recommendation model, which can extract a point-of-interest fusion feature representation and a user fusion feature representation from the two knowledge graphs in combination with a user review sentiment embedding sequence to accurately predict a point of interest most likely to be visited by the target user at a next moment. The disclosure has the characteristics of high precision and strong scalability, and can provide support for personalized user behavior trajectory prediction.","['G06F16/9537', 'G06N5/022', 'G06F16/367', 'G06F16/9535', 'G06F18/251', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
CN113053367B,"Speech recognition method, speech recognition model training method and device","The application discloses a voice recognition method, a voice recognition model training method and a voice recognition model training device, and relates to the technical fields of artificial intelligence, deep learning and voice recognition. The specific implementation scheme is as follows: acquiring voice to be recognized; inputting the voice to be recognized into a dialect acoustic model to obtain a phoneme posterior probability distribution and a dialect acoustic vector corresponding to a characteristic sequence of the voice to be recognized; decoding the voice to be recognized according to the phoneme posterior probability distribution corresponding to the feature sequence and the trained dialect statistical language model to obtain N candidate word sequences; inputting the N candidate word sequences into a trained first neural network language model to obtain N first text vectors, and inputting the N candidate word sequences into a trained second neural network language model to obtain N second text vectors; and determining a voice recognition result of the voice to be recognized from the N candidate word sequences according to the dialect acoustic vector, the N first text vectors and the N second text vectors.","['G10L15/063', 'G10L15/16', 'G10L15/22', 'G10L15/26']"
CN118862875A,A Chinese event extraction method based on pre-trained language model,"The invention relates to the field of event extraction, and discloses a Chinese event extraction method based on a pre-training language model. Firstly, the invention provides a method for constructing an event extraction model based on a pre-training language model, which comprises the following steps: re-dividing the event extraction task, and respectively constructing a sentence-level event detection model and an event element labeling model based on the high-quality pre-training language model; secondly, the invention provides an event extraction method based on a pre-training language model, which comprises the following steps: and carrying out event classification on the sample through the sentence-level event detection model, and carrying out element labeling on the sample through the event element labeling model. The method can effectively extract the event on the Chinese data, optimize the problem of over-fitting caused by insufficient data quality in the current event extraction model construction process through multi-stage data enhancement, and improve the accuracy of event extraction.","['G06F40/279', 'G06F16/353', 'G06F40/166', 'G06F40/242', 'G06F40/284', 'G06F40/289', 'G06F40/295', 'G06N3/047', 'G06N3/048', 'G06N3/08']"
CN112308326B,Biological network link prediction method based on meta-path and bidirectional encoder,"The invention belongs to the field of computer science, and discloses a biological network link prediction method based on a meta-path and a bidirectional encoder. Firstly, constructing a multi-source heterogeneous medicine information network, and designing various semantic paths for sequence sampling to form a large-scale semantic information base; secondly, organically fusing a depth Transformer coder and a mask language model (masked language model) to design a depth bidirectional coding characterization model and effectively extract a low latitude characterization vector of each node; finally, a biological link prediction such as a disease-protein association relation, a protein-drug interaction, a drug-side effect association relation and the like is carried out by utilizing an Inductive matrix completion (Inductive matrix completion) technology, and a drug research and development technology system of disease-target-drug-side effect is further completed.","['G06Q10/04', 'G06F40/30', 'G06N3/08', 'G16B20/00', 'G16B30/10', 'Y02A90/10']"
CN118841002B,"Audio answer scoring method, device, equipment, storage medium and product","The application provides an audio response scoring method, device, equipment, storage medium and product, wherein the method comprises the steps of obtaining target response audio and a first task instruction, wherein the first task instruction comprises description information of a scoring task, question information corresponding to the target response audio and scoring rules, inputting the target response audio and the first task instruction into a pre-trained multi-modal large language model, outputting scoring results of the target response audio by the multi-modal large language model, and executing a first training task and a second training task to obtain the multi-modal large language model, wherein the first training task is used for scoring the first sample response audio based on a transcribed text of the first sample response audio, and the second training task is used for determining the scoring of the first sample response audio by processing the first sample response audio, and the scoring results of the first training task are used as scoring labels of the second training task.","['G10L15/01', 'G10L15/063', 'G10L15/16', 'G10L15/18', 'G10L15/1815', 'G10L15/22', 'G10L15/26', 'G10L2015/223']"
CN114970506B,Grammatical error correction method and system based on multi-granularity grammatical error template learning and fine-tuning,"The invention discloses a grammar error correction method and system based on multi-granularity grammar error template learning fine adjustment. The method of the invention respectively executes the template learning process of the correct and incorrect judgment task, the template learning process of the incorrect type prediction task and the template learning process of the modification mode prediction task in the pre-training language model. And then calculating training loss of the pre-training language model by using three task losses through multi-task learning, and adjusting model parameters of the pre-training language model by using the training loss to obtain the fine-tuned pre-training language model. Then, training a grammar correction model by using the fine-tuned pre-training language model and the generated grammar correction method of fusing the pre-training language model. And finally, inputting the statement to be corrected into a grammar correction model to obtain a correct statement of the statement to be corrected. The invention considers the dependence and interaction among three granularity grammar error correction information through multitask learning, and provides rich and multi-angle grammar error correction information for the grammar error correction model.","['G06F40/253', 'G06F16/35', 'G06F40/30', 'G06N3/08']"
CN111680484B,Answer model generation method and system for visual general knowledge reasoning question and answer,"The invention provides a method and a system for generating a visual general knowledge reasoning question-answering model, wherein the method comprises the following steps: s1, corpus preprocessing, generating text semantic vectors for inquiry and response, and generating image target characteristic vectors for images. And S2, fusing the text semantic vector and the image target characteristic vector. S3, response modeling is carried out, and vector representation r of each candidate response is obtained i . S4, representing the vector of each candidate response by correlation comparison to obtain a correlation response vector z i . S5, utilizing the relevance response vector z i And (5) constructing a classifier and training a model. By the method and the device, the generated model reduces the complexity of the visual common sense reasoning question-answer task, and other response information is referred in response, so that the machine can select the optimal answer more easily, and the accuracy of the model is greatly improved.","['G06F40/186', 'G06F18/2411', 'G06F18/251', 'G06F40/30', 'Y02D10/00']"
US20230360388A1,"Training a generative artificial intelligence / machine learning model to recognize applications, screens, and user interface elements using computer vision","Techniques for training a generative artificial intelligence (AI) / machine learning (ML) model to recognize applications, screens, and UI elements using computer vision (CV) and to recognize user interactions with the applications, screens, and UI elements are disclosed. Optical character recognition (OCR) may also be used to assist in training the generative AI/ML model. Training of the generative AI/ML model may be performed without other system inputs such as system-level information (e.g., key presses, mouse clicks, locations, operating system operations, etc.) or application-level information (e.g., information from an application programming interface (API) from a software application executing on a computing system), or the training of the generative AI/ML model may be supplemented by other information, such as browser history, heat maps, file information, currently running applications and locations, system level and/or application-level information, etc.","['G06V10/82', 'G06F11/3438', 'G06V10/74', 'G06V10/774', 'G06V30/10', 'G06V40/193', 'G06V40/20', 'G06F9/451', 'G06F9/45529', 'G06N3/044', 'G06N3/0455', 'G06N3/0475', 'G06N3/084', 'G06V2201/02']"
US11822558B2,Efficient index lookup using language-agnostic vectors and context vectors,"Technology is described herein for searching an index, including operations of: obtaining a source data item; generating a source context-supplemented vector based on the source data item; and searching the index to find one or more target context-supplemented vectors that are determined to match the source context-supplemented vector. Each context-supplemented vector, which is associated with a particular data item, is made up of two parts: a language-agnostic vector and a context vector. The language-agnostic vector expresses the meaning of the particular data item in a manner that is independent of a natural language that is used to express the particular data item, while the context vector expresses a context associated with the formation of the particular data item. More generally, the technology's use of context vectors allows it to perform index search operations in a more efficient manner, compared to a search engine that does not use context vectors.","['G06F16/24575', 'G06F16/9024', 'G06F16/2237', 'G06F16/3344', 'G06F16/3347', 'G06F16/90332', 'G06F16/951', 'G06F16/9538', 'G06F40/30', 'G06N20/00', 'G06N3/045', 'G06N3/084', 'G06N3/094']"
WO2025025290A1,Knowledge fusion multi-modal interaction method and apparatus based on improved alignment method,"A knowledge fusion multi-modal interaction method and apparatus based on an improved alignment method. The method comprises: acquiring multi-modal data (S101); constructing a query transformation model (S102); on the basis of a multi-stream feature extraction method, performing improved processing on a vision transformer to obtain a multi-scale vision transformer (S103); on the basis of a joint text method, training the multi-scale vision transformer to obtain a target vision transformer (S104); freezing the target vision transformer, and on the basis of the target vision transformer, performing vision-text alignment training processing on the query transformation model to obtain a target query transformation model (S105); connecting the target query transformation model to a language model to obtain a knowledge fusion multi-modal interaction model (S106); and inputting the multi-modal data into the knowledge fusion multi-modal interaction model to obtain an interaction result (S107). In the embodiments of the present invention, the knowledge fusion multi-modal interaction model aligns visual representation with text representation, so that the mutual information between the visual representation and the text representation can be maximized, and the present invention can be widely applied to the technical field of artificial intelligence.","['G06F18/251', 'G06F18/253', 'G06F40/284', 'G06N3/0455', 'G06N3/0464', 'G06N3/0499', 'G06N3/082', 'G06V20/46']"
US20240004638A1,Systems and methods for detection of software vulnerability fix,"Methods and systems are described for detecting and reporting a vulnerability fix in a code repository. A commit obtained from the code repository is preprocessed to generate file-level token sequences each representing a file-level code change for respective files. Respective file-level code change embedding vectors are generated by inputting each file-level token sequence into a transformer model, each file-level code change embedding vector being a vector representation of the file-level code change for the respective file. The file-level code change embedding vectors are combined into a commit-level code change embedding vector that represents all code changes contained in the commit. A predicted commit-level vulnerability fix score is generated by inputting the commit-level code change embedding vector into a classifier. A vulnerability fix report is outputted, containing an identification of the commit and the predicted commit-level vulnerability fix score.","['G06F8/71', 'G06F8/65', 'G06F21/577', 'G06N3/0455', 'G06N3/084', 'G06F2221/033', 'G06F8/427']"
US20240338746A1,Generative content based on user session signals,"An online system employs real-time and pre-generated images for recommendation. The system leverages generative machine-learning models, such as diffusion models, to generate images dynamically. The selection and creation of these images rely upon user data and session data, which are collected during a user's application session. These data are employed to generate a text prompt string, which directs the image generation process. For instances where real-time computation may be a resource constraint, the system utilizes pre-generated images linked to user-context clustersâ€”data set groupings related to user characteristics and session context. This method enables the system to present tailored recommendations to the user, making use of both dynamic generation and pre-existing image resources, thereby optimizing the balance between customization, computational resources, and latency.","['G06Q30/0631', 'G06Q30/0643']"
CN113221567A,Judicial domain named entity and relationship combined extraction method,"The invention discloses a named entity and relationship joint extraction method in the judicial field, which is an entity relationship extraction method of a BILSTM network and attention mechanism set based on a BERT pre-training language model, realizes joint learning of two tasks through parameter sharing, and fully utilizes the relation between the tasks to optimize a result. Selecting a BERT pre-training language model training word vector to complete the conversion work of the data set word vector; then, acquiring more complete context feature information by using a BILSTM neural network, thereby extracting the text depth word vector features; and finally, acquiring a class label of the character through a softmax classifier to realize entity identification, and judging the association relationship between the current character and the previous character by using an attention mechanism to realize the combined extraction of the entity and the multiple relationships.","['G06F40/295', 'G06F16/353', 'G06F40/30', 'G06N3/044', 'G06N3/048']"
US12299015B2,Generative summarization dialog-based information retrieval system,"Embodiments of the disclosed technologies include generating a search prompt based on an input portion of an online dialog involving a user of a computing device. The search prompt includes a dialog summarization instruction configured to instruct a generative artificial intelligence model to generate and output a dialog summary. The search prompt is sent to a first generative model. In response to the search prompt, a search query is generated and output by the first generative model based on the dialog summary. The search query is sent to a search system. Search result data is determined based on an execution of the search query by the search system. At least some of the search result data is included in an output portion of the online dialog. The output portion is configured to be displayed at the computing device in response to the input portion of the online dialog.","['G06F16/3329', 'G06F16/243', 'G06F16/334', 'G06F16/338', 'G06F16/90332', 'G06F16/9535']"
CN116702716A,Method and device for automatically rewriting document content,"The application discloses a method and a device for automatically rewriting document contents. The method for automatically rewriting the document content comprises the following steps: acquiring document information to be rewritten; obtaining a grade to be rewritten; and according to the grade to be rewritten, the document information to be rewritten is rewritten, so that the post-rewriting document information is formed. The method for automatically rewriting the document contents provided by the application rewrites the document information to be rewritten according to the rewriting level selected by the user, thereby helping a document writer save a great deal of time, and the written document is moistened, so that smooth and diversified expression is generated on the premise of ensuring semantic consistency, the document writer is helped to diverge thinking, and the work is completed more efficiently.","['G06F40/166', 'G06F40/30', 'G06Q50/18', 'Y02D10/00']"
US20240062011A1,Techniques for using named entity recognition to resolve entity expression in transforming natural language to a meaning representation language,"Techniques are disclosed herein for using named entity recognition to resolve entity expression while transforming natural language to a meaning representation language. In one aspect, a method includes accessing natural language text, predicting, by a first machine learning model, a class label for a token in the natural language text, predicting, by a second machine-learning model, operators for a meaning representation language and a value or value span for each attribute of the operators, in response to determining that the value or value span for a particular attribute matches the class label, converting a portion of the natural language text for the value or value span into a resolved format, and outputting syntax for the meaning representation language. The syntax comprises the operators with the portion of the natural language text for the value or value span in the resolved format.","['G06F40/295', 'G06F40/211', 'G06F40/284', 'G06F40/35']"
CN113159168B,Pre-training model accelerated reasoning method and system based on redundant word deletion,"The embodiment of the invention provides a pre-training model accelerated reasoning method and a system based on redundant word deletion, wherein the method comprises the following steps: fine tuning is carried out on a pre-training language model by using a sentence sequence and a sample label of a given downstream task to obtain a fine-tuned initial model; adding a plurality of word selection layers in the fine-tuned initial model, and only keeping words passing through the word selection layers from the input words to enter the next layer in the process of transmitting words forward layer by layer in the fine-tuned initial model; the word selection layer is obtained by training based on sample data and a predetermined word information amount label. The accelerated reasoning method of the pre-training language model provided by the embodiment of the invention can not only reduce the resource consumption of the pre-training language model during reasoning, but also expand the method to long text processing to obtain better effect. Meanwhile, the method can quickly obtain the model with better relative performance under different accelerated speeds, and has good practicability.","['G06F18/2415', 'G06F40/284', 'G06N3/048', 'G06N3/082', 'G06N5/04']"
US12346673B1,Large language models utilizing element-wise operation-fusion,"Techniques for using a quantized and/or fused model are described. In some examples, a service is to receive a request to use a trained model, the request including input data; apply a trained model to the input data, the application of the trained model includes fusing one or more matrix multiplication operations with element-wise operations; and output a result from the trained model.","['G06F8/35', 'G06F40/20']"
WO2022188584A1,Similar sentence generation method and apparatus based on pre-trained language model,"The present application provides a similar sentence generation method and apparatus based on a pre-trained language model. The method comprises: acquiring a sentence to be processed; inputting, into a trained generative model, the sentence to be processed, so as to acquire a plurality of candidate similar sentences; generating a plurality of discriminative sentence pairs according to the sentence to be processed and the plurality of candidate similar sentences; and inputting the plurality of discriminative sentence pairs into a trained discriminative model, so as to acquire a discrimination result, and acquiring a target similar sentence from among the plurality of candidate similar sentences according to the discrimination result.","['G06F40/194', 'G06F18/22', 'G06F40/211', 'G06F40/30']"
CN117218725A,Real-time sign language recognition and translation system and method based on edge equipment,"The invention discloses a real-time sign language recognition and translation system and method based on edge equipment, comprising the following steps: the video data acquisition module is used for acquiring sign language video data; a video pre-processing module comprising: the device comprises a video frame size adjustment module, a data normalization module and a video frame graying module; the sign language model module is used for obtaining a sign language model; the structure re-parameterization module optimizes the sign language model obtained by the sign language model module to obtain an optimized sign language model; and ensuring that the output of the optimized model is unchanged. According to the invention, the sign language key local area characteristics and the global frame characteristics are extracted, the characteristics are constructed into an area perception space-time diagram, the sign language characteristics are extracted by utilizing a shallow layer diagram convolution network, a sign language model is trained based on the extracted characteristics, a structural re-parameterization technology is adopted to optimize the model, and the optimized model is adopted to recognize the sign language video as a word sequence and translate the word sequence into a natural language text.",[]
CN116720004B,"Recommendation reason generation method, device, equipment and storage medium","The application discloses a recommendation reason generation method, a device, equipment and a storage medium, which relate to the technical field of recommendation in the AI field. In addition, the problem that the object ID and the embedded token of the token in the original language model are not in the same token space caused by introducing a new token ID is effectively avoided, and the difficulty of fine tuning the language model is greatly reduced, so that the text generating capability of the large language model is better exerted.","['G06F16/9535', 'G06F16/3344', 'G06F40/289', 'G06F40/30']"
US20230351115A1,Document image processing including tokenization of non-textual semantic elements,"A method of document image processing comprises, based on at least a document page image, generating a plurality of semantic tokens that includes a plurality of word tokens and a plurality of special tokens. Each special token among the plurality of special tokens represents a non-textual semantic element of the document image, and generating the plurality of semantic tokens includes predicting, for each special token among the plurality of special tokens, a token type of the special token. The method also comprises generating, for each semantic token among the plurality of semantic tokens, a corresponding semantic token embedding among a plurality of semantic token embeddings; and applying a trained model to process an input that is based on the plurality of semantic token embeddings and a plurality of visual token embeddings based on at least the document page image to generate a semantic processing result.","['G06F40/30', 'G06F40/284', 'G06F40/40', 'G06V10/82', 'G06V30/10', 'G06V30/274', 'G06V30/413', 'G06V30/414']"
CN114141238A,Voice enhancement method fusing Transformer and U-net network,"The invention discloses a voice enhancement method fusing a Transformer and a U-net network, which comprises the following steps: s1, collecting an original clean voice data set and a noisy voice data set, and dividing the collected data sets into a training set, a verification set and a test set; s2, constructing a voice enhancement model fusing a Transformer and a U-net network; s3, training the voice enhancement model constructed in the step S2 by using the training set and the verification set in the step S1; and S4, inputting the test set in the step S1 as the voice signal to be enhanced into the trained voice enhancement model, and outputting a clean voice signal. The invention adds a Transformer module in the U-net network, effectively extracts the local and global context characteristic information; and simultaneously, three types of loss functions of time domain loss, time-frequency domain loss and perceptual loss are used for training the voice enhancement network together, so that higher voice intelligibility and perceptual quality are obtained.","['G10L15/063', 'G10L15/183', 'G10L19/008', 'G10L19/24', 'G10L21/02', 'G10L21/0224', 'G10L21/0232', 'G10L2015/0635']"
CN113919366A,Semantic matching method and device for power transformer knowledge question answering,"The invention discloses a semantic matching method and device for power transformer knowledge question answering. The semantic matching method for the power transformer knowledge question answering comprises the following steps: performing semantic intention and semantic slot position joint identification on a question sentence input by a user by using a trained semantic identification model, and determining the query intention of the user and slot position information in the question sentence, wherein the slot position information comprises a slot position type, a slot position name and a slot position value; linking the slot position value with an entity in a knowledge map library to determine a standard entity name, an attribute name or a relationship name which is corresponding to the slot position value and belongs to the same slot position type; converting the determined standard entity name, attribute name or relationship name into a query statement of a search engine, and acquiring an answer matched with a question statement input by a user based on the query statement; and displaying answers to the user by adopting a corresponding display strategy according to the query intention of the user.","['G06F40/35', 'G06F16/288', 'G06F16/3329', 'G06F16/355', 'G06N3/044', 'G06N3/08', 'G06Q50/06']"
WO2024077002A2,Natural intelligence for natural language processing,"Backpropagation enabled neural network optimization, but is still expensive at scale. An efficient alternative for neural networks must reduce scaling costs and provide high-efficiency optimizations. The presented solution for feed-forward layers is extended compositionally, for transformers containing feed-forward and self-attention layers. These train complex models modularized by self-attentive feed-forward units (SAFFUs), defining efficient architectures that generalize over much less data. Results demonstrate outperformance of backpropagation (alone), compared to application of backpropagation after explicit solutions, which uncover better optima from less data. Ablations train a roadmap of 245 transformers to determine specifications for the SAFFU- transformer, showing several architectural variants appear quite performant. The most performant models appear to not be the most parameterized. Summarizing, well-generalized SAFFU models are efficiently reachable, their architectural explorability via explicit solutions is cheaper and more robust than by backpropagation, and their explicit solutions could be useful for low-resource hardware, where Al might be embodied.","['G06F40/284', 'G06F40/216', 'G06F40/237', 'G06F40/44', 'G06F40/56', 'G06N3/0464', 'G06N3/084']"
US20230290187A1,"Liveness detection method and apparatus, electronic device, and storage medium","A liveness detection method includes: obtaining a reflected audio signal and video data of a object in response to receiving a liveness detection request; performing signal processing and time-frequency analysis on the reflected audio signal to obtain time-frequency information of a processed audio signal, and extracting motion trajectory information of the object from the video data; respectively extract features from the time-frequency information and the motion trajectory information to obtain an audio feature and a motion feature of the object; calculating first global attention information of the object according to the audio feature, and calculating second global attention information of the object according to the motion feature; and fusing the first global attention information with the second global attention information to obtain fused global information, and determining a liveness detection result of the object based on the fused global information.","['G01S15/88', 'G06V40/45', 'G01S15/58', 'G01S15/86', 'G01S7/539', 'G06F18/253', 'G06T7/246', 'G06V10/431', 'G06V10/776', 'G06V10/806', 'G06V10/82', 'G06V20/46', 'G06V20/95', 'G06V40/171', 'G06V40/40', 'G06T2207/10016', 'G06T2207/30201', 'G06T2207/30241', 'G06V40/172']"
US20240184834A1,Systems and methods for a language model-based customized search platform,"Embodiments described herein provide systems and methods for a customized search platform that provides users control and transparency in their searches. The system may use a ranker and parser to utilize input data and contextual information to identify search applications, sort the search applications, and present search results via user-engageable elements. The system may also use input from a user to personalize and update search results based on a user's interaction with user-engageable elements.","['G06F16/951', 'G06F16/9532', 'G06F16/9535', 'G06F16/9538', 'G06F16/954', 'G06N3/0455', 'G06N3/0475', 'G06N3/084', 'G06N3/048', 'G06N3/09']"
US12292915B1,Security for generative models using attention analysis,"Devices and techniques are generally described for security threat mitigation for generative machine learning models. In some examples, first prompt data including first data associated with a first natural language input and a first span may be determined. An LLM may determine first plan data using the first prompt data. The first plan data may include a call to the first API. A first classifier model may determine a first trust score for the first span. A first attention score may be determined for the first span and the first action plan. Second plan data may be generated based on at least one of the first trust score and the first attention score or the second trust score and the second attention score.","['G06F16/35', 'G06F16/383', 'G06F40/30', 'G06F40/40']"
CN111881726B,Living body detection method and device and storage medium,"The invention provides a method, a device and a storage medium for detecting living organisms, wherein the method comprises the following steps: acquiring image frame data of a lip region of an object to be detected and audio characteristic data of the object to be detected in a first video file of the object to be detected; determining a target frame sequence position by adopting a preset feature fusion model based on the image frame data and the audio feature data; the target frame sequence position is used for representing a target starting frame sequence position and/or a target ending frame sequence position of an image frame with lip language in the image frame data; intercepting the image frame data based on the target frame sequence position to obtain target image frame data; performing lip language identification on the target image frame data to obtain a first identification result, and performing audio identification on the audio characteristic data to obtain a second identification result; and performing living body detection on the object to be detected based on the first recognition result and the second recognition result. The embodiment of the invention can improve the precision of the living body detection.","['G06V40/45', 'G06V40/20', 'G10L15/22', 'G10L15/25']"
US20240402999A1,Systems and methods for generating code using language models trained on computer code,"Disclosed herein are methods, systems, and computer-readable media for generating computer code based on natural language input. In an embodiment, a method may comprise one or more of: receiving a docstring representing natural language text specifying a digital programming result; generating, using a trained machine learning model, and based on the docstring, a computer code sample configured to produce respective candidate results; causing the computer code sample to be executed; identifying, based on the executing, a computer code sample configured to produce a particular candidate result associated with the digital programming result; performing at least one of outputting, via a user interface, the identified computer code sample, compiling the identified computer code sample, transmitting the identified computer code sample to a recipient device, storing the identified computer code sample, and/or re-executing the identified computer code sample.","['G06F8/30', 'G06F8/33', 'G06F8/73']"
CN117149974A,A knowledge graph question and answer method optimized for subgraph retrieval,"The invention discloses a knowledge graph question-answering method based on subgraph retrieval optimization, which comprises the steps of firstly constructing a data set required by entity identification and entity link model training based on a question-answering data set; secondly, building an entity identification model by utilizing the fine-tuned pre-training language model; then recalling a candidate entity list from the mapping dictionary and the knowledge graph according to the identified question entity, and realizing an entity link model fusing multiple features through entity disambiguation; then adopting a multi-strategy optimized cluster searching process, and providing a sub-graph searching algorithm based on relation merging and entity sorting; and finally, building a question-answer matching model based on the pre-training language model and the twin network architecture, and sequencing candidate answer paths in the question related subgraph to obtain final answers, so as to realize the intelligent question-answer function of the knowledge graph.","['G06F16/3329', 'G06F16/3344', 'G06F16/3347', 'G06F16/367', 'G06N3/0442', 'G06N3/0455', 'G06N3/047', 'Y02D10/00']"
CN117520499A,"Training method, using method, device, equipment and medium of general language model","The application discloses a training method, a using method, a device, equipment and a medium of a general language model, and belongs to the technical field of artificial intelligence. The method comprises the following steps: acquiring sample data of at least one mode, wherein the sample data of at least one mode is marked with label information, and the label information comprises sample credibility; inputting sample data of at least one mode into a universal language model, and obtaining prediction information corresponding to the sample data of at least one mode through a target judgment network in the universal language model, wherein the prediction information comprises prediction credibility, and the target judgment network is used for enabling the universal language model to have the capability of credibility judgment based on scenes; model parameters of the generic language model are trained based on the prediction information and the tag information. The scheme can be applied to the fields of large language models and multi-mode large models, can be applied to natural language generation and question-answering system scenes, and can be used for judging the credibility of the predicted data corresponding to the input data in the target scene.","['G06F16/3329', 'G06F16/3344', 'G06N3/0475']"
EP4352661A1,Automatic xai (autoxai) with evolutionary nas techniques and model discovery and refinement,"An exemplary model search may provide optimal explainable models based on a dataset. An exemplary embodiment may identify features from a training dataset, and may map feature costs to the identified features. The search space may be sampled to generate initial or seed candidates, which may be chosen based on one or more objectives and/or constraints. The candidates may be iteratively optimized until an exit condition is met. The optimization may be performed by an external optimizer. The external optimizer may iteratively apply constraints to the candidates to quantify a fitness level of each of the seed candidates. The fitness level may be based on the constraints and objectives. The candidates may be a set of data, or may be trained to form explainable models. The external optimizer may optimize the explainable models until the exit conditions are met.","['G06N3/126', 'G06N3/08', 'G06F18/2163', 'G06F18/217', 'G06F18/22', 'G06N5/025', 'G06N3/006', 'G06N3/045', 'G06N3/048', 'G06N3/049', 'G06N3/084', 'G06N5/045', 'G06N7/01']"
AU2011200795B2,Systems and methods for asset condition monitoring in electric power substation equipment,"SYSTEMS AND METHODS FOR ASSET CONDITION MONITORING IN ELECTRIC POWER SUBSTATION EQUIPMENT Certain embodiments of the invention may include systems and methods for asset condition monitoring in electrical power substation equipment. According to an example embodiment of the invention, a method is provided for detecting behavior in electrical substation equipment. The method may include receiving realtime data associated with a plurality of parameters associated with the substation equipment, storing at least a portion of the realtime data over a period of time, wherein the stored data comprises historical data, comparing the realtime and historical data, and generating a report based at least in part on a comparison of the realtime data and historical data.",['G05B23/0227']
CN116050496A,"Method, device, medium, and equipment for determining image description information generation model","The disclosure relates to the technical field of image processing, in particular to a method and a device for determining a picture description information generation model, a computer readable storage medium and electronic equipment. The method comprises the following steps: acquiring a first reference model and a model to be trained, wherein the model to be trained comprises a single-mode image encoder, a single-mode text encoder, a multi-mode text encoder and a multi-mode text decoder; determining a first penalty function based on the single-mode image encoder, the single-mode text encoder, the multi-mode text encoder, and the multi-mode text decoder; training the model to be trained according to the first loss function and the first reference model to obtain a target model to be trained; determining a picture description information generation model based on a single-mode image encoder and a multi-mode text decoder in a target model to be trained; wherein the model parameters of the model to be trained are less than the model parameters of the first reference model. The picture description information generation model obtained by the method has the advantages of less parameters and high accuracy.","['G06N3/08', 'G06V10/774', 'G06V10/82']"
CN113672708A,"Language model training method, question and answer pair generation method, device and equipment","The application is applicable to the technical field of information processing, and provides a language model training method, a question and answer pair generation device and equipment. The language model training method comprises the steps of obtaining a training text; generating a plurality of training text sequences according to the training texts; each training text sequence comprises the training text, answer texts contained in the training text and question texts corresponding to the answer texts; training the language model based on the supervised learning algorithm by taking the training text and the answer text in each training text sequence as features and taking the problem text in each training text sequence as a label to obtain the trained target language model. And when the target language model is obtained based on the language model training method, the method is suitable for obtaining an application scene of the question corresponding to the answer according to the answer prediction.","['G06F16/3329', 'G06F40/211', 'G06F40/295', 'G06F40/30']"
CN114417785A,"Knowledge point labeling method, model training method, computer equipment and storage medium","The embodiment of the application provides a knowledge point labeling method, a model training method, computer equipment and a storage medium, wherein the labeling method comprises the following steps: acquiring a target text; inputting the target text into a preset deep language model to obtain a text content representation corresponding to the target text; acquiring knowledge point representations corresponding to a preset knowledge point tag set, wherein the knowledge point representations are obtained by processing knowledge point tags in the knowledge point tag set based on a deep language model; and matching the text content representation and the knowledge point representation to obtain a knowledge point label corresponding to the target text. Based on a deep language model, deeper semantic modeling can be carried out on a target text, and the obtained text content representation can be better adapted to a multi-label classification task; the knowledge point representation contains the information of a knowledge point system, and the recognition effect of the knowledge points can be improved by combining the knowledge point representation with the text content representation.","['G06F40/117', 'G06F16/35', 'G06F40/30']"
CN110209822A,"Sphere of learning data dependence prediction technique based on deep learning, computer","The invention belongs to computer network data electric powder prediction, a kind of sphere of learning data dependence prediction technique based on deep learning, computer are disclosed, the paper and patent data of disclosed general data, sphere of learning are collectedï¼›Sphere of learning term vector is trained on academic corpus using the term vector technology of deep learningï¼›Semantic relevant other fields can be predicted according to term vector for given field, realize the prediction of related sphere of learning.System of the invention includes: data collection module, for collecting disclosed dataï¼›Term vector training module trains sphere of learning term vector for the term vector technology using deep learning on academic corpusï¼›Sphere of learning prediction module realizes the prediction of related sphere of learning for that can predict semantic relevant other fields according to term vector for given field.The present invention realizes the relevant quick and Accurate Prediction in field by sphere of learning term vector of the construction based on deep learning, by term vector.","['G06F16/35', 'G06F18/214', 'G06F40/289']"
CN117390139B,Method for evaluating working content accuracy of substation working ticket based on knowledge graph,"The invention discloses a method for evaluating the accuracy of working contents of a power transformation working ticket based on a knowledge graph, which can automatically analyze and evaluate the accuracy of the working contents of the power transformation working ticket, converts equipment information, operation steps, working requirements and the like in the working ticket into a computable and inferable form through natural language processing and machine learning technology, realizes the accuracy evaluation of the working contents, supports efficient query and reasoning by utilizing a graph database of the knowledge graph, rapidly retrieves and analyzes knowledge and information related to the working contents, realizes standardization and consistency evaluation of the power transformation working ticket based on the method of the knowledge graph, performs consistency check on the working contents by establishing a unified knowledge model and rule, ensures that the equipment information, the operation steps and the working requirements in the working ticket accord with unified standards and specifications, and expands and updates the knowledge graph according to new data and knowledge, and continuously evolves and perfects the knowledge graph.","['G06F16/3344', 'G06F16/367', 'G06F40/295', 'G06F40/30', 'G06N3/0464', 'G06N3/08', 'G06N5/022', 'Y04S10/50']"
CN113535957B,"Conversation emotion recognition network model system based on dual knowledge interaction and multitask learning, construction method, equipment and storage medium","The application discloses a dialogue emotion recognition network model based on dual knowledge interaction and multitask learning, a construction method, electronic equipment and a storage medium, and belongs to the technical field of natural language processing. The problem that the direct interaction of the words and knowledge is neglected by the existing Emotion Recognition in conversion (ERC) model is solved; the ERC task can only be provided with limited emotional information by using an auxiliary task which is weakly related to the main task. The present application utilizes common sense knowledge in a large-scale knowledge graph to enhance word-level representation. A self-matching module is used to integrate the knowledge representation and the utterance representation, allowing complex interaction between the two. And taking the emotion polarity intensity prediction task at the phrase level as an auxiliary task. The label of the auxiliary task is from the emotion polarity intensity value of the emotion dictionary, is obviously highly related to the ERC task, and provides direct guidance information for emotion perception of the target utterance.","['G06F16/35', 'G06F16/367', 'G06F40/242', 'G06F40/284', 'G06N3/08', 'G06N5/02']"
CN113947114B,Gas turbine rotor fault diagnosis method based on Transformer neural network and shaft trajectory,"The invention relates to a gas turbine rotor fault diagnosis method based on a transducer neural network and an axis track, which comprises the following steps: 1) Collecting original vibration signals of a rotor X, Y to synthesize an original axis track of the rotor, and purifying the original axis track by adopting a self-adaptive wavelet threshold method; 2) Performing image processing on the purified new axis track, converting the axis track into a single-channel image with consistent size, extracting image features of the axis track by adopting an independent component analysis method, and constructing a training sample; 3) Constructing a fault diagnosis model based on a transducer neural network, training through a training sample, realizing fault diagnosis of a gas turbine rotor according to the trained fault diagnosis model, and identifying the type of the rotor fault. Compared with the prior art, the method adopts the neural network at the front edge of the natural language processing field to improve the rotor fault diagnosis accuracy based on the axis track to a certain extent, improves the diagnosis efficiency, and has certain guiding significance on the operation and maintenance of the power plant.","['G06F2218/12', 'G06F18/2134', 'G06F18/2411', 'G06F18/2415', 'G06N3/045', 'G06N3/08']"
WO2024081075A1,Source code patch generation with retrieval-augmented transformer,A source code patch generation system uses the context of a buggy source code snippet of a source code program and a hint to predict a source code segment that repairs the buggy source code snippet. The hint is a source code segment that is semantically-similar to the buggy source code snippet where the similarity is based on a context of the buggy source code snippet. An autoregressive deep learning model uses the context of the buggy source code snippet and the hint to predict the most likely source code segment to repair the buggy source code snippet.,"['G06F11/362', 'G06F8/36', 'G06F8/65', 'G06F8/71', 'G06F8/75', 'G06N3/0455', 'G06N3/084', 'G06N3/0895', 'G06N3/09']"
US20230336823A1,Real-Time Adaptive Content Generation with Dynamic Sentiment Prediction,"A system to dynamically encode, analyze, and subsequently decode user sentiment to adaptively generate temporally coherent media in real-time, thereby eliciting intended sentiment in the user. The enclosed system design unites sentiment analysis and generative media frameworks within a variational autoencoding structure, thus facilitating a continuous feedback loop between the user and the media that persistently adapts to the user's evolving sentiment.","['H04N21/4668', 'G06F16/735', 'H04N21/4667']"
US20250232138A1,"Computer-generated content based on text classification, semantic relevance, and activation of deep learning large language models","The disclosure relates to systems and methods of automatically generating unique content including natural language text based on a corpus of previously generated response documents and discrete requirements defined in a requirements specification. The system may use generative stitching that includes multi-layer processes that execute to influence the generation of unique content including natural language text through an artificial intelligence (AI) language transformer model trained to output the content based on previously written material that is semantically relevant to the discrete requirements and is weighted against labeled attributes. The labeled attributes may determine the influence asserted against the language transformer, thereby generating unique on-target content that may be combined to create a computer-generated response document.","['G06F40/56', 'G06F40/40', 'G06F40/131', 'G06F40/169', 'G06F40/30', 'G06N20/00']"
US20250112878A1,Knowledge graph assisted large language models,"Techniques for a knowledge-graph system to use large language models (LLMs) to build knowledge graphs to answer queries submitted to a chatbot by users. The knowledge-graph system builds the knowledge graph using answers produced by an LLM for novel queries. The chatbot will continue to use the LLM to answer novel queries, but the chatbot may harness the knowledge graph to answer repeat questions to gain various efficiencies over LLM-backed chatbots. For example, the knowledge-graph system may easily debug or otherwise improve the answers in knowledge graphs, store provenance information in knowledge graphs, and augment the knowledge graphs using other data sources. Thus, the reliability and correctness of chatbots will be improved as the bugs and inaccuracies in answers provided by the LLM will be corrected in the knowledge graphs, but the chatbots can still harness the abilities of LLMs to provide answers across various subject-matter domains.","['H04L51/02', 'G06F40/20', 'G06F40/35']"
WO2023108324A1,Comparative learning enhanced two-stream model recommendation system and algorithm,"A comparative learning enhanced two-stream model recommendation system and algorithm. A long-term preference interest of a user can be acquired, long-term and short-term preferences of the user can be combined, and dynamic preference data of the user is captured by using an implicit preference, thereby better improving the recommendation accuracy. The method comprises: firstly, performing learning on a time sequence feature in data by using the characteristics of a Transformer, so as to obtain a long-term interest of a user; then, learning and exploring, by using a GCN, feature information of a spatial structure during an article conversion process; and finally, combining, by means of location coding and global graph coding, the feature information obtained by the Transformer and the GCN, and also performing representation learning assistance of a model by using a comparative learning method.",['G06F16/9537']
CN113674734B,"Information query method and system based on voice recognition, equipment and storage medium","The application discloses an information query method, an information query system, computer equipment and a computer readable storage medium based on voice recognition. The technical scheme of the method comprises the following steps: the encoding step is used for inputting audio data, extracting characteristic values by using a transducer encoder, and outputting a two-dimensional characteristic value sequence; decoding: after carrying out streaming voice decoding and recognition by adopting a decoder combining a transducer and an N-gram based on a two-dimensional characteristic value sequence, screening and outputting M results of first text sorting, carrying out non-streaming voice decoding and recognition by adopting a transducer model based on the two-dimensional characteristic value sequence and M results of first matched text sorting, and outputting N results of second text sorting, wherein N and M are positive integers which are greater than or equal to 1, and N is less than or equal to M; and the assignment weighting step is used for carrying out assignment weighting based on the hot word dictionary in the text sorting result output by the decoding step and outputting an optimal query result. The invention improves the accuracy of voice recognition by adding the language model and the hotword weight function.","['G10L15/02', 'G06F16/3343', 'G06F16/338', 'G06F16/35', 'G10L15/063', 'G10L15/183', 'G10L15/26', 'G10L19/0018', 'Y02D10/00']"
US12008830B2,System for template invariant information extraction,"A system for template invariant information extraction. The system comprises of processor, a first neural network model and a second neural network model. The processor is configured to recognize and extract entities and location of the entities in the input document using the first neural network model. The processor is further configured to classify whether the input document belongs to at least a template of the documents of the first training dataset using the second neural network model. The second neural network model comprises a linear classifier configured to generate a plurality of confidence scores for the input document corresponding to a unique template of the documents of the first training dataset. A threshold value to classify the input document belonging to template of the documents of the first training dataset is determined, Classification is done by comparing the confidence score with the threshold value.","['G06F16/93', 'G06F40/186', 'G06F40/295', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06V30/416']"
CN117688560A,Semantic analysis-oriented intelligent detection method for malicious software,"The invention provides a semantic analysis-oriented intelligent malicious software detection method, which comprises the steps of constructing a semantic detection model, combining an XLNet big language model, a convolutional neural network, a two-way long-short-term memory neural network and a self-adaptive attention mechanism, extracting an API function call sequence of malicious software as a feature for training the model, and achieving the purpose of detecting the malicious software through static analysis of the feature; the model utilizes the characteristics of context semantics and relevance of an API function, uses an XLnet large language model as characteristic representation, uses a convolutional neural network and a two-way long-short-term memory neural network to extract the characteristics secondarily, inputs the extracted characteristics into a self-adaptive attention mechanism layer, carries out weight distribution on the extracted characteristics, further highlights key information, improves classification accuracy and reduces noise influence; experimental results show that the method is not influenced by the variation and the variation of malicious codes, and is superior to detection methods of traditional models and other large language models.","['G06F21/56', 'G06F40/289', 'G06F40/30', 'G06N3/0455', 'G06N3/0464', 'G06N3/08']"
CN118377871A,"Sensitive word auditing method, device, equipment and medium based on large language model","The embodiment of the application provides a sensitive word auditing method, device, equipment and medium based on a large language model. The method comprises the following steps: acquiring a text to be checked; sequentially carrying out M-level sensitive word matching processing on the text to be checked to obtain M-level sensitive word matching results; the M-level sensitive word matching result is whether a preset sensitive word of the level exists in the text to be checked or not; wherein M is an integer greater than or equal to 2; if the M-level sensitive word matching result is determined to be that the sensitive word with the level exists, determining the user intention of the text to be checked; and determining whether the text to be checked passes the check according to the user intention. The method can improve accuracy and efficiency of sensitive word auditing.","['G06F16/3329', 'G06F18/22']"
CN113468888A,Entity relation joint extraction method and device based on neural network,"The invention discloses a neural network-based entity relationship joint extraction method and device. Firstly, fusing a pre-trained ERNIE word vector, a CWV word vector and relative position information of words by using a single-layer Transformer network; secondly, a handshake labeling strategy is improved, and a vectorized entity type label is introduced to fully utilize entity type information; then, obtaining a candidate entity relation triple in the sentence by a label decoding method; and finally, indexing articles, sentences, entities and relationships by using the UCL according to the characteristics that the UCL national standard can efficiently organize the content and effectively associate the information. The method can directly extract the entity relationship triples on the sentences, avoid the problems of redundant entities, nested entities, overlapping relationships and the like, can quickly check the entity relationship by using a small amount of labeled data, and can objectively and normatively index the data.","['G06F40/295', 'G06F40/117', 'G06F40/126', 'G06F40/30', 'G06N3/04', 'G06N3/084']"
CN113779220B,A Multi-hop Question Answering Method for Mongolian Language Based on Three-Channel Cognitive Graph and Graph Attention Network,"A Mongolian multi-hop question-answering method based on a three-channel cognitive map and a graph annotating semantic network utilizes machine translation to translate Mongolian query questions into Chinese query questions; performing part-of-speech tagging after separating sentences and words of a Chinese query question; converting the part-of-speech tagged Chinese problem statement into a Chinese query statement, and inputting the Chinese query statement into a cognitive map server; the cognitive map server simulates a human cognitive system in cognition, performs parallel calculation by using three channels, and finally gives an answer with the highest probability through normalization, query sequencing and query selection, and returns a result. According to the invention, questions and answers with different requirements are carried out by virtue of the advantages of the three channels, the accuracy and the speed of a question and answer system in natural language processing are greatly improved, and meanwhile, the channels 1, 2 and 3 can be reused in translation and question and answer stages, so that the speed is improved by nearly one time, and the question and answer quality is improved as a whole.","['G06F16/3329', 'G06F40/295', 'G06F40/30', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
US20230186024A1,"Text Processing Method, Device and Storage Medium","Provided are a text processing method, a device and a storage medium, relating to a field of computer technology, and especially to a field of artificial intelligence, such as natural language processing and deep learning. The specific implementation scheme includes: performing text processing on first text, by using a text processing acceleration operator; and processing, in parallel and faster, content after the text processing, by using the text processing acceleration operator. Text processing and parallel acceleration are carried out by the text processing acceleration operator, which can improve the speed of text processing.","['G06F40/279', 'G06F40/205', 'G06F16/35', 'G06F16/355', 'G06F40/284', 'G06F40/289', 'G06F40/47', 'G06F40/53', 'G06N3/04', 'G06N3/0455', 'G06N3/08']"
CN115357728A,Transformer-based large model knowledge graph representation method,"The invention discloses a Transformer-based large model knowledge graph representation method, which comprises the following steps of: (1) Randomly sampling subgraphs including center triples from the knowledge graph to construct a mask subgraph sequence; (2) Extracting embedded representation of nodes in the mask subgraph sequence by using a Transformer, taking a multidimensional power of an adjacent matrix as structural information during extraction, and adding a coding vector of the structural information into an attention mechanism of the Transformer to obtain embedded representation of the mask nodes; (3) Performing word sense prediction on the embedded representation of the mask node by using a classifier to obtain a word sense prediction result; (4) constructing a loss function and optimizing model parameters; (5) And (5) completing the knowledge graph by using the parameter optimization model. The method can fully capture the structural information and the context semantic information in the knowledge graph.","['G06F16/367', 'G06F16/353', 'G06F40/30']"
US20230139347A1,Per-embedding-group activation quantization,A processor-implemented method for providing per-embedding-group activation quantization includes receiving sequential data at a first layer of a transformer neural network. The sequential data is processed via the first layer of the transformer neural network to generate an activation tensor. The activation tensor is split into multiple groups of embeddings. Each of the embeddings groups has a different set of quantization parameters. Each of the embedding groups is quantized separately based on the corresponding quantization parameters of the different set of quantization parameters. The quantized embedding groups are multiplied with a set of weights to generate an output.,"['G06F40/20', 'G06F40/30', 'G06F40/40', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/048', 'G06N3/063', 'G06N3/08', 'G06N3/084']"
CN112100351A,A method and device for constructing an intelligent question answering system through question generation data sets,"A method and apparatus for constructing an intelligent question-answering system through a question generation data set, by constructing a travel domain knowledge map; analyzing a question of a natural language proposed by a user, performing word segmentation and word vector training on the question, and adding a preset tourism field dictionary by using a jieba tool in the word segmentation process; performing entity extraction on the natural language question by using a Bert-BilSTM + CRF model; matching the extracted entity with an entity in a knowledge graph; if the knowledge graph has a matched entity, selecting the entity; if the knowledge graph has no matched entities, performing semantic similarity calculation, and selecting the closest entities; matching the selected entities and attributes with triples in a knowledge graph; and returning the corresponding attribute value as an answer of the question to the user. The invention also provides a device, a terminal and a readable storage medium for realizing the method. The invention can conveniently and accurately return the information required by the user.","['G06F16/3329', 'G06F18/24', 'G06F40/211', 'G06F40/295', 'G06F40/30', 'G06N3/045', 'G06N3/049', 'G06N3/08']"
US20250094464A1,Knowledge re-ranking techniques,"Techniques are disclosed herein for selecting document chunks that are most relevant to a query. The techniques include receiving a query and comparing a plurality of stored text passages to the query using a first similarity metric. Based on the comparison, a subset of the plurality of stored text passages that are most similar to the query are selected. A plurality of sentences from the subset of the plurality of stored text passages are identified. The identified sentences are ranked based on the query and a second similarity metric. A subset of the sentences are selected based on the ranking. The subset of the sentences or a derivative thereof are output in response to the query.","['G06F16/383', 'G06F16/316', 'G06F16/334', 'G06F16/3344', 'G06F40/289', 'G06F40/30', 'G06F40/40', 'G06F40/205']"
CN115329783A,Tibetan Chinese neural machine translation method based on cross-language pre-training model,"The invention provides a Tibetan Chinese neural machine translation method based on a cross-language pre-training model, and relates to the technical field of language translation. Preprocessing preset Tibetan-Chinese parallel data to obtain a corpus to be processed; synonym replacement and translation are carried out on the corpus in a data enhancement mode; segmenting the Tibetan-Chinese parallel corpus into words, segmenting the words by using a subword-nmt algorithm, segmenting all the words into sub-word units, reconstructing a new word list, and optimizing the new word list by using a VOLT (volatile organic language translation) model; training the Tibetan-Chinese parallel linguistic data by using a multilingual pre-training translation model comprising a plurality of language pairs in an mRASP model based on a transform-big neural network machine translation architecture to obtain a translation model; and evaluating the translation model by adopting different length penalty factors during decoding. It can improve the translation effect and make up the expression gap between languages.","['G06F40/58', 'G06F40/30', 'G06F40/42', 'G06N3/04']"
CN119274778A,A method for diagnosis and treatment of livestock and poultry parasitic diseases based on a large visual language model,"The invention discloses a diagnosis and treatment method for livestock and poultry intestinal parasitic diseases based on a visual language model, which is characterized in that a parasite image-text characteristic alignment data set is utilized to train the visual language model, so that the large language model has visual ability to parasites; then, constructing a diagnosis and treatment dialogue data set of the livestock and poultry parasitic diseases, training the LoRA fine-tuning visual language large model by using the diagnosis and treatment dialogue data set to obtain a diagnosis and treatment dialogue model, and realizing diagnosis and treatment of the livestock and poultry parasitic diseases by using the diagnosis and treatment dialogue model. The invention can improve the diagnosis and treatment efficiency of the parasitic diseases of the livestock and the poultry, reduce the misdiagnosis rate and optimize the diagnosis and treatment flow.","['G16H50/20', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N5/04', 'G06T7/0012', 'G06V10/26', 'G06V10/56', 'G06V10/765', 'G06V10/82', 'G16H10/60', 'G16H50/70', 'G06T2207/20081', 'G06T2207/20084', 'Y02A40/70']"
US20240281222A1,Artificial intelligence system for generating database code from natural language inputs,"An apparatus comprises at least one processing device that includes a processor coupled to a memory. The at least one processing device is configured to receive natural language input from one or more user devices relating at least in part to data of one or more databases, to apply the natural language input to an artificial intelligence (AI) system, to generate in the AI system database code for the one or more databases based at least in part on the natural language input, and to execute the generated database code against at least a portion of the one or more databases. The natural language input may be received in association with a database query, which is decomposed into a sequence of processing steps formulated in natural language using corresponding text templates, and from which one or more prompts are generated for application to the AI system.","['G06F8/35', 'G06F16/95']"
US20240273291A1,Generative collaborative publishing system,"Embodiments of the disclosed technologies include, in response to input of a first prompt to a generative language model, outputting, by the generative language model, a first document including a first piece of writing, where the first piece of writing is based on the first prompt. Feedback is received for the first document, where the feedback includes a rating for the first piece of writing. Using the generative language model, a second prompt different from the first prompt is generated, where the second prompt is based on the feedback. In response to input of the second prompt to the generative language model, the generative language model outputs a second document different from the first document, where the second document includes a second piece of writing based on the second prompt. The second document is published to a network.","['G06F40/279', 'G06F16/383', 'G06F40/56']"
US20240020546A1,Frozen Model Adaptation Through Soft Prompt Transfer,Systems and methods for prompt tuning can utilize previously-learned prompts for the initialization of tuning for prompts on different tasks that may differ from the task associated with the previously-learned prompt. The prompt being utilized for initialization can be a generic prompt and/or may be a prompt selected based on a determined similarity between two or more task embeddings.,"['G06N3/09', 'G06N5/022', 'G06N3/096', 'G06N3/084']"
EP4398157A1,Training a machine learning model based on text and image data,"A method comprising: accessing a training data point comprising i) a present image of a target, ii) a prior image of the target captured from an earlier occasion, and iii) and a corresponding textual report on the present and prior images; and training a machine learning model by: a) using an image encoder of the machine learning model, encoding the present image into a present image embedding, and encoding the prior image into a prior image embedding, b) using a difference encoder of the machine learning model, generating a difference embedding representing a difference between the present image embedding and the prior image embedding, c) using a text encoder of the machine learning model, encoding the textual report into a text embedding, d) comparing the text embedding with a temporal image embedding comprising the present image embedding and difference embedding.","['G06N3/088', 'G06N3/0455', 'G06N3/0895', 'G06N3/09', 'G06N3/091', 'G06N3/092', 'G06N3/096', 'G16H30/40', 'G16H50/20', 'G06N3/0464']"
US8180964B1,Optimization of cache configuration for application design,"A system generates application code based on a received model, determines an optimal cache configuration based on the application code, executes the application code in a target cache environment, generates cache performance results based on the executed application code, and re-determines the optimal cache configuration based on the cache performance results.","['G06F8/30', 'G06F11/3442', 'G06F11/3466', 'G06F12/0802', 'G06F2201/885']"
CN103248059B,Power distribution network reactive voltage optimization method and system,"The invention discloses a reactive power optimization method for a power distribution network, which comprises the steps of firstly building an actual operation model of the power distribution network, then carrying out load flow calculation, analyzing voltage quality, utilizing a reactive power optimization management tool to formulate a reactive power optimization scheme, adopting a simulation analysis method to carry out inspection and adjustment on the appointed optimization scheme, and finally carrying out reactive power optimization on the power distribution network according to the adjusted reactive power optimization scheme. The invention also discloses a system for optimizing the reactive voltage of the power distribution network, which is a system corresponding to the method. The method and the system solve the problem of reactive power optimization of the power distribution network, and provide theoretical basis and decision support for planning, running, transformation and management of the whole power distribution network.","['Y02E40/30', 'Y02E40/70', 'Y04S10/50']"
US20240078376A1,Generating machine learning pipelines using natural language and/or visual annotations,"Implementations are disclosed for automatically generating computer code that implements a machine learning-based processing pipeline based on multiple different modalities of input. In various implementations, one or more annotations created on a demonstration digital image to annotate one or more visual features depicted in the demonstration digital image may be processed to generate annotation embedding(s). Natural language input describing one or more operations to be performed based on the one or more annotations also may be processed to generate one or more logic embeddings. The annotation embedding(s) and the logic embedding(s) may be processed using a language model to generate, and store in non-transitory computer-readable memory, target computer code. The target computer code may implement a machine learning-based processing pipeline that performs the one or more operations based on the one or more annotations.","['G06F40/169', 'G06F16/583', 'G06F40/30']"
US20240320476A1,"System and method for capturing, managing and enriching prompts in a data processing environment","A prompt capture and enrichment system having a prompt capture unit for receiving prompts from different data sources to form input prompts; a prompt enrichment unit for automatically enriching one or more prompt attributes of the input prompts and for generating enriched prompts; a prompt filtering unit for filtering the enriched prompts based on prompt attributes and then generating filtered prompts and for generating a truthfulness score associated with the filtered prompts indicative of a truthfulness of the filtered prompts; a prompt matching unit for matching one of the enriched or filtered prompts with one of the input prompts to determine if a match exists based on user information and prompt attributes; and a storage unit including a blockchain for storing the input prompts, the enriched prompts, and the filtered prompts.","['G06N3/0475', 'G06N20/00', 'G06N5/022']"
CN111950296B,Comment target emotion analysis based on BERT fine tuning model,"The invention discloses comment target emotion analysis based on a BERT fine tuning model, which comprises a BCR-CRF target extraction model and a BCR-DA target emotion classification model, wherein the BCR-DA target emotion classification model is divided into an online course comment emotion analysis, a BCR-CRF target extraction model, a BCR-DA target emotion analysis model and experimental results on a real Chinese online course comment data set; the BCR-CRF target extraction model comprises the following steps: the first step: performing intra-domain unsupervised training BERT pre-training model BCR based on a large-scale Chinese comment corpus; and a second step of: introducing a CRF layer, adding grammar constraint to an output sequence of a semantic representation layer in a BCR model, ensuring the rationality of the dependency relationship between prediction labels, and accurately extracting comment targets in a course comment text; and a third step of: and constructing a BCR-DA model classification containing double attention layers to express emotion polarities of course comment targets. The invention can accurately analyze the target emotion contained in the online course comments and has important significance for understanding the emotion change of the learner and improving the course quality.","['G06F40/30', 'G06F16/35', 'G06F18/214']"
WO2022153722A1,Systems and methods for semi-supervised extraction of text classification information,"Disclosed embodiments relate to extracting classification information from input text. Techniques can include obtaining input text, identifying a plurality of tokens in the input text, pre-training a machine learning model, determining tagging information of the plurality of tokens using a first classification layer of the machine learning model, pairing sequences of tokens using the tagging information associated with the plurality of tokens, wherein the paired sequences of tokens are determined by a second classification layer, determining one or more attribute classifiers to apply to the one or more paired sequences, wherein the attribute classifiers are determined by a third classification layer of the machine learning model, evaluating sentiments of the paired sequences, wherein the sentiments of the paired sequences are determined by a fourth classification layer of the language machine learning model, aggregating sentiments of the paired sequences associated with an attribute classifier, and storing the aggregated sentiments.","['G06F40/284', 'G06F40/289', 'G06N20/00', 'G06N3/045', 'G06N3/084', 'G06N3/088', 'G06N5/04']"
CN118899077A,A Chinese medicine tongue diagnosis and analysis system and method based on multimodal large model,"The invention relates to the technical field of multi-mode large models, in particular to a traditional Chinese medicine tongue diagnosis analysis system and method based on a multi-mode large model, which are used for generating qualitative and quantitative description texts of traditional Chinese medicine features of tongue images and facial image images in batches through Tonguea I-AP I interfaces, and constructing a method for fine-tuning a data set of visual text fused traditional Chinese medicine inspection diagnosis instructions, thereby providing important data support for training in the vertical field of traditional Chinese medicine tongue diagnosis of the TongueLMM large model. The TongueLMM language decoder module is used for carrying out fine adjustment of fusion vision and language instructions on the general LLM large language generation model, and the image understanding capability of the LLM language model is stimulated by training LLM to carry out follow-up learning on the multi-modal instructions of the visual text. The language decoder module can be adapted to various LLM models which support better Chinese, the project LLM selects LLaMA a network model, and the traditional Chinese medicine inspection and inquiry instruction fine tuning data set transfer learning is used for obtaining higher performance in three training tasks of traditional Chinese medicine tongue surface image interpretation, multiple questions and answers and traditional Chinese medicine physique reasoning.","['G16H50/20', 'G06F18/22', 'G06F18/256', 'G06N3/0455', 'G06N3/0464', 'G06N3/096', 'G06N5/04', 'G16H30/40', 'G16H50/70', 'Y02A90/10']"
US20250077851A1,Remediation generation for situation event graphs,"Described systems and techniques determine an event graph of a causal chain of events representing a situation within a network, the event graph including event text characterizing at least one event of the causal chain of events. The event graph may then be processed using a large language model that includes at least one topological context adapter that includes a graph adapter and a text adapter, including processing the event graph with the graph adapter and the event text with the text adapter. The at least one topological context adapter may be trained using existing narratives describing past situations, and/or may be trained using worklog data describing past situations and corresponding actions taken to remedy the past situations. Outputs of the graph adapter and the text adapter may be combined to generate a narrative of the situation that explains the causal chain of events and/or instructions to remedy the situation.","['G06N3/0499', 'G06N3/045']"
US20250252315A1,"Reinforcement learning method and system based on sequential decision-making, device, and medium","Provided are a reinforcement learning method and system based on sequential decision-making, a device, and a medium. The method includes: preprocessing historical trajectory data of reinforcement learning to generate preprocessed historical trajectory data to train a Transformer network model, and transforming the reinforcement learning into a language conversion model task by using a text conversion mechanism in the Transformer network model, to generate a trained Transformer network model, where the Transformer network model is used to predict action information at a next time point in a historical environment, and determine a maximum target reward value in a historical environmental state to obtain a complete trajectory in the historical environmental state; and predicting action information at the next time point in a real environmental state by using the trained Transformer network model, to obtain a complete trajectory in the real environmental state.","['G06F18/2431', 'G06N3/092', 'G06F18/15', 'G06N3/006', 'G06N3/0455', 'G06N3/08', 'Y04S10/50']"
CN117171312A,A multi-round dialogue management method for power dispatching knowledge graph,"The invention discloses a multi-round dialogue management method for a power dispatching knowledge graph, which relates to the technical field of power dispatching, and comprises the following steps: acquiring input information of a dispatcher; carrying out semantic understanding on the input information to obtain corresponding intention and slot position values, thereby obtaining corresponding dialogue states; inputting the dialogue state into an action determining network to obtain an action corresponding to the input information; when the action is finished, generating a scheduling strategy based on the action and feeding back to a dispatcher, and carrying out power scheduling according to the scheduling strategy by the dispatcher; when the action is a question or confirmation, generating a reply sentence based on the action and feeding back the reply sentence to the dispatcher, acquiring updated input information of the dispatcher determined based on the reply sentence, and returning to perform semantic understanding on the input information to obtain a corresponding intention and slot value, so as to obtain a corresponding dialogue state until the action is ended. The invention improves the working efficiency of power dispatching.",[]
US8095477B2,Load fuzzy modeling device and method for power system,"A load fuzzy modeling device for the power system based on the causality diagram, the composite cloud generator and the improved T-S fuzzy model, comprising: a hardware device, including: the sensors, a signal conditioning and filter module, an A/D conversion module, a causality conversion module, a DSP module, a memory module, a display and keyboard module connected with the DSP module, respectively and a communication module connected with the PC. The voltage and the current are respectively acquired from the system through a voltage transformer and a current transformer, and then are transmitted into the filter circuit for filtering, thus are amplified through a amplifier LM258P as the voltage within the range of âˆ’5V Ëœ+5V so as to input into the A/D conversion modules for sampling; the digital signal is simply processed by the causality conversion module according to certain analysis algorithm principle, and the processed signal is transmitted into the DSP to process, the final results thus displayed on the LCD screen and transmitted into the PC through the commutation module.","['G06N7/04', 'G05B13/0275', 'G05B17/02']"
US20240249077A1,Systems and methods for in-context learning using small-scale language models,"Embodiments described herein provide a data driven framework that (i) translates demonstration examples to a fixed-length soft promptâ€”a sequence of soft tokens; and (ii) learns a global (not generated from demonstrations) soft prompt. The framework then combines the global prompt, the translated prompts and the original context to create an augmented context which is given as final input for the backbone LM to use.","['G06N3/084', 'G06F40/284', 'G06N3/044', 'G06N3/045', 'G06N3/0455']"
US20240419830A1,Network infrastructure for user-specific generative intelligence,"Network infrastructure for user-specific generative intelligence. Providing user-specific context to a generically trained LLM introduces a variety of complications (privacy, resource utilization, training costs, etc.). Various aspects of the present disclosure provide novel user-specific data structures, privacy and access control, layers of data, and session management, within a network infrastructure for generative intelligence. For example, user-specific embedding vectors may be used to provide user context to a generically trained foundation model. In some variants, edge devices capture multiple modalities of user context (images, audio; not just text). Privacy and access control mechanisms also allow a user to control information that is captured and sent to the foundation model. Session management further decouples a user's conversational state from the foundation model's session state. These concepts and others may be used to emulate e.g., a chatbot based virtual assistant that responds based on user context.","['H04N23/64', 'G06F16/242', 'G06F16/3329', 'G06F16/3347', 'G06F16/583', 'G06F16/587', 'G06F21/6218', 'G06F21/6227', 'G06F21/6254', 'G06F40/284', 'G06F40/35', 'G06F40/40', 'G06F9/547', 'G06V10/235', 'G06V10/25']"
CN115546908A,"A living body detection method, device and equipment","The embodiment of the specification discloses a method, a device and equipment for detecting a living body. The scheme can comprise the following steps: and obtaining semantic feature information and visual feature information of the image to be detected, and performing feature fusion processing on the obtained semantic feature information and visual feature information to obtain a fusion feature vector of the image to be detected. And then, a living body detection model built based on an attention mechanism is used for processing the fusion characteristic vector, and further, a living body detection result aiming at the image to be detected is obtained.","['G06V40/45', 'G06N3/08', 'G06V10/44', 'G06V10/82']"
CN116136957B,"Text error correction method, device and medium based on intention consistency","The invention discloses a text error correction method, a device and a medium based on intention consistency, wherein the method comprises the following steps: collecting original error correction data for preprocessing and labeling, and constructing an error correction data set of a text error correction model; constructing a text correction model based on a neural network, inputting a text to be corrected into the text correction model, and outputting first character probability distribution; calculating an error correction loss value as a first loss value by using the first character probability distribution and the corresponding real label; calculating an error correction loss value based on the intention consistency score as a second loss value using the first character probability distribution; training a text error correction model in a two-stage training mode based on the first loss value and the second loss value to obtain a trained final text error correction model; and inputting the text to be corrected into a text correction model obtained through training to correct the errors, and outputting the corrected text. The invention can effectively reduce the error correction rate of the text error correction model, improve the error correction accuracy and has strong practicability.","['G06F40/232', 'G06N3/08', 'Y02D10/00']"
US20240370736A1,Multi modal prompts for zero-shot mixed tasks,"Multi modal models comprising an encoder and decoder are described. The encoder projects inputs into embeddings, which are used to generate a multi modal prompt, which is provided to the decoder. The encoder input comprises context information. The multi modal prompt comprises mixed types of data. This mixed data is converted into embeddings and combined to form the multi modal prompt. For example, text may be converted to embeddings using a text encoder and images may be converted to embeddings using an image encoder. The same encoder used for context can be used (encoder weight sharing). The mixed embeddings are then fed into the decoder's multi-attention head to guide output generation. A model can be trained to learn the generic associativity of multi modal prompts. Once trained using generic tasks, a model can be deployed to tackle multiple tasks zero-shot, without finetuning on new data types.","['G06N3/0455', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/084', 'G06N3/088', 'G06N3/096']"
CN114757171A,"Training method of pre-trained language model, language model training method and device","The present disclosure provides a method for training a pre-training language model, a method for training a language model, and an apparatus thereof, including: obtaining a sample text, and executing at least two pre-training tasks according to the sample text to obtain a pre-training language model, wherein the pre-training tasks comprise: the method comprises a forward causal language modeling task, a reverse causal language modeling task and a mask language modeling task, wherein the forward causal language modeling task is a modeling task for predicting a later word in a sample text by the earlier word in the sample text, the reverse causal language modeling task is a modeling task for predicting a previous word in the sample text by the later word in the sample text, and the mask language modeling task is a modeling task for predicting a word at a mask position in the sample text by a word at a non-mask position in the sample text.","['G06F40/274', 'G06F40/284', 'G06N3/044', 'G06N3/08']"
JP2024177129A,Extracting information from reports using large-scale language models,"To provide a method for extracting information from reports, such as security reports or medical records, using machine learning-artificial intelligence (ML-AI) models.SOLUTION: A computer-implemented method for extracting and mapping structured information to a data model includes a step of acquiring text data from one or more unstructured data sources. Rephrased text data is determined using a Large Language Model (LLM), a preprocessing prompt, and the text data. Extracted data is determined using the LLM, an extraction prompt, the data model, and the rephrased text data. The extracted data is mapped to the data model. The method can be applied, for example, to medical use cases or cyber threat detection, among others, to improve the data models and support decision making.SELECTED DRAWING: Figure 3","['G06F40/295', 'G06F40/205']"
CN113947072A,Text error correction method and text error correction device,"The invention relates to a text error correction method and a text error correction device. The method comprises the following steps: a language model construction step, namely collecting a specific type of text, analyzing the specific type of text to form a document corpus aiming at the specific type of text, and training by utilizing a pre-training language model based on the document corpus to obtain the language model aiming at the specific type of text; a text error correction model construction step, namely constructing a text error correction task, finely adjusting the language model obtained in the language model construction step, and constructing an end-to-end text error correction model; and a text error correction step of inputting the text to be corrected into the text error correction model obtained in the text error correction model construction step to obtain the corrected text and the position information of the error. According to the text error correction method and the text error correction device, the problem that IPO verification needs a large number of manual service pain points is solved, the linguistic data do not need to be marked manually, and a large amount of labor cost is saved.","['G06F40/216', 'G06F40/232', 'G06N3/08']"
CN115482382A,An Image Semantic Segmentation Method Based on Transformer Architecture,"An image semantic segmentation method based on a transform architecture belongs to the technical field of medical image segmentation. The invention provides a method for extracting image context characteristics by a layered Swin Transformer encoder with a moving window. First, the image block is input into a transform-based encoder-decoder architecture, where the design of the moving window mechanism makes the features more comprehensive. Nested and dense jump connection in the UNet + + model can fully extract context features, so that the feature fusion is more sufficient.","['G06V10/26', 'G06N3/04', 'G06N3/08', 'G06V10/52', 'G06V10/806', 'G06V10/82']"
WO2024258605A1,Large language model integrations for pivot tables in spreadsheet environments,"Technology is disclosed herein for the integration of spreadsheet environments with LLM services. In an implementation, an application receives a natural language input from a user associated with a spreadsheet hosted by the application. The application generates a prompt that includes metadata about the spreadsheet and identifies a required format for descriptions of pivot tables. The application sends a prompt to a large language model (LLM) service to elicit a reply that includes a description of the pivot table having the required format. The application receives a reply to the prompt from the LLM service that includes the description of the pivot table in the required format. The application generates pivot table according to the reply from the LLM service.","['G06F40/18', 'G06F40/20', 'G06N3/0455']"
CN112329392A,Target encoder construction method and device for bidirectional encoding,"The application relates to a method and a device for constructing a target encoder of bidirectional encoding. The method comprises the following steps: acquiring a training text; coding the training text into a first sequence according to a coding relation stored in the dictionary, wherein the coding sequence of the first sequence is consistent with the character sequence of the training text; sequentially covering each element in the first sequence according to the character sequence of the training text to obtain a plurality of second sequences; rearranging the elements in each second sequence according to the sequence opposite to the current arrangement sequence to obtain a plurality of third sequences; the second sequence and the third sequence are input from a coding language model, and the model is output as a target coder. This application adopts forward encoding input and reverse encoding input to carry out the training that the feature extracted when constructing the target encoder to through the positive and negative bidirectional encoding in the true spatial sense, promote the feature representation ability of encoder, and then solved the technical problem that the translation accuracy of sentence end lasts the reduction.","['G06F40/126', 'G06F18/214', 'G06N3/045', 'Y02T10/40']"
US12321355B2,Unified search systems and methods,"A genealogy online system may cause to display, at a graphical user interface associated with a genealogy online system, a search box, the genealogy online system configured to provide functions comprising family-tree building and historical record search. The genealogy online system may receive a query from a user entered at the search box. The genealogy online system may use a machine learning language model to determine an intent of the user associated with the query. The genealogy online system may cause to display, at the graphical user interface as a result of the query, one or more links to one or more functions of the genealogy online system based on the intent determined by the machine learning language model.","['G06F16/24575', 'G06F16/243', 'G06F18/2415', 'G06F40/30', 'G06N20/00', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G06V10/82']"
US20230259692A1,Systems and methods for computer generation of a modifiable product description,"Generative language models are able to generate a sequence of text that may closely mimic a native human speaker's own generated text. However, technical challenges exist when implementing a generative language model for generating product descriptions. The model may output certain inaccuracies due to the predictive nature of generating the output. Further, the model does not have the ability to identify words from the product description that a merchant may want to modify, nor the ability to provide meaningful alternatives to such words. In some embodiments, a natural language processor might be built and/or trained using classification data. The natural language processor may identify one or more words and/or phrases in a product description as a candidate for modification. The product description might then be displayed on a merchant-facing user interface with an indication that the candidate for modification may be modified.","['G06F40/56', 'G06F40/166', 'G06F40/289', 'G06F40/40']"
CN108597517A,"Punctuation mark adding method, device, computer equipment and storage medium","This application involves a kind of punctuation mark adding method, device, computer equipment and storage mediums.The method includesï¼šObtain the target text of punctuation mark to be addedï¼›The first weighted finite state interpreter is constructed according to the target text and default punctuation mark collectionï¼›The language model trained according to the samples of text with punctuation mark is converted into the second weighted finite state interpreterï¼›First weighted finite state machine and the second weighted finite state interpreter are mergedï¼›Optimum route search is carried out in the third weighted finite state interpreter obtained to merging, and obtains the target text with punctuation mark.The accuracy for the punctuation mark that addition is corresponded to target text can be improved using this method.","['G10L15/26', 'G06F40/289', 'G10L15/08', 'G10L15/30', 'G10L2015/088']"
US12361335B1,Validating vector constraints of outputs generated by machine learning models,"The technology evaluates the compliance of an AI application with predefined vector constraints. The technology employs multiple specialized models trained to identify specific types of non-compliance with the vector constraints within AI-generated responses. One or more models evaluate the existence of certain patterns within responses generated by an AI model by analyzing the representation of the attributes within the responses. Additionally, one or more models can identify vector representations of alphanumeric characters in the AI model's response by assessing the alphanumeric character's proximate locations, frequency, and/or associations with other alphanumeric characters. Moreover, one or more models can determine indicators of vector alignment between the vector representations of the AI model's response and the vector representations of the predetermined characters by measuring differences in the direction or magnitude of the vector representations.","['G06F11/3684', 'G06F11/3688', 'G06F11/3692', 'G06N20/00', 'G06N20/20', 'G06N5/045']"
US20240419977A1,Systems and methods for training an autonomous machine to perform an operation,"Computer-implemented methods are included for training an autonomous machine to perform a target operation in a target environment. The methods include receiving a natural language description of the target operation and a natural language description of the target environment. The methods further include generating a prompt such as a reward and/or goal position signature by combining the natural language description of a target task or goal and the natural language description of the target environment. The methods then generate a reward or goal position function by prompting a large language model with the generated prompt. The methods further include computing a state description using a model of the target environment, and training a policy for the autonomous machine to perform the target task or goal using the generated function and state description.","['G06F8/311', 'G06N20/00', 'G06N3/006', 'G06N3/09']"
US20240273286A1,Generative collaborative publishing system,"Embodiments of the disclosed technologies include generating, by a generative language model, a first version of a first document, generating a second version of the first document by dividing the first version of the first document into a plurality of segments, where a first segment of the plurality of segments includes a subset of the digital content generated by the generative language model; enabling contributions to the first segment; enabling contributions to a second segment of the plurality of segments; receiving a first contribution to the second version of the first document, where the first contribution includes digital content generated by a first user of the network; creating a first segment-contribution pair by linking the first contribution with the first segment; receiving a second contribution to the second version of the first document; and creating a second segment-contribution pair by linking the second contribution with the second segment, where at least one of the first segment-contribution pair or the second segment-contribution pair is capable of being used to generate, by the generative language model, a second document.","['G06F40/258', 'G06F40/117', 'G06F40/166', 'G06F40/197', 'G06N3/0475', 'G06N3/08', 'G06Q10/101', 'G06F8/33', 'H04L67/06']"
KR20230156635A,Method and apparatus for learning task-specific adapter,"The present invention relates to a method for learning a task-specific adapter which comprises the steps of: generating a second sentence encoder model by adding a task-specific adapter to a first sentence encoder model having a pre-learned transformer structure; inputting an input sentence obtained from a pre-set learning database into the second sentence encoder model and obtaining a correct label corresponding to the input sentence; performing forward propagation from an upper layer to a lower layer of the second sentence encoder model based on the input sentence to obtain a predicted label; setting an adapter learning loss function by adding a first function based on a first loss function and a first weight and a second function based on a second loss function and a second weight; and performing backpropagation based on the predicted label obtained by the forward propagation, the correct label, and the adapter learning loss function to correct parameters of the adapter. The first loss function is set based on a cross entropy error, and the second loss function is set based on metric learning technique. Accordingly, a response speed required to predict the intent of a sentence can be increased by reducing the required resources.","['G06N3/084', 'G06F40/35', 'G06F16/3329', 'G06N3/04', 'G06N3/045', 'G06N3/0895', 'G06N3/096']"
CN113158671A,Open domain information extraction method combining named entity recognition,"The invention discloses an open domain information extraction method combining named entity identification, which comprises the following steps: inputting a sentence to be extracted into a pre-established and trained triple predicate extractor, and outputting a predicate phrase; inputting the predicate phrases and sentences to be extracted into a pre-established and trained triple element extractor, and outputting the positions of the triple element phrases; the triplet elements include: subject, object and idiom; inputting the sequence hidden state representation and the position of the triple element phrase obtained from the triple element extractor into a pre-established and trained named entity recognizer, and outputting the named entity category of the triple element phrase; the triple predicate extractor is used for extracting predicates related to triples appearing in an input sentence; the triple element extractor is used for extracting element phrases related to triples appearing in the input sentence; the named entity identifier is used to identify a named entity class of the triplet element phrase.","['G06F40/295', 'G06F16/3344', 'G06F16/3346', 'G06F16/35', 'G06F40/211', 'G06F40/216', 'G06F40/284']"
US11941356B2,Systems and methods for multi-scale pre-training with densely connected transformer,"Embodiments described herein propose a densely connected Transformer architecture in which each Transformer layer takes advantages of all previous layers. Specifically, the input for each Transformer layer comes from the outputs of all its preceding layers; and the output information of each layer will be incorporated in all its subsequent layers. In this way, a L-layer Transformer network will have L(L+1)/2 connections. In this way, the dense connection allows the linguistic information learned by the lower layer to be directly propagated to all upper layers and encourages feature reuse throughout the network. Each layer is thus directly optimized from the loss function in the fashion of implicit deep supervision.","['G06F40/20', 'G06N3/045', 'G06N3/047', 'G06N3/084', 'G10L15/16']"
US12322380B2,Generating audio using auto-regressive generative neural networks,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating a prediction of an audio signal. One of the methods includes receiving a request to generate an audio signal conditioned on an input; processing the input using an embedding neural network to map the input to one or more embedding tokens; generating a semantic representation of the audio signal; generating, using one or more generative neural networks and conditioned on at least the semantic representation and the embedding tokens, an acoustic representation of the audio signal; and processing at least the acoustic representation using a decoder neural network to generate the prediction of the audio signal.","['G10L13/00', 'G06N3/0455', 'G06N3/0475', 'G06N3/09', 'G10H1/0008', 'G10L13/06', 'G10L15/063', 'G10L15/16', 'G10L15/1815', 'G10L19/038', 'G10L21/0272', 'G10L25/30', 'G10H2210/056', 'G10H2250/311']"
CN112217947B,"Method, system, equipment and storage medium for transcribing text by customer service telephone voice","The invention provides a method, a system, equipment and a storage medium for transcribing text by customer service telephone voice, wherein the method comprises the following steps: collecting customer service telephone voice to be transcribed; extracting the voice characteristics of the customer service telephone voice to be transcribed; inputting the extracted voice features into a trained voice recognition model, wherein the voice recognition model comprises an acoustic encoder and a decoder, the acoustic encoder comprises a self-attention mechanism-based Transformer module, and the decoder comprises a word-based Transducer model; and acquiring the text output by the voice recognition model. The method improves the accuracy of speech recognition, improves the stability and the deduction speed of the model, and improves the accuracy and the efficiency of the telephone speech transcription text in the customer service telephone scene.","['H04M3/53', 'G10L15/063', 'G10L19/16']"
US12271710B2,Elidable text for prompt crafting,"An elidable text is constructed that prioritizes the content included in a prompt to a large language model having a fixed-size context window. The elidable text is generated from developer-generated instructions or automatically for source code within a source code editor. A source code editor may include a feature that selects certain lines of code as important or focused which are assigned a high-priority value. A changed line, a line of source code at a current cursor position, lines of source code at the beginning of a file and those that output data are considered focused lines. Non-focused lines are assigned a priority based on a distance from a focused line. The elidable text constrains the data included in a prompt to the context window size by replacing the lowest-valued lines of text and source code with a replacement string.","['G06F8/33', 'G06F8/37']"
CN116259075A,Pedestrian attribute recognition method based on prompt fine-tuning pre-trained large model,"A pedestrian attribute identification method based on a prompt fine tuning pre-training large model belongs to the technical field of computer vision and solves the problems of suboptimal and poor generalization capability caused by insufficient utilization of the relation between pedestrian images and attribute labels in the prior art. According to the invention, the visual and text encoders of the CLIP are adopted to extract image features and attribute features, the multi-mode transducer module fuses the two modal features, a prediction result is obtained through the feedforward network, the pedestrian attribute recognition problem is modeled as a visual language fusion problem, a pre-trained visual language large model is used as a backbone network to extract the visual and text features with better inter-modal connection, and the multi-mode transducer is used to model the visual and text connection, so that attribute semantic information is fully utilized, and the better generalization capability of the pre-trained large model is reserved in a prompting fine-tuning mode, and the model practicability is stronger.","['G06V40/10', 'G06N3/04', 'G06N3/08', 'G06V10/40', 'G06V10/764', 'G06V10/806', 'G06V10/82', 'Y02T10/40']"
CN116910187B,"Astronomical information extraction method, astronomical information extraction device, electronic device and storage medium","The application relates to an astronomical information extraction method, an astronomical information extraction device, an electronic device and a storage medium, wherein the astronomical information extraction method comprises the following steps: labeling initial text features and initial visual features in each astronomical document from the astronomical documents to be processed; performing feature fusion on the initial text features and the initial visual features to obtain a multi-modal knowledge extraction model which is input into a complete joint characterization vector and is based on a transform architecture, calculating a predicted probability value of the astronomical information features by using an encoder in the multi-modal knowledge extraction model, and outputting the predicted multi-modal astronomical information features according to the predicted probability value by using a decoder in the multi-modal knowledge extraction model; based on the labeling of various types of features in astronomical literature, the prediction of final astronomical information features is realized by utilizing a multi-modal knowledge extraction model, so that multi-modal astronomical knowledge in astronomical literature can be extracted.","['G06F16/3344', 'G06F16/35', 'G06N3/0455', 'G06N3/0464', 'G06N3/048', 'G06N3/08', 'Y02D10/00']"
US20240319965A1,Systems And Methods For Automating Analyses Of User Experience Tests With Generative Language Models,"Techniques are described herein for using artificial intelligence (AI) and machine learning (ML) to automate, accelerate, and enhance various aspects of user experience testing. Embodiments incorporate generative language models into user experience testing applications to extract key findings for improving product designs and driving product optimizations. In some embodiments, programmatic processes conduct a dialogue with a generative language model by engineering a set of input prompts as a function of prompt fragments, user experience test results, and test contexts. The AI-generated findings may drive actions directed to optimizing product designs and improving user experiences.","['G06Q10/0637', 'G06F11/3438', 'G06F16/345', 'G06F30/27', 'G06F40/30', 'G06F40/40', 'G06F40/56', 'G06F8/20', 'G06Q30/0203']"
US20250124001A1,Apparatus and method for data ingestion for user-specific outputs of one or more machine learning models,"An apparatus for data ingestion and manipulation, the apparatus including at least a processor and a memory communicatively connected to the at least a processor, the memory containing instructions configuring the at least a processor to receive a resource data file from one or more data acquisition systems, classify the resource data file to one or more educational categorizations, generate an educational module as a function of the resource data file and the classification of the educational categorizations wherein the education module comprises one or more machine learning models, retrieve a user profile of a plurality of user profiles as a function of a user input, create user-specific outputs as a function of the educational module, the user profile, and a conversational input and generate a virtual avatar model as a function of the user-specific outputs.","['G06F16/16', 'G06N3/006', 'G09B5/065', 'G09B7/02', 'G06N20/00']"
US11990058B2,Machine grading of short answers with explanations,"An example method embodying the disclosed technology comprises: digitally storing Teacher models and a Student model at a server computer; training each model with a corpus of unlabeled training data using Masked Language Modeling; fine-tuning each Teacher model for an ASAG task with labeled ground truth data; executing each Teacher model to generate and digitally store a respective set of class probabilities on an unlabeled task-specific data set for the ASAG task; further training the Student model by a linear ensemble of the Teacher models using KD; receiving, at the server computer, digital input comprising a target response text and a corresponding target reference answer text; programmatically inputting the target response text and the corresponding target reference answer text to the Student model, thereby outputting a corresponding predicted binary label; displaying correction data indicating the corresponding predicted binary label in a GUI; and, optionally, displaying explainability data in the GUI.","['G06F40/35', 'G09B7/04', 'G06F17/18', 'G06F40/20', 'G06F40/30', 'G06N20/20', 'G06N3/045', 'G06N3/0895', 'G06N3/096']"
CN117911786A,Interpretable visual positioning method based on multi-modal semantics,"The invention discloses an interpretable visual positioning method based on multi-mode semantics, which comprises the following steps of: acquiring a data set to be processed, and dividing the data set into: a test set and a training set; constructing an interpretable visual positioning model, training the interpretable visual positioning model according to a training set to obtain a trained interpretable visual positioning model, inputting test set data into the trained interpretable visual positioning model, and performing interpretable visual positioning according to multi-mode semantics such as instructions, pictures and the like in the test set. If the method is applied to the robot system, the requirements of the user can be met more comprehensively and accurately when the implicit instruction is processed, the explanation is generated and the accurate positioning is realized, and more intelligent and transparent experience is provided for the user.","['G06V10/764', 'G06N3/0455', 'G06N3/0464', 'G06V10/806', 'G06V10/811', 'G06V10/82']"
CN118736355A,Performance evaluation method and device of visual language model in positioning task,"The application provides a performance evaluation method and a device of a visual language model in a positioning task, which are characterized in that the positioning task comprises a positioning task based on attribute distinction, a positioning task based on relation distinction and a positioning task based on primary and secondary distinction, and the method comprises the following steps: constructing an evaluation data set for performing performance evaluation by a visual language model, aiming at an evaluation sample in the evaluation data set, executing a positioning task on the evaluation sample by the visual language model to obtain a positioning heat map in the evaluation sample, and determining a performance evaluation score of the evaluation sample according to the positioning heat map; and determining the average value of the performance evaluation scores as the average accuracy of the evaluation data set as the performance evaluation result of the visual language model. The application can verify the combined reasoning capacity of the visual language model in three dimensions of distinguishing the attribute, distinguishing the relation and distinguishing the primary and secondary, and is used for solving the defect that the pre-trained visual language model in the prior art has poor performance in the aspect of combined reasoning capacity.","['G06V10/776', 'G06F40/14', 'G06N3/084', 'G06N5/04', 'G06V10/25', 'G06V10/82', 'G06V20/70']"
CN117009650A,Recommendation method and device,"The application provides a recommendation method and a recommendation device in the field of artificial intelligence, which are used for carrying out recall, recommendation and the like based on a language model, so that automatic recommendation by using the language model is realized, and the generalization capability is strong. The method comprises the following steps: firstly, acquiring a user characteristic set, wherein the user characteristic set can comprise texts or keywords for describing user images; generating a recall path based on the user feature set through a language model, wherein the recall path represents a path recommended for a user, the language model is used for classifying input data, generating corresponding weights for each category, and generating the recall path according to the weights of each category; a recommendation list is then generated for the user based on the recall path through the language model.","['G06F16/9535', 'G06F18/24', 'G06N3/042', 'G06N3/044', 'G06N3/0464', 'G06N3/084', 'G06N3/096']"
WO2025030425A1,Method and system for enhancing ability of large language model to answer questions of specific field,"The present application relates to the technical field of drug screening, and in particular to a method and system for enhancing the ability of a large language model to answer questions of a specific field. The method comprises the following steps: using a large language model to extract a relation triple from a data set, wherein the data set comprises structured data and unstructured data; on the basis of the relation triple, constructing a knowledge graph of G protein coupled receptors (GPCRs), and storing the knowledge graph in a form of a graph database, wherein the knowledge graph comprises a GPCR entity and a relation thereof; using a pre-trained large language model to search the graph database for related information of a specific field to obtain prompt information, wherein the specific field is the field of GPCRs; and on the basis of the prompt information and the pre-trained large language model, obtaining an answer result, wherein the answer result is returned to a user by the pre-trained large language model. In the present application, by constructing a knowledge graph, the large language model is endowed with accurate and real-time field knowledge of GPCRs, and the ability of the large language model to answer questions of the field of GPCRs is enhanced.",['G06F16/332']
WO2024072140A1,Apparatus and method for controlling a robot photographer with semantic intelligence,"An electronic device for controlling a photographic system may obtain a video stream and a user query for a target event, obtain a set of photos from the video stream, obtain at least one photoshoot suggestion based on the user query via a language model, obtain a snapped photo for the target event based on the at least one photoshoot suggestion, in response to a given video frame included in the video stream satisfying a target content criterion, and output one or more photos selected from the set of photos and the snapped photo as event photos.","['B25J9/1679', 'B25J11/00', 'B25J13/003', 'B25J19/023', 'B25J9/163', 'B25J9/1697', 'G06F3/167', 'G06F40/00', 'G06T7/70', 'H04N23/61', 'H04N23/64', 'H04N23/66', 'H04N23/667', 'H04N23/695', 'H04N23/90', 'G06T2207/10016', 'G06T2207/30244']"
CN118013021B,"Medical answer method, device, equipment and medium based on large language model","The application provides a medicine answering method, device, equipment and medium based on a large language model, and the method is applied to the technical field of artificial intelligence. The method comprises the following steps: continuously pre-training the large language model of the base by adopting a training data set to obtain a candidate model; a training dataset comprising: data of general fields and data of medical fields; at least part of network parameters in the candidate model are adjusted by adopting a training instruction set, so that a medical solution model is obtained; a training instruction set comprising: instructions for the general domain and instructions for the medical domain; and when the medicine inquiry information is subjected to medicine solution, determining target medicine solution data matched with the medicine inquiry information by adopting a medicine solution model. According to the method, the medical solution data can be rapidly and accurately provided for the patient end through the medical solution model, so that the workload of a doctor end can be reduced, and the accuracy of the output medical solution data can be improved.","['G06F16/3329', 'G06N3/08', 'G06N5/041']"
US12136043B1,Transforming conversational training data for different machine learning models,"A method includes receiving a command to transform a first statement in a first conversational training set into training data for a second training set, where the first conversational training set trains a first machine learning model in a first knowledge area, and where the second conversational training set trains a second machine learning model in a second knowledge area. The method also includes analyzing language of the first statement using a third machine learning model, where the third machine learning model is trained to recognize patterns of language variation between the first and second knowledge areas. The method also includes, responsive to the analyzing, selecting an original segment of the first statement. The method also includes, responsive to the selecting, transforming the first statement into a second statement, for the second conversational training set, that includes a substitute segment that at least partially replaces the original segment.","['G06F40/169', 'G06F40/284', 'G06F40/30', 'G06F40/35', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06N5/022', 'G06F40/253']"
US12380060B2,Graph spatial split,"A method for reducing latency and increasing throughput in a reconfigurable computing system includes receiving a compute graph for execution on a reconfigurable dataflow processor comprising a grid of compute units and grid of memory units interconnected with a switching array. The compute graph includes a node specifying an operation on a tensor. The node may be split into multiple nodes that each specify the operation on a distinctive portion of the tensor to produce a first modified compute graph. The first modified compute graph may be executed. In addition, the multiple nodes may be within a single meta-pipeline stage and may be processed in parallel. Furthermore, the compute graph may further comprise a separate node for gathering the distinctive portions of the tensor into a complete tensor, to produce a second modified compute graph.","['G06F15/825', 'G06F8/45', 'G06F9/3867', 'G06F9/5016', 'G06F9/5066', 'G06F9/544', 'G06N3/063', 'G06F2209/509']"
CN111950540A,"A method, system, device and medium for knowledge point extraction based on deep learning","The invention discloses a knowledge point extraction method, a knowledge point extraction system, a knowledge point extraction device and a knowledge point extraction medium based on deep learning, wherein the method comprises the following steps: acquiring an original data set by a crawler method and/or an OCR text recognition method; preprocessing the original data set to acquire knowledge representation data; determining a Bert pre-training model according to the knowledge characterization data; optimizing the Bert pre-training model according to a dynamic mask method and a mixed mask method; and extracting knowledge points in the original data set according to the optimized Bert model. By means of the Bert pre-training model, complicated characteristic engineering steps can be omitted, and the problem of ambiguity can be solved; the invention optimizes the bert model, extracts knowledge points more accurately, and can be widely applied to the technical field of deep learning.","['G06V10/22', 'G06F16/355', 'G06F18/214', 'G06F18/2415']"
US20080133233A1,Medical assistance device,"A medical assistance device comprises: a voice dictionary used for transforming voice into characters; a storage configured to store cipher information indicating correspondence of terms included in the voice dictionary and the terms for ciphering; a voice entering part configured to enter voice; a transformer configured to transform the entered voice into a term string, based on the voice dictionary and the cipher information; a display transformer configured to transform the transformed term string into information for display; and a display controller configured to instruct a display device to display the information for display.",['G16H40/63']
WO2024174911A1,"Code generation method and apparatus, storage medium and electronic device","The present disclosure relates to a code generation method and apparatus, a storage medium and an electronic device, which aim to improve the accuracy of automatically generating codes. The method comprises: acquiring a target text, the target text comprising a program code text to be supplemented or a natural language text used for describing a code function; and inputting the target text into a code generation model to obtain target program codes generated on the basis of the target text, the code generation model being obtained by training a code understanding task and a code generation task, the code understanding task being used for the code generation model to learn grammatical features and semantic features of sample program codes, and the code generation task being used for the code generation model to learn a process of generating new program codes on the basis of the sample program codes.","['G06F8/447', 'G06N5/02']"
US11620457B2,Learning to fuse sentences with transformers for summarization,"Systems and methods for sentence fusion are described. Embodiments receive coreference information for a first sentence and a second sentence, wherein the coreference information identifies entities associated with both a term of the first sentence and a term of the second sentence, apply an entity constraint to an attention head of a sentence fusion network, wherein the entity constraint limits attention weights of the attention head to terms that correspond to a same entity of the coreference information, and predict a fused sentence using the sentence fusion network based on the entity constraint, wherein the fused sentence combines information from the first sentence and the second sentence.","['G06F40/40', 'G06F40/289', 'G06F40/16', 'G06F40/166', 'G06F40/216', 'G06F40/284', 'G06F40/295', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'G06N3/09', 'G06F40/30']"
CN116842127A,Self-adaptive auxiliary decision-making intelligent method and system based on multi-source dynamic data,"The invention discloses a self-adaptive auxiliary decision-making intelligent method and system based on multi-source dynamic data. Comprising the following steps: the multisource data acquisition and processing module is used for preprocessing dynamic data obtained by own sensor equipment, such as a radar sensor, an infrared sensor and a visible light sensor, and static data marked by an expert; the effective information mining module acquires image features and text features through a vision-language model, fuses the image features corresponding to the multi-source data of the same target through the position-guided feature fusion module, and performs target identification by utilizing the text features and the fused image features; the battlefield situation visualization module is used for visualizing the battlefield situation by using the three-dimensional scene reconstruction model based on the own target and the position information thereof and the identified enemy target and the position information thereof; and predicting the battlefield situation, namely predicting the position information of the target at the future moment by utilizing the network structure and the historical position information of the target, so as to predict the whole future battlefield situation.","['G06F16/316', 'G06F16/3331', 'G06F18/214', 'G06F18/25', 'G06F40/126', 'G06F40/194', 'G06F2216/03']"
CN118918095A,Transformer defect detection method based on pixel-text matching of CLIP model,"A transformer defect detection method based on pixel-text matching of a CLIP model belongs to the technical field of transformer equipment defect detection, solves the problem of how to effectively detect the transformer under the condition that no enough related data sets exist, converts original image-text matching in the CLIP into pixel-text matching, and guides the detection of the transformer defect by using a pixel-text score graph; the defect data of the transformer equipment are collected and converted into image text pairs, the image text pairs are input into a model, the multi-mode data are mapped into the same multi-mode space, image embedding and text embedding for representing normal and abnormal states are extracted, pixel-text score diagrams are calculated, the score diagrams are fed to an FPN image decoder and supervised by using real labels, after training, the model is used for a data set of the defects of the transformer equipment, and the final segmentation result of the defects of the transformer equipment is obtained.","['G06T7/0002', 'G06N3/045', 'G06V10/26', 'G06V10/75', 'G06V10/774', 'G06V10/82', 'G06V30/14']"
CN114298287A,"Prediction method and device based on knowledge distillation, electronic equipment, storage medium","The embodiment provides a prediction method and device based on knowledge distillation, electronic equipment and a storage medium, and belongs to the technical field of machine learning. The method comprises the following steps: performing self-attention distillation on the second neural network model according to the transformer layer of the first neural network model; training a backbone classifier of a backbone network according to downstream task data, and updating backbone parameters of the backbone network; performing self-distillation on the main classifier of the main network after fine adjustment to obtain a branch classifier; training a branch classifier according to the label-free task data to obtain a target classification model; based on a sample self-adaptive mechanism, the to-be-classified data input to the target classification model is subjected to self-adaptive reasoning to obtain a prediction result.",[]
US12019988B2,"Multilingual, end-to-end-aspect based sentiment analysis with opinion triplets predictions","A computer-implemented method for training a neural end-to-end aspect based sentiment analysis (ABSA) system includes: inputting a batch of samples of a dataset into the neural end-to-end ABSA system, where the neural end-to-end ABSA system includes: a contextual language encoder configured to embed tokens with context; a first self-attention network configured to, based on an output of the contextual language encoder, detect an aspect term and provide a first output corresponding to the aspect term; and a second self-attention network configured to, based on the output of the contextual language encoder, detect the aspect term and provide a second output corresponding to the aspect term; and based on the inputted batch of samples and a consistency loss function, selectively adjusting weights of the neural end-to-end ABSA system based on consistent aspect term detection by the first self-attention network and the second self-attention network.","['G06N3/084', 'G06F40/284', 'G06F40/30', 'G06N3/049', 'G06N3/08', 'G10L15/16', 'G06N3/044', 'G06N3/045']"
WO2021147041A1,"Semantic analysis method and apparatus, device, and storage medium","A semantic analysis method and apparatus, a device, and a storage medium, relating to the field of artificial intelligence, and specifically relating to the field of natural language understanding. Said method comprises: extracting a structured entity vector from a text to be analyzed, the structured entity vector being used to indicate the identifier of the entity and the attributes of the entity; performing feature extraction on the structured entity vector, to obtain an entity feature; fusing the entity feature, a lexical feature of the text and a syntactic feature of the text, to obtain a semantic feature of the text; and decoding the semantic feature to obtain semantic information of the text. Said method uses the attributes of an entity to enhance the semantic understanding capability.","['G06F40/30', 'G06F16/3344', 'G06F40/211', 'G06F40/279']"
US20240184982A1,Hierarchical text generation using language model neural networks,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating long textual works using language model neural networks. For example, the textual works can be generated hierarchically by performing a hierarchy of generation steps using the same language model neural network.","['G06F40/20', 'G06F40/56']"
CN119128070A,"Large language model training methods, equipment and media in the agricultural field","The application provides an agricultural field large language model training method, equipment and medium, wherein the agricultural field large language model training method comprises the steps of collecting agricultural field data book information, preprocessing to obtain a first Chinese data set and a first minority national language data set, combining the first Chinese data set and the first minority national language data set to generate a first mixed language data set, processing by adopting a word segmentation library, performing secondary processing by adopting a preset special character processing method, constructing a word segmentation table, constructing a WordPiece word sheet model, processing and dividing the word segmentation table, converting each word in the word segmentation table into a word vector by adopting a word embedding algorithm, constructing a coding component and a decoder component which adopt a self-attention mechanism based on a Transformer framework, generating an agricultural field large language model, and training the agricultural field large language model by taking the word vector as input to obtain a trained agricultural field large language model. To solve the problem of large language models supporting Chinese and minority languages.","['G06F16/3329', 'G06F16/3344', 'G06F16/35', 'G06F16/36', 'G06N3/0455', 'G06N3/0499', 'G06N3/084', 'G06Q50/02', 'Y02A40/10']"
US20240012840A1,Method and apparatus with arabic information extraction and semantic search,"An Arabic information extraction apparatus includes one or more processors configured to: receive a query comprising a long query or a short query; extract, using one or more language models, a named entity and a keyword from the query to generate extracted information; classify, using one or more classification models, the query to generate a classified query; convert the classified query and the extracted information into a dense vector representation; and determine and output a similarity match between the dense vector representation and a document vector representation of a knowledge base comprising an Islamic law document.","['G06F16/3344', 'G06F16/3334', 'G06F16/35', 'G06F40/279', 'G06F40/295']"
WO2025090955A1,Efficiently serving machine-learned model computations with high throughput and low latency,"An example method includes receiving input requests to process a plurality of input sequences using the machine-learned sequence processing model to generate a plurality of output sequences respectively corresponding to the plurality of input sequences; generating a plurality of initial attention tensors respectively for the plurality of input sequences, wherein: one or more respective initial attention tensors are generated for each respective input sequence in parallel over input elements of the respective input sequence; and the one or more respective initial attention tensors are generated in one or more batches having a first batch size using a prefill system that comprises one or more prefill computing devices and executes one or more layers of the machine-learned sequence processing model; and autoregressively generating, using the plurality of initial attention tensors, a plurality of output elements for each of the plurality of output sequences in one or more batches having a second batch size, wherein: the plurality of output elements are generated using a generation system that comprises one or more generation computing devices.","['G06F9/5044', 'G06F9/5027', 'G06N3/063', 'G06F2209/509', 'G06N3/045']"
US20250165717A1,Multilevel Data Analysis,"A graphical, hierarchical document stream browser and environment for semantic (e.g. framing) and performance data analysis and interactive visualization integrates three scales: entities (competitive), entity (diachronic), and document (linguistic). The document level includes annotation and computational linguistics facilities; the entity level has calendrical and time-series focus. All levels emphasize deep linkage and network (i.e. connective/relational space) view of objects, with user-configurable connectivity. Large language model (LLM) integrations provide synthetic advisories, public opinions, reports, plot insights, comparisons; traditional natural language processing techniques and neural models are also employed. A smart plot system includes a â€œplot cartâ€ and interpreter with an analysis snippet library. Graph structure may arise via adjustable blending or perceptual optimization of canned attribute-related distance functions or via link-induction query language with deep â€œsemantic stored procedureâ€ subexpressions, or feed into graph neural network-style inference for predictions. Most non-LLM ongoing computational load is client-side, using precomputed hierarchical summary files.","['G06F40/30', 'G06F16/3344', 'G06T11/206', 'G06T2200/24']"
CN114372414A,Multi-modal model construction method and device and computer equipment,"The embodiment of the invention relates to a multi-modal model construction method, a device, computer equipment, a storage medium and a computer program product, which can be applied to the field of maps, wherein the method sets an iteration cycle to construct a multi-modal model, can acquire large-scale pre-training data from at least one information flow task in the current iteration cycle, and accordingly completes self-supervision pre-training of the multi-modal pre-training model in the current iteration cycle based on the pre-training data.","['G06F30/27', 'G06F18/2155', 'G06N3/045', 'G06N3/08']"
US20220366192A1,Formula and recipe generation with feedback loop,"Techniques to mimic a target food item using artificial intelligence are disclosed. A formula generator is trained using ingredients and using recipes and, given a target food item, determines a formula that matches the given target food item. A flavor generator is trained using recipes and their associated flavor information and, given a formula, the flavor generator determines a flavor profile for the given formula. The flavor profile may be used to assist the formula generator in generating a subsequent formula. A recipe generator is trained using recipes and, given a formula, determines a cooking process for the given formula. A food item may be cooked according to a recipe, and feedback, including a flavor profile, may be provided for the cooked food item. The recipe and its feedback may be added to a training set for the flavor generator.","['G06K9/6263', 'G06F16/2423', 'G06F16/2457', 'G06F16/248', 'G06F18/214', 'G06F18/2178', 'G06K9/6256', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06V10/774', 'G06V10/7784']"
US7895038B2,Signal enhancement via noise reduction for speech recognition,"Speech enhancement techniques for extemporaneous noise without a noise interval and unknown extemporaneous noise are provided with a method of signal enhancement including subtracting a given reference signal from an input signal containing a target signal and a noise signal by spectral subtraction; applying an adaptive filter to the reference signal; and controlling a filter coefficient of the adaptive filter in order to reduce components of the noise signal in the input signal. In signal enhancement, a database of a signal model concerning the target signal expressing a given feature by a given statistical model is provided, and the filter coefficient is controlled based on the likelihood of the signal model with respect to an output signal from the spectral subtraction means.",['G10L21/0208']
CN112420025A,Intelligent voice recognition conversion method and system for power dispatching,"The scheme collects voice data in the power dispatching process in real time and stores the voice data through an audio file; constructing a speech character matching model base, and training an RNN-T speech recognition model based on the model base; converting the audio file into a text file by using the trained RNN-T voice recognition model; optimizing the text file by using a distributed word vector tool, and filtering repeated texts; extracting key information related to the report content from the optimized text file by using the trained BP neural network to form structured data which accords with the dispatching log specification of the power grid dispatching management system; the problem that the dispatching telephone information can only be manually recorded by a dispatcher, the efficiency is low, and the call quality problem needs to be repeatedly confirmed for multiple times is effectively solved, the working efficiency is improved, and the dispatching personnel can timely and accurately handle the abnormity and the fault of the power grid equipment.","['G10L15/063', 'G10L15/16']"
US20240362422A1,Revising large language model prompts,"A computing system for revising large language model (LLM) input prompts is provided herein. In one example, the computing system includes at least one processor configured to receive, via a prompt interface, a prompt from a user including an instruction for a trained LLM to generate an output, and generate a first response to the prompt. The at least one processor is configured to assess the first response according to assessment criteria to generate an assessment report for the first response, and generate a revised prompt in response to second input including the first prompt, the first response, the assessment report, and a prompt revision instruction for the LLM to revise the prompt in view of the assessment report. The at least one processor is configured to, in response to final input including the revised prompt, generate a final response to the revised prompt, and output the final response.","['G06F40/40', 'G06F40/253', 'G06F9/543', 'G06N20/00']"
CN118194988A,A vertical financial big model system and method for realizing efficient processing of tabular data,"The invention relates to a financial large model system in the vertical field for realizing the function of efficiently processing form data, which comprises a form encoder and a large language model, wherein the output end of the form encoder is connected with the large language model, the form encoder extracts information from an input form to generate a global form expression form, and the global form expression form and a text query of a user are fed into the large language model for reasoning; the large language model is used for identifying the query intention of a user, decomposing a complex demand task into a plurality of simple tasks and generating an output of a command chain and text reply. The invention also relates to a method for realizing efficient processing of the form data. The vertical-field financial large model system and the method for realizing the function of efficiently processing the form data provide a brand-new solution for the interaction of the natural language and the form data, break through the possibility of data analysis given by the LLM technology, and effectively solve the problems and difficulties faced by the large model application in the securities industry in the aspect of pursuing easier access and understanding of the data.","['G06N5/022', 'G06F16/243', 'G06F16/2455', 'G06F16/2462', 'G06F16/248', 'G06N3/0455', 'G06N3/08', 'G06N5/041']"
CN113297364A,Natural language understanding method and device for dialog system,"The invention belongs to the technical field of intelligent dialogue, in particular to a natural language understanding method and a device oriented to a dialogue system, which comprises a word embedding layer, a coding representation layer and a joint learning layer, has reasonable structure and is definitely based on a collected specific field data set and 1) an original BERT-WWM model 2) an original ERNIE model 3) and a pre-trained joint learning model 4) a 3-layer BERT-WWM model after knowledge distillation. Compared experiments are carried out on the four models, on the basis of a specific field data set, 3) the models are better than 1) and 2) models in the two performance indexes of the intention classification accuracy and slot position identification F1, and 4) the model parameter scale after knowledge distillation is greatly reduced, the inference delay is effectively reduced, and the performance loss is small.","['G06F16/3329', 'G06F16/3344', 'G06F16/35', 'G06F40/242', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
US20210357752A1,"Model Processing Method, Apparatus, Storage Medium, and Processor","A method, an apparatus, a storage medium, and a processor for model processing are disclosed. The method includes: obtaining an original language model; determining a task that needs to be processed by the original language model; and converting the original language model based on features of the task to obtain a target language model for processing the task. The present disclosure solves the technical problem of the difficulty of effectively using a model.","['G06N3/08', 'G06F40/20', 'G06F40/56', 'G06F40/186', 'G06N3/04', 'G06N3/045', 'G06N3/082', 'G06N5/025']"
CN112749262B,"Question-answering processing method and device based on artificial intelligence, electronic equipment and storage medium","The invention provides a question-answering processing method and device based on artificial intelligence, electronic equipment and a computer readable storage medium; the method comprises the following steps: respectively carrying out coding processing on the questions and answers corresponding to the questions to obtain coding features of the corresponding questions and coding features of the corresponding answers; carrying out fusion processing on the coding features of the questions and the coding features of the answers to obtain fusion features; performing problem prediction processing on the fusion characteristics to obtain similar problems similar to the problems; and constructing a knowledge graph based on the questions, the similar questions and the corresponding answers, and responding to the question-answering service request based on the knowledge graph. The intelligent degree of the intelligent question-answering service can be improved through the intelligent question-answering service method and the intelligent question-answering service system.","['G06F16/3329', 'G06F16/367', 'G06F18/253', 'G06F40/126', 'G06F40/35', 'G06N3/045', 'G06N3/084', 'G06Q10/04', 'G06Q10/10']"
CN117153393B,Cardiovascular disease risk prediction method based on multi-mode fusion,"The invention discloses a cardiovascular disease risk prediction method based on multi-mode fusion. Acquiring patient sign data in the medical data set; aggregating various types of text reports generated by patients during a hospital; extracting time sequence characteristics by using the acquired patient sign data and using a graph neural network GNN-encoder as a network of a time sequence backbone; extracting text characteristics from patient text reports in the aggregated various text reports by using a pre-training-based hierarchical BioBERT model as a language backbone network; the time sequence features and the text features are spliced together, and the fused features are obtained as embedded vectors of the physical condition of the patient in a feature space through a self-attention depth fusion network; constructing a binary classifier based on a multi-layer perceptron to obtain a binary classification result of whether cardiovascular diseases exist; constructing a multi-element classifier based on a multi-layer perceptron to obtain the predicted subtype of the patient. The method is used for solving the problems that the text report is too long and the semantic features are difficult to extract effectively without losing full text information in the medical text data set.","['G16H50/30', 'G06F18/2132', 'G06F18/241', 'G06N3/042', 'G06N3/0455', 'G06N3/08', 'G16H10/60', 'G16H15/00', 'Y02A90/10']"
CN115114436A,Paragraph information fused discourse level text continuity classification method,"The invention discloses a chapter-level text continuity classification method fusing paragraph information, belongs to the technical field of natural language processing, and solves the problems that a chapter vector obtained by the conventional method generally lacks context information and paragraph structure information of chapters is generally ignored. The method of the invention comprises the following steps: sentence division is carried out on the discourse-level text, and sentences of the discourse-level text are obtained; obtaining a sentence vector according to sentences of chapter-level texts by using an XLNet pre-training model; constructing a discourse structure matrix of discourse level texts according to the relation of the paragraphs; initializing a Tree-Transformer construction model by utilizing a chapter structure matrix; building a model by using the initialized Tree-Transformer, and updating the sentence vector; acquiring chapter-level semantic vector representation of chapter-level text according to the updated sentence vector; and (4) inputting the expression of the chapter-level semantic vectors into the classification layer to obtain a chapter-level semantic consistency classification result. The invention has important effect on the directions of multi-task learning, document vectorization and the like in information retrieval and natural language processing.","['G06F16/353', 'G06F16/3347', 'G06F40/30', 'G06N3/08']"
WO2021135477A1,"Probabilistic graphical model-based text attribute extraction method and apparatus, computer device and storage medium","A probabilistic graphical model-based text attribute extraction method and apparatus, a computer device, and a storage medium, relating to artificial intelligence neural network technology. The method comprises: inputting into a BERT neural network model a received text to be processed, and obtaining corresponding text representation output; inputting the text representation output into a multi-task learning classification model so as to obtain a corresponding entity type; sequentially performing recursion, vector concatenation, feature fusion and essential-attribute extraction on the entity type so as to obtain the essential attributes in the entity and start and end positions of the essential attributes; and sequentially performing entity representation vector extraction, vector concatenation and feature fusion, and non-essential-attribute extraction on the essential attributes and the start and end positions of the essential attributes, so as to obtain non-essential attributes in the entity and start and end positions of the non-essential attributes. The invention improves the accuracy of attribute extraction from data. Furthermore, there are no data format restrictions on text to be processed; thus, any structured data or unstructured data may be inputted.","['G06F40/295', 'G06F18/24', 'G06F40/216', 'G06N3/045', 'G06N3/049', 'G06N3/08']"
US20240362272A1,Machine-learned multi-modal artificial intelligence (ai) models for understanding and interacting with video content,"A video analysis system receives one or more queries from users of client devices. The video analysis system trains a machine-learned video encoder and/or a decoder coupled to receive video data and a prompt including a user query and generate an output for responding to the user query. A set of video embeddings are generated by extracting frame data, audio data, or text data from the video content, and applying a machine-learned video encoder to the frame data, the audio data, or the text data to generate the set of video embeddings. The video analysis system also generates a set of prompt embeddings representing at least a portion of the query in a latent space. The video analysis system applies at least a component of a machine-learned decoder to the input tensor to generate an output including a set of output embeddings.","['G06F40/40', 'G06F16/735', 'G06V20/46']"
WO2025007653A1,"Image generation method and apparatus based on artificial intelligence, electronic device, computer readable storage medium, and computer program product","The present application provides an image generation method and apparatus based on artificial intelligence, an electronic device, a computer readable storage medium, and a computer program product. The method comprises: acquiring a content text and acquiring a style image having a target style; carrying out text encoding processing on the content text to obtain a content text code of the content text, and extracting a style code from the style image; and on the basis of a dual-path cross attention mechanism corresponding to the style code and the content text code, carrying out inverse diffusion processing on a noise image to obtain a target image, wherein the target image is matched with the content of the content text, and the target image has the target style.","['G06F40/126', 'G06F40/30', 'G06T11/60', 'Y02D10/00']"
CN116259407B,"Disease diagnosis method, device, equipment and medium based on multi-mode data","The invention relates to the technical field of medical diagnosis and treatment, and particularly discloses a disease diagnosis method, device, equipment and medium based on multi-mode data, wherein the method comprises the following steps: respectively extracting a problem text feature, a disease text feature, an audio feature and an image feature; mapping the symptom text features, the audio features and the image features to the same dimension, and performing feature alignment treatment; acquiring fusion feature vectors by fusion feature aligned disorder text features, audio features and image features; the text features and the fusion feature vectors of the splicing problems acquire splicing vectors; placing the spliced vector into a pre-trained language model to generate a diagnosis result; the method can fully mine the relevance and the difference between different modal data, solves the problem that the relationship between the multi-modal data is difficult to capture due to the difference between the data, effectively fuses various features, can effectively relieve the fatigue of doctors, and improves the disease diagnosis accuracy.","['G16H50/20', 'G06V10/806', 'G06V10/811', 'G06V10/82', 'G16H10/60', 'Y02A90/10']"
US20240037375A1,Systems and Methods for Knowledge Distillation Using Artificial Intelligence,"An artificial intelligence (AI)-based knowledge distillation and paper production computing system processes instructions to use machine learning models to automatically review papers from a large corpus of papers and distill knowledge using science of science methods and AI-based modeling techniques. The AI-based knowledge distillation and paper production computing system processes instructions to leverage network science and machine learning tools to analyze papers with respect to a given topic to find relevant scientific publications, organize and group publications based on topic similarity and relation to the topic in general, and distill and summarize the message and content of these publications into a coherent set of statements.","['G06F16/345', 'G06N3/0455', 'G06N20/10', 'G06F40/30', 'G06N3/045']"
US12242473B2,"Graph-based Natural Language Processing (NLP) for querying, analyzing, and visualizing complex data structures","A system with graph-based Natural Language Processing (NLP) for querying, analyzing, and visualizing complex data structures is described. Such a system executes a generalized AI language model; defines and migrates a training dataset into a graph database by exposing data sources to an executing AI language model that self-defines a structure and self-writes an executable script to query the original data sources and self-writes code to load the extracted data into a graph database in the form of new nodes and new relationships with directionality between the nodes. The system further includes means for loading the extracted data into the graph database; condensing the information stored within the graph database into a condensed data structure representing the full architecture of the data in a natural language format; and responding to human language inquiries with responsive text, speech, and visualizations using the data loaded into the graph database.","['G06F16/243', 'G06F16/9024', 'G06N20/00']"
CN117292783A,Medical image report generating system,"The application relates to a medical image report generating system, which comprises processing equipment and a human-computer interaction interface; the man-machine interaction interface is used for inputting a description text and a report generation instruction corresponding to the medical image and displaying the generated medical image report; the processing equipment is used for inputting the description text into a pre-trained diagnostic opinion generation model based on the report generation instruction, outputting diagnostic opinion corresponding to the description text, and constructing the diagnostic opinion generation model based on a large language model; based on the medical image, the description text and the diagnostic comments, a corresponding medical image report is generated, the problem that the corresponding diagnostic comments are given according to the image description in the related technology needs to consume higher time and labor cost is solved, and the working efficiency of doctors is improved.","['G16H15/00', 'G06N3/08', 'G16H30/40']"
US20240061835A1,System and method of selective fine-tuning for custom training of a natural language to logical form model,"Systems and methods fine-tune a pretrained machine learning model. For a model having multiple layers, an initial set of configurations is identified, each configuration establishing layers to be frozen and layers to be fine-tuned. A configuration that is optimized with respect to one or more parameters is selected, establishing a set of fine-tuning layers and a set of frozen layers. An input for the model is provided to a remote system. An output of the set of frozen layers of the model, given the provided input, is received back and locally stored. The set of fine-tuning layers of the model is loaded from the remote system. The model is fine-tuned by retrieving the locally stored output of the set of frozen layers, and updating weights associated with the set of fine-tuning layers of the machine learning model.","['G06F16/24522', 'G06F16/252']"
US12314294B1,Method and systems for generating a projection structure using a graphical user interface,"A system for generating a projection structure using a graphical user interface, wherein the system comprises: a display device configured to display a graphical user interface (GUI); a computing device comprising a memory; and a processor, wherein the memory contains instructions configuring the processor to: receive a plurality of datasets, wherein each dataset comprises a plurality of parameters; generate a first projection structure as a function of the datasets; display the first projection structure through the GUI; receive natural language query data corresponding to the first projection structure using a chatbot interface; generate response data as a function of the query data; map, as a function of a look-up table, each categorization to a database entry index; obtain response data from the database; adjust the parameters as a function of the response data; generate a second projection structure; and display it in the GUI.","['G06F16/31', 'G06F16/33295', 'G06F16/35', 'G06T11/206']"
US20240420491A1,Network infrastructure for user-specific generative intelligence,"Network infrastructure for user-specific generative intelligence. Providing user-specific context to a generically trained LLM introduces a variety of complications (privacy, resource utilization, training costs, etc.). Various aspects of the present disclosure provide novel user-specific data structures, privacy and access control, layers of data, and session management, within a network infrastructure for generative intelligence. For example, user-specific embedding vectors may be used to provide user context to a generically trained foundation model. In some variants, edge devices capture multiple modalities of user context (images, audio; not just text). Privacy and access control mechanisms also allow a user to control information that is captured and sent to the foundation model. Session management further decouples a user's conversational state from the foundation model's session state. These concepts and others may be used to emulate e.g., a chatbot based virtual assistant that responds based on user context.","['G06V20/70', 'G06F16/245', 'G06F40/284', 'G06V10/764']"
US12307247B1,Transformer-based programming code value quantification system,"Disclosed herein are computer-implemented systems and methods for code value assessment. For example, in one aspect, the system comprises, an input module configured to receive program code submissions from developers, a processing unit equipped with Text-Based Models with Contextual Understanding (TBM-CUs) configured to evaluate the functional meaning, purpose, and value of submitted code segments and distinguish between code contributions from human programmers and machine learning systems, a visualization module configured to present the assessed value of the analyzed code over various time periods and dimensions, offering insights into trends, patterns, and comparative performance, a communication module configured to translate programming code meaning or function into plain language summaries for non-technical stakeholders, and an AI peer review module configured to automatically review, accept or reject code contributions.","['G06F8/77', 'G06F11/3616', 'G06F8/75', 'G06N20/00', 'G06N3/045']"
CN103166323B,System for monitoring secondary circuit model on line in real time based on protection device,"The invention belongs to the technical field of power systems and discloses a system for monitoring a secondary circuit model on line in real time based on a protection device. The system comprises a network communication module, a model resolution module, a graph generation module, a model storage module and a real-time comparison module, wherein the model resolution model is connected with the graph generation module and the model storage module and outputs a substation configuration description (SCD) static model and a resolved configured IED description (CID) on-line model respectively; the real-time comparison module is connected with the network communication module and the model storage module, receives a generic object oriented substation event (GOOSE) real-time model, a verified SCD static model and a resolved CID on-line model, compares and outputs a comparison result to the network communication module, and the comparison result is uploaded to a background system to realize real-time on-line monitoring. By the system, a phenomenon that a virtual terminal and a secondary circuit of the protection device make mistakes due to SCD file modification, CID configuration errors, secondary circuit real-time connection obstacles corresponding to GOOSE messages and the like can be timely found, so that operational risks of a substation are reduced.",['Y04S10/40']
CN119398109B,"Domain knowledge injection method, device, equipment and medium for language embedding model","The application relates to the technical field of embedded models and provides a method, a device, equipment and a medium for injecting domain knowledge based on a language embedded model, wherein the method comprises the steps of obtaining a target language model with the same architecture as the embedded model; selecting and copying a first target knowledge layer from a plurality of first original transformers of a target language model, inserting the first target knowledge layer into the target language model to generate a new language model, pre-training the first target knowledge layer by using preset domain knowledge data to obtain a first target domain knowledge layer, inserting the first target domain knowledge layer into a corresponding position of an embedded model to obtain a new first embedded model, and performing small sample comparison learning on the new first embedded model by using the domain knowledge data to obtain the target embedded model only by updating the weight of the domain knowledge layer. According to the technical scheme, full-parameter training is not needed, the searching capability on professional data is improved well, and catastrophic forgetting can be avoided.","['G06N3/0455', 'G06N3/096', 'Y02D10/00']"
WO2021182199A1,"Information processing method, information processing device, and information processing program","In this information processing method, task processing is performed using a trained model that has been trained to output output data having a different modality to the modality of input data, taking input data having a plurality of different modalities as an input. The trained model is trained using pre-trained models (M1, M2, M3) that have been pre-trained by means of pre-training data including the modalities of the input data and the output data.",['G06N20/00']
CN116306672A,A data processing method and device thereof,"A data processing method relates to the field of artificial intelligence, comprising the following steps: acquiring a first text, wherein the first text is a program code; predicting a first editing position and an editing action type through a first neural network according to the first text; and predicting an object code corresponding to the first editing position through a natural language generation model according to the first editing position and the context code in the first editing position in the first text. The method and the device predict the position needing to be edited and the action of editing in the code through the first neural network, then perform code synthesis based on editing information, further realize the prediction of the editing position, and further realize the editing of the code.","['G06F40/30', 'G06N3/04', 'G06N3/08', 'G06N5/04']"
US20230083000A1,Systems and methods for detection and correction of ocr text,"OCR-text correction system and method embodiments are described. The OCR-text correction embodiments comprise or cooperate with a transformer-based sequence-to-sequence language model. The model is pretrained to denoise corrupted text and is fine-tuned using OCR-correction-specific examples. Text obtained at least in part through OCR is applied to the fine-tuned pretrained transformer model to detect at least one error in a subset of the text. Responsive to detecting the at least one error, the fine-tuned pretrained transformer model outputs an updated subset of the text to correct the at least one error.","['G06V30/133', 'G06V30/19147', 'G06V30/26']"
US20250068417A1,"Systems and methods for a bifurcated model architecture for generating concise, natural language summaries of blocks of code","Systems and methods are described for generating code summaries using bifurcated model architectures. The bifurcated model architecture may comprise a first model that generates code summaries based on native code script, and the second model may compare, de-duplicate, and/or categorize the code summaries into clusters that perform the same or similar functions. That is, the systems and methods may comprise a first model (e.g., a large language model) to predict new content (e.g., a code summary for a given native script code). The outputs of the first model are then inputted into a natural language processing (NLP) model to compare the code summaries. For example, the first model of the bifurcated architecture performs a crucial pre-processing step that prevents the NLP model from generating cluster descriptions that are specific to semantic structures, programming languages, and/or code formatting.",['G06F8/73']
WO2025112453A1,"Autonomous driving model, method, apparatus and vehicle capable of achieving multi-modal interaction","Provided are an autonomous driving model, method, apparatus and vehicle capable of achieving multi-modal interaction. An input layer (210) in an autonomous driving model (200) is configured to receive historical decision information (201), sensing information (202), traffic information (203) and interaction information (204) at a current moment; an encoding layer (220) is configured to encode input information; an autoregressive inference layer (230) is configured to obtain a hidden state for a next moment; and a decoding layer (240) is configured to perform decoding so as to obtain interaction information for the next moment and autonomous driving decision-making information for the next moment. The autonomous driving model (200) may understand a current driving environment by inferring the sensing information (202), the traffic information (203) and the interaction information (204), and gains a better understanding of the influence of historical operations on the autonomous driving process by inferring historical decision-making data, thereby making an output result of the autonomous driving model achieve better interpretability and controllability.","['G06N3/042', 'B60W50/00', 'B60W60/00', 'G06N3/0455', 'G06N3/08', 'G06N5/04', 'B60W2050/0002']"
US20240346342A1,Generative machine learning models for genealogy,"Disclosed herein are methods, systems, and non-transitory computer readable mediums for generating a shareable genealogical summary for a target individual. An example method includes receiving a request from a user to generate a shareable genealogical summary about a target user. The method generates the shareable genealogical summary comprising a genealogical history of the target user. The method provides genealogical information for the target user to a machine-learning language model. The genealogical information includes a family tree. The method receives a response generated by executing the machine-learning language model from a model serving system. The method provides the shareable genealogical summary for display to the user.","['G06N5/022', 'G06F40/106', 'G06F40/284', 'G06F40/30', 'G06F40/56', 'G06N3/044', 'G06N3/045', 'G06N5/01', 'G06N7/01']"
US20250232217A1,Copilot implementation: training an expansion machine learning tool,"Apparatus and methods are disclosed for implementing a copilot as a network of microservices including specialized large language models (LLMs) or other trained machine learning (ML) tools. This architecture supports flexible, customizable, or dynamically determinable dataflow. Compared to much larger competing LLMs, comparable or superior performance is achieved for certain tasks, while significantly reducing computation time and hardware requirements, even to a single compute node with a single GPU. An expansion microservice converts client input tokens into associated tokens to diversify targeted tasks presented to a core microservice. An ML tool in the expansion microservice is pretrained in multiple stages for varying tasks using varying general and target-specific corpora. Synthesized training data derived from a knowledge graph can also be used. The expansion ML tool is subsequently fine-tuned in one or multiple stages. Variations and additional techniques are disclosed.","['G06F16/2433', 'G06F16/2455', 'G06F16/252', 'G06F16/258', 'G06F16/3329', 'G06F40/284', 'G06N20/00', 'G06N5/022']"
US12079587B1,Multi-task automatic speech recognition system,"Disclosed herein are methods, systems, and computer-readable media for generating an output transcript from an input audio segment using a multi-task transformer model. In some embodiments, the transformer model can be trained to transcribe or translate audio data in multiple languages using labeled audio data. The labeled audio data can include first audio segments associated with first same-language transcripts of the first audio segments and second audio segments associated with second different-language transcripts of the second audio segments. In some embodiments, a vocabulary of the model can include special purpose and time stamp tokens. The special purpose tokens can specify tasks for the model to perform.",['G06F40/58']
US12260179B2,Incorporating unstructured data into machine learning-based phenotyping,"Implementations are described herein for incorporating unstructured data into machine learning-based phenotyping. In various implementations, natural language textual snippet(s) may be obtained. Each natural language textual snippet may describe environmental or managerial features of an agricultural plot that exist during a crop cycle. A sequence-to-sequence machine learning model may be used to encode the natural language snippet(s) into embedding(s) in embedding space. The embedding(s) may semantically represent the environmental or managerial features of the agricultural plot. Using one or more phenotypic machine learning models, phenotypic prediction(s) may be generated about the agricultural plot based on the one or more semantic embeddings and additional structured data about the agricultural plot. Output may be provided at one or more computing devices that is based on one or more of the phenotypic predictions.","['G06N3/0455', 'G06F40/126', 'G06F40/237', 'G06F40/30', 'G06N3/042', 'G06N3/0442', 'G06N3/045', 'G06Q10/06', 'G06Q50/02', 'G06V10/82', 'G06V20/188', 'G10L15/1815', 'G10L15/22', 'G06F40/18', 'G06N3/0464', 'G06N3/084', 'G06N3/09']"
US20240354424A1,System and methods for unbiased transformer source code vulnerability learning with semantic code graph,"The present disclosure presents vulnerability code detection systems and related methods. One such method comprises executing, by a client computing device, a joint RoBERTa and graph convolutional neural network model that is configured to detect a code vulnerability attack on a computing device. The model can analyze the code structure and its connections and identify any irregularities or patterns that could be used to exploit vulnerabilities. Once the GCNN model has analyzed the code, it can provide insights to the user or system administrator about potential vulnerabilities and provide suggested actions to remediate them.",['G06F21/577']
US20240046074A1,"Methods, systems and computer program products for media processing and display","The present disclosure overcomes the above-noted and other deficiencies by providing systems and methods for image processing and data analysis that may be utilized for identifying, classifying, researching and analyzing subjects and/or objects including, but not limited to vehicles, vehicle parts, vehicle artifacts, cultural artifacts, geographical locations, etc. To identify all of the subjects and/or objects in a photo, alone or in combination with a geographical location and/or a cultural heritage subject and/or object, and then to associate a narrative with them represents a unique challenge.","['G06N3/0455', 'G06N3/044', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06V10/74', 'G06V10/764', 'G06V10/774', 'G06V10/776', 'G06V10/82', 'G10L15/063', 'G10L15/16', 'G10L15/18']"
US8867614B2,Image coding method and image decoding method,"According to an embodiment, an image coding method includes a transforming and quantizing step of orthogonally transforming and quantizing each of a plurality of blocks as a unit within a target region of an input image to obtain coefficients; and a variable-length coding step of coding a plurality of coefficient strings each having coefficients of same frequency component among the blocks within the target region by using a plurality of variable length coding schemes: The variable-length coding step includes switching among the variable-length coding schemes for each coefficient string.","['H04N19/00121', 'H04N19/176', 'H04N19/0009', 'H04N19/00157', 'H04N19/00278', 'H04N19/00296', 'H04N19/00775', 'H04N19/00781', 'H04N19/124', 'H04N19/13', 'H04N19/14', 'H04N19/18', 'H04N19/60', 'H04N19/61']"
CN221960554U,Matrix multiplication computing device for AI accelerator integrated circuit and AI accelerator device,"The present utility model relates to a matrix multiplication calculation device for an AI accelerator integrated circuit and an AI accelerator device. A matrix multiplication computing device is provided for compressing weights of a neural network in a computing accelerator. The apparatus includes a memory configured to store weight matrix elements in a first format, the first format including a scale factor and a column of mantissa blocks. The first register is configured to receive the scaling factor for each weight matrix column via a crossbar device coupled to the memory, the converter is configured to determine a maximum exponent using the scaling factor for the column, and the second register is configured to store the maximum exponent. Further, the first register is configured to receive a mantissa block of a column and the converter is configured to determine a conversion block using all scale factors and mantissa blocks. The weight buffer receives the conversion block and the maximum exponent as elements of a second format that is used by the computing device to determine matrix multiplication outputs, which are stored in the output buffer.","['G06N3/0455', 'G06F17/16', 'G06N3/0499', 'G06N3/063']"
WO2021217772A1,"Ai-based interview corpus classification method and apparatus, computer device and medium","An AI-based interview corpus classification method and apparatus, a computer device, and a readable storage medium. Said method comprises: classifying interviewees by fully considering several corpus features of an interviewees' corpus in an interview scenario related to a prediction result according to a lightweight model obtained by training on the basis of a GPT model, and storing the classification result in a blockchain network node. The interviewees are classified by means of the lightweight model obtained from the GPT model. Because a loss function of each layer of the lightweight model relative to the GPT model is calculated, the accuracy of an output result of the lightweight model and the consistency between the output result of the lightweight model and the output result of the GPT model can be ensured, thereby solving the technical problem of low accuracy of a classification result obtained by using a lightweight network to classify interviewees in the prior art.","['G06F18/24', 'G06F16/3329', 'G06F18/213', 'G06F40/30']"
US20250190717A1,"Method performed by an electronic device, electronic device and computer-readable storage media","A language model updating method, performed by an electronic device includes, acquiring a first training sequence including at least one of text used for training or a token sequence corresponding to the text used for training; extracting a first general knowledge representation of the first training sequence using a first language model; using a second language model to extract a second general knowledge representation of the first training sequence and determine a plurality of prediction results corresponding to the first training sequence; and updating the second language model based on the first general knowledge representation, the second general knowledge representation, and the plurality of prediction results, wherein each of the first general knowledge representation and the second general knowledge representation include at least one of language patterns or semantic relationships, extracted from the first training sequence.","['G06N3/0455', 'G06F40/40', 'G06F40/284', 'G06F40/30', 'G06N3/042', 'G06N3/084', 'G06N3/096', 'G06N5/02', 'G06N5/022']"
CN113343235B,"Application layer malicious effective load detection method, system, device and medium based on Transformer","The invention discloses a method, a system, equipment and a medium for detecting malicious effective loads of an application layer based on a Transformer, wherein the method comprises the following steps: constructing an application layer user request sample set, wherein each sample comprises an application layer user request and malicious effective load information thereof, and the malicious effective load information comprises one or more groups of specific malicious effective loads and categories thereof; carrying out data preprocessing on the sample set to obtain a user request and a category real label of each word element; training a pre-constructed transform-based deep neural network model by using the preprocessed sample set; predicting the category of the malicious effective load of each word element in the application layer user request to be detected by using the model obtained by training; and finally combining continuous and same-class lemmas, and determining malicious effective load information in the user request. The invention can accurately and effectively detect the malicious effective load information in the application layer user request.","['G06F21/562', 'G06F18/214', 'G06N3/045', 'G06N3/08']"
CN111414513A,Music genre classification method and device and storage medium,"The embodiment of the invention discloses a music genre classification method, a music genre classification device and a storage medium. The scheme includes the steps of collecting positive and negative samples of music of a preset genre, extracting first lyric text data of the music in the positive and negative samples, preprocessing the first lyric text data, extracting audio data of the music in the positive and negative samples, preprocessing the audio data, compressing the audio data, extracting first audio features in the compressed audio data, extracting semantic codes of the first lyric text data according to a trained preset language model, combining the semantic codes with the first audio features, inputting the combined data to a music classification model generated based on the language model for training, and determining the music genre of the music to be classified according to the trained music classification model. According to the scheme provided by the embodiment of the application, the genre of the music to be classified is determined through the trained classification model, and the accuracy of music genre classification is improved.","['G06F16/65', 'G06F16/683', 'G06F16/685', 'Y02D10/00']"
CN112364170B,"Data emotion analysis method and device, electronic equipment and medium","The invention provides a data emotion analysis method, a device, electronic equipment and a medium, wherein the method comprises the following steps: the method comprises the following steps: collecting comment information; obtaining the score of the user in the comment information; obtaining emotion polarities of the comment information by adopting keyword matching and dictionary rules based on a dictionary, wherein the emotion polarities comprise neutral, negative and positive; analyzing the probability of the comment information belonging to different emotion polarities based on machine learning; converting the scores, the emotion polarities and the probabilities of the emotion polarities into the same range by adopting a mapping method; and obtaining the emotion polarity and the score of the comment information by adopting a weighted voting fusion mode according to the scores, the emotion polarities and the emotion polarities converted into the same range. The method has good generalization effect and high analysis accuracy.","['G06F16/353', 'G06F16/3346', 'G06F40/242', 'G06F40/289', 'G06N20/00']"
US20240427810A1,System and method for training a multi-tenant language model,"A system and method are disclosed for reducing false responses from a large language model. The method includes: mapping a data field from a first source to a semantic layer, the semantic layer including a plurality of data fields; storing data from the first source in a database based on the semantic layer; tokenizing each data field for a first large language model (LLM); fine-tuning the first LLM based on the tokenized semantic layer; providing a prompt to the first LLM, which configures it to generate an output answer; providing the output answer to a second LLM, which configures it to generate a query for the database; executing the query on the database to generate a database output based on the stored data; and providing the output answer in a user interface (UI) in response to determining that the database output and the output answer are within a predefined threshold.","['G06F40/284', 'G06F16/3344', 'G06F16/34', 'G06F16/9024']"
WO2025112451A1,"Autonomous driving method, apparatus and vehicle capable of following instructions for self-recovery","The present disclosure relates to the field of computer technology and particularly relates to the technical fields of autonomous driving and artificial intelligence. Provided are an autonomous driving method, apparatus and vehicle capable of following instructions for self-recovery. The autonomous driving method comprises: acquiring input information; encoding the input information to obtain an input tensor corresponding to the input information; performing autoregressive inference on the input tensor to obtain a hidden state for a first moment after a current moment; and performing decoding on the basis of the hidden state of the first moment to obtain interaction information for the first moment and autonomous driving decision-making information for the first moment, wherein the interaction information for the first moment comprises a signal used for indicating that assistance is required during autonomous driving. In this way, prompt information for natural language interaction can be used to instruct an autonomous driving model to control a vehicle, thereby achieving the rapid generation of a self-recovery scheme. In addition, the vehicle can perform autonomous self-recovery on the basis of self-recovery scheme instructions.",['B60W60/001']
WO2022151966A1,"Processing method and apparatus for language model, text generation method and apparatus, and medium","Disclosed are a processing method and apparatus for a language model, a text generation method and apparatus, and a medium. A language model is deployed in an electronic device, and in the same feature layer of the language model, a plurality of calculation operations between calculations of a plurality of target types are merged into a fused calculation operation. The processing method comprises: upon it being determined that a fused calculation operation is to be executed, a CPU of an electronic device sending, to a GPU, an operation instruction that includes a plurality of calculation operations; and in response to receiving the operation instruction, the GPU processing the plurality of calculation operations.","['G06F40/216', 'G06F40/279']"
CN116595252A,A data processing method and related device,"The data processing method can be applied to the field of artificial intelligence and comprises the following steps: acquiring first data; the first data comprises attribute information of a user or an article in multiple dimensions; generating second data according to the first data, wherein the second data is natural language text for describing the attribute information; according to the first data, a first characteristic representation is obtained through an embedded network, wherein the first characteristic representation comprises a plurality of embedded vectors, and each embedded vector corresponds to one dimension; obtaining a second feature representation through a natural language processing model according to the second data, wherein the second feature representation comprises semantic features of the second data; and updating the embedded network according to the difference between the second characteristic representation and the first characteristic representation to obtain the updated embedded network. The application combines the cooperative information and the semantic information for modeling, and can fully utilize the knowledge of the language model, thereby improving the prediction effect.","['G06F16/9535', 'G06F16/3344', 'G06F40/30', 'G06N3/048', 'G06N3/084', 'G06Q30/0631']"
CA3127965A1,Machine learning guided polypeptide analysis,"Systems, apparatuses, software, and methods for identifying associations between amino acid sequences and protein functions or properties. The application of machine learning is used to generate models that identify such associations based on input data such as amino acid sequence information. Various techniques including transfer learning can be utilized to enhance the accuracy of the associations.","['G06N3/088', 'G16B15/20', 'G06N3/045', 'G16B20/00', 'G06N20/10', 'G06N3/044', 'G06N3/0442', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/048', 'G06N3/08', 'G06N3/082', 'G06N3/084', 'G06N3/0895', 'G06N3/09', 'G06N3/094', 'G06N3/096', 'G06N5/01', 'G06N5/022', 'G06N7/01', 'G16B40/20', 'G16B40/30']"
WO2024245343A1,Data processing method and related device,"A data processing method, which relates to the field of artificial intelligence. The method comprises: acquiring a first feature representation and a second feature representation, wherein the first feature representation is obtained after feature extraction is performed on first text, the second feature representation is obtained after feature extraction is performed on a prompt, and the prompt instructs to perform compression according to a target compression rate; compressing the first feature representation and the second feature representation at the target compression rate, so as to obtain a compressed feature representation; and obtaining second text according to the compressed feature representation and by means of a large language model, wherein the second text is used as reply text of the first text. In the present application, a compression rate is carried in a prompt, such that apriority of compression information can be provided for a large model, and thus the large model can generate more accurate reply text when there is a loss in an input.","['G06F16/3329', 'G06F16/1744', 'G06F16/3344', 'G06F16/35']"
CN118350800A,Laboratory fault prediction operation and maintenance system and method,"The invention discloses a laboratory fault prediction operation and maintenance system and a method, which relate to the technical field of laboratory operation and maintenance and comprise the following steps: acquiring historical operation data of laboratory equipment in advance, determining laboratory environment parameters matched with the historical operation data according to the historical operation data, and respectively constructing a laboratory equipment inspection threshold value and a laboratory environment monitoring threshold value for fault diagnosis; and constructing a GPT large language model, and constructing a basic environment data corpus and a laboratory knowledge base according to laboratory equipment inspection thresholds and laboratory equipment operation and laboratory environment change correlation curves. According to the invention, a correlation curve of laboratory equipment operation and laboratory environment change is constructed, a laboratory environment monitoring threshold is obtained, the operation mode and environment parameter setting of the laboratory equipment are dynamically adjusted, the smooth performance of an experiment process is ensured by dynamic combination, meanwhile, the probability of laboratory equipment failure is reduced to the greatest extent, and the prospective prediction and efficient operation and maintenance of the laboratory equipment failure are realized.","['G01D21/02', 'G06N3/045', 'G06N3/088', 'G06N5/02', 'G06Q10/04', 'G06Q10/20']"
CN114911892A,"Interactive Layer Neural Networks for Search, Retrieval and Sorting","An interaction layer neural network for searching, retrieving and ranking. A language system includes a controller. The controller may be configured to receive queries and documents, tokenize the queries into query token sequences and the documents into document token sequences, generate a token pair matrix for each query and document token, retrieve a pre-computed similarity score produced by a neural conditional translation probability network for each entry in the token pair matrix, wherein the neural network has been trained in a ranking task using a corpus of pairs of queries and corresponding related documents, produce a ranking score for each document relative to each query via a sum-sum aggregate of each similarity score for the corresponding query; and outputting the document and the associated ranking score of the document.","['G06N3/02', 'G06F16/24578', 'G06F16/319', 'G06F16/3329', 'G06F16/3331', 'G06F16/338', 'G06F40/216', 'G06F40/284', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/047', 'G06N3/0495', 'G06N3/0499', 'G06N3/08', 'G06N3/09', 'G06F16/3346', 'G06F16/3347', 'G06N3/048', 'G06N7/01']"
US20240058704A1,"Method, etc. for generating trained model for predicting action to be selected by user","One or more embodiments of the invention is a method for generating a trained model for predicting an action to be selected by a user in a game that proceeds in accordance with actions selected by the user, while updating game states, the method including: determining weights for individual history-data element groups; generating training data from data of game states and actions included in the history-data element groups; and generating a trained model on the basis of the generated training data, wherein the generation of training data includes generating a number of items of game state text as game state text corresponding to one game state, having different orders of a plurality of text elements, the number being based on the determined weight, and generating training data including pairs of the individual generated items of game state text and corresponding action text.","['A63F13/67', 'A63F13/798', 'G06F40/40', 'G06N20/00', 'A63F2300/206']"
CN117198267A,"Local dialect voice intelligent recognition and question-answering method, system, equipment and medium","Local dialect voice intelligent recognition and question answering method, system, equipment and medium, weNet2.0 is Mandarin voice recognition model M 1 The method comprises the steps of carrying out a first treatment on the surface of the Acquiring a local dialect text translation task training sample set R; the transducer model is a local dialect text translation model M 2 The method comprises the steps of carrying out a first treatment on the surface of the Inputting R into M 2 In the pair M 2 Iterative training to obtain a local dialect text translation model M 2 'A'; inputting user test dialect audio into M 1 The resulting intermediate text vector input M 2 ' in, get Mandarin text; inputting Mandarin text into a customizable retrievable question-answer model, if Mandarin text and Q 1 And Q 2 After the matching degree is calculated, the knowledge base of question and answer is hit, a corresponding answer text is output, otherwise, the Mandarin text is input into a generated large language model, and the answer text is generated; inputting the reply text into a TTS model to obtain corresponding audio; system, device, and medium: for implementing the method; the method solves the problem of low recognition accuracy of the local dialect voice recognition method.",[]
