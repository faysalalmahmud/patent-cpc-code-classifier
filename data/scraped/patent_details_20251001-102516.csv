publication_number,title,abstract,cpc_codes
US20200074169A1,System And Method For Extracting Structured Information From Image Documents,"A system and method for extracting structured information from image documents is disclosed. An input image document is obtained, and the input image document may be analyzed to determine a skeletal layout of information included in the input image document. A measure of similarity between the determined skeletal layout and each of the document templates may be determined. A document template may be selected as a matched template, based on the determined measure of similarity. Box areas from the input image document may be cropped out, and optical character recognition (OCR) may be performed on the box areas. Obtained recognized text may be automatically processed using directed search to correct errors made by the OCR. Statistical language modeling may be used to classify the input image document into a classification category, and the classified input image document may be processed according to the classification category.","['G06K9/00449', 'G06Q40/08', 'G06F17/2765', 'G06F18/22', 'G06F40/174', 'G06F40/279', 'G06K9/00456', 'G06K9/00463', 'G06K9/00483', 'G06Q50/18', 'G06V10/75', 'G06V30/412', 'G06V30/413', 'G06V30/414', 'G06V30/418', 'G06K2209/01', 'G06V30/10']"
US20210209774A1,"Image adjustment method and apparatus, electronic device and storage medium","An image adjustment method and apparatus, an electronic device and a storage medium are provided. The image adjustment method includes: generating a combination image of a target person and a target clothing based on a target clothing image and a target person image; obtaining an adjustment parameter of the target clothing in the target clothing image based on image features of the target clothing image and image features of the combination image; obtaining a deformation image of the target clothing according to the adjustment parameter and the target clothing image.","['G06T7/33', 'G06T3/147', 'G06K9/00362', 'G06K9/46', 'G06Q30/0643', 'G06T11/60', 'G06T3/0075', 'G06T5/50', 'G06V10/267', 'G06V10/464', 'G06V10/764', 'G06V10/82', 'G06V40/10', 'G06V40/113', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30196', 'G06T2210/16', 'Y02P90/30']"
US9253511B2,Systems and methods for performing multi-modal video datastream segmentation,"Systems and methods are described that can provide users with personalized video content feeds. In several embodiments, a multi-modal segmentation process is utilized that relies upon cues derived from video, audio and/or text data present in a video data stream. In a number of embodiments, video streams from a variety of sources are segmented. Links are identified between video segments and between video segments and online articles containing additional information relevant to the video segments. The additional information obtained by linking a video segment to an additional source of data can be utilized in the generation of personalized playlists. In the context of news programming, the dynamic mixing and aggregation of news videos from multiple sources can greatly enrich the news watching experience. In several embodiments, processes for linking video segments to additional sources of data can be implemented as part of a video search engine service.","['H04N21/23418', 'G06F16/24578', 'G06F16/319', 'G06F16/435', 'G06F16/4387', 'G06F16/5866', 'G06F16/738', 'G06F16/78', 'G06F16/951', 'G06K9/00718', 'G06V20/41', 'H04N21/222', 'H04N21/233', 'H04N21/23424', 'H04N21/237', 'H04N21/25891', 'H04N21/26258', 'H04N21/2665', 'H04N21/2668', 'H04N21/8126', 'H04N21/8405', 'H04N21/8455', 'H04N21/8456', 'H04N21/854']"
US8638993B2,Segmenting human hairs and faces,"Systems for segmenting human hairs and faces in color images are disclosed, with methods and processes for making and using the same. The image may be cropped around the face area and roughly centered. Optionally, the illumination environment of the input image may be determined. If the image is taken under dark environment or the contrast between the face and hair regions and background is low, an extra image enhancement may be applied. Sub-processes for identifying the pose angle and chin contours may be performed. A preliminary mask for the face by using multiple cues, such as skin color, pose angle, face shape and contour information can be represented. An initial hair mask by using the abovementioned multiple cues plus texture and hair shape information may be created. The preliminary face and hair masks are globally refined using multiple techniques.","['G06V40/162', 'G06V40/171']"
WO2025007949A1,Method for detecting defect on surface of copper clad laminate on basis of multi-scale gridding,"The present application provides a method for detecting a defect on the surface of a copper clad laminate on the basis of multi-scale gridding, said method comprising: S1, acquiring and capturing an image of a copper clad laminate by means of a line scan camera; S2, segmenting a copper clad region and a non-copper clad region, and rotating the copper clad region to the front; S3, dividing the copper clad region in the image according to set grid parameters, and performing a pyramid operation on a divided grid graph; S4, calculating the mean of each layer; S5, fusing the means of all of the layers by using a mean degradation coefficient, and obtaining a final background mean; and S6, by using a background-foreground difference, searching for a final defect and classifying same. According to the present application, the amount of computation is reduced, and local information of the background is retained, thus increasing detection speed, and also ensuring the stability of precision. A good detection effect can be obtained when high-speed detection must be performed on a material.","['G01N21/8851', 'G06T3/02', 'G06N3/048', 'G06N3/08', 'G06T5/40', 'G06T5/70', 'G06T7/0004', 'G06T7/001', 'G06T7/11', 'G06T7/194', 'G06T7/62', 'G01N2021/8854', 'G01N2021/8887', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30141', 'Y02P90/30']"
CN107038424B,A gesture recognition method,"The invention belongs to the technical field of gesture recognition, and relates to a gesture recognition method, which comprises the following steps: acquiring a depth image sequence in real time through Kinect; acquiring the positions of the palm center and the elbow center in real time through the Kinect, and extracting the hand contour according to the depth information of each frame of image; calculating the positions of the fingertips and the positions of the finger roots according to the center positions of the palms, the elbow and the hand contours, and extracting features; matching the features extracted in the step3 with gesture features in a template library, entering a classifier, selecting the closest gesture as the recognized gesture according to the classification standard of the classifier, and storing the recognized gesture of each frame in a queue Q; and analyzing the recognition results of the frame and the previous four frames in the queue Q, and selecting the gesture with the largest occurrence number as the final recognition result. According to the method, the Kinect is used for obtaining the depth information, and the digital image analysis technology is combined, so that the gesture of the operator can be rapidly and accurately recognized.","['G06V40/28', 'G06F18/24']"
CN112907510A,Surface defect detection method,"A surface defect detection method comprises the steps of collecting images of a defect position through a camera; the method mainly aims to detect and classify the defect image by adopting an Adaboost and DCNN fusion mode under the condition that a defect image training sample is insufficient, so that the recognition precision of the surface defect is remarkably improved, and a basis is provided for the surface defect detection of a complex irregular object.","['G06T7/0004', 'G01N21/8851', 'G06F18/214', 'G06F18/2411', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06T7/12', 'G06T7/13', 'G06T7/136', 'G01N2021/8854', 'G01N2021/8887', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/30164', 'Y02P90/30']"
WO2019192397A1,End-to-end recognition method for scene text in any shape,"An end-to-end recognition method for a scene text in any shape, comprising: extracting a text feature by means of a feature pyramid network for generating a candidate text box by a region extraction network; and then adjusting the position of the candidate text box by means of a fast region classification and regression branch to obtain more accurate text bounding box position information; inputting the text bounding box position information into a segmentation branch to obtain a predicted symbol sequence by means of a pixel voting algorithm; finally, processing the predicted symbol sequence by means of a weighted edit distance algorithm to locate the most matching word of the predicted sequence in a given dictionary to obtain the final text recognition result. The method can detect and recognize scene texts in any shape in a natural image, comprising a horizontal text, a multi-directional text, and a curved text, and can perform an end-to-end training completely. The detection and recognition method has a strong practical application value.","['G06V10/267', 'G06N3/084', 'G06F18/214', 'G06N3/044', 'G06N3/045', 'G06V10/25', 'G06V10/454', 'G06V10/82', 'G06V20/63', 'G06V30/153']"
CN104317391B,A kind of three-dimensional palm gesture recognition exchange method and system based on stereoscopic vision,"The invention discloses a kind of three-dimensional palm gesture recognition method and system based on stereoscopic vision, belong to computer vision and human-computer interaction technique field.The present invention includes：The collection of data is carried out using stereo Vision collecting device, the three-dimensional data including depth information is obtained；Palm is split by obtained three-dimensional data, then with plane fitting, the posture of palm in three dimensions is obtained；According to the palm posture of Tracking Recognition, predefined standard gesture is matched, specific three-dimensional applications is carried out, realizes man-machine interaction.Posture of the invention by recognizing palm, so that the precision of identification is higher, the operation that less appearance is misidentified, while the problem of solving the high operand for being generally basede on finger tip identification, realize more real-time human-computer interaction function, it is ensured that interactive accuracy and ease for use.","['G06F3/017', 'G06V40/20']"
US8411932B2,"Example-based two-dimensional to three-dimensional image conversion method, computer readable medium therefor, and system","An example-based 2D to 3D image conversion method, a computer readable medium therefore, and a system are provided. The embodiments are based on an image database with depth information or with which depth information can be generated. With respect to a 2D image to be converted into 3D content, a matched background image is found from the database. In addition, graph-based segmentation and comparison techniques are employed to detect the foreground of the 2D image so that the relative depth map can be generated from the foreground and background information. Therefore, the 3D content can be provided with the 2D image plus the depth information. Thus, users can rapidly obtain the 3D content from the 2D image automatically and the rendering of the 3D content can be achieved.","['H04N13/261', 'H04N13/128', 'H04N2213/003']"
US10109055B2,Multiple hypotheses segmentation-guided 3D object detection and pose estimation,"A machine vision system and method uses captured depth data to improve the identification of a target object in a cluttered scene. A 3D-based object detection and pose estimation (ODPE) process is use to determine pose information of the target object. The system uses three different segmentation processes in sequence, where each subsequent segmentation process produces larger segments, in order to produce a plurality of segment hypotheses, each of which is expected to contain a large portion of the target object in the cluttered scene. Each segmentation hypotheses is used to mask 3D point clouds of the captured depth data, and each masked region is individually submitted to the 3D-based ODPE.","['G06T7/004', 'G06V10/464', 'G06F18/2411', 'G06K9/4604', 'G06K9/6202', 'G06K9/6269', 'G06T7/0081', 'G06T7/11', 'G06T7/162', 'G06T7/70', 'G06T7/73', 'H04N13/204', 'H04N13/239', 'H04N13/271', 'G06T2200/04', 'G06T2207/10028', 'G06T2207/20072', 'G06T2207/20076', 'H04N13/243', 'H04N13/344', 'H04N2013/0092']"
US10373380B2,3-dimensional scene analysis for augmented reality operations,"Techniques are provided for 3D analysis of a scene including detection, segmentation and registration of objects within the scene. The analysis results may be used to implement augmented reality operations including removal and insertion of objects and the generation of blueprints. An example method may include receiving 3D image frames of the scene, each frame associated with a pose of a depth camera, and creating a 3D reconstruction of the scene based on depth pixels that are projected and accumulated into a global coordinate system. The method may also include detecting objects, and associated locations within the scene, based on the 3D reconstruction, the camera pose and the image frames. The method may further include segmenting the detected objects into points of the 3D reconstruction corresponding to contours of the object and registering the segmented objects to 3D models of the objects to determine their alignment.","['G06T19/006', 'G06T7/10', 'G06T2207/10021', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/30244']"
US6788809B1,System and method for gesture recognition in three dimensions using stereo imaging and color vision,"A system and method for recognizing gestures. The method comprises obtaining image data and determining a hand pose estimation. A frontal view of a hand is then produced. The hand is then isolated the background. The resulting image is then classified as a type of gesture. In one embodiment, determining a hand pose estimation comprises performing background subtraction and computing a hand pose estimation based on an arm orientation determination. In another embodiment, a frontal view of a hand is then produced by performing perspective unwarping and scaling. The system that implements the method may be a personal computer with a stereo camera coupled thereto.","['G06F3/017', 'G06V20/64', 'G06V40/107']"
CN111104867B,Recognition model training and vehicle re-recognition method and device based on part segmentation,"The invention provides a recognition model training and vehicle re-recognition method and device based on component segmentation, which relate to the technical field of image recognition and comprise the following steps: inputting an input image into a component segmentation network and a global feature extraction network of the identification model, and acquiring a plurality of mask images and a global feature image of the input image; each mask map corresponds to one view angle of an object to be identified in the input image; determining a plurality of local features corresponding to a plurality of mask graphs one by one according to the mask graphs and the global feature graph; determining a viewing angle perception distance between different input images according to a plurality of local features of the different input images; according to the visual angle perception distance, a loss function is constructed, and parameters of the recognition model are updated according to the loss function, so that training of the recognition model is completed.","['G06V20/40', 'G06F18/24147', 'G06N3/045', 'G06N3/08', 'G06V10/44', 'Y02T10/40']"
US12333690B2,"Method and device for removing handwritten content from text image, and storage medium","Provided by the present disclosure are a method and device for removing handwritten content from a text image, and a storage medium. The method for removing handwritten content from a text image comprises: acquiring an input image of a text page to be processed, wherein the input image comprises a handwritten area, and the handwritten area comprises the handwritten content; using an image segmentation model to recognize the input image so as to obtain an initial handwritten pixel of the handwritten content; performing blurring processing on the initial handwritten pixel to obtain a handwritten pixel mask area; determining the handwritten content according to the handwritten pixel mask area; and removing the handwritten content from the input image to obtain an output image.","['G06V30/147', 'G06V10/273', 'G06T5/77', 'G06T5/70', 'G06T7/11', 'G06T7/13', 'G06V10/28', 'G06V30/22', 'G06V30/2455']"
CN109344701B,Kinect-based dynamic gesture recognition method,"The invention discloses a dynamic gesture recognition method based on Kinect, which comprises the following steps of: collecting a color image sequence and a depth image sequence of the dynamic gesture by using Kinect V2; carrying out preprocessing operations such as hand detection and segmentation; extracting the space characteristic and the time sequence characteristic of the dynamic gesture, and outputting a space-time characteristic; inputting the output space-time features into a simple convolutional neural network to extract the space-time features of higher layers, and classifying by using a dynamic gesture classifier; and training dynamic gesture classifiers of the color image sequence and the depth image sequence respectively, and fusing and outputting by using a random forest classifier to obtain a final dynamic gesture recognition result. The invention provides a dynamic gesture recognition model based on a convolutional neural network and a convolutional long-time memory network, the spatial characteristics and the temporal characteristics of a dynamic gesture are respectively processed by the two parts, and a random forest classifier is adopted to fuse the classification results of a color image sequence and a depth image sequence, so that the recognition rate of the dynamic gesture is greatly improved.","['G06V40/28', 'G06F3/017']"
CN111563502B,"Image text recognition method and device, electronic equipment and computer storage medium","The application provides a text recognition method and device for an image, electronic equipment and a computer readable storage medium, and relates to the field of image processing. The method comprises the following steps: receiving an image to be detected; calling the trained character recognition model to process the image to be detected, or inputting the image to be detected into the character recognition model, so that the character recognition model generates at least two anchor frames with different inclination angles in the image to be detected based on the inclination angles in the preset anchor frame parameters; determining a region to be identified containing at least one keyword character in the image based on each anchor frame with different inclination angles; identifying at least one keyword character in the area to be identified, and performing mask processing on the area to be identified and each keyword character to generate labeling information of the area to be identified and each keyword character; and displaying the recognized keyword characters and the recognized labeling information in the image to be detected. The application effectively improves the character recognition precision.","['G06V10/25', 'G06F18/214', 'G06V10/242', 'G06V30/153', 'G06V30/10']"
US11074445B2,"Remote sensing image recognition method and apparatus, storage medium and electronic device","A remote sensing image recognition method, apparatus and device, a storage medium and a computer program. The remote sensing image recognition method comprises: performing resolution reduction processing on a remote sensing image to be recognized to obtain a resolution reduction processed remote sensing image (S100); segmenting at least one remote sensing image block from the resolution reduction processed remote sensing image (S110); determining, from the at least one remote sensing image block, a remote sensing image block to be processed (S120); inputting, into a neural network, the remote sensing image block to be processed, and obtaining, through the neural network, classification probability information of pixels in the input remote sensing image block to be processed (S130); and determining a recognition result of the remote sensing image to be recognized, according to the classification probability information of the pixels in the remote sensing image block to be processed (S140).","['G06K9/0063', 'G06N3/08', 'G06F18/2415', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G06N7/01', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/13']"
US8103109B2,Recognizing hand poses and/or object classes,"There is a need to provide simple, accurate, fast and computationally inexpensive methods of object and hand pose recognition for many applications. For example, to enable a user to make use of his or her hands to drive an application either displayed on a tablet screen or projected onto a table top. There is also a need to be able to discriminate accurately between events when a user's hand or digit touches such a display from events when a user's hand or digit hovers just above that display. A random decision forest is trained to enable recognition of hand poses and objects and optionally also whether those hand poses are touching or not touching a display surface. The random decision forest uses image features such as appearance, shape and optionally stereo image features. In some cases, the training process is cost aware. The resulting recognition system is operable in real-time.","['G06F3/017', 'G06F18/24323', 'G06F3/0425', 'G06V10/764', 'G06V40/28']"
US8787658B2,Image segmentation using reduced foreground training data,"Methods of image segmentation using reduced foreground training data are described. In an embodiment, the foreground and background training data for use in segmentation of an image is determined by optimization of a modified energy function. The modified energy function is the energy function used in image segmentation with an additional term comprising a scalar value. The optimization is performed for different values of the scalar to produce multiple initial segmentations and one of these segmentations is selected based on pre-defined criteria. The training data is then used in segmenting the image. In other embodiments further methods are described: one places an ellipse inside the user-defined bounding box to define the background training data and another uses a comparison of properties of neighboring image elements, where one is outside the user-defined bounding box, to reduce the foreground training data.","['G06V10/235', 'G06T7/11', 'G06T7/12', 'G06T7/143', 'G06T7/194']"
CN112639396B,"Dimension measuring apparatus, dimension measuring method, and semiconductor manufacturing system","The present disclosure relates to a dimension measuring apparatus that eliminates shortening of time required for dimension measurement and operator-induced errors. A dimension measuring device for measuring the dimension of a measurement object using an input image is provided, wherein a 1 st image in which each region of the input image is labeled with a label in a region-by-region manner is generated by machine learning, an intermediate image including a label indicating each region of the 1 st image is generated based on the generated 1 st image, a 2 nd image in which each region of the input image is labeled with a label in a region-by-region manner is generated based on the input image and the generated intermediate image, the coordinates of boundary lines of adjacent regions are obtained using the generated 2 nd image, the coordinates of feature points defining the dimension condition of the measurement object are obtained using the obtained coordinates of the boundary lines, and the dimension of the measurement object is measured using the obtained coordinates of the feature points.","['G06T7/62', 'G06T7/194', 'G01B15/04', 'G01B7/02', 'G01N23/2251', 'G06F18/214', 'G06T7/0004', 'G06T7/12', 'G06T7/33', 'G06T7/60', 'G06T7/73', 'G06V10/25', 'G06V10/273', 'G06V10/56', 'H01L21/67069', 'H01L22/12', 'G01B15/00', 'G01B2210/56', 'G06T2207/10061', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20092', 'G06T2207/20152', 'G06T2207/30148', 'G06T2207/30204', 'G06V2201/06']"
US8879813B1,Systems and methods for automated interest region detection in retinal images,"Embodiments disclose systems and methods that aid in screening, diagnosis and/or monitoring of medical conditions. The systems and methods may allow, for example, for automated identification and localization of lesions and other anatomical structures from medical data obtained from medical imaging devices, computation of image-based biomarkers including quantification of dynamics of lesions, and/or integration with telemedicine services, programs, or software.","['G06T5/94', 'G06T7/0014', 'A61B3/0025', 'A61B3/12', 'A61B3/14', 'G06F16/583', 'G06F16/5866', 'G06K9/0061', 'G06T3/14', 'G06T3/18', 'G06T3/40', 'G06T5/20', 'G06T7/0012', 'G06T7/0016', 'G06V10/267', 'G06V10/44', 'G06V10/50', 'G06V10/758', 'G06V40/18', 'G06V40/193', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'G16Z99/00', 'G06F16/51', 'G06T2207/10024', 'G06T2207/20016', 'G06T2207/20032', 'G06T2207/20036', 'G06T2207/30041', 'G06T2207/30096', 'G06T2207/30104', 'G06T2207/30168', 'G06V2201/03', 'G06V40/14']"
US10055013B2,Dynamic object tracking for user interfaces,"Systems and approaches provide for user interfaces that are based on object tracking. For example, the object may be a user's head or face. As the user moves his head or face and/or tilts a computing device, the content displayed on the computing device will adapt to the user's perspective. The content may include three-dimensional (3D) graphical elements projected onto a two-dimensional (2D) plane and/or the graphical elements can be associated with textural shading, shadowing or reflections that change according to user or device motion to give the user the impression that the user is interacting with the graphical elements in 3D environment. A state of motion of the device can be determined and jitter and/or latency corresponding to the rendering of content can be altered so as to minimize or decrease jitter when the device is stationary and/or to decrease or minimize latency when the device is in motion.","['G06F3/012', 'G06F3/0304', 'G06F3/04815', 'G06F2200/1637']"
US9262016B2,Gesture recognition method and interactive input system employing same,"A gesture recognition method comprises capturing images, processing the images to identify at least two clusters of touch points associated with at least two pointers, recognizing a gesture based on motion of the clusters, and updating a display in accordance with the recognized gesture.","['G06T7/20', 'G06F3/0428', 'G06F3/03', 'G06F3/042', 'G06F3/0425', 'G06F3/04883', 'G06F2203/04109', 'G06F2203/04808']"
US20190096135A1,Systems and methods for visual inspection based on augmented reality,"A system for visual inspection includes: a scanning system configured to capture images of an object and to compute a three-dimensional (3-D) model of the object based on the captured images; an inspection system configured to: compute a descriptor of the object based on the 3-D model of the object; retrieve metadata corresponding to the object based on the descriptor; and compute a plurality of inspection results based on the retrieved metadata and the 3-D model of the object; and a display device system including: a display; a processor; and a memory storing instructions that, when executed by the processor, cause the processor to: generate overlay data from the inspection results; and show the overlay data on the display, the overlay data being aligned with a view of the object through the display.","['G06T19/006', 'G02B27/0172', 'G06F18/2148', 'G06F18/24', 'G06F18/24765', 'G06K9/6257', 'G06K9/626', 'G06K9/6267', 'G06T17/00', 'G06T17/20', 'G06T19/20', 'G06T7/0006', 'G06T7/001', 'G06T7/11', 'G06T7/50', 'G06T7/73', 'G06V10/764', 'G06V10/82', 'G06V20/64', 'G02B2027/0138', 'G02B2027/014', 'G02B2027/0141', 'G06K2209/27', 'G06T2200/08', 'G06T2207/10008', 'G06T2207/10028', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2219/2004', 'G06T2219/2012', 'G06T2219/2016', 'G06V10/25', 'G06V2201/10']"
US9390506B1,Selective object filtering and tracking,"A computer-implemented method for tracking an object in a video including, in a first video frame: filtering the first video frame to delineate foreground from background and to select from the foreground an object to be tracked, said object identified at least by contour, center and color; constructing a first virtual bounding box minimally enclosing said object; using a tracking algorithm to establish a target location for the object in the second video frame based on similarity to the object identified in the first video frame; constructing a second virtual bounding box minimally enclosing the target shape at the target location; extending the second virtual bounding box; filtering the second video frame to delineate foreground from background using weighted values propagated from at least the first frame; scrutinizing the output of the second video frame by matching at least shape and color of the target shape at the target location.","['G06T7/248', 'G06T7/0067', 'G06K9/00765', 'G06K9/3241', 'G06K9/468', 'G06T5/20', 'G06T7/004', 'G06T7/0081', 'G06T7/194', 'G06T7/408', 'G06T2207/10016']"
CN110197146B,"Face image analysis method based on deep learning, electronic device and storage medium","The invention relates to a face image analysis method based on deep learning, an electronic device and a readable storage medium, wherein the method comprises the following steps: acquiring a human face picture to be analyzed, and calling a human face multi-attribute detection model to identify the human face picture to obtain a first preset type of human face attribute characteristics; calling a face angle discrimination model to identify the face picture to obtain a second preset type of face attribute features; calculating the facial image characteristics of a third preset type of the facial image by using a preset image processing rule; and respectively converting the recognized human face attribute characteristics of each first preset type, the recognized human face attribute characteristics of each second preset type and the recognized human face image characteristics of each third preset type into corresponding picture quality parameter values, and substituting the picture quality parameter values into a predetermined picture quality comprehensive parameter calculation formula to calculate the image quality comprehensive parameter values corresponding to the human face picture. The invention can comprehensively and accurately analyze the quality of the face image.","['G06N3/045', 'G06V40/161', 'G06V40/168', 'G06V40/172']"
US11361505B2,Model retrieval for objects in images using field descriptors,"Techniques are provided for one or more three-dimensional models representing one or more objects. For example, an input image including one or more objects can be obtained. From the input image, a location field can be generated for each object of the one or more objects. A location field descriptor can be determined for each object of the one or more objects, and a location field descriptor for an object of the one or more objects can be compared to a plurality of location field descriptors for a plurality of three-dimensional models. A three-dimensional model can be selected from the plurality of three-dimensional models for each object of the one or more objects. A three-dimensional model can be selected for the object based on comparing a location field descriptor for the object to the plurality of location field descriptors for the plurality of three-dimensional models.","['G06T17/00', 'G06F18/22', 'G06K9/6215', 'G06T17/20', 'G06T19/20', 'G06T7/75', 'G06V10/751', 'G06V20/64', 'G06V20/653', 'G06T2207/20084', 'G06T2219/2016']"
US20080240499A1,Jointly Registering Images While Tracking Moving Objects with Moving Cameras,A method tracks a moving object by registering a current image in a sequence of images with a previous image. The sequence of images is acquired of a scene by a moving camera. The registering produces a registration result. The moving object is tracked in the registered image to produce a tracking result. The registered current image is registered with the previous image using tracking result for all the images in the sequence.,['G06V10/24']
US9524440B2,System and method for superimposed handwriting recognition technology,"A system and method that is able to recognize a user's natural superimposed handwriting without any explicit separation between characters. The system and method is able to process single-stroke and multi-stroke characters. It can also process cursive handwriting. Further, the method and system can determine the boundaries of input words either by the use of a specific user input gesture or by detecting the word boundaries based on language characteristics and properties. The system and method analyzes the handwriting input through the processes of segmentation, character recognition, and language modeling. These three processes occur concurrently through the use of dynamic programming.","['G06K9/00865', 'G06V30/2268', 'G06F18/24', 'G06F3/04883', 'G06K9/00422', 'G06K9/52', 'G06K9/6267', 'G06K9/66', 'G06T5/002', 'G06T5/006', 'G06T5/70', 'G06T5/80', 'G06T7/0079', 'G06T7/10', 'G06T7/13', 'G06V10/42', 'G06V30/194', 'G06V30/274', 'G06V30/333', 'G06V30/36', 'G06T2207/20084']"
CN111801680B,Systems and methods for visual search and augmented reality,"Various embodiments of the present disclosure provide systems and methods for visual search and augmented reality in which an on-screen body of visual markers overlaid on an interface indicates the current state of an image recognition process. In particular, the visual marker body may take a number of actions, where a particular action indicates a particular state. Thus, the user can judge the current state of the scanning process by visually marking the behavior of the subject. The behavior of the visual marker body may also indicate recommended actions that may be taken to improve the scan conditions or otherwise facilitate the process to the user. In various embodiments, as the scanning process goes from one state to another, the visually marked on-screen body may correspondingly move or seamlessly transition from one behavior to another.","['G06T19/006', 'G06F16/532', 'G06F16/5838', 'G06T13/80', 'G06T7/20', 'G06T7/248', 'G06V10/17', 'G06V10/40', 'G06V20/20', 'G06T2207/30204', 'G06V10/62']"
US8233726B1,Image-domain script and language identification,"Disclosed herein is a method, computer system and computer program product for identifying a writing system associated with a document image containing one or more words written in the writing system. Initially, a document image fragment is identified based on the document image, wherein the document image fragment contains one or more pixels from one or more of the words in the document image. A set of sequential features associated with the document image fragment is generated, wherein each sequential feature describes one dimensional graphic information derived from the one or more pixels in the document image fragment. A classification score for the document image fragment is generated responsive at least in part to the set of sequential features, the classification score indicating a likelihood that the document image fragment is written in the writing system. The writing system associated with the document image is identified based at least in part on the classification score for the document image fragment.",['G06V30/2445']
CN109544456B,Panoramic environment perception method based on fusion of 2D image and 3D point cloud data,"The invention discloses a panoramic environment sensing method based on two-dimensional image and three-dimensional point cloud data fusion, which is characterized in that multi-frame point cloud data are unified to a world coordinate system by applying a sample consistency initial alignment (SAC-IA) algorithm and an Iterative Closest Point (ICP) algorithm; performing similar feature matching and image splicing on the gray level images corresponding to the point cloud data, eliminating cracks in image splicing areas of the spliced images by adopting a pixel weighting method, and smoothing the brightness values of pixel points in the splicing areas to obtain optimized panoramic images; dividing all point cloud data in a world coordinate system by using depth information to obtain point cloud data in different depth ranges, and projecting the point cloud data into two-dimensional images and carrying out binarization respectively; and performing information fusion on the generated binary image and the panoramic image to realize panoramic visual perception of the environmental information at different depths. The method can meet the technical requirements of rapid panoramic information reconstruction of the detection scene with complex background and strong interference, and can effectively reduce the difficulty of subsequent scene understanding, path planning and other work.","['G06T3/4038', 'G06T7/10', 'G06T7/33', 'G06T2207/10004', 'G06T2207/10028', 'G06T2207/20221']"
US11710243B2,"Method for predicting direction of movement of target object, vehicle control method, and device","A method for predicting a direction of movement of a target object, a method for training a neural network, a smart vehicle control method, a device, an electronic apparatus, a computer readable storage medium, and a computer program. The method for predicting a direction of movement of a target object comprises: acquiring an apparent orientation of a target object in an image captured by a camera device, and acquiring a relative position relationship of the target object in the image and the camera device in three-dimensional space (S100); and determining, according to the apparent orientation of the target object and the relative position relationship, a direction of movement of the target object relative to a traveling direction of the camera device (S110).","['G06V20/56', 'G06T7/20', 'B60W30/09', 'B60W30/0956', 'B60W50/14', 'B60W60/0015', 'G06F18/213', 'G06F18/214', 'G06T7/11', 'G06T7/70', 'G06V10/764', 'G06V10/82', 'G06V20/58', 'G06V40/103', 'G08G1/166', 'B60W2420/403', 'B60W2420/42', 'B60W2554/4029', 'B60W2554/80', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196', 'G06T2207/30244', 'G06T2207/30252', 'G06T2207/30256', 'G06T2207/30261']"
US9275289B2,Feature- and classifier-based vehicle headlight/shadow removal in video,"A method for removing false foreground image content in a foreground detection process performed on a video sequence includes, for each current frame, comparing a feature value of each current pixel against a feature value of a corresponding pixel in a background model. The each current pixel is classified as belonging to one of a candidate foreground image and a background based on the comparing. A first classification image representing the candidate foreground image is generated using the current pixels classified as belonging to the candidate foreground image. The each pixel in the first classification image is classified as belonging to one of a foreground image and a false foreground image using a previously trained classifier. A modified classification image is generated for representing the foreground image using the pixels classified as belonging to the foreground image while the pixels classified as belonging to the false foreground image are removed.","['G06K9/00825', 'G06T7/215', 'G06F18/2411', 'G06K9/00744', 'G06K9/6269', 'G06T7/0081', 'G06T7/194', 'G06T7/251', 'G06T7/254', 'G06V10/764', 'G06V20/46', 'G06V20/584', 'G06T2207/10048', 'G06T2207/20032', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/30232', 'G06T2207/30242']"
US10839535B2,Systems and methods for providing depth map information,"A method for providing depth map information based on image data descriptive of a scene. In one embodiment, after generating an initial sequence of disparity map data, performing a smoothing operation or an interpolation to remove artifact introduced in the disparity map data as a result of segmenting the image data into superpixels.","['G06T7/593', 'G06F18/22', 'G06K9/6215', 'G06T3/4007', 'G06T5/002', 'G06T5/20', 'G06T5/70', 'G06T7/10', 'G06T7/50', 'G06V10/757']"
US11763565B2,Fine-grain object segmentation in video with deep features and multi-level graphical models,"Techniques related to automatically segmenting a video frame into fine grain object of interest and background regions using a ground truth segmentation of an object in a previous frame are discussed. Such techniques apply multiple levels of segmentation tracking and prediction based on color, shape, and motion of the segmentation to determine per-pixel object probabilities, and solve an energy summation model to generate a final segmentation for the video frame using the object probabilities.","['G06V20/49', 'G06T7/11', 'G06F17/18', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/09', 'G06N7/01', 'G06T11/20', 'G06T7/168', 'G06V20/46', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2210/12']"
US10386919B2,Rendering rich media content based on head position information,"Rich media content, such as advertising, can be provided for display on a two-dimensional screen to give the user an impression that the screen is a window into a three-dimensional (3D) environment. For example, the user's head can be tracked and the graphical elements of the advertisement can be rendered based on the position of the user's head relative to a computing device such that the graphical elements appear to have 3D depth. A full or substantially full 3D view of a product can be presented. Additional information, such as a product description, features, pricing, user ratings, user reviews, among others, can also be displayed based on the position of the user's head relative to the computing device. A 3D video can also be presented, and a user can view different perspectives of the video based on the position of the user's head with respect to the computing device.","['G06F3/012', 'G06F3/017', 'G06F3/0482', 'G06F3/04845', 'G06Q30/0261']"
US7148913B2,Vision-based pointer tracking and object classification method and apparatus,"A vision-based pointer tracking and object recognizing system is presented. The system comprises a computer system including a processor, a memory coupled with the processor, an input coupled with the processor for receiving a series of image frames from a camera, and an output coupled with the processor for outputting a path of a pointer. The computer system includes a tracking system and a recognition system which are configured to accurately and efficiently track the fingertip of a user in order to allow the user to perform coarse segmentation of an object in preparation for object classification. The tracking system includes a Robust Tracking Filter that accommodates uncertainties. The object recognition system incorporates an invariant object recognition technique that allows objects to be recognized despite changes in user viewpoint.","['G06F3/042', 'G06F3/011', 'G06F3/0304', 'G06V40/107']"
US11430134B2,Hardware-based optical flow acceleration,"An optical flow accelerator (OFA) which provides hardware-based acceleration of optical flow and stereo disparity determination is described. A system is described which includes an OFA configured to determine a first optical flow using a first disparity search technique, and to determine a second optical flow using a second disparity search technique that is different from the first disparity search technique. The system also includes a processor configured to combine the first optical flow and the second optical flow to generate a third optical flow. In some implementations, the first and second disparity search techniques are based upon Semi-Global Matching (SGM). In some implementations, the OFA is further configurable to determine stereo disparity.","['G06T7/269', 'G06F9/3877', 'G06T5/002', 'G06T5/70', 'G06T7/207', 'G06T7/215', 'G06V10/147', 'G06V10/25', 'G06V10/751', 'G06V10/764', 'G06V10/82', 'G06V20/58', 'G06T2207/10012', 'G06T2207/20016', 'G06T2207/20084']"
CN111275139B,"Handwritten content removal method, handwritten content removal device, and storage medium","A handwritten content removing method, a handwritten content removing apparatus, and a storage medium. The handwritten content removing method comprises the following steps: acquiring an input image of a text page to be processed, wherein the input image comprises a handwriting area, and the handwriting area comprises handwriting content; identifying the input image to determine the handwritten content in the handwriting area; and removing the handwriting content in the input image to obtain an output image.","['G06V30/2455', 'G06T5/77', 'G06T11/001', 'G06V10/25', 'G06V10/28', 'G06V10/82', 'G06V30/146', 'G06V30/15', 'G06V30/155', 'G06V30/162', 'G06V30/1801', 'G06V30/226', 'G06V30/41', 'G06V30/42', 'G06T2207/30176', 'G09B3/02']"
CN108520501B,A video rain and snow removal method based on multi-scale convolutional sparse coding,"A video rain and snow removing method based on multi-scale convolution sparse coding simultaneously estimates rain and snow components and a moving foreground in a video under the assumption of a low-rank background. Firstly, acquiring video data containing rain and snow noise, and initializing a model; establishing a generation model of a rain and snow picture according to the characteristics of rain and snow and the video foreground; according to the structural characteristics of imaging of rain and snow in a video, namely that moving rain and snow are repeatable and multi-scale rain strip local blocks on the image, establishing a multi-scale convolution sparse coding model about the rain and snow; establishing a moving object detection model according to the characteristic of video foreground sparsity; integrating the model into a rain and snow removing/model under a maximum likelihood estimation framework; and applying the rain and snow removing video and the rain and snow removing model to obtain the rain and snow removing video and other statistical variables, and outputting the rain and snow removing video. The invention aims to establish a high-quality video rain and snow removing model based on a rain and snow generating principle and rain and snow noise structural characteristics, so that the video rain and snow removing technology can be widely applied to more complex actual scenes.","['G06T5/70', 'G06T2207/10016', 'G06T2207/20081']"
US5465304A,"Segmentation of text, picture and lines of a document image","In a character recognition system, a method and apparatus for segmenting a document image into areas containing text and non-text. Document segmentation in the present invention is comprised generally of the steps of: providing a bit-mapped representation of the document image, extracting run lengths for each scanline from the bit-mapped representation of the document image; constructing rectangles from the run lengths; initially classifying each of the rectangles as either text or non-text; correcting for the skew in the rectangles; merging associated text into one or more text blocks; and logically ordering the text blocks.","['G06V30/413', 'G06T9/005']"
CN111767846B,"Image recognition method, device, equipment and computer storage medium","The application discloses an image recognition method, relates to the technical field of artificial intelligence, and particularly relates to the technical field of image processing. The specific implementation scheme is as follows: performing facial feature recognition on the preprocessed facial image, and marking the position of the facial feature in the facial image to obtain a marked facial image; determining face images under multiple scales of the marked face images, inputting the multi-scale face images into a backbone network model for feature extraction, and obtaining wrinkle features of the face images of each scale under multiple scales; and fusing the wrinkle characteristics of each scale in the same area of the face image to obtain the wrinkle identification result of the face image. The embodiment of the application has higher wrinkle recognition precision and is not easily influenced by external environments such as illumination and the like.","['G06V40/168', 'G06F18/253', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06V10/267', 'G06V10/464', 'G06V10/52', 'G06V10/806', 'G06V40/171', 'G06V40/172', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30201']"
CN114399522B,An edge detection method based on Canny operator with high and low thresholds,"The invention provides an edge detection method of a Canny operator based on a high-low threshold value, which mainly comprises the steps of 1) conducting smoothing filtering processing on a source image, using a switch median filter to replace Gaussian filtering to remove noise on the image, 2) adopting a sobel operator to calculate the gradient amplitude and the direction of the image after the smoothing filtering processing and the noise removing in the step 1, 3) conducting non-maximum suppression on the gradient amplitude obtained in the step 2 to obtain an edge image with single pixel width, 4) adopting a k-means clustering algorithm to obtain clustering centers with high and low gradient values, 5) obtaining an otsu threshold value of a gradient by an otsu algorithm, and obtaining the high-low threshold value among the high clustering center, the otsu threshold value and the low clustering center, 6) utilizing the high-low threshold value to process the edge image with the single pixel width obtained in the step 3, and obtaining a binary edge, and 7) adopting area morphological opening operation to remove the interference edge of the binary edge, and obtaining a final edge image. The Canny algorithm has the advantages of high positioning precision, strong self-adaptability, good interference point removing effect and the like.","['G06T7/13', 'G06F18/23213', 'G06T5/70', 'G06T7/155', 'G06T2207/20036']"
US10482681B2,Recognition-based object segmentation of a 3-dimensional image,"Techniques are provided for segmentation of objects in a 3D image of a scene. An example method may include receiving, 3D image frames of a scene. Each of the frames is associated with a pose of a depth camera that generated the 3D image frames. The method may also include detecting the objects in each of the frames based on object recognition; associating a label with the detected object; calculating a 2D bounding box around the object; and calculating a 3D location of the center of the bounding box. The method may further include matching the detected object to an existing object boundary set, created from a previously received image frame, based on the label and the location of the center of the bounding box, or, if the match fails, creating a new object boundary set associated with the detected object.","['G06T19/20', 'G06K9/00624', 'G06T7/12', 'G06V20/00', 'G06T2207/10028', 'G06T2219/2008']"
US20090052783A1,"Similar shot detecting apparatus, computer program product, and similar shot detecting method","If a difference between feature values of frames from different shots is within a predetermined error range, one or more target frames are selected from each of the shots. Based on face areas in the selected target frames, the feature values of the target frames are calculated. If a difference between the feature values of the target frames is within a predetermined error range, these shots, from which these similar frames are originally retrieved, are assigned with the same shot attribute and set to be similar shots.","['G06V20/40', 'H04N5/147', 'G06F16/7328', 'G06F16/784', 'G06V40/161']"
CN108229322B,"Video-based face recognition method and device, electronic equipment and storage medium","The embodiment of the application discloses a face recognition method, a face recognition device, electronic equipment and a storage medium based on videos, wherein the face recognition method comprises the following steps: forming a group of face sequences aiming at face images which appear in a plurality of continuous video frames in a video and have positions in the video frames meeting a preset displacement requirement, wherein the face sequences are a set of face images belonging to the same person in the video frames; and aiming at a group of face sequences, carrying out face recognition by utilizing a preset face library at least according to the face features in the face sequences.","['G06V40/168', 'G06V10/82', 'G06F18/217', 'G06V10/454', 'G06V10/993', 'G06V20/40', 'G06V40/166', 'G06V40/167', 'G06V40/172']"
WO2021017261A1,"Recognition model training method and apparatus, image recognition method and apparatus, and device and medium","Disclosed are a recognition model training method and apparatus, an image recognition method and apparatus, and a device and a medium. The recognition model training method comprises: acquiring an original positive sample image and an original negative sample image carrying first labeling information; performing down-sampling processing on the original positive sample image and the original negative sample image, and performing screenshot processing by means of a screenshot tool to acquire a target positive sample image and a target negative sample image carrying second labeling information; inputting the target positive sample image and the target negative sample image into an MB-FCN model for model training so as to acquire a target MB-FCN detector; taking, as a group of target training data, the original positive sample image and the target positive sample image with a current image identifier in the first labeling information matching a source image identifier in the second labeling information; and inputting the target training data into a GAN model for model training so as to acquire a target GAN model. The target GAN model can help to improve the facial recognition accuracy of a blurred image.","['G06F18/22', 'G06N3/045', 'G06N3/088', 'G06V40/168', 'G06V40/172', 'Y02T10/40']"
CN111951237B,Visual appearance detection method,"The invention provides an appearance visual detection method, which comprises the following steps: an optical guiding step: the similarity of the images to be compared is calculated by taking the reference image as a standard, so that the optical imaging consistency of the images acquired by the same batch of products on different machines is ensured; a visual guidance step: the feeding accuracy of small parts is required to be preset, otherwise, the mechanical arm cannot carry out normal feeding, the deviation angle, the X position and the Y position of the product are required to be obtained through a visual guidance algorithm before the product is fed, and the machine is informed to adjust so as to ensure the feeding accuracy of the product. The machine vision detection method based on the deep learning and the traditional image processing is innovatively adopted, so that the requirement of similarity comparison in the image acquisition stage exists, the consistent images are acquired, and the accuracy of subsequent data for depth model detection and the accuracy of image detection are ensured.","['G06T7/0004', 'G01B11/00', 'G06F18/22', 'G06F18/2431', 'G06N3/08', 'G06T5/70', 'G06T7/11', 'G06T7/136', 'G06T7/194', 'G06T2207/20048', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30164']"
CN112950477B,A High Resolution Salient Object Detection Method Based on Dual Path Processing,"The invention discloses a high-resolution saliency target detection method based on dual-path processing, which comprises the following steps: image processing, namely firstly preprocessing an input high-resolution image by a HeadBottleneck module comprising depth separable convolution to reduce the parameter number of the original high-resolution image; extracting features with larger receptive fields by a global context path through a backbone network R-ResNet based on ResNet-50 improvement; the multi-scale feature extraction, the features obtained by the R-ResNet backbone network are sent to a multi-scale feature extraction and enhancement module; and generating a saliency map, wherein the space detail keeping branch extracts detailed edge information through an edge information guiding module, and then complementarily fuses the detailed edge information with the features obtained by the global context branch to obtain a final saliency map result. The invention can realize accurate detection and rapid segmentation of the salient object in the high-resolution image, and finally output the salient object graph, thereby providing a solution for high-resolution salient object detection.","['G06T3/4053', 'G06N3/045', 'G06N3/048', 'G06N3/08', 'G06T3/4007', 'G06V10/44', 'Y02D10/00']"
CN111862201B,A relative pose estimation method for spatial non-cooperative targets based on deep learning,"The invention discloses a space non-cooperative target relative pose estimation method based on deep learning, which comprises the following steps: utilizing software to manufacture a data set of a non-cooperative target and expanding the data set; designing a target detection network based on a convolutional neural network, judging whether the target is a non-cooperative target, and detecting a 2D boundary box around the non-cooperative target; dividing a non-cooperative target around a 2D boundary frame, searching the center of the non-cooperative target based on kirchhoff, estimating the depth from the center of the non-cooperative target to a camera, and converting 2D pixel coordinates into 3D coordinates by combining depth values; designing a key point extraction network, extracting key points such as corner points, triaxial end points and the like of a non-cooperative target, and obtaining rotation quaternary representation rotation from regression of the key points; and fine-tuning the estimated relative pose through iterative optimization. The invention can treat the conditions of heavy shielding, sensor noise, lower estimation precision when the object is symmetrical, and the like, and can meet the real-time speed requirement.","['G06T7/70', 'G06F30/15', 'G06F30/27', 'G06N3/045', 'G06T17/00', 'G06T7/11', 'Y02T10/40', 'Y02T90/00']"
US20200193206A1,Scene and user-input context aided visual search,"Provided is a technique for determining a context of an image and an object depicted by the image based on the context. A trained context classification model may determine a context of an image, and a trained object recognition model may determine an object depicted by the image based on the image and the context. Provided is also a technique for determining an object depicted within an image based on an input location of an input detected by a display screen. An object depicted within an image may be detected based on a distance in feature space between an image feature vector of an image and a feature vector of the object, and a distance in pixel-space between an input location of an input and location of the object within the image.","['G06K9/3233', 'G06V20/20', 'G06F18/214', 'G06F18/24', 'G06K9/4604', 'G06K9/6256', 'G06K9/6267', 'G06T1/0014', 'G06T5/009', 'G06T5/92', 'G06V10/235', 'G06V20/70', 'H04N25/00', 'G06T2207/20132']"
US12333853B2,Face parsing method and related devices,"A facial parsing method and apparatus, a facial parsing network training method and apparatus, an electronic device and a non-transitory computer-readable storage medium, which relate to the field of artificial intelligence. The facial parsing method includes inputting a facial image into a pre-trained facial parsing neural network; extracting a semantic feature of the facial image by using a semantic perception sub-network; extracting a boundary feature of the facial image by using a boundary perception sub-network; and processing the cascaded semantic feature and boundary feature by using a fusion sub-network, to obtain a facial region to which each pixel in the facial image belongs. The method can improve the resolution capability of a neural network for boundary pixels between different facial regions of a facial image, thereby improving the precision of facial parsing.","['G06V40/162', 'G06V10/774', 'G06V40/161', 'G06F18/2193', 'G06F18/253', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/09', 'G06V10/80', 'G06V10/82', 'G06V40/16', 'G06V40/168', 'G06V40/172']"
CN111582294B,Method for constructing convolutional neural network model for surface defect detection and application thereof,"The invention discloses a method for constructing a convolutional neural network model for surface defect detection and application thereof, wherein the method comprises the following steps: (1) collecting and importing original pictures; (2) Preprocessing an original picture, determining the preprocessed original picture as an original training sample and storing the original training sample; (3) Labeling the original training sample to generate a labeling sample; (4) Transforming the original training sample to generate a new training sample, so as to realize training sample enhancement; (5) Training samples are used as input data, and the samples are marked for corresponding processing and are used as reference output: and (5) until the accuracy rate of stable convergence is reached through multiple iterations, storing after the convolutional neural network model is generated. The invention overcomes a plurality of adverse effects caused by interference factors such as random product defect morphology, complex texture, low contrast and the like under the condition of inputting a small amount of samples, thereby improving the defect recognition rate.","['G06F18/241', 'G01N21/8851', 'G06F18/214', 'G01N2021/8887']"
CN113781402B,Method and device for detecting scratch defects on chip surface and computer equipment,"The application relates to a method, a device, computer equipment and a storage medium for detecting scratch defects on the surface of a chip. The method comprises the following steps: acquiring an original image of a chip to be detected, and preprocessing the original image to obtain a preprocessed image; clustering each pixel according to the similarity among the pixels in the preprocessed image to obtain a plurality of super pixels; according to the plurality of super pixels, performing significance analysis on the preprocessed image to obtain a significant image; threshold segmentation is carried out on the salient image so as to separate a target foreground image containing scratches on the surface of the chip from a target background image; the target foreground image is a chip surface scratch image obtained through detection. The method has high detection accuracy and strong robustness for the scratches on the chip surface.","['G06T7/11', 'G06F18/23', 'G06T5/70', 'G06T7/0004', 'G06T7/136', 'G06T7/90']"
US10949702B2,System and a method for semantic level image retrieval,A system and method for retrieval of similar images related to query images is provided. The query images are pre-processed for noise removal by selecting filtering technique based on noise variance estimation in each query image with respect to pre-set noise variance threshold value. The pre-processed query images are pre-classified for determining class one image identifier. Image types are generated from pre-processed query images for determining class two image identifier. Features are extracted from pre-classified query images based on class one image identifier and from generated images based on class two image identifier. The images similar to query images are retrieved which have features similar to extracted features of pre-classified query images and generated images. The retrieved similar images are ranked for determining most similar images with respect to query images. Similarity between query images and retrieved similar images is analyzed for re-ranking retrieved similar images.,"['G06N3/088', 'G06K9/4623', 'G06F16/535', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/0475', 'G06N3/08', 'G06N3/09', 'G06N3/094', 'G06T5/002', 'G06T5/20', 'G06T5/70', 'G06V20/00', 'G06N20/00', 'G06N7/01']"
CN111328396B,Pose estimation and model retrieval for objects in images,"Techniques for selecting a three-dimensional model are provided. An input image containing an object may be obtained and a pose of the object in the input image may be determined. One or more candidate three-dimensional models representing one or more objects in the determined pose may be obtained. From the one or more candidate three-dimensional models, a candidate three-dimensional model may be determined to represent the object in the input image.","['G06T19/20', 'G06F18/22', 'G06T19/006', 'G06T7/75', 'G06V10/82', 'G06V20/653', 'G06T2207/10016', 'G06T2207/20084', 'G06T2210/12', 'G06T2219/2016', 'G06V2201/10']"
CN110060237B,"Fault detection method, device, equipment and system","The invention discloses a fault detection method, a fault detection device, equipment and a fault detection system, and belongs to the technical field of image processing. The method comprises the following steps: obtaining an original product image of an industrial product to be detected; inputting the original product image into a segmentation network to perform feature extraction on the original product image to obtain a feature map, performing pooling processing on the feature map by using at least two pooling layers with different sizes to obtain multi-level feature information, and determining a fault area in the original product image according to the multi-level feature information; inputting the fault area into a fault identification model to determine the fault type of the fault area. By the method, the fault of the industrial product can be accurately detected, and the detection efficiency can be improved.","['G06F18/23213', 'G06F18/24', 'G06T7/0008', 'G06T7/62', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30141']"
US10810435B2,Segmenting objects in video sequences,"In implementations of segmenting objects in video sequences, user annotations designate an object in any image frame of a video sequence, without requiring user annotations for all image frames. An interaction network generates a mask for an object in an image frame annotated by a user, and is coupled both internally and externally to a propagation network that propagates the mask to other image frames of the video sequence. Feature maps are aggregated for each round of user annotations and couple the interaction network and the propagation network internally. The interaction network and the propagation network are trained jointly using synthetic annotations in a multi-round training scenario, in which weights of the interaction network and the propagation network are adjusted after multiple synthetic annotations are processed, resulting in a trained object segmentation system that can reliably generate realistic object masks.","['G06N3/08', 'G06K9/00765', 'G06K9/00671', 'G06K9/00744', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/09', 'G06T7/11', 'G06T7/174', 'G06T7/194', 'G06V10/764', 'G06V10/82', 'G06V20/10', 'G06V20/20', 'G06V20/46', 'G06V20/49', 'G06V20/70', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/20216']"
CN108548820B,Cosmetic paper label defect detection method,"The invention provides a method for detecting defects of a paper label of a cosmetic, which comprises the steps of judging whether the label position is correct, judging whether Chinese character recognition is correct, judging whether the date position is correct and judging whether the date recognition is correct. The invention designs a Chinese character extraction algorithm and a recognition algorithm of different color information in the cosmetic label; a pretreatment algorithm and a high-precision recognition algorithm for the mechanically-beaten characters in the cosmetic label are designed. The invention has stable algorithm, is convenient for system overhaul and maintenance, and can automatically detect all the products without manual watching by only adjusting corresponding control parameters and pressing a start button during production.","['G01N21/8851', 'B07C5/34', 'G01N21/95607', 'G06V10/56', 'G06V30/1478', 'G06V30/153', 'G01N2021/8887', 'G01N2021/95615', 'G06V30/10', 'Y02P90/30']"
CN107103613B,A kind of three-dimension gesture Attitude estimation method,"The present invention discloses a kind of three-dimension gesture Attitude estimation method, comprising: S1, obtains multiple gesture depth maps and carries out the segmentation of gesture prospect and background, to obtain multiple gesture foreground pictures and be randomly divided into training set and test set；S2, the physical tags figure that each gesture foreground picture is constructed according to gesture model figure, physical tags figure include that multiple coordinate points that manpower respectively refers to identification point are represented in gesture foreground picture, and the value of each coordinate points includes corresponding coordinate value and depth value with reference to identification point；S3, multiple the gesture foreground pictures chosen in training set are trained full convolutional neural networks, and accordingly export multiple prediction label figures；S4, compare deviation between physical tags figure and prediction label figure, while updating network parameter；S5, it is iterated with constantly reducing deviation, until network parameter is restrained；S6, input gesture depth map to be estimated, export corresponding label figure, as estimated result.","['G06T7/207', 'G06T3/147', 'G06T7/194', 'G06T7/215']"
US8731300B2,Handwritten word spotter system using synthesized typed queries,"A wordspotting system and method are disclosed for processing candidate word images extracted from handwritten documents. In response to a user inputting a selected query string, such as a word to be searched in one or more of the handwritten documents, the system automatically generates at least one computer-generated image based on the query string in a selected font or fonts. A model is trained on the computer-generated image(s) and is thereafter used in the scoring the candidate handwritten word images. The candidate or candidates with the highest scores and/or documents containing them can be presented to the user, tagged, or otherwise processed differently from other candidate word images/documents.","['G06V30/2276', 'G06F18/243', 'G06F18/295', 'G06V30/19187']"
US9269012B2,Multi-tracker object tracking,"Systems and approaches are provided for tracking an object using multiple tracking processes. By combining multiple lightweight tracking processes, object tracking can be robust, use a limited amount of power, and enable a computing device to respond to input corresponding to the motion of the object in real time. The multiple tracking processes can be run in parallel to determine the position of the object by selecting the results of the best performing tracker under certain heuristics or combining the results of multiple tracking processes in various ways. Further, other sensor data of a computing device can be used to improve the results provided by one or more of the tracking processes.","['G06K9/3241', 'G06T7/246', 'G06K9/00228', 'G06K9/00369', 'G06T7/2033', 'G06V10/255', 'G06V40/103', 'G06V40/161', 'G06K2009/3291', 'G06T2207/10016', 'G06T2207/30201', 'G06V10/62']"
WO2021017260A1,"Multi-language text recognition method and apparatus, computer device, and storage medium","A multi-language text recognition method and apparatus, a computer device, and a storage medium. The method comprises: obtaining an image to be recognized, the image to be recognized comprising original text corresponding to at least two languages (S201); performing layout analysis recognition on the image to be recognized, obtaining at least one text line image, and determining the text line position of each text line image in the image to be recognized (S202); performing language recognition on each text line image to obtain a target language corresponding to the text line image (S203); querying a recognition model database on the basis of the target language to obtain a target OCR recognition model corresponding to the target language (S204); performing transfer recognition on the text line image by means of the target OCR recognition model to obtain target text corresponding to the text line image (S205); and obtaining, on the basis of the target text corresponding to the text line image and the text line position, target recognition text corresponding to the image to be recognized (S206). According to the method, each text line image can be recognized by means of a target OCR recognition model, thereby improving the multi-language text recognition accuracy.","['G06F18/241', 'G06N3/045', 'G06N3/08', 'G06V10/22', 'G06V10/267', 'G06V30/1478', 'Y02D10/00']"
US11328524B2,Systems and methods for automatic data extraction from document images,"Described systems and methods allow the automatic extraction of structured information from images of structured text documents such as invoices and receipts. Some embodiments employ optical character recognition (OCR) technology to extract individual text tokens (e.g., words) and token bounding boxes from a document image. A feature vector of each text token comprises a first part determined according to a character content of the text token, and a second part determined according to an image content of the token's bounding box. A neural network classifier produces a label indicative of a type of information (e.g. “billing address”, “due date”, etc.) carried by each text token. In some embodiments, documents are linearized by ordering text tokens in a sequence according to a reading order of a natural language (e.g., English, Arabic) in which the respective document is formulated. Token feature vectors are fed to the classifier in the order indicated by the token sequence.","['G06F16/56', 'G06F16/5846', 'G06F40/284', 'G06V10/40', 'G06V10/82', 'G06V30/19173', 'G06V30/412', 'G06V30/413', 'G06V30/414', 'G06V30/416', 'G06V30/10']"
WO2022100495A1,Method for automatically segmenting ground-glass pulmonary nodule and computer device,"The present invention relates to a method for automatically segmenting a ground-glass pulmonary nodule and a computer device. Said method comprises the following steps: acquiring medical image raw data acquired by a computer tomography device; pre-processing the medical image raw data; and taking the pre-processed image as an input of a trained fully convolutional residual network based on an ASPP structure and an attention mechanism, to obtain a ground-glass pulmonary nodule segmentation result, wherein the fully convolutional residual network based on the ASPP structure and the attention mechanism takes a plurality of Conv2D convolutional layers as a basic architecture, a residual module and an attention module being provided between adjacent Conv2D convolutional layers, and an ASPP structure is provided in the fully convolutional residual network to capture multi-scale information of the ground-glass pulmonary nodule. Compared with the prior art, the present invention has the advantages of rapidness, accuracy, etc.","['G06T7/11', 'G06F18/214', 'G06F18/2415', 'G06N3/045', 'G06N3/08', 'G06V10/25', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30064', 'G06V2201/032']"
US9911219B2,"Detection, tracking, and pose estimation of an articulated body","Techniques related to pose estimation for an articulated body are discussed. Such techniques may include extracting, segmenting, classifying, and labeling blobs, generating initial kinematic parameters that provide spatial relationships of elements of a kinematic model representing an articulated body, and refining the kinematic parameters to provide a pose estimation for the articulated body.","['G06T13/40', 'G06F18/24323', 'G06F3/017', 'G06F3/0304', 'G06F3/044', 'G06K9/00355', 'G06K9/4604', 'G06K9/6282', 'G06T15/60', 'G06T17/10', 'G06T7/11', 'G06T7/143', 'G06T7/207', 'G06T7/251', 'G06T7/75', 'G06V10/44', 'G06V10/764', 'G06V40/28', 'G06T2200/04', 'G06T2200/08', 'G06T2207/10004', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/30241', 'G06T2213/08', 'G06T2215/12', 'G06T2219/2016']"
CN108335305B,"Image segmentation method and apparatus, electronic device, program, and medium","The embodiment of the invention discloses an image segmentation method and device, electronic equipment, a program and a medium, wherein the method comprises the following steps: extracting the features of the image through a neural network, and outputting the features of at least two different levels; performing retracing fusion on the features of the at least two different levels at least once to obtain a first fusion feature; wherein the one-fold fusion comprises: in the network depth direction of the neural network, fusing the characteristics of different levels output by the network layers with different network depths respectively according to two different level directions in sequence; and segmenting at least partial region of the image based on the first fusion feature to obtain a segmentation result. The embodiment of the invention designs a frame based on deep learning to solve the problem of image segmentation, and can improve the accuracy of the image segmentation result.","['G06T7/11', 'G06T2207/20084']"
US11017230B2,Systems and methods for depicting vehicle information in augmented reality,"Systems and methods are provided for depicting information about a vehicle in augmented reality. An exemplary method may include determining identification information of a vehicle based on input received from a client device and obtaining, from an information source, vehicle information based on the identification information. The method may also include analyzing the vehicle information using an artificial intelligence engine and extracting vehicle history data from the vehicle information based on the analysis. The method may also include identifying a portion of the vehicle associated with the vehicle history data and providing instructions, based on the vehicle history data, to an augmented reality-enabled device to superimpose an indicator on a visual representation of the vehicle at a location corresponding to the portion of the vehicle.","['G06K9/00671', 'G06Q40/08', 'G06F40/279', 'G06N3/0464', 'G06N5/04', 'G06V20/20', 'G07C5/0825', 'G07C5/085', 'G06N3/045', 'G06N3/063', 'G06T11/60', 'G06T2200/24', 'G06V30/10', 'G07C5/008']"
US10462445B2,Systems and methods for estimating and refining depth maps,A method for improving accuracy of depth map information derived from image data descriptive of a scene. In one embodiment Mutual Feature Map data are created based on initial disparity map data values and the image data descriptive of the scene. The Mutual Feature Map data are applied to create a series of weighting functions representing structural details that can be transferred to the first disparity values to restore degraded features or replace some of the first disparity values with values more representative of structural features present in the image data descriptive of the scene.,"['H04N13/128', 'G06K9/4604', 'G06K9/6212', 'G06T5/20', 'G06T7/593', 'G06V10/44', 'G06V10/758', 'H04N13/239', 'H04N13/257', 'H04N13/271', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20032', 'G06T2207/30256', 'G06T2207/30261', 'H04N2013/0081']"
CN111369571B,"Method, device and electronic equipment for judging accuracy of three-dimensional object pose","The application provides a three-dimensional object pose accuracy judging method and device and electronic equipment, and belongs to the technical field of computer vision. Wherein the method comprises the following steps: acquiring a grid model of a target object to be checked, a scene image where the target object is located and a first pose of the target object in the scene image; according to the first pose, projecting a grid model of the target object to obtain a projection corresponding to the target object at present, wherein the projection comprises an outer contour and an inner structure line; judging whether the fitting degree of the projection and the scene image is larger than or equal to a first threshold value; if so, the first pose is determined to be accurate. Therefore, through the three-dimensional object pose accuracy judging method, the accuracy of the first pose is verified according to the projection corresponding to the target object determined by the first pose, and the accuracy of the three-dimensional object pose accuracy judging is improved.","['G06T7/70', 'G06T19/006', 'G06T7/11', 'G06T7/13', 'G06T7/136', 'G06T7/50']"
CN108898610B,An object contour extraction method based on mask-RCNN,"The invention discloses an object contour extraction method based on mask-RCNN, which comprises the steps of firstly obtaining a mask-RCNN model through training, inputting an RGB image of an object contour to be extracted into the mask-RCNN model to carry out semantic segmentation, obtaining a binary mask image corresponding to the RGB image through mask-RCNN network processing, inputting the RGB image and the corresponding binary mask image into a contour refinement module, providing a contour feature description mode, accurately representing the direction and angle information of the object contour, carrying out self-adaptive correction on the binary mask image contour obtained based on the mask-RCNN through a contour correction algorithm, and finally realizing real-time accurate extraction of the image example contour under the conditions of low image quality such as low resolution, target blurring and low time and space complexity.","['G06T7/13', 'G06N3/045', 'G06T5/30', 'G06T5/70']"
US9367766B2,Text line detection in images,"Techniques for detecting and recognizing text may be provided. For example, an image may be analyzed to detect and recognize text therein. The analysis may involve detecting text components in the image. For example, multiple color spaces and multiple-stage filtering may be applied to detect the text components. Further, the analysis may involve extracting text lines based on the text components. For example, global information about the text components can be analyzed to generate best-fitting text lines. The analysis may also involve pruning and splitting the text lines to generate bounding boxes around groups of text components. Text recognition may be applied to the bounding boxes to recognize text therein.","['H04N1/00331', 'G06K9/6261', 'G06F18/2163', 'G06K9/00442', 'G06K9/00463', 'G06K9/72', 'G06T7/0081', 'G06T7/11', 'G06V20/582', 'G06V30/414', 'G06K9/00449', 'G06V20/63', 'G06V30/412']"
CN105243374B,"Three-dimensional face identification method, system and the data processing equipment using it","The invention discloses a kind of three-dimensional face identification method, system and using its data processing equipment, the embodiment of the present invention detects a series of key points with stronger generic associative on face first, these key points are distributed on Different Individual face that there are obvious differences.On this basis, the fusion recognition to three-dimensional face is realized with point cloud registering by local feature matching.Firstly, the present invention can effectively reduce expression shape change to the adverse effect of recognition of face using local feature；Secondly, the present invention comprehensively utilizes face part and global information, fusion recognition is realized using characteristic matching and point cloud registering, further improves the precision of recognition methods.Compared to existing method, accuracy of identification of the invention and robustness are greatly promoted.","['G06V40/165', 'G06V20/653']"
CN107492091B,Label appearance detection method based on machine vision and terminal equipment,"The invention is suitable for the technical field of label appearance detection, and provides a label appearance detection method based on machine vision and a terminal device, wherein the label appearance detection method comprises the following steps: collecting a label image to be detected; detecting characteristic points of the label image to be detected to obtain a first characteristic point and a first characteristic description vector; matching and comparing the difference value of the to-be-detected label image with the standard image template according to the first characteristic point and the first characteristic description vector of the to-be-detected label image and the second characteristic point and the second characteristic description vector of the standard image template to obtain a difference value image; the difference comparison is pixel difference comparison; and when the pixel value of the difference image is larger than a preset pixel value, judging that the label image to be detected has defects. The invention can improve the detection efficiency of the appearance of the label and reduce the error rate.","['G06T7/001', 'G06T7/13', 'G06T2207/20224', 'G06T2207/30108']"
US7630551B2,Method and system for line extraction in digital ink,"A method and system of line extraction in a digital ink sequence of handwritten text data points is provided in which a stroke sequence comprised of a sequence of strokes is obtained, the strokes are segmented into a sequence of substrokes by applying a stroke segmentation algorithm, angular differences between neighboring groups of substrokes, in the sequence of substrokes, are calculated, and the positions of the extrema of the angular differences are determined, thereby identifying the substrokes at line breaks and enabling line extraction.",['G06V30/333']
CN111241947B,"Training method and device for target detection model, storage medium and computer equipment","The application relates to a training method, a training device, a computer readable storage medium and computer equipment of a target detection model, wherein the training method comprises the following steps: acquiring a feature map of a sample image during training, and determining an initial detection frame in the feature map according to a preset rotation angle, a preset scale and a preset target aspect ratio; the position of each initial detection frame is adjusted to obtain the position information of the prediction detection frame, and the network parameters of the regression network are adjusted according to the position information and the real position information in the labeling information of the sample image; predicting the prediction probability of the target corresponding to each preset category according to the target detection area determined by the position information of the prediction detection frame; and adjusting network parameters of the classification network according to the real category information and the prediction probability in the labeling information of the sample image to obtain a target detection model for target detection of the image. According to the scheme provided by the application, the target detection model can identify the rotation angle of the target in the image, and the positioned target detection frame is more accurate.","['G06V20/40', 'G06F18/23', 'G06F18/241', 'G06V10/267', 'Y02T10/40']"
US9336459B2,Interactive content generation,"Generation of interactive content. In an embodiment, a representation of candidate object(s) in content of a digital media asset are received. For each of the candidate object(s), feature(s) of the candidate object are compared to corresponding feature(s) of a plurality of reference objects to identify reference object(s) that match the candidate object. For each of the matched candidate object(s), a hotspot package is generated. The hotspot package may comprise a visual overlay which comprises information associated with the reference object(s) matched to the respective candidate object.","['G06K9/6215', 'G06F16/73', 'G06F16/51', 'G06F16/583', 'G06F17/30247', 'G06F18/22', 'G06K9/4604', 'G06K9/52', 'G06T11/60', 'G06T5/00', 'G06T7/0044', 'G06T7/0081', 'G06T7/0083', 'G06T7/11', 'G06T7/12', 'G06T7/74', 'G06V10/44', 'G06V20/20', 'G06K2009/4666']"
US5987158A,Apparatus for automated identification of thick cell groupings on a biological specimen,A thick group of cells classifier. Image data acquired from an automated microscope from a cytological specimen is processed by a computer system. The computer applies filters at different stages. Obvious artifacts are eliminated from analysis early in the processing. The first stage of processing is image segmentation where objects of interest are identified. The next stage of processing is feature calculation where properties of each segmented thick group object are calculated. The final step is object classification where every segmented thick group object is classified as being abnormal or as belonging to a cellular or non-cellular artifact.,"['G06T7/0012', 'G06V20/69', 'G06T2207/10056', 'G06T2207/30024']"
CN108460411B,"Instance division method and apparatus, electronic device, program, and medium","The embodiment of the invention discloses an instance segmentation method and device, electronic equipment, a program and a medium, wherein the method comprises the following steps: extracting the features of the image through a neural network, and outputting the features of at least two different levels; extracting the regional characteristics corresponding to at least one example candidate region in the image from the characteristics of at least two different levels, and fusing the regional characteristics corresponding to the same example candidate region to obtain first fusion characteristics of each example candidate region; and carrying out example segmentation based on each first fusion feature to obtain an example segmentation result of the corresponding example candidate region and/or an example segmentation result of the image. The embodiment of the invention designs a frame based on deep learning to solve the problem of example segmentation, and can obtain more accurate example segmentation results.","['G06F18/213', 'G06F18/24', 'G06V10/25', 'G06V10/40', 'G06F18/253']"
CN109902622B,Character detection and identification method for boarding check information verification,"The invention relates to a character detection and identification method for boarding pass information verification, and belongs to the field of computer vision. The method comprises the following steps: s1: reading a boarding check image, and acquiring a boarding check test image and a training image; s2: positioning each text block by a text line detection method of a multitask full convolution neural network model based on a fuzzy region; s3: through learning of a text recognition model based on CTC and a self-attention mechanism, recognition of text lines, namely located text blocks is achieved; s4: and establishing a common character library of the boarding check so as to learn an n-gram language model and assist in optimizing a text line recognition result. The invention adopts automatic detection and identification of boarding card character information, realizes Chinese and English mixed text line identification, and obtains more comprehensive personal information.",[]
US7236632B2,Automated techniques for comparing contents of images,"Automated techniques for comparing contents of images. For a given image (referred to as an “input image”), a set of images (referred to as “a set of candidate images”) are processed to determine if the set of candidate images comprises an image whose contents or portions thereof match contents included in a region of interest in the input image.","['G06F16/5854', 'G06T7/97', 'G06F16/5838', 'G06V30/40']"
CN115908269B,"Visual defect detection method, visual defect detection device, storage medium and computer equipment","The application discloses a visual defect detection method, a visual defect detection device, a storage medium and computer equipment. The method comprises the following steps: acquiring a target image of an ink area of an object to be detected; performing edge extraction processing on the target image, and determining a gradient image of an interested region in the target image, wherein the gradient image comprises edge contour information; generating a defect image corresponding to the gradient image according to the absolute value of the image gradient of the pixel point in the edge contour information; and determining defect information of the ink area according to the defect image. The method can be used for rapidly and accurately identifying the tiny defects, so that defect detection of unidirectional uneven background is realized, the reliability of defect detection is greatly improved, and the yield of products is improved.",['Y02P90/30']
US11328172B2,Method for fine-grained sketch-based scene image retrieval,"A sketch-based image retrieval method, device and system, to improve accuracy of image searching from a scene sketch image. For example, the image retrieval method, device and system can be used to retrieve a target scene image from a collection of stored images in a storage (i.e., an image collection). The image retrieval method includes: segmenting the scene sketch image using an image segmentation module into semantic object-level instances, and fine-grained features are obtained for each object instance, generating an attribute graph which integrates the fine-grained features for each semantic object instance detected from the query scene sketch image, generating a feature graph by using a graph encoder module from the attribute graph, and computing a similarity or distance between the feature graphs of the query scene sketch image and the scene images in the image collection by a graph matching module and the most similar scene images are returned.","['G06K9/6215', 'G06F16/532', 'G06F18/22', 'G06F16/56', 'G06F16/5854', 'G06F18/2163', 'G06K9/6232', 'G06K9/6261', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/11', 'G06V10/426', 'G06V10/443', 'G06V10/454', 'G06V10/757', 'G06V10/761', 'G06V10/82', 'G06V10/84', 'G06N5/01', 'G06T2207/20016', 'G06T2207/20084']"
CN107609549B,Text detection method for certificate image in natural scene,"The invention discloses a text detection method of a certificate image in a natural scene, which comprises the following steps: selecting common Chinese characters to manufacture Chinese character pictures to form a data set 1, performing random rotation and cutting operations on the marked certificate images, and fusing different background pictures by using a Poisson clone mode to form a data set 2; training a character classification model of the VGG16 network by adopting a data set 1, initializing a full convolution neural network model by using the obtained parameters after the model is converged, and training the model by using a data set 2; processing the image by using a trained full convolution neural network model, and obtaining the classification condition of each pixel point according to a maximum probability method to form a text-non-text binary image; obtaining a text region by using a connected region method, binarizing an original image, and extracting only character information in the text region in a text-non-text region binary image to obtain a text binary image; correcting the image by a maximum variance method; and projecting the corrected image again, and refining the text-non-text region binary image.",[]
CN108304798B,Street level order event video detection method based on deep learning and motion consistency,"A street level order event video detection method based on deep learning and motion consistency relates to the field of artificial intelligence and the field of computer vision. The method comprises the following steps: 1) and (3) an algorithm framework: 2) detecting a target; 3) calculating the motion consistency; 4) and (5) event judgment. Compared with the prior art, the method has the advantages that the target detection deep learning network is designed, the scene recognition model is trained, the motion information in the scene is calculated, the behavior state of the scene is analyzed, the event is jointly judged under multiple conditions in a mode of combining the target detection technology in a static video frame and the target behavior analysis technology in a dynamic video in the field of video intelligent analysis, the detection system for the outdoor business affairs and the lane occupation business affairs is designed, and the automatic detection of the events is accurately and quickly completed.","['G06V20/41', 'G06F18/2413', 'G06V20/44', 'G06V2201/07']"
US11262597B2,"Method, device, and computer program for virtually adjusting a spectacle frame","A virtual try-on process for spectacles includes an approximate positioning and a fine positioning of a spectacle frame on a head of a user. Provided for this purpose are 3D models of the head and the spectacle frame, as well as head metadata based on the model of the head and frame metadata based on the model of the frame. The head metadata contains placement information, in particular a placement point, which can be used for the approximate positioning of the spectacle frame on the head, and/or a placement region which describes a region of the earpiece part of the frame for placement on the ears of the head. A rapid and relatively simple computational positioning of the spectacle frame on the head and a more accurate positioning using a subsequent precise adjustment can be achieved with the aid of the metadata.","['G06Q30/0621', 'A61B5/1079', 'G02C13/003', 'G02C7/027', 'G02C7/028', 'G06F30/17', 'G06F30/27', 'G06T19/20', 'G06T7/33', 'G16H20/40', 'G16H30/40', 'G16H40/63', 'G06T2207/30196', 'G06T2210/16', 'G06T2219/2004']"
US11720798B2,Foreground-background-aware atrous multiscale network for disparity estimation,"A system for disparity estimation includes one or more feature extractor modules configured to extract one or more feature maps from one or more input images; and one or more semantic information modules connected at one or more outputs of the one or more feature extractor modules, wherein the one or more semantic information modules are configured to generate one or more foreground semantic information to be provided to the one or more feature extractor modules for disparity estimation at a next training epoch.","['G06N3/084', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/11', 'G06T7/174', 'G06T7/194', 'G06T7/593', 'G06T7/596', 'G06V10/454', 'G06V10/82', 'G06V30/1916', 'G06V30/274', 'G06N3/082', 'G06T2207/20081', 'G06T2207/20084']"
CN111914628B,Training method and device of face recognition model,"The application discloses a training method and device of a face recognition model, relates to the technical fields of artificial intelligence, deep learning and computer vision, and particularly relates to the technical field of face recognition. The specific implementation scheme is as follows: and after the first training image is a non-occlusion face image and the plurality of occlusion object images are obtained, respectively fusing the plurality of occlusion object images to the non-occlusion face image to generate a plurality of second training images, and inputting the first training image and the second training image into a face recognition model to train the face recognition model. Therefore, the face recognition model is trained by adopting the non-occlusion face images and the fused second training images, so that the trained face recognition model can accurately recognize the non-occlusion face images and the occlusion face images at the same time, and the technical problems that the accuracy rate is low and even the face images with the occlusion objects cannot be recognized when the existing face recognition model recognizes the face images with the occlusion objects are solved.","['G06V40/166', 'G06V40/171', 'G06F18/214', 'G06V10/247', 'G06V10/267', 'G06V10/273', 'G06V10/34', 'G06V10/72', 'G06V10/7715', 'G06V10/774', 'G06V40/162', 'G06V40/172', 'Y02T10/40']"
CN110147767B,Three-dimensional gesture attitude prediction method based on two-dimensional image,"The embodiment of the application provides a three-dimensional gesture attitude prediction method based on a two-dimensional image, which comprises the following steps: establishing a two-dimensional image data set marked with gesture key points; dividing a first training sample in the two-dimensional image data set through a palm segmentation neural network to obtain a gesture contour region; matching and training the gesture outline area and the gesture key points to obtain a two-dimensional gesture prediction model; performing gesture key point scoring training on the two-dimensional gesture prediction model according to a second training sample in the two-dimensional image data set to obtain a gesture evaluation neural network; inputting the gesture outline region extracted from the image to be detected into a gesture evaluation neural network to obtain a two-dimensional gesture prediction graph and a gesture key point score graph; and matching the two-dimensional gesture prediction graph with a pre-stored three-dimensional gesture graph through a gesture priori network, and outputting the three-dimensional gesture prediction graph according to the gesture key point score graph. According to the method and the device, the three-dimensional gesture is predicted based on the two-dimensional gesture image, and the prediction accuracy is high.","['G06F18/214', 'G06V40/107', 'Y02D10/00']"
CN108604301B,Keypoint-based point pair features for scalable automatic global registration for large RGB-D scans,"A method, system, and apparatus provide the ability to globally register point cloud scans. First and second three-dimensional (3D) point clouds are acquired. The point clouds have a common subset of points and there is no a priori knowledge of the alignment between the point clouds. A particular point that may be identified in another point cloud is detected. Information about the normal of each of the detected specific points is retrieved. A descriptor (which only describes 3D information) is constructed on each of the detected specific points. A matching pair of descriptors is determined. Rigid transformation hypotheses are estimated (based on matching pairs) and the transformation is represented. Hypotheses are accumulated into the fitting space, selected based on density, and validated based on the score. Then, one of the hypotheses is selected as registration.","['G06V20/653', 'G01S7/00', 'G06T17/005', 'G06T17/10', 'G06T7/00', 'G06T7/33', 'G06V10/462', 'G06V10/757', 'G06T2200/04', 'G06T2207/10024', 'G06T2207/10028']"
US8027521B1,Method and system for robust human gender recognition using facial feature localization,"The present invention is a method and system to provide a face-based automatic gender recognition system that utilizes localized facial features and hairstyles of humans. Given a human face detected from a face detector, it is accurately localized to facilitate the facial/hair feature detection and localization. Facial features are more finely localized using the geometrically distributed learning machines. Then the position, size, and appearance information of the facial features are extracted. The facial feature localization essentially decouples geometric and appearance information about facial features, so that a more explicit comparison can be made at the recognition stage. The hairstyle features that possess useful gender information are also extracted based on the hair region segmented, using the color discriminant analysis and the estimated geometry of the face. The gender-sensitive feature vector, made up from the extracted facial and hairstyle features, is fed to the gender recognition machines that have been trained using the same kind of gender-sensitive feature vectors of gallery images.",['G06V40/172']
US5452374A,Skew detection and correction of a document image representation,"In a character recognition system, a method and apparatus for correcting the skew of a document image. Skew correction is typically performed during segmentation of the document image into text and non-text parts. Skew correction generally involves skew angle determination and correction of the document image based on the skew angle. A skew angle is determined through the steps of: providing a set of associated rectangles representing the document image, identifying a column edge associated with the set of associated rectangles, comparing rectangles from the set of associated rectangles to identify those that are in the same column and suitably far apart, calculating a tangential angle between the rectangles identified and identifying the most common tangential angle as the skew angle. Once the skew angle is determined, correction of the document image is made by constructing real skewed rectangles from corresponding extracted rectangles and rotating each of the real skewed rectangles around an origin coordinate for a distance based on the skew angle.","['G06T7/70', 'G06V30/1478', 'G06V30/414', 'G06V30/10']"
US8831381B2,Detecting and correcting skew in regions of text in natural images,"An electronic device and method use a camera to capture an image of an environment outside the electronic device followed by identification of regions, based on pixel intensities in the image. At least one processor automatically computes multiple values of an indicator of skew in multiple regions in the image respectively. The multiple values are specific to the multiple regions, and thereafter used to determine whether unacceptable skew is present across the regions, e.g. globally in the image as a whole. When skew is determined to be unacceptable, user input is requested to correct the skew, e.g. by displaying on a screen, a symbol and receiving user input (e.g. by rotating an area of touch or rotating the electronic device) to align a direction of the symbol with a direction of the image, and then the process may repeat (e.g. capture image, detect skew, and if necessary request user input).","['G06K9/00456', 'G06V30/18095', 'G06K9/3258', 'G06K9/36', 'G06T11/60', 'G06V20/63', 'G06V30/413', 'G06V30/10']"
US9607391B2,Image object segmentation using examples,"Systems and methods are disclosed herein for using one or more computing devices to automatically segment an object in an image by referencing a dataset of already-segmented images. The technique generally involves identifying a patch of an already-segmented image in the dataset based on the patch of the already-segmented image being similar to an area of the image including a patch of the image. The technique further involves identifying a mask of the patch of the already-segmented image, the mask representing a segmentation in the already-segmented image. The technique also involves segmenting the object in the image based on at least a portion of the mask of the patch of the already-segmented image.","['G06T7/0079', 'G06T7/11', 'G06F18/22', 'G06K9/6215', 'G06T7/174', 'G06T7/251', 'G06T2207/10024', 'G06T2207/20076', 'G06T2207/20081']"
US9720934B1,Object recognition of feature-sparse or texture-limited subject matter,"An object recognition system can be adapted to recognize subject matter having very few features or limited or no texture. A feature-sparse or texture-limited object can be recognized by complementing local features and/or texture features with color, region-based, shape-based, three-dimensional (3D), global, and/or composite features. Machine learning algorithms can be used to classify such objects, and image matching and verification can be adapted to the classification. Further, multiple modes of input can be integrated at various stages of the object recognition processing pipeline. These multi-modal inputs can include user feedback, additional images representing different perspectives of the object or specific regions of the object including a logo or text corresponding to the object, user behavior data, location, among others.","['G06F17/30247', 'G06F16/583', 'G06F17/30705', 'G06F18/22', 'G06F18/24', 'G06V10/40', 'G06V10/42', 'G06V10/75', 'G06V10/764']"
WO2020140698A1,"Table data acquisition method and apparatus, and server","A table data acquisition method and apparatus, and a server. The method comprises: obtaining image data of text to be processed; extracting a combined graph from the image data, the combined graph being a graph containing morphological vertical lines and morphological horizontal lines crossing each other; dividing the combined graph into a plurality of rectangular units; performing optical character recognition on the rectangular units respectively, and determining text information of the rectangular units; and according to the position coordinates of the rectangular units, combining the rectangular units containing the text information to obtain table data. By first obtaining graphic features such as morphological vertical lines and morphological horizontal lines in image data and obtaining a combined graph according to the graphic features, then dividing the combined graph into a plurality of rectangular units for optical character recognition to obtain text information of the rectangular units, and carrying out combination reduction according to the position coordinates to obtain table data, the technical problems of big errors and inaccuracy in table data extraction in an existing method are solved.",['G06V30/413']
CN110111338B,Visual tracking method based on superpixel space-time saliency segmentation,"The embodiment of the invention discloses a visual tracking method based on superpixel space-time saliency segmentation, which comprises the following steps: modeling the appearance of the target by combining a color histogram of the superpixel in a space domain and a sparse optical flow in a time domain, wherein the model represents the color, the scale and the motion characteristics of the target; applying a graph model-based saliency detection algorithm to superpixel-based target tracking; the separation of foreground super pixels and background super pixels is completed based on an iterative threshold segmentation algorithm, and the target is accurately cut by clustering the correlation coefficient matrix; and training the SVM classifier by using the cut samples, and using the trained classifier for target foreground recognition of subsequent frames. Implementing embodiments of the present invention, the use of superpixels can greatly reduce the computational time of complex image processing and provide flexibility compared to high-level and low-level features.","['G06F18/23', 'G06F18/2411', 'G06T7/11', 'G06T7/194', 'G06T2207/10004']"
CN111259899B,Code spraying character detection method,"The invention discloses a method for detecting code-spraying characters, which comprises the following steps: collecting a code spraying image of a qualified sample as a reference image; step (2) selecting a character area to be detected in a picture frame on the reference image; collecting a code spraying sample image to be detected on the production line; if the character area of the current image has deviation relative to the character area of the reference image, tracking the character area to be detected by adopting a target tracking algorithm based on feature point matching; step (5) if the character area to be detected may contain a plurality of lines of characters, inputting the character area image into a semantic segmentation neural network, and segmenting the plurality of lines of character areas into a single line; step (6) correcting the distorted and/or inclined single-row characters; step (7) adopting an improved convolution cyclic neural network algorithm to identify single-line characters; and (8) judging whether all samples are detected, if not, repeating the steps (3) - (8), and otherwise, ending the detection.","['G06V30/153', 'G06F18/232', 'G06F18/2415', 'G06N3/045', 'G06N3/08', 'G06V10/22', 'G06V10/267']"
EP3872700A2,"Image processing method, apparatus, electronic device, and storage medium","The disclosure provides an image processing method, an image processing apparatus, an electronic device and a storage medium, which belongs to the field of computer technologies, and specifically relates to computing vision, image processing, face recognition, and deep learning technologies in artificial intelligence. The method includes: performing skin color recognition on a face image to be processed to determine a target skin color of a face contained in the face image; obtaining a reference transformation image corresponding to the face image by processing the face image using any style transfer model in response that a style transfer model set does not comprise a style transfer model corresponding to the target skin color; and obtaining a target transformation image matching the target skin color by adjusting a hue value, a saturation value, and a lightness value of each pixel in the target region based on the target skin color.","['G06T3/04', 'G06V10/25', 'G06T3/60', 'G06T7/10', 'G06T7/11', 'G06T7/90', 'G06V10/758', 'G06V10/772', 'G06V20/20', 'G06V40/162', 'G06T2207/20081', 'G06T2207/30201']"
CN114782499B,A method and device for extracting static areas of images based on optical flow and view geometry constraints,"The invention discloses an image static region extraction method and device based on optical flow and view geometric constraint. Filtering and preprocessing the acquired image, extracting characteristic angular points with direction information, matching the characteristic angular points after preprocessing the characteristic angular points, calculating an optical flow vector field by using a sparse optical flow method, primarily separating static characteristic points, further verifying the static characteristic points by using epipolar geometric constraint, optimizing the separation result of the static characteristic points, finally clustering the non-static characteristic points, dividing pixel areas of the non-static characteristic points, outputting an image static area after image morphological processing, and providing the image static area for visual SLAM for positioning and map construction. The invention combines the characteristic optical flow and geometric constraint, solves the interference of the dynamic target on the visual SLAM system, improves the positioning and mapping precision of the visual SLAM system in the dynamic environment, ensures the real-time performance of the system and realizes the reliable, quick, high-precision and low-delay visual positioning and mapping system.","['G06T7/269', 'G06F18/23', 'G06T7/12', 'G06T7/136', 'G06T7/62', 'G06T2207/20164']"
US8249333B2,Segmenting image elements,"A method of segmenting image elements into a foreground and background is described, such that only the foreground elements are part of a volume of interest for stereo matching. This reduces computational burden as compared with computing stereo matching over the whole image. An energy function is defined using a probabilistic framework and that energy function approximated to require computation only over foreground disparities. An optimization algorithm is used on the energy function to perform the segmentation.","['G06T7/11', 'G06T7/00', 'G06T1/00', 'G06T7/30', 'G06T2207/10012', 'G06T2207/20076']"
CN111986178B,"Product defect detection method, device, electronic device and storage medium","The application discloses a product defect detection method, a product defect detection device, electronic equipment and a storage medium, and relates to the fields of computer vision, image processing, deep learning and the like. The specific implementation scheme is as follows: acquiring an image to be detected of a target product; determining a color characteristic difference value between the image to be detected and the template image according to the color characteristic value of the image to be detected and the color characteristic value of the pre-stored template image; taking the color characteristic difference value as a characteristic value of the first channel to obtain an input image comprising the first channel; and obtaining defect information of the target product according to the input image and the target detection model. The embodiment of the application can improve the defect detection effect on products with small size and weak texture characteristics in industrial scenes.","['G06T7/001', 'G06T7/90', 'G06V10/40', 'G06T2207/10024']"
US10755089B2,Hierarchical differential image filters for skin analysis,"There is provided a framework including systems and methods for analyzing skin parameters from images or videos showing skin. Using a series of Hierarchical Differential Image Filters (HDIF), it becomes possible to detect different skin features such as wrinkles, spots, and roughness. The hierarchical differential image filter computes two enhancements to an image showing skin at two different levels of enhancement, determines a differential image using the two enhancements and computes the skin analysis rating using the differential image. These skin ratings are comparably accurate to actual ratings by dermatologists.","['G06K9/00362', 'G06V40/10', 'G06K9/4609', 'G06K9/6857', 'G06V10/443', 'G06V30/2504', 'G06K2209/05', 'G06K9/527', 'G06V10/52', 'G06V2201/03']"
CN110610453B,Image processing method and device and computer readable storage medium,"The embodiment of the invention discloses an image processing method, an image processing device and a computer readable storage medium, wherein when a click instruction on a video is detected, a first image of a current frame of the video and a click position indicated by the click instruction are obtained; determining an initial contour image corresponding to a target object at a click position on the first image; adjusting the initial contour image according to the image characteristic information of the target object to obtain a target contour image; and dividing the target contour image into a plurality of subarea images, and acquiring three-dimensional information of each subarea image to construct so as to obtain a three-dimensional image corresponding to the target object. Therefore, the initial contour image corresponding to the clicked target object in the first image in the video is determined through the computer vision technology, the initial contour image is perfected through the image characteristic information to obtain the target contour image, three-dimensional image construction is carried out according to the target contour image, and the image processing efficiency is greatly improved.","['G06F3/0488', 'G06T17/00', 'G06T3/08', 'G06T7/12', 'G06T7/136', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'Y02D10/00']"
US9417700B2,Gesture recognition systems and related methods,"In one embodiment of the invention, the a method includes the steps of receiving one or more raw frames from one or more cameras, each of the one or more raw frames representing a time sequence of images, determining one or more regions of the one or more received raw frames that comprise highly textured regions, segmenting the one or more determined highly textured regions in accordance textured features thereof to determine one or more segments thereof, determining one or more regions of the one or more received raw frames that comprise other than highly textured regions, and segmenting the one or more determined other than highly textured regions in accordance with color thereof to determine one or more segments thereof. One or more of the segments are then tracked through the one or more raw frames representing the time sequence of images.","['G06F3/017', 'G06K9/00335', 'G06K9/3216', 'G06K9/4671', 'G06V10/245', 'G06V10/462', 'G06V40/20']"
CN112163449B,A lightweight multi-branch feature cross-layer fusion image semantic segmentation method,"The invention relates to a light multi-branch feature cross-layer fusion image semantic segmentation method, which comprises the following steps of: 1) Acquiring an original image, inputting the original image into a convolution downsampling network to obtain images with different resolutions, retaining the spatial position information of an object, and simultaneously reducing the information redundancy in the images; 2) Respectively inputting images with different resolutions into a semantic feature extraction branch and a cross-layer connection branch to respectively obtain a deep layer feature and a shallow layer feature; 3) Respectively carrying out importance distinguishing on the deep layer characteristics and the shallow layer characteristics, and then carrying out characteristic cross-layer fusion; 4) And after classifying, recombining and upsampling the fused feature map, outputting a semantic segmentation result. Compared with the prior art, the method has the advantages of high calculation efficiency, high precision and the like.","['G06V20/10', 'G06N3/045', 'G06N3/08', 'G06V10/267', 'G06V10/464']"
CN110717470B,"Scene recognition method and device, computer equipment and storage medium","The embodiment of the invention discloses a scene recognition method, a device, computer equipment and a storage medium, wherein the scene recognition method comprises the following steps: performing shot segmentation on the target video, and extracting audio data and an image sequence in the segmented shot segments; respectively inputting the audio data and the image sequence into corresponding deep learning network models to obtain audio characteristics and image characteristics; fusing the audio features and the image features to obtain fusion features; and inputting the fusion characteristics into a scene classifier, and identifying the scenes of the corresponding lens fragments to obtain scene categories. The technical scheme of the embodiment of the invention can improve the accuracy of video scene recognition.","['G06V20/41', 'G06N3/045', 'G06N3/08']"
US10671835B2,Object recognition,"The present disclosure relates to methods, devices, and systems for object recognition. For example, the systems, devices, and methods described herein may be used to recognize types, orientations and positions of objects, such as objects (e.g., planar industrial parts) in a bin picking industrial environment. In an aspect of the present disclosure, a system for object recognition may project first 3D point cloud surface data to a 2D representation data. The system may perform a matching operation to evaluate a 2D object pose of the 2D representation data and to match the 2D representation data to a 2D object template. After a match is identified, the system may project the 2D representation data to 3D space to obtain a coarse 3D object pose.","['G06T7/74', 'G06V20/647', 'G06K9/00208', 'G06K9/00214', 'G06T7/10', 'G06T7/70', 'G06V20/653', 'G06T2207/10028']"
CN112200111B,An Occlusion Robust Pedestrian Re-identification Method Based on Global and Local Feature Fusion,"A global and local feature fused occlusion robust pedestrian re-identification method relates to a computer vision technology. The method comprises the following steps: 1) preparing training data; 2) designing and training a model; the model comprises a ResNet-50 framework network, a global branch, a local branch and a semantic branch, wherein the global branch utilizes SPC loss to extract global characteristics, the local branch extracts local characteristics, the semantic branch predicts a human semantic label, and the three branches can be combined together to carry out end-to-end training. 3) And extracting global features and local features of all pedestrian images in the pedestrian re-recognition data training set and the test set and semantic labels of the predicted pedestrian images by using the trained model, and calculating indicators of non-shielding areas. 4) And respectively calculating the similarity between each pedestrian image in the query set and all the pedestrian images in the database, and sequencing the pedestrian images from large to small according to the similarity, thereby completing the pedestrian re-identification. The performance of the recognition is significantly improved.","['G06V40/103', 'G06F18/241', 'G06N3/045', 'G06N3/08', 'G06V10/44', 'Y02T10/40']"
US8111873B2,Method for tracking objects in a scene,"A method for tracking objects in a scene being viewed by a sequence of digital images comprises: separating a region of interest in the digital images into a plurality of spatially smaller, overlapping modules, and defining a hidden Markov model for each module. The method further comprises observing detections of positions of possible objects at different times, considering a set of states at a point of time (t1) such that the set of states comprises a state having a global optimum probability, and backtracking through the sequence of digital images for each hidden Markov model to shrink the set of states that are possible for earlier parts of the sequence of digital images, such that a single optimal state is found for an earlier point of time (t2<t1), whereby tracks of detected objects through the scene are determined up to the earlier point of time (t2).",['G06V20/52']
WO2022127454A1,"Method and device for training cutout model and for cutout, equipment, and storage medium","A method and device for training a cutout model and for cutout, equipment, and a storage medium, related to the field of computer vision. The method comprises: inputting a sample image into a first image segmentation model and a second image segmentation model to respectively produce an initial sample mask image and an initial sample trimap image corresponding to the sample image; optimizing the initial sample trimap image by utilizing the initial sample mask image to produce a target sample trimap image; inputting the target sample trimap image and the sample image into a cutout model to produce a sample alpha image corresponding to the sample image; and training an image processing model on the basis of the sample alpha image and of a tagged alpha image corresponding to the sample image. This increases the accuracy of segmenting the trimap image, thus increasing the cutout precision and accuracy of the cutout model.","['G06T7/11', 'G06N3/084', 'G06T5/30', 'G06T7/12', 'G06T7/194', 'G06T2207/20036', 'G06T2207/20081']"
US10430649B2,Text region detection in digital images using image tag filtering,"Text region detection techniques and systems for digital images using image tag filtering are described. These techniques and systems support numerous advantages over conventional techniques through use of image tags to filter text region candidates. A computing device, for instance, may first generate text region candidates through use of a variety of different techniques, such as text line detection. The computing device then assigns image tags to the text region candidates. The assigned image tags are then used by the computing device to filter the text region candidates based on whether image tags assigned to respective candidates are indicative of text.","['G06V30/413', 'G06V20/63', 'G06K9/00456', 'G06K9/3258']"
US11676390B2,"Machine-learning model, methods and systems for removal of unwanted people from photographs","Methods and systems for fully-automatic image processing to detect and remove unwanted people from a digital image of a photograph. The system includes the following modules: 1) Deep neural network (DNN)-based module for object segmentation and head pose estimation; 2) classification (or grouping) of wanted versus unwanted people based on information collected in the first module; 3) image inpainting of the unwanted people in the digital image. The classification module can be rules-based in an example. In an example, the DNN-based module generates, from the digital image: 1. A list of object category labels, 2. A list of object scores, 3. A list of binary masks, 4. A list of object bounding boxes, 5. A list of crowd instances, 6. A list of human head bounding boxes, and 7. A list of head poses (e.g., yaws, pitches, and rolls).","['G06V20/53', 'G06T5/60', 'G06T5/77', 'G06T7/11', 'G06T7/74', 'G06V10/25', 'G06V10/454', 'G06V10/82', 'G06V40/162', 'G06V40/172', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196', 'G06T2207/30242', 'G06V2201/07']"
US4928313A,Method and system for automatically visually inspecting an article,A method and system are disclosed for automatically visually inspecting an article such as an electronic circuit wherein both reference and non-reference algorithms are utilized to detect circuit defects. The system includes a pipelined cellular image processor which is utilized to implement the non-reference algorithm and an arithmetic logic unit (ALU) is coupled to the output of the cellular image processor to perform the reference method. The non-reference method includes a spaces and traces algorithm and the reference method includes a topology matching algorithm. The system also includes an algorithm for locating and gauging critical areas of the circuit with sub-pixel accuracy. The cellular image processor is supported by a matched host image processor system,"['G06T7/001', 'G01N21/956', 'G06T2207/30148']"
CN108229303B,"Detection recognition and training method, device, equipment and medium for detection recognition network","The embodiment of the invention discloses a training method, a device, equipment and a medium for detecting, identifying and detecting an identification network, wherein the detecting and identifying method comprises the following steps: inputting an image to be processed into a detection and identification network; the detection identification network comprises a sharing network layer, a detection network layer and an identification network layer; outputting the sharing layer characteristics of the image to be processed through the sharing network layer; inputting the sharing layer characteristics into the detection network layer, outputting the detection layer characteristics of the image to be processed through the detection network layer, and acquiring text box information including characters in the image to be processed based on the detection layer characteristics; and inputting the sharing layer characteristics and the text box information into the identification network layer, and outputting the text content in the text box through the identification network layer. The embodiment of the invention reduces repeated feature extraction on the image and improves the processing efficiency; the efficiency and the speed of character detection and identification are improved.","['G06V30/413', 'G06F18/214', 'G06V10/267', 'G06V30/10']"
CN113160192B,Visual sense-based snow pressing vehicle appearance defect detection method and device under complex background,"The invention provides a visual-based snow pressing vehicle appearance defect detection method under a complex background, which comprises the following steps of: s1: acquiring an image of a region to be detected of a target object by using a mobile terminal in a random shooting mode, and transmitting image information to an upper computer; s2: the upper computer receives the image information and detects the defects of the image; s3: and the upper computer stores the detection result and finally generates a detection report. The invention combines object-level defect detection and semi-supervised defect classification, and provides a semi-supervised defect detection method for realizing object-level automatic labeling under a complex background.","['G06T7/0004', 'G06F18/2414', 'G06F18/2415', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T2207/30252']"
CN107316031B,Image feature extraction method for pedestrian re-identification,"The invention discloses an image feature extraction method for pedestrian re-identification, which is used for carrying out pedestrian re-identification through aligning local descriptor extraction and grading global feature extraction; extracting the alignment local descriptors, processing the original image by adopting affine transformation, and performing summation pooling operation on image block features of adjacent regions to obtain the alignment local descriptors; aligning the local descriptors to reserve space information between blocks inside the image; the hierarchical global feature extraction is to obtain global features by classifying the positioned pedestrian region blocks and solving corresponding feature mean values. By adopting the technical scheme of the invention, the problem of characteristic misalignment caused by pedestrian attitude change and the like in pedestrian re-identification can be solved, and the influence of irrelevant backgrounds on the re-identification is eliminated, so that the accuracy and the robustness of the pedestrian re-identification are improved.","['G06N20/00', 'G06V40/23', 'G06F18/2163', 'G06F18/22', 'G06V10/36', 'G06V10/443', 'G06V10/50', 'G06V10/56', 'G06V40/10', 'G06V40/103']"
US7593550B2,Distance iris recognition,"A system for one dimensional segmentation of an iris of an eye into a map of the iris and classification of the map into unaffected areas and affected areas. Also, the system may provide for regular shape fitting of the areas for normalization and identifying the unaffected areas as symmetric segments. Further, the system may assign weights to the unaffected areas and the affected areas of the map of the iris and an enrolled map of an iris and their corresponding bins for matching purposes.","['G06V40/193', 'G06V40/18']"
US9292933B2,Method and apparatus for shape based deformable segmentation of multiple overlapping objects,The present invention provides a system and method for simultaneous variational and adaptive segmentation of single non-overlapping and multiple overlapping/occluded-objects in an image scene. The method according to the present invention synergistically combines shape priors with boundary and region-based active contours in a level set formulation with a watershed scheme for model initialization for identifying and resolving multiple object overlaps in an image scene. The method according to the present invention comprises learning a shape prior for a target object of interest and applying the shape prior into a hybrid active contour model to segment overlapping and non-overlapping objects simultaneously.,"['G06T7/12', 'G06T7/0089', 'G06T7/0083', 'G06T7/0087', 'G06T7/0097', 'G06T7/149', 'G06V10/26', 'G06V20/695', 'G06T2207/20081', 'G06T2207/20116', 'G06T2207/20124', 'G06T2207/20152', 'G06T2207/30024', 'G06T2207/30096']"
US11720766B2,Systems and methods for text and barcode reading under perspective distortion,"A method for automatically recognizing content of labels on objects includes: capturing visual information of an object using a scanning system including one or more cameras, the object having one or more labels on one or more exterior surfaces; detecting, by a computing system, one or more surfaces of the object having labels; rectifying, by the computing system, the visual information of the one or more surfaces of the object to compute one or more rectified images; and decoding, by the computing system, content of a label depicted in at least one of the one or more rectified images.","['G06K7/1447', 'G06K7/1417', 'G06K7/1443', 'G06T7/521', 'G06T7/593']"
US11727661B2,Method and system for determining at least one property related to at least part of a real environment,"A method for determining at least one property related to at least part of a real environment comprises receiving a first image of a first part of a real environment captured by a first camera, wherein the first camera is a thermal camera and the first image is a thermal image and the first part of the real environment is a first environment part, providing at least one description related to at least one class of real objects, wherein the at least one description includes at least one thermal property related to the at least one class of real objects, receiving a second image of the first environment part and of a second part of the real environment captured by a second camera, wherein the second part of the real environment is a second environment part, providing an image alignment between the first image and the second image, determining, for at least one second image region contained in the second image, at least one second probability according to the image alignment, pixel information of the first image, and the at least one description, wherein the at least one second probability relates to the at least one class of real objects, and wherein the at least one second image region comprises at least part of the first environment part, determining at least one image feature derived from pixel information of at least one third image region contained in the second image according to the at least one second probability, wherein the at least one third image region comprises at least part of the second environment part, and performing at least one computer vision method to determine at least one property related to at least part of the second environment part according to the determined at least one image feature.","['G06V10/143', 'G06T7/11', 'G06T7/248', 'G06T7/74', 'G06T2207/10048', 'G06T2207/20076', 'G06T2207/20212', 'G06V40/161']"
WO2022272230A1,Computationally efficient and robust ear saddle point detection,"A computer-implemented method includes receiving a two-dimensional (2-D) side view face image of a person, identifying a bounded portion or area of the 2-D side view face image of the person as an ear region-of-interest (ROI) area showing at least a portion of an ear of the person, and processing the identified ear ROI area of the 2-D side view face image, pixel-by-pixel, through a trained fully convolutional neural network model (FCNN model) to predict a 2-D ear saddle point (ESP) location for the ear shown in the ear ROI area. The FCNN model has an image segmentation architecture.","['G06V10/82', 'G06V40/165', 'G02C7/027', 'G06F18/21', 'G06F18/2413', 'G06T17/00', 'G06T19/20', 'G06T7/246', 'G06T7/50', 'G06T7/73', 'G06V10/255', 'G06V30/18057', 'G06V40/171', 'G06T2207/20084', 'G06T2207/30201', 'G06T2219/2004']"
CN109522908B,Image Saliency Detection Method Based on Region Label Fusion,"The image saliency detection method based on region label fusion in the embodiment of the invention uses a super-pixel segmentation algorithm to preprocess an image and over-segments the image into a plurality of image region blocks; obtaining region similarity by using a Gaussian kernel function, performing spectral clustering of a super-pixel region by using the region similarity to obtain a label set of image segmentation, and storing boundary information of an image according to the label set; acquiring salient features of the image, and fusing the salient features under a conditional random field model to obtain a roughness salient map; propagating the boundary information by using the label set, comparing and fusing the boundary information with the coarseness saliency map to obtain the reconstruction of the coarseness saliency map; and carrying out binarization processing on the reconstructed thickness saliency map by adopting a self-adaptive threshold segmentation mode, labeling the saliency areas into uniform labels by using label indication vectors, and processing isolated points in the saliency areas to obtain more effective saliency area detection.","['G06T7/11', 'G06F18/22', 'G06F18/23213', 'G06F18/253', 'G06V10/464', 'G06V10/56', 'G06T2207/20021', 'G06T2207/20221']"
CN111753727B,"Method, apparatus, device and readable storage medium for extracting structured information","The embodiment of the application discloses a method, a device, electronic equipment and a computer readable storage medium for extracting structured information, and relates to the technical fields of deep learning, image processing, natural language processing and cloud computing. One embodiment of the method comprises the following steps: acquiring an image to be processed, and identifying and obtaining a wireless table area in the image to be processed; performing semantic segmentation operation on the wireless table area by using a deep model which can extract multi-scale features and is used for segmentation to obtain each text block, so as to obtain each segmented text block; and extracting according to each text block to obtain target structural information. The embodiment provides an automatic structured information extraction scheme of a detail bill and a bill, and particularly aims at a wireless table area, by using a deep model which can extract multi-scale features for dividing and obtaining each text block, the text block dividing effect is better, and the accuracy of the extracted structured information is improved.","['G06V30/412', 'G06F18/23', 'G06F18/24', 'G06N3/045', 'G06N3/08', 'G06V10/267', 'G06V10/464']"
US8589410B2,Visual search using multiple visual input modalities,"Systems, methods, and computer-readable storage media for web-scale visual search capable of using a combination of visual input modalities are provided. An edgel index is created that includes shape-descriptors, including edgel-based representations, that correspond to each of a plurality of images. Each edgel-based representation includes pixels that depicts edges or boundary contours of an image and is created, at least in part, by segmenting the image into a plurality of image segments and performing a multi-phase contour detection on each segment. Upon receiving a search query having a visual query input, the visual query input is converted into shape-descriptors, including an edgel-based representation, and the shape-descriptors, including the edgel-based representation, of each of the plurality of images is compared with the shape-descriptors, including the edgel-based representation, of the visual query input to identify at least one image of the plurality of images that matches the visual query input.","['G06F16/5854', 'G06F16/5838', 'G06F16/316', 'G06F16/3328', 'G06F16/532', 'G06F16/951']"
US8805017B2,Gesture pre-processing of video stream to reduce platform power,"Techniques are disclosed for processing a video stream to reduce platform power by employing a stepped and distributed pipeline process, wherein CPU-intensive processing is selectively performed. The techniques are particularly well-suited for efficient hand-based navigational gesture processing of a video stream, in accordance with some embodiments. The stepped and distributed nature of the process allows for a reduction in power needed to transfer image data from a given camera to memory prior to image processing. In one example case, for instance, the techniques are implemented in a user's computer system wherein initial threshold detection (image disturbance) and optionally user presence (hand image) processing components are proximate to or within the system's camera, and the camera is located in or proximate to the system's primary display. The computer system may be any mobile or stationary computing system having a display and camera that are internal and/or external to the system.","['G06F3/017', 'G06V40/20']"
CA3017646C,Label and field identification without optical character recognition (ocr),Systems of the present disclosure allow fields and labels to be identified in a digital image of a form without performing OCR. A digital image of a form can be partitioned into image segments using computer-vision image-segmentation techniques. Features for each image segment can be extracted using computer-vision feature-detection methods. The features extracted from an image segment can be included in an input instance for a machine-learning model. The machine-learning model can assign a classification to the input instance. The classification can associate the input instance with a field type or a label type.,"['G06V30/412', 'G06N20/00', 'G06T3/02', 'G06T7/11', 'G06V30/262', 'G06V30/40', 'G06V30/10']"
US10620712B2,Interactive input system and method,"A method for human-machine interaction includes monitoring a movement of an object by a sensor that detects positions of the object over time, generating a time-dependent velocity of the object based on the movement of the object, detecting a tapping event of the object tapping on a surface by detecting a sudden change of the time-dependent velocity, and determining a position of the object at a time when the tapping event occurs as a tapping position of the object.","['G06F3/017', 'G06F3/013', 'G06F3/0425', 'G06K9/00201', 'G06K9/00335', 'G06K9/209', 'G06K9/52', 'G06K9/6214', 'G06K9/6232', 'G06V20/64', 'G06V40/20']"
JP5379085B2,Method and system for classifying connected groups of foreground pixels in a scanned document image based on marking type,"Methods and systems for classifying markings on images in a document are undertaken according to marking types. The document containing the images is supplied to a segmenter which breaks the images into fragments of foreground pixel structures that are identified as being likely to be of the same marking type by finding connected components, extracting near-horizontal or -vertical rule lines and subdividing some connected components to obtain the fragments. The fragments are then supplied to a classifier, where the classifier provides a category score for each fragment, wherein the classifier is trained from the groundtruth images whose pixels are labeled according to known marking types. Thereafter, a same label is assigned to all pixels in a particular fragment, when the fragment is classified by the classifier.","['G06V30/15', 'G06F18/214', 'G06V30/18076', 'G06V30/19147', 'G06V30/413', 'G06V30/10']"
CN114303120B,virtual keyboard,"Systems, apparatuses (or devices), methods, and computer-readable media for generating virtual content are provided. For example, a device (e.g., an augmented reality device) may obtain an image of a scene of a real world environment, wherein the real world environment is viewable through a display of the augmented reality device when virtual content is displayed by the display. The device may detect at least a portion of the user's physical hand in the image. The device may generate a virtual keyboard based on detecting at least a portion of the physical hand. The device may determine a location of the virtual keyboard on a display of the augmented reality device relative to at least a portion of the physical hand. The device may display the virtual keyboard at that location on the display.","['G06F3/011', 'G02B27/0172', 'G06F1/163', 'G06F3/012', 'G06F3/014', 'G06F3/017', 'G06F3/0304', 'G06F3/0426', 'G06F3/0487', 'G06T19/006', 'G02B2027/0138', 'G02B2027/014', 'G02B2027/0178', 'G02B2027/0187', 'G06F2203/04803']"
WO2021164228A1,Method and system for selecting augmentation strategy for image data,"The present application relates to the field of artificial intelligence, and provided in the embodiments thereof are a method and system for selecting an augmentation strategy for image data. The present application relates to the technical field of artificial intelligence. The method comprises: from within an augmentation strategy set, selecting a plurality of strategy subsets to be determined to perform sample augmentation on a preset sample training set so as to obtain a plurality of augmented sample training sets; training an initialized classification model by using each augmented sample training set so as to obtain a plurality of trained classification models; inputting a preset sample validation set into each trained classification model to obtain classification accuracy degrees corresponding to the trained classification models; and determining an optimal strategy subset from among the plurality of strategy subsets by using a Bayesian optimization algorithm on the basis of the classification accuracy degrees corresponding to each trained classification model. The technical solution provided in the embodiments of the present application is able to solve the problem of difficulty in determining which augmentation strategy is most effective for the current type of image sample.","['G06F18/214', 'G06F18/24', 'G06N3/045', 'G06N3/08', 'Y02T10/40']"
CN112258618B,Semantic mapping and positioning method based on fusion of prior laser point cloud and depth map,"The invention discloses a semantic mapping and positioning method based on the fusion of a prior laser point cloud and a depth map, which comprises the following steps: s1, collecting prior laser point cloud data; s2, acquiring a depth image and an RGB image, generating an RGB-D point cloud based on the depth image, and carrying out initialization registration on the prior laser point cloud and the RGB-D point cloud; s3, providing camera pose constraint through the registered prior laser point cloud to correct the camera pose; s4, constructing a three-dimensional geometric point cloud map by adopting a front and back window optimization method; s5, performing geometric incremental segmentation on the three-dimensional geometric point cloud map, performing object identification and semantic segmentation on the RGB image, and fusing geometric incremental segmentation and semantic segmentation results to obtain a 3D geometric segmentation map of semantic enhanced geometric segmentation; and S6, performing semantic association and segmentation probability distribution updating on the object to complete the construction of the semantic map. The invention can effectively eliminate the accumulated error of large-scale indoor map building and positioning, and has high precision and real-time performance.","['G06T15/04', 'G06T7/12', 'G06T7/13', 'G06T7/85', 'G06T2207/10012', 'G06T2207/10032']"
US20180189974A1,Machine learning based model localization system,"A method for deriving an image sensor's 3D pose estimate from a 2D scene image input includes at least one Machine Learning algorithm trained a priori to generate a 3D depth map estimate from the 2D image input, which is used in conjunction with physical attributes of the source imaging device to make an accurate estimate of the imaging device 3D location and orientation relative to the 3D content of the imaged scene. The system may optionally employ additional Machine Learning algorithms to recognize objects within the scene to further infer contextual information about the scene, such as the image sensor pose estimate relative to the floor plane or the gravity vector. The resultant refined imaging device localization data can be applied to static (picture) or dynamic (video), 2D or 3D images, and is useful in many applications, most specifically for the purposes of improving the realism and accuracy of primarily static, but also dynamic Augmented Reality (AR) applications.","['G06T7/70', 'G06N3/08', 'G06N3/04', 'G06T7/50', 'G06T7/536', 'G06T7/77', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30244']"
CN108549864B,Vehicle-mounted thermal imaging pedestrian detection-oriented region-of-interest filtering method and device,"The invention discloses a vehicle-mounted thermal imaging pedestrian detection-oriented RoIs filtering method and device. The method for filtering the RoIs refers to a method for designing three-layer cascade filters to filter the non-pedestrian RoIs: the first layer filters the RoIs with abnormal sizes by calculating the pixel height of the pedestrian and the height-width ratio of the RoIs and setting corresponding threshold intervals; the second layer calculates the vertical distance between the upper and lower boundaries of the second layer and the road surface reference of the current image one by one through the RoIs, calculates a threshold value based on the height of the RoIs pixel, and filters the RoIs with abnormal positions; and the third layer searches possible pedestrian head regions according to the brightness vertical projection difference curve of each Rois, compares the difference degree of the Haar-like characteristics of the head regions and the adjacent background regions, and filters the Rois missing the pedestrian head. The method can reduce the calculation overhead of pedestrian detection and enhance the scene adaptability of the classifier on the premise of considering the accuracy rate of pedestrian detection. The Rois filtering device comprises a size abnormal Rois filter, a position abnormal Rois filter and a head missing pedestrian Rois filter.","['G06V20/56', 'G06V40/103', 'G06F18/214', 'G06F18/24', 'G06V10/143', 'G06V10/25', 'G06V10/30', 'G06V10/462', 'G06V10/507', 'G06V20/40', 'G06V40/10']"
WO2022116423A1,"Object posture estimation method and apparatus, and electronic device and computer storage medium","A target object posture estimation method, comprising: obtaining a three-dimensional point cloud according to a scene depth map of a target object; extracting a target object point set from the three-dimensional point cloud; calculating a visibility loss value of the target object according to the three-dimensional point cloud and the target object point set; calculating a key point loss value of the target object by means of performing Hough voting on the target object point set; performing semantic segmentation on pixel points of the scene depth map, so as to obtain a semantic loss value of the target object; and calculating a posture of the target object according to the visibility loss value, the key point loss value, the semantic loss value and a multi-task joint model. Further provided are a target object posture estimation apparatus, a device and a storage medium. The method further relates to blockchain technology, and a scene depth map can be stored in a blockchain node. By means of the method, the posture of a target object to be grabbed can be accurately analyzed, thereby improving the grabbing precision of a mechanical arm.","['G06T7/70', 'G06T7/10', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084']"
CN110148084B,"Method, device, equipment and storage medium for reconstructing 3D model from 2D image","The embodiment of the invention discloses a method, a device, equipment and a storage medium for reconstructing a 3D model from a 2D image. Comprising the following steps: acquiring two-dimensional images corresponding to at least two visual angles of a three-dimensional object respectively; inputting the two-dimensional images corresponding to the at least two visual angles into a set neural network to perform information fusion and 3D model reconstruction, and obtaining the 3D model of the three-dimensional object. According to the method for reconstructing the 3D model by the 2D image, provided by the embodiment of the invention, the two-dimensional images with at least two view angles corresponding to each other are input into the set neural network to perform information fusion and 3D model reconstruction, so that the 3D model of the three-dimensional object is obtained, the accuracy of reconstructing the 3D model can be improved, and a user can understand the three-dimensional object more easily by checking the 3D model.","['G06N3/04', 'G06T17/10', 'G06T17/00', 'G06N3/08', 'G06T3/08', 'G06T7/50', 'G06T7/55', 'G06T2207/20081', 'G06T2207/20084', 'G06T2210/32']"
US8941588B2,Fast fingertip detection for initializing a vision-based hand tracker,"Systems and methods for initializing real-time, vision-based hand tracking systems are described. The systems and methods for initializing the vision-based hand tracking systems image a body and receive gesture data that is absolute three-space data of an instantaneous state of the body at a point in time and space, and at least one of determine an orientation of the body using an appendage of the body and track the body using at least one of the orientation and the gesture data.","['G06F3/017', 'G06F3/0304', 'G06F3/0325', 'G06K9/00375', 'G06V30/1448', 'G06V40/107', 'G06K2009/3225', 'G06V10/245']"
US12067746B2,Systems and methods for using computer vision to pick up small objects,"A method for estimating a pose of an object includes: receiving, by a processor, an observed image depicting the object from a viewpoint; computing, by the processor, an instance segmentation map identifying a class of the object depicted in the observed image; loading, by the processor, a 3-D model corresponding to the class of the object; computing, by the processor, a rendered image of the 3-D model in accordance with an initial pose estimate of the object and the viewpoint of the observed image; computing, by the processor, a plurality of dense image-to-object correspondences between the observed image of the object and the 3-D model based on the observed image and the rendered image; and computing, by the processor, the pose of the object based on the dense image-to-object correspondences.","['G06T7/75', 'B25J13/08', 'B25J9/1697', 'G05B19/4155', 'G06T7/269', 'G06T7/55', 'G05B2219/50391', 'G06T2207/10024', 'G06T2207/10052', 'G06T2207/20084', 'G06T2207/30164']"
WO2022217876A1,"Instance segmentation method and apparatus, and electronic device and storage medium","Provided in the embodiments of the present disclosure are an instance segmentation method and apparatus, and an electronic device and a storage medium. The method comprises: acquiring first semantic information of an image to be processed, a first instance feature of an instance to be segmented in said image, and a first instance mask corresponding to the first instance feature; and on the basis of the first semantic information, the first instance feature and the first instance mask, performing at least two stages of semantic fusion processing, so as to obtain a second instance mask, wherein upsampling is performed on the first instance feature output in the previous stage of semantic fusion processing, so as to obtain an instance feature of the next stage; a corresponding instance mask of the instance feature is obtained on the basis of the instance feature of the next stage; the instance feature of the next stage, the instance mask of the next stage, and corresponding semantic information of the next stage are taken as input features of the next stage of semantic fusion processing; and the resolution of the semantic information in the input features of each stage of semantic fusion processing is the same as the resolution of the instance feature. In this way, semantic information, an instance feature and an instance mask of an instance to be segmented are refined by means of a plurality of stages, wherein an instance feature and detailed information supplemented by semantic segmentation, which are output in a previous stage, are received in each stage, such that the segmentation effect of the instance to be segmented can be greatly improved.","['G06T7/11', 'G06F18/241', 'G06N3/045', 'G06N3/08', 'G06T7/187', 'G06V10/28', 'G06V10/44', 'G06V10/751', 'G06T2207/10004', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084']"
US10586337B2,Producing a segmented image of a scene,"A computer-implemented method of computer vision in a scene that includes one or more transparent objects and/or one or more reflecting objects comprises obtaining a plurality of images of the scene, each image corresponding to a respective acquisition of a physical signal, the plurality of images including at least two images corresponding to different physical signals; and generating a segmented image of the scene based on the plurality of images. This improves the field of computer vision.","['G06T7/143', 'G06T7/11', 'G06F18/23', 'G06T7/174', 'G06T7/187', 'G06T7/50', 'G06T7/97', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/10048']"
US11681418B2,Multi-sample whole slide image processing in digital pathology via multi-resolution registration and machine learning,"When reviewing digital pathology tissue specimens, multiple slides may be created from thin, sequential slices of tissue. These slices may then be prepared with various stains and digitized to generate a Whole Slide Image (WSI). Review of multiple WSIs is challenging because of the lack of homogeneity across the images. In embodiments, to facilitate review, WSIs are aligned with a multi-resolution registration algorithm, normalized for improved processing, annotated by an expert user, and divided into image patches. The image patches may be used to train a Machine Learning model to identify features useful for detection and classification of regions of interest (ROIs) in images. The trained model may be applied to other images to detect and classify ROIs in the other images, which can aid in navigating the WSIs. When the resulting ROIs are presented to the user, the user may easily navigate and provide feedback through a display layer.","['G06F3/0484', 'G06F18/2148', 'G06F18/217', 'G06F18/2431', 'G06F18/40', 'G06F3/04845', 'G06F3/0485', 'G06T3/0075', 'G06T3/147', 'G06T3/4053', 'G06T7/0012', 'G06T7/11', 'G06T7/136', 'G06T7/194', 'G06T7/32', 'G06T7/33', 'G06V10/50', 'G06V10/764', 'G06V10/7788', 'G06V10/82', 'G06V20/695', 'G06V20/698', 'G16H30/40', 'G16H50/20', 'G16H50/70', 'G06F2203/04806', 'G06T2200/24', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/20016', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20092', 'G06T2207/20104', 'G06T2207/30024']"
CN109804302B,Method and apparatus and computer program for determining a representation of an spectacle lens edge,"The invention relates to a computer-implemented method for determining a representation of an edge (26) of an eyeglass lens (28) or of a left eyeglass lens (28) and a right eyeglass lens (29) of an eyeglass wearer (20). According to the invention, the following steps are carried out for this purpose: providing image data b (x) for a spectacle wearer (20) having a spectacle frame (24) worn, calculating information data i (x) derived from the image data b (x), calculating a deterministically optimized cost function e (u) associating the information data i (x) with spectacle lens data u (x), wherein spectacle lens data u (x) describes a spatial extent of at least one spectacle lens (28) held in the spectacle frame (24), and setting a curve of the spectacle lens (28) or of edges (26) of the left spectacle lens (28) and the right spectacle lens (29) by optimizing the cost function e (u).","['G02C13/003', 'G02C7/028', 'G06T7/12', 'B24B13/06', 'G02C13/005', 'G02C7/027', 'G06T7/149', 'G06V40/171', 'G06T2207/10012', 'G06T2207/10024', 'G06T2207/10028']"
US12243231B2,Computer supported review of tumors in histology images and post operative tumor margin assessment,"A computer apparatus and method for identifying and visualizing tumors in a histological image and measuring a tumor margin are provided. A CNN is used to classify pixels in the image according to whether they are determined to relate to nontumorous tissue, or one or more classes for tumorous tissue. Segmentation is carried out based on the CNN results to generate a mask that marks areas occupied by individual tumors. Summary statistics for each tumor are computed and supplied to a filter which edits the segmentation mask by filtering out tumors deemed to be insignificant. Optionally, the tumors that pass the filter may be ranked according to the summary statistics, for example in order of clinical relevance or by a sensible order of review for a pathologist. A visualization application can then display the histological image having regard to the segmentation mask, summary statistics and/or ranking. Tumor masses extracted by resection are painted with an ink to highlight its surface region. The CNN is trained to distinguish ink and no-ink tissue as well as tumor and no-tumor tissue. The CNN is applied to the histological image to generate an output image whose pixels are assigned to the tissue classes. Tumor margin status of the tissue section is determined by the presence or absence of tumor-and-ink classified pixels. Tumor margin involvement and tumor margin distance are determined by computing additional parameters based on classification-specified inter-pixel distance parameters.","['G06T7/0012', 'G06F18/2148', 'G06F18/24', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/11', 'G06V10/95', 'G06V20/693', 'G06V20/695', 'G06V20/698', 'G16H30/40', 'G06F3/04842', 'G06T2200/24', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024', 'G06T2207/30096', 'G06V2201/03']"
CN111462120B,"Defect detection method, device, medium and equipment based on semantic segmentation model","The invention discloses a defect detection method based on a semantic segmentation model, which comprises the following steps: acquiring image data of a workpiece to be detected; inputting image data into a trained semantic segmentation model, and acquiring a defect prediction Label image of the workpiece; the semantic segmentation model comprises a feature extraction network and a feature prediction network which are sequentially connected; the characteristic extraction network is used for predicting the occurrence position of the defect area and comprises a characteristic extraction layer, a characteristic compression layer, a characteristic flattening layer and a characteristic classification layer; the characteristic prediction network is used for classifying each pixel point in the defect area determined by the characteristic extraction network, and outputting a defect prediction Label image of the workpiece after positioning and segmenting the defect area; the defect detection of the workpiece is carried out based on the defect prediction Label graph, so that the model calculation amount can be effectively reduced, and the visual detection of the workpiece defects can be rapidly realized with high precision.","['G06T7/0008', 'G06F18/2415', 'G06T7/11', 'G06T7/62', 'G06V10/44', 'G06V10/56', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30164']"
US11037325B2,Information processing apparatus and method of controlling the same,"An information processing apparatus has a hold unit for holding a plurality of features of a physical space as a plurality of held features, each of the plurality of held features being related with a position in a first image captured by an image capturing apparatus at a first time, and information of an availability for a position/orientation calculation of the image capturing apparatus. The information processing apparatus associates a plurality of detected features, which are detected in a second image captured by the image capturing apparatus at a second time after the first time, with the plurality of held features, and, based on the availabilities related with the associated held features, adjusts a detected feature, among the plurality of detected features, to be used in the calculation of the position/orientation of the image capturing apparatus.","['G06T7/73', 'G06F18/22', 'G06K9/00664', 'G06K9/46', 'G06K9/6201', 'G06T7/55', 'G06T7/60', 'G06V10/40', 'G06V10/75', 'G06V20/10', 'G06K9/00671', 'G06K9/209', 'G06T2207/10021', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/30244', 'G06V10/147', 'G06V20/20']"
CN111488826B,"Text recognition method and device, electronic equipment and storage medium","The application relates to the technical field of computer vision, and provides a text recognition method, a device, electronic equipment and a storage medium, which are used for realizing text recognition of different languages in an image and improving recognition accuracy, wherein the method comprises the following steps: extracting features of an image to be identified, and obtaining a feature map corresponding to the image to be identified, wherein the image to be identified contains texts of at least one language class; according to the feature diagram, performing position detection on the text in the image to be identified, obtaining position information of the text in the image to be identified, and performing language identification on the text in the image to be identified, obtaining language information of the text in the image to be identified; and identifying the text in the image to be identified based on the acquired position information and language information. According to the method and the device for recognizing the texts in the images to be recognized, language prediction is carried out on each text in the images to be recognized, user specification is not needed, each text is flexibly recognized according to the recognition method corresponding to each recognized language, and recognition accuracy is higher.","['G06V30/413', 'G06F18/214', 'G06F18/2415']"
US20110078191A1,Handwritten document categorizer and method of training,"A method and an apparatus for training a handwritten document categorizer are disclosed. For each category in a set into which handwritten documents are to be categorized, discriminative words are identified from the OCR output of a training set of typed documents labeled by category. A group of keywords is established including some of the discriminative words identified for each category. Samples of each of the keywords in the group are synthesized using a plurality of different type fonts. A keyword model is then generated for each keyword, parameters of the model being estimated, at least initially, based on features extracted from the synthesized samples. Keyword statistics for each of a set of scanned handwritten documents labeled by category are generated by applying the generated keyword models to word images extracted from the scanned handwritten documents. The categorizer is trained with the keyword statistics and respective handwritten document labels.","['G06F16/35', 'G06F18/214', 'G06V30/1444', 'G06V30/19147', 'G06V30/2276', 'G06V30/10']"
CN113128442B,Chinese character handwriting style identification method and scoring method based on convolutional neural network,"The invention relates to a handwriting style identification method based on a convolutional neural network. The handwriting style identification method based on the convolutional neural network comprises the following steps: acquiring a calligraphy work image; preprocessing the calligraphic work image to identify each single-word image; inputting the single-word images into a handwriting font style recognition model to obtain handwriting font style recognition results of each single word; the handwriting style recognition model is a ResNet neural network and comprises a first convolution layer, a first pooling layer, 4 convolution layers realized based on Bottleneck residual blocks, a global average pooling layer, 2 full-connection layers and a softmax layer. The handwriting style identification method based on the convolutional neural network has the advantages of high identification accuracy and less calculation amount. The invention also provides a Chinese character handwriting scoring method based on the convolutional neural network, which integrates font style identification, handwriting content identification and handwriting Chinese character aesthetic feeling scoring, and has more comprehensive functions.","['G06V30/2264', 'G06F18/214', 'G06F18/22', 'G06F18/2415', 'G06N3/045', 'G06N3/047', 'G06N3/048', 'G06N3/08', 'G06V10/242', 'G06V10/267']"
CN111325203B,An American license plate recognition method and system based on image correction,"The invention relates to an American license plate recognition method and system based on image correction, wherein a text detection module, an image correction module, a text recognition module and a text classification module comprise the following steps: preprocessing an image file of a data set, enhancing the data, and generating a training set and a test set; designing a text detection module, detecting text information in the image, realizing the segmentation of the text and the background in the image, and obtaining a text image only containing the text information; correcting the text image by adopting an image correction module, and converting the originally distorted or inclined text image into a horizontal direction; identifying the corrected text image to obtain information such as letters, numbers and the like contained in the text image; and constructing a text classification module, and screening the license plate number, the continent name and the annual inspection date from all the text information to finish license plate identification. The method solves the problems of complex background patterns, fuzzy target text image deformation, complex text information and large calculation amount during off-line training by using a neural network during American license plate recognition.","['G06V20/62', 'G06F18/214', 'G06N3/044', 'G06N3/045', 'G06V20/625', 'G06V30/10']"
US11275961B2,"Character image processing method and apparatus, device, and storage medium","Provided are character image processing methods and apparatuses, devices, storage medium, and computer programs. The character image processing method mainly comprises: obtaining at least one image block containing a character in a character image to be processed; obtaining image block form transformation information of the image block on the basis of a neural network, the image block form transformation information being used for changing a character orientation in the image block to a predetermined orientation, and the neural network being obtained by means of training using an image block sample having form transformation label information; performing form transformation processing on the character image to be processed according to the image block form transformation information; and performing character recognition on the character image to be processed which is subjected to the form transformation.","['G06N3/08', 'G06V10/242', 'G06K9/3208', 'G06K9/00456', 'G06K9/325', 'G06K9/342', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G06V10/267', 'G06V10/82', 'G06V20/62', 'G06V30/1463', 'G06V30/158', 'G06V30/18057', 'G06V30/1916', 'G06V30/19173', 'G06V30/413', 'G06K2209/01', 'G06V30/10']"
WO2021012254A1,"Target detection method, system, and mobile platform","A target detection method, a system, and a mobile platform. The method comprises: acquiring a target to be processed detected by a millimeter-wave radar within a preset region; acquiring target auxiliary information detected by an auxiliary sensor within the preset region; and using the target auxiliary information to optimize said target, and obtaining the resulting detected target. The invention employs an auxiliary sensor to optimize a target detected by a millimeter-wave radar, thereby solving the problem in which a millimeter-wave radar has a low resolution and is less sensitive to weak reflectors, and increasing the accuracy of a target detection result.","['G01S13/931', 'G06F18/25', 'G06V20/56']"
CN111951253B,"Method, device and readable storage medium for detecting surface defects of lithium battery","The invention relates to a method, a device and a readable storage medium for detecting surface defects of a lithium battery, wherein the method comprises the following steps: taking the collected image of the surface of the lithium battery as a sample to be processed; performing one or more downsampling operations on the sample to be processed to obtain a feature map; performing up-sampling operation on the feature mapping chart, and performing channel fusion on the feature mapping chart and an image to be sampled before the down-sampling operation is performed on the feature mapping chart to obtain a fusion feature chart; acquiring target characteristic information of the sample to be processed according to at least one of the characteristic mapping chart and the fusion characteristic chart; training a network model based on the tandem grouping rolling blocks by using a sample data set to obtain a detection model; and detecting the image to be detected through the detection model so as to detect whether the image to be detected comprises defects or not. The method can effectively improve the detection rate and realize the effective detection of the defects with weak characteristic information and multiple scales.","['G06T7/0004', 'G06F18/253', 'G06T2207/20081', 'G06T2207/20084']"
CN111191583B,Space target recognition system and method based on convolutional neural network,"The invention discloses a space target recognition system and method based on a convolutional neural network, which are used for solving the technical problem that the existing deep learning cannot achieve both recognition accuracy and calculated amount. The method comprises model training and target recognition, wherein the model training comprises the steps of inputting positive and negative samples, extracting features by using a sparse convolutional neural network, and training a classifier to form a learning dictionary; the target recognition comprises input image preprocessing, input model matching and output result. The beneficial technical effects of the invention are as follows: on the premise of ensuring accuracy, the recognition efficiency is improved, the storage space is saved, and the more efficient hardware design is facilitated.","['G06V20/41', 'G06F18/214', 'G06F18/241', 'G06N3/045', 'G06N3/082', 'G06N3/084', 'G06V10/513', 'G06V2201/07', 'Y02D10/00']"
US20220020175A1,"Method for Training Object Detection Model, Object Detection Method and Related Apparatus","An object detection model training method, object detection method and related apparatus, relate to the field of artificial intelligence technologies such as computer vision, deep learning. An implementation includes: obtaining training sample data including a first remote sensing image and position annotation information of an anchor box of a subject to be detected in the first remote sensing image, where the position annotation information includes angle information of the anchor box relative to a preset direction; obtaining an object feature map of the first remote sensing image based on an object detection model, performing object detection on the subject to be detected based on the object feature map to obtain an object bounding box, and determining loss information between the anchor box and the object bounding box based on the angle information; updating a parameter of the object detection model based on the loss information.","['G06T7/73', 'G06F18/214', 'G06F18/24', 'G06K9/6256', 'G06N3/045', 'G06N3/08', 'G06T7/194', 'G06T7/75', 'G06V10/25', 'G06V10/267', 'G06V10/44', 'G06V10/469', 'G06V10/764', 'G06V10/82', 'G06V20/13', 'G06N3/084', 'G06T2207/10032', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30181', 'G06T2210/12', 'G06V2201/07']"
US11132543B2,Unconstrained appearance-based gaze estimation,"A method, computer readable medium, and system are disclosed for performing unconstrained appearance-based gaze estimation. The method includes the steps of identifying an image of an eye and a head orientation associated with the image of the eye, determining an orientation for the eye by analyzing, within a convolutional neural network (CNN), the image of the eye and the head orientation associated with the image of the eye, and returning the orientation of the eye.","['G06N3/08', 'G06K9/00604', 'G06F18/214', 'G06F18/2413', 'G06K9/00228', 'G06K9/00255', 'G06K9/00617', 'G06K9/00973', 'G06K9/00986', 'G06K9/3216', 'G06K9/4628', 'G06K9/6256', 'G06K9/627', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/09', 'G06N3/096', 'G06T7/11', 'G06T7/70', 'G06T7/73', 'G06V10/245', 'G06V10/454', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06V10/94', 'G06V10/955', 'G06V40/161', 'G06V40/166', 'G06V40/19', 'G06V40/197', 'G06N3/04', 'G06T2200/04', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30201', 'G06T2210/52']"
WO2020215565A1,"Hand image segmentation method and apparatus, and computer device","A hand image segmentation method and apparatus, and a computer device, which relate to the technical field of computers, are applicable for detection and segmentation of a hand image in a picture, and efficiently avoid the problem of the influence of hand image detection caused by skin color confusion, illumination, and deformation during hand image segmentation. The method comprises: acquiring a sample image comprising a complete hand image (101); marking the coordinate location of a hand region in the sample image (102); using the sample image having the coordinate location marked as a training set, and training same on the basis of a Faster R-CNN algorithm to obtain a hand recognition model of which the training result meets a preset standard (103); using the hand recognition model to detect whether an image to be recognized comprises a hand image (104); and outputting, according to the detection result, the hand image segmentation result of the image to be recognized (105).","['G06F18/214', 'G06V40/28']"
US8218862B2,Automatic mask design and registration and feature detection for computer-aided skin analysis,"Methods and systems for automatically generating a mask delineating a region of interest (ROI) within an image containing skin are disclosed. The image may be of an anatomical area containing skin, such as the face, neck, chest, shoulders, arms or hands, among others, or may be of portions of such areas, such as the cheek, forehead, or nose, among others. The mask that is generated is based on the locations of anatomical features or landmarks in the image, such as the eyes, nose, eyebrows and lips, which can vary from subject to subject and image to image. As such, masks can be adapted to individual subjects and to different images of the same subjects, while delineating anatomically standardized ROIs, thereby facilitating standardized, reproducible skin analysis over multiple subjects and/or over multiple images of each subject. Moreover, the masks can be limited to skin regions that include uniformly illuminated portions of skin while excluding skin regions in shadow or hot-spot areas that would otherwise provide erroneous feature analysis results. Methods and systems are also disclosed for automatically registering a skin mask delineating a skin ROI in a first image captured in one imaging modality (e.g., standard white light, UV light, polarized light, multi-spectral absorption or fluorescence imaging, etc.) onto a second image of the ROI captured in the same or another imaging modality. Such registration can be done using linear as well as non-linear spatial transformation techniques.","['A61B5/441', 'G06V10/754', 'G06V40/162', 'G06V40/171']"
US5293429A,System and method for automatically classifying heterogeneous business forms,"Business forms are a special class of documents typically used to collect or distribute data; they represent a vast majority of the paperwork need to conduct business. The present invention provides a pattern recognition system that classifies digitized images of business forms according to a predefined set of templates. The process involves a training phase, during which images of the template forms are scanned, analyzed and stored in a data dictionary, and a recognition phase, during which images of actual forms are compared to the templates in the dictionary to determine their class membership. The invention provides the feature extraction and matching methods, as well as the organization of the form dictionary. The performance of the system was evaluated using a collection of computer generated test forms. The methodology for creating these forms, and the results of the evaluation are also described. Business forms are characterized by the presence of horizontal and vertical lines that delimit the useable space. The present invention identifies these so called regular lines in bi-level digital images to separate text from graphics before applying an optical character recognizer; or as a feature extractor in a form recognition system. The approach differs from existing vectorization, line extraction, and text-graphics separation methods, in that it focuses exclusively on the recognition of horizontal and vertical lines.",['G06V30/40']
US10176367B2,Computer implemented method for sign language characterization,"A sign language recognizer is configured to detect interest points in an extracted sign language feature, wherein the interest points are localized in space and time in each image acquired from a plurality of frames of a sign language video; apply a filter to determine one or more extrema of a central region of the interest points; associate features with each interest point using a neighboring pixel function; cluster a group of extracted sign language features from the images based on a similarity between the extracted sign language features; represent each image by a histogram of visual words corresponding to the respective image to generate a code book; train a classifier to classify each extracted sign language feature using the code book; detect a posture in each frame of the sign language video using the trained classifier; and construct a sign gesture based on the detected postures.","['G06K9/00355', 'G06F3/017', 'G06K9/00389', 'G06V40/113', 'G06V40/28', 'G06V10/464']"
CN112801008B,"Pedestrian re-recognition method and device, electronic equipment and readable storage medium","The application provides a pedestrian re-identification method, a pedestrian re-identification device, electronic equipment and a readable storage medium, and relates to the technical field of image processing. According to the method, the first branch network is added into the pedestrian re-recognition model for extracting the pedestrian segmentation attention feature map, so that the model can pay more attention to the features of the area where the pedestrian is located when the pedestrian re-recognition is carried out, the features which are more beneficial and more obvious to the pedestrian re-recognition in the image can be extracted, a better recognition effect is achieved for the pedestrian re-recognition under the condition that the pedestrian is blocked, and the accuracy of the pedestrian re-recognition can be effectively improved.","['G06V40/103', 'G06F18/253', 'G06V10/462']"
US11227158B2,Detailed eye shape model for robust biometric applications,"Systems and methods for robust biometric applications using a detailed eye shape model are described. In one aspect, after receiving an eye image of an eye (e.g., from an eye-tracking camera on an augmented reality display device), an eye shape (e.g., upper or lower eyelids, an iris, or a pupil) of the eye in the eye image is calculated using cascaded shape regression methods. Eye features related to the estimated eye shape can then be determined and used in biometric applications, such as gaze estimation or biometric identification or authentication.","['G06K9/00617', 'G06V40/197', 'G06F18/2155', 'G06K9/00604', 'G06K9/0061', 'G06K9/00973', 'G06K9/6259', 'G06V10/94', 'G06V40/19', 'G06V40/193']"
WO2019232862A1,"Mouth model training method and apparatus, mouth recognition method and apparatus, device, and medium","A mouth model training method and apparatus, a mouth recognition method and apparatus, a device, and a medium. The mouth model training method comprises: acquiring a face image sample and marking same to obtain face image sample data, and extracting a feature vector of the face image sample (S10); dividing the face image sample data into training sample data and verification sample data (S20); training a support vector machine classifier by using the training sample data to obtain a critical surface of the support vector machine classifier (S30); calculating a vector distance between the feature vector of a verification sample in the verification sample data and the critical surface (S40); acquiring a preset true positive rate or a preset false positive rate, and acquiring a classification threshold according to the vector distance and marked data corresponding to the verification sample data (S50); and acquiring a mouth determination model according to the classification threshold (S60). By means of the mouth model training method, a mouth determination model having a high accuracy for determining whether the mouth is covered can be obtained.","['G06V40/171', 'G06F18/214', 'G06F18/2411']"
US9042650B2,Rule-based segmentation for objects with frontal view in color images,"A method of labeling pixels in an image in which pixels in the image that represent human skin of one or more people are detected and one or more regions in the image are identified, where each region in the one or more regions includes all or a portion of a human face of a person in the one or people in the image. Pixels that represent each face in the image are identified using the pixels that represent skin and the regions that include faces of the people, thereby identifying a position of each face in the image. From this, a face mask for each face and a rough body map corresponding to each face is determined using the positions of the identified faces. Further still, a torso map corresponding to each face is determined using determined face positions. Then, the extracted face masks and the torso maps are used to refine a skin map. A person or people map is determined using the skin map and the rough body map.","['G06V40/162', 'G06K9/00234', 'G06T7/11', 'G06T7/0081', 'G06T2207/30196', 'G06T2207/30201']"
WO2022170844A1,"Video annotation method, apparatus and device, and computer readable storage medium","The present application relates to the technical field of computer vision and image processing, and provides a video annotation method, apparatus and device and a computer readable storage medium, capable of improving the annotation efficiency of video data to a certain extent and reduce labor costs. The method comprises: acquiring a video frame sequence about a working scene by means of an RGB-D image acquisition device, video frames in the video frame sequence comprising an object to be annotated; obtaining target device attitude parameters when the RGB-D image acquisition device acquires the video frames, and constructing a three-dimensional scene point cloud of the working scene according to the target device attitude parameters; setting a three-dimensional object model at the position where said object is located in the three-dimensional scene point cloud to obtain first object information of said object in the three-dimensional scene point cloud; and annotating, according to the first object information and the target device attitude parameters, second object information on said object comprised in the video frames.","['G06T7/207', 'G06N3/045', 'G06N3/08', 'G06T7/13', 'G06T7/73', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084']"
WO2019232866A1,"Human eye model training method, human eye recognition method, apparatus, device and medium","A human eye model training method, a human eye recognition method, an apparatus, a device and a medium. The human eye model training method comprises: acquiring a face image sample and marking the face image sample so as to obtain face image sample data, and extracting a feature vector of the face image sample, wherein the face image sample data comprises the face image sample and marking data (S10); dividing the face image sample data into training sample data and verification sample data (S20); using the training sample data to train a support vector machine classifier so as to obtain a critical plane of the support vector machine classifier (S30); calculating a vector distance of a feature vector of a verification sample in the verification sample data and the critical plane (S40); acquiring a preset true positive rate or a preset false positive rate, and acquiring a classification threshold according to the vector distance and the marking data corresponding to the verification sample (S50); acquiring a human eye determination model according to the classification threshold (S60). The described method may obtain a human eye determination model that is highly accurate in determining whether a human eye is occluded.","['G06V40/165', 'G06F18/214', 'G06F18/2411', 'G06V40/18']"
US11694334B2,Segmenting objects in vector graphics images,"In implementations of segmenting objects in vector graphics images, an object segmentation system can obtain points that identify an object in a vector graphics image, and determine a region of interest in the image that includes the object based on the points that identify the object. The object segmentation system can generate a heat map from the points that identify the object in the image, and a rasterized region from rasterizing the region of interest. The object segmentation system can generate a mask from the rasterized region and the heat map, the mask identifying pixels of the object in the rasterized region, and determine, from the mask, paths of the vector graphics corresponding to the object.","['G06T7/11', 'G06T11/206', 'G06T11/60', 'G06T7/143', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20101']"
US20210398335A1,"Face editing method, electronic device and readable storage medium thereof","A face editing method, an electronic device and a readable storage medium, which relate to the field of image processing and deep learning technologies, are disclosed. A face editing implementation in the present disclosure includes: acquiring a face image in an image to be processed; converting an attribute of the face image according to an editing attribute to generate an attribute image; segmenting semantically the attribute image, and then processing a semantic segmentation image according to the editing attribute to generate a mask image; and merging the attribute image with the image to be processed using the mask image to generate a result image.","['G06T11/60', 'G06T5/50', 'G06T3/4053', 'G06T7/11', 'G06T7/30', 'G06T2207/10004', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30201']"
CN103984953B,Semantic segmentation method based on multiple features fusion Yu the street view image of Boosting decision forests,"A kind of semantic segmentation method based on multiple features fusion Yu the street view image of Boosting decision forests, comprises the following steps：Step 1, super-pixel segmentation is carried out to image；Step 2, multi-feature extraction；Step 3, Fusion Features；Step 4, training study and Classification and Identification；Effectively be merged for 2D features and 3D features by the present invention, significantly improves the discrimination of target, compared with prior art, segmentation result is consistent, connective good, edge accurate positioning, introduce Boosting decision forest classification mechanisms, it is ensured that the stability of target classification.",[]
US10936862B2,System and method of character recognition using fully convolutional neural networks,"Embodiments of the present disclosure include a method for extracting symbols from a digitized object. The method includes processing the word block against a dictionary. The method includes comparing the word block against a word in the dictionary, the comparison providing a confidence factor. The method includes outputting a prediction equal to the word when the confidence factor is greater than a predetermined threshold. The method includes evaluating properties of the word block when the confidence factor is less than the predetermined threshold. The method includes predicting a value of the word block based on the properties of the word block. The method further includes determining an error rate for the predicted value of the word block. The method includes outputting a value for the word block, the output equal to a calculated value corresponding to a value of the word block having the lowest error rate.","['G06N3/08', 'G06K9/00409', 'G06F18/2414', 'G06F18/28', 'G06K9/00422', 'G06K9/00463', 'G06K9/6255', 'G06K9/6273', 'G06K9/72', 'G06N3/044', 'G06N3/0442', 'G06N3/0445', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/047', 'G06N3/0472', 'G06N3/09', 'G06V10/82', 'G06V30/19173', 'G06V30/262', 'G06V30/333', 'G06V30/36', 'G06V30/414', 'G06K2209/01', 'G06V30/10']"
CN108010021B,Medical image processing system and method,"The embodiment of the invention discloses a medical image processing system and a medical image processing method. The system comprises: one or more processors; memory for storing one or more programs that, when executed by the one or more processors, cause the one or more processors to implement: acquiring a medical image; inputting the medical image into a first artificial intelligent network to obtain a first classification probability map of the medical image, wherein each point in the first classification probability map corresponds to a pixel point of the medical image, and the first artificial intelligent network is obtained in advance according to a medical image sample and a corresponding target pixel point; and determining target pixel points in the medical image according to the first classification probability map, wherein the set of the target pixel points is a target object. The embodiment of the invention improves the detection rate of the target pixel point and ensures the accuracy of target object detection.","['G06T7/0012', 'G06F18/241', 'G06T2207/10081', 'G06T2207/20032', 'G06T2207/20084', 'G06T2207/30061']"
EP1859411B1,Tracking objects in a video sequence,"A video surveillance system ( 10 ) comprises a camera ( 25 ), a personal computer (PC) ( 27 ) and a video monitor ( 29 ). Video processing software is provided on the hard disk drive of the PC ( 27 ). The software is arranged to perform a number of processing operations on video data received from the camera, the video data representing individual frames of captured video. In particular, the software is arranged to identify one or more foreground blobs in a current frame, to match the or each blob with an object identified in one or more previous frames, and to track the motion of the or each object as more frames are received. In order to maintain the identity of objects during an occlusion event, an appearance model is generated for blobs that are close to one another in terms of image position. Once occlusion takes place, the respective appearance models are used to segment the resulting group blob into regions which are classified as representing one or other of the merged objects.","['G06T7/277', 'G06T7/194', 'G06T7/215', 'G06T7/251', 'G06T2207/10016']"
US12244970B2,Method and system for providing at least one image captured by a scene camera of a vehicle,"The present disclosure relates to a method of providing at least one image of at least one real object captured by at least one scene camera of a plurality of scene cameras mounted on a vehicle. The method includes: providing camera poses of respective scene cameras of the plurality of scene cameras relative to a reference coordinate system associated with the vehicle, providing user attention data related to a user captured by an information capturing device, providing at least one attention direction relative to the reference coordinate system from the user attention data, determining at least one of the scene cameras among the plurality of scene cameras according to the at least one attention direction and the respective camera pose of the at least one of the scene cameras, and providing at least one image of at least one real object captured by the at least one of the scene cameras.","['H04N7/181', 'B60R11/04', 'G06F3/013', 'G06V40/19', 'B60R2300/105', 'G06V20/56']"
CN112686304B,Target detection method and device based on attention mechanism and multi-scale feature fusion and storage medium,"The invention relates to a target detection method, equipment and a storage medium based on an attention mechanism and multi-scale feature fusion, and the method, the equipment and the storage medium refer to the following steps: inputting an image to be detected into a target detection model to obtain the category of a target in the image, the coordinates of the center of an enclosing frame and the length and width of the enclosing frame; the generation process of the target detection model comprises the following steps: and training the sample image marked with the category of the target, the coordinates of the center of the surrounding frame and the length and the width of the surrounding frame to obtain a target detection model. The invention provides a combination FPN and Scale Fusion (SF) module to improve the problem of insufficient Fusion of multiple features of an FPN network structure. It is proposed that a lightweight non-localization is applied to the head network classification part of the detector for improving the accuracy of pixel level classification.",[]
US8107726B2,System and method for class-specific object segmentation of image data,"Systems and methods for processing an image to determine whether segments of the image belong to an object class are disclosed. In one embodiment, the method comprises receiving digitized data representing an image, the image data comprising a plurality of pixels, segmenting the pixel data into segments at a plurality of scale levels, determining feature vectors of the segments at the plurality of scale levels, the feature vectors comprising one or more measures of visual perception of the segments, determining one or more similarities, each similarity determined by comparing two or more feature vectors, determining, for each of a first subset of the segments, a first measure of probability that the segments is a member of an object class, determining probability factors based on the determined first measures of probability and similarity factors based on the determined similarities, and performing factor graph analysis to determine a second measure of probability for each of a second subset of the segments based on the probability factors and similarity factors.","['G06T7/33', 'G06T7/11', 'G06F18/214', 'G06F18/295', 'G06T7/162', 'G06T7/73', 'G06V30/19147', 'G06V30/19187', 'G06V30/2504', 'H04N23/00', 'G06T2207/10016', 'G06T2207/20016']"
CN109961049B,Cigarette brand identification method under complex scene,"The invention discloses a cigarette brand identification method in a complex scene, which comprises the steps of carrying out gray processing on an original color image, eliminating noise interference by combining image filtering, roughly positioning the edge of a preprocessed image by utilizing an improved Sobel edge operator, carrying out thinning processing such as mathematical morphology operation and the like to obtain a block-shaped communicated candidate region of a binary image, and sending the candidate region into a fast RCNN model of a deep learning neural network for accurate positioning and identification. According to the method, the candidate area is intercepted through edge detection, so that the interference of the background on the detection performance is reduced, and meanwhile, the improved Sobel operator is combined with the characteristics of the shelf cigarette picture to emphatically detect the edge in the vertical direction; according to the invention, the fast RCNN detection model modifies the size and proportion of the anchor frame in the regional suggestion network according to the size and shape characteristics of the cigarette, so that the missing detection probability of small targets is reduced, and the detection and identification accuracy is improved.","['G06T7/11', 'G06T7/13', 'G06V10/30', 'G06V10/454', 'G06V20/10']"
US10387724B2,Iris recognition via plenoptic imaging,"Iris recognition can be accomplished for a wide variety of eye images by using plenoptic imaging. Using plenoptic technology, it is possible to correct focus after image acquisition. One example technology reconstructs images having different focus depths and stitches them together, resulting in a fully focused image, even in an off-angle gaze scenario. Another example technology determines three-dimensional data for an eye and incorporates it into an eye model used for iris recognition processing. Another example technology detects contact lenses. Application of the technologies can result in improved iris recognition under a wide variety of scenarios.","['G06T5/73', 'G06K9/0061', 'G06K9/00604', 'G06T5/003', 'G06T5/50', 'G06V40/19', 'G06V40/193', 'G06T2200/21', 'G06T2207/10052', 'G06T2207/30041']"
CN111709420B,"Text detection method, electronic device and computer readable medium","The embodiment of the invention discloses a text detection method, electronic equipment and a computer readable medium, wherein the text detection method comprises the following steps: performing feature extraction and image segmentation on a text image to be detected, and at least acquiring a text region probability map of the text image to be detected and image features of the text image to be detected; carrying out binarization on the text region probability map to obtain a text region binary map; acquiring at least one text connected domain according to the text area binary image and the image characteristics; acquiring a text region approximate boundary of at least one text connected domain; and obtaining a text detection result of the text image to be detected according to the approximate boundary of the text region and a preset boundary threshold value. By the embodiment of the invention, the speed and the efficiency of text detection, particularly intensive text detection, are improved.","['G06V20/62', 'G06F18/23', 'G06F18/253', 'G06V30/153']"
WO2022088620A1,"State detection method and apparatus for camera lens, device and storage medium","A state detection method and apparatus for a camera lens, a device and a storage medium. The method comprises: performing abnormality detection on a target image captured by a camera so as to obtain a final abnormal region in the target image (S11); and analyzing the final abnormal region to determine whether the camera lens is in an abnormal state (S12). According to the method, the detection difficulty of the camera lens can be reduced, and the detection costs are reduced.","['G06T7/0004', 'H04N17/002', 'G01M11/0242', 'G03B43/00', 'G06T7/0002', 'H04N23/55', 'G06T2207/20036', 'G06T2207/20048']"
US11854208B2,Systems and methods for trainable deep active contours for image segmentation,"Systems and methods for image segmentation using neural networks and active contour methods in accordance with embodiments of the invention are illustrated. One embodiment includes a method for generating image segmentations from an input image. The method includes steps for receiving an input image, identifying a set of one or more parameter maps from the input image, identifying an initialization map from the input image, and generating an image segmentation based on the set of parameter maps and the initialization map.","['G06T7/149', 'G06N3/04', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06T7/11', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20116']"
US9659213B2,System and method for efficient recognition of handwritten characters in documents,"A system and computer-implemented method for efficient recognition of one or more handwritten characters in one or more documents is provided. The system comprises an image input module configured to receive the one or more documents as one or more images. The system further comprises a snippet extraction module to extract one or more fields of information from the one or more received images. Furthermore, the system comprises a segmentation module to segment the one or more extracted fields of information into one or more segments. Also, the system comprises a feature extraction module configured to extract one or more handwritten characters from each of the one or more segments. In addition, the system comprises a character recognition module configured to recognize the one or more extracted handwritten characters. The system further comprises a language processing module configured to detect and correct errors in the one or more recognized characters.","['G06F40/174', 'G06K9/00422', 'G06F17/2705', 'G06F17/273', 'G06F40/205', 'G06F40/232', 'G06K9/00409', 'G06K9/4642', 'G06T7/0081', 'G06T7/11', 'G06V10/50', 'G06V30/36', 'G06K2009/4666', 'G06T2207/20021', 'G06T2207/30176', 'G06V30/333']"
US9411417B2,Eye gaze tracking system and method,"An eye gaze tracking system is disclosed. The system includes a gaze data acquisition system including a plurality of light sources and a plurality of image sensors. The plurality of light sources are arranged to emit light to a head of the user, and the plurality of image sensors are configured to receive the light. In an embodiment, the system further includes a gaze tracking module including an ocular feature extraction module, a point of regard (POR) calculation module and a POR averaging module. The ocular feature extraction module is configured to process the gaze data and to extract ocular features, and is configured to determine a confidence value associated with an accuracy of the parameters. The POR calculation module is configured to determine a POR from the ocular features. The POR averaging module is configured to determine an average POR.","['G06F3/013', 'G06T7/004', 'G06T7/70', 'G06V40/193', 'G06T2207/10016', 'G06T2207/30041', 'G06T2207/30196']"
TWI808386B,"Image recognition method, device, terminal and storage medium","The embodiments of the application disclose an image recognition method, device, terminal and storage medium. The embodiments of the application can obtain a to-be-recognized image, which is an image of a to-be-recognized identification card; perform text area recognition on the to-be-recognized image to correspondingly obtain a text area image of target text of the to-be-recognized identification card; determine a text direction of the target text based on the text area image; adjust a direction of the text area image based on the text direction to obtain an adjusted text area image; and perform text recognition on the adjusted text area image to obtain text content of the target text. For text recognition, this application can automatically correct a skewed or upside-down text area image based on the text direction of the target text, thereby improving accuracy of text recognition and efficiency of image recognition.","['G06V20/63', 'G06V10/22', 'G06F18/24', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0464', 'G06N3/0495', 'G06N3/08', 'G06N3/09', 'G06V10/267', 'G06V10/82', 'G06V30/1478', 'G06V30/148', 'G06V30/18', 'G06V30/19127']"
US10157309B2,Online detection and classification of dynamic gestures with recurrent convolutional neural networks,"A method, computer readable medium, and system are disclosed for detecting and classifying hand gestures. The method includes the steps of receiving an unsegmented stream of data associated with a hand gesture, extracting spatio-temporal features from the unsegmented stream by a three-dimensional convolutional neural network (3DCNN), and producing a class label for the hand gesture based on the spatio-temporal features.","['G06N3/084', 'G06K9/00355', 'G06F18/2137', 'G06F18/214', 'G06F18/2413', 'G06F18/2415', 'G06K9/6251', 'G06K9/6256', 'G06K9/6277', 'G06N3/044', 'G06N3/0442', 'G06N3/0445', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/0895', 'G06N3/09', 'G06V10/454', 'G06V10/56', 'G06V10/764', 'G06V10/82', 'G06V20/49', 'G06V20/64', 'G06V40/28']"
CN109284059B,"Handwriting drawing method and device, interactive intelligent panel and storage medium","The invention relates to a handwriting drawing method and device, an interactive intelligent panel and a storage medium, which are applied to the interactive intelligent panel. The method comprises the following steps: acquiring a touch point; the touch points are points triggered by handwriting writing events on the interactive intelligent flat touch screen; calculating the width of the touch point and a skeleton line of the handwriting corresponding to the touch point; determining the outline of the handwriting; the contour line is obtained by calculation according to the width of the touch point; acquiring an inflection point in a contour line, and generating a Bezier curve corresponding to the inflection point as the smoothed contour line; the trend of the Bezier curve is determined by the trend of the skeleton line; and drawing the handwriting according to the smoothed contour line. The embodiment of the invention solves the problem that the edge of the drawn handwriting is relatively hard, and improves the handwriting writing effect based on the interactive intelligent panel.","['G06F3/04883', 'G06V30/32']"
CN117274216B,Ultrasonic carotid plaque detection method and system based on level set segmentation,"The invention relates to a carotid plaque detection method and system under ultrasound based on level set segmentation. It comprises the following steps: performing energy minimization operation on the level set energy functional to determine a level set function based on the level set energy functional with the minimum energy, and performing binary segmentation on the carotid artery ROI image by using the determined level set function to obtain a level set segmented binary ROI image; performing morphological processing on the obtained level set segmentation binary ROI image to obtain a plurality of candidate connected areas after the morphological processing; and in the carotid ROI image, calculating the sub-region gray average value of the sub-region positively corresponding to each candidate communication region, and reserving the sub-region with the sub-region gray average value not smaller than the gray threshold value otsu_threshold, wherein the reserved sub-region is the position in the carotid ROI image where the carotid plaque is located. The invention can effectively realize the segmentation and detection of the ultrasonic carotid plaque and improve the accuracy and reliability of carotid plaque detection.","['G06T7/0012', 'G06T7/136', 'G06T7/187', 'G06T7/90', 'G06T2207/10132', 'G06T2207/30101', 'Y02A90/10']"
US12223645B2,"Method and device for identifying abnormal cell in to-be-detected sample, and storage medium","Methods, apparatus, device, and storage medium for identifying an abnormal cell in a to-be-detected sample are disclosed. The method includes obtaining, by a device, multi-layer images of a to-be-detected sample, the to-be-detected sample comprising a single cell and a cell cluster; obtaining, by the device, multi-layer image blocks of the single cell and multi-layer image blocks of the cell cluster according to the multi-layer images; obtaining, by the device, a first identification result by a first image identification network according to the multi-layer image blocks of the single cell; obtaining, by the device, a second identification result by a second image identification network according to the multi-layer image blocks of the cell cluster; and determining, by the device, whether an abnormal cell exists in the to-be-detected sample according to the first identification result and the second identification result.","['G06T7/0012', 'G06N3/045', 'G06T7/11', 'G06V10/809', 'G06V20/698', 'G06T2207/10012', 'G06T2207/10056', 'G06T2207/30024', 'G06T2207/30096', 'G06T2207/30242']"
US11276177B1,Segmentation for image effects,"Systems, methods, and computer-readable media are provided for foreground image segmentation. In some examples, a method can include obtaining a first image of a target and a second image of the target, the first image having a first field-of-view (FOV) and the second image having a second FOV; determining, based on the first image, a first segmentation map that identifies a first estimated foreground region in the first image; determining, based on the second image, a second segmentation map that identifies a second estimated foreground region in the second image; generating a third segmentation map based on the first segmentation map and the second segmentation map; and generating, using the second segmentation map and the third segmentation map, a refined segmentation mask that identifies the target as a foreground region of the first and/or second image.","['G06T7/11', 'G06T7/174', 'G06T3/40', 'G06T5/002', 'G06T5/50', 'G06T5/70', 'G06T7/194', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/10048', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20092', 'G06T2207/20132', 'G06T2207/20221', 'H04N23/90', 'H04N5/247']"
CN106910242B,Method and system for 3D reconstruction of indoor complete scene based on depth camera,"The invention discloses a method and a system for performing indoor complete scene three-dimensional reconstruction based on a consumption-level depth camera. The method comprises the steps of obtaining a depth image and carrying out self-adaptive bilateral filtering; carrying out visual odometer estimation by utilizing the filtered depth image, automatically segmenting an image sequence based on visual contents, carrying out closed-loop detection between segments, and carrying out global optimization; and performing weighted volume data fusion according to the optimized camera track information so as to reconstruct an indoor complete scene three-dimensional model. The embodiment of the invention realizes the edge protection and the noise removal of the depth map through the self-adaptive bilateral filtering algorithm, can effectively reduce the accumulated error in the estimation of the visual odometer and improve the registration precision based on the automatic segmentation algorithm of the visual content, and can effectively keep the geometric details of the surface of the object by adopting the weighted volume data fusion algorithm. Therefore, the technical problem of how to improve the three-dimensional reconstruction precision in the indoor scene is solved, and a complete, accurate and refined indoor scene model can be obtained.","['G06T17/00', 'G06T5/50', 'G06T5/70', 'G06T2207/20028', 'G06T2207/20221', 'G06T2207/30244']"
US6128003A,Hand gesture recognition system and method,"Noise problems in processing small images or large-granularity images are reduced by representing hand images as rotational vectors calculated using a real-valued centroid. The hand region is therefore sectored independently of pixel quantization. Color segmentation is used to identify hand-color regions, followed by region labelling to filter out noise regions based on region size. Principal component analysis is used to plot gesture models.","['G06V40/28', 'G06V10/507']"
CN112052839B,"Image data processing method, apparatus, device and medium","The embodiment of the application provides an image data processing method, an image data processing device and an image data processing medium, wherein the method relates to a target detection technology in artificial intelligence, and comprises the following steps: acquiring object contour characteristics corresponding to a source image, and generating a contour mask image containing edge pixel points according to the object contour characteristics, wherein the source image comprises a target object, and the contour formed by the edge pixel points in the contour mask image is associated with the contour of the target object; carrying out transformation processing on edge pixel points contained in the contour mask image to obtain M discontinuous initial line segments corresponding to the edge pixel points; the contour formed by the M initial line segments is associated with the contour of the target object; and determining a target vertex associated with the target object according to the line segment intersection points among the M initial line segments, and determining an object edge shape for representing the outline of the target object in the source image according to the target vertex. By adopting the embodiment of the application, the detection accuracy of the target object in the image can be improved.","['G06V30/412', 'G06F18/253', 'G06N3/045', 'G06N3/084', 'G06V10/22', 'G06V10/25', 'G06V10/44', 'G06V2201/07']"
US12340538B2,Systems and methods for generating and using visual datasets for training computer vision models,"A system for collecting data for training a computer vision model for shape estimation includes: an imaging system configured to capture one or more images; and a processing system including a processor and memory storing instructions that, when executed by the processor, cause the processor to: receive one or more input images from the imaging system; estimate a pose of an object depicted in the one or more images; render a shape estimate from a 3-D model of the object posed in accordance with the pose of the object; and generate a data point of a training dataset, the data point including one or more images based on the one or more input images and a label corresponding to the one or more images, the label including the shape estimate.","['G06T7/75', 'G06V10/24', 'G02B27/288', 'G06N3/08', 'G06T17/00', 'G06T7/344', 'G06T7/50', 'G06V10/145', 'G06V10/26', 'G06V10/761', 'G06V10/774', 'G06V10/82', 'G06V20/653', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/30108', 'G06V2201/06', 'G06V2201/12']"
US9939893B2,Eye gaze tracking,"Methods and systems for eye gaze tracking are provided, in which a plurality of images of at least one eye are received and, for each image of the eye(s), a plurality of stable vascular points associated with a vasculature of the eye(s) are identified. Based on the identified stable vascular points in the received images of the eye(s), a gaze angle of the eye(s) is determined.","['G06F3/013', 'G06K9/00597', 'G06T7/246', 'G06V40/18', 'G06T2207/30041', 'G06T2207/30201']"
US10402680B2,Methods and apparatus for image salient object detection,"A method and an apparatus for extracting a saliency map are provided in the embodiment of the present application, the method includes: conducting first convolution processing, first pooling processing and normalization processing on an original image via a prediction model to obtain eye fixation information from the original image, where the eye fixation information is used for indicating a region at which human eye gaze; conducting second convolution processing and second pooling processing on the original image via the prediction model to obtain semantic description information from the original image; fusing the eye fixation information and the semantic description information via element-wise summation function; and conducting detection processing on the fused eye fixation information and semantic description information via the prediction model to obtain a saliency map from the original image. It is used for improving the efficiency of extracting the saliency map from image.","['G06K9/4671', 'G06V10/56', 'G06T7/143', 'G06F18/24143', 'G06K9/00597', 'G06K9/42', 'G06K9/4685', 'G06K9/52', 'G06T3/4007', 'G06T7/11', 'G06T7/194', 'G06V10/454', 'G06V10/462', 'G06V10/464', 'G06V10/764', 'G06V10/82', 'G06V40/18', 'G06V40/193', 'G06T2207/10024', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104']"
EP1550082B1,Three dimensional face recognition,"A method of cropping a representation of a face for electronic processing, said method comprising: selecting a first geodesic contour about an invariant reference point on said face, setting a region within said first geodesic contour as a first mask, selecting a second geodesic contour about a boundary of said identified first region, setting a region within said second geodesic contour as a second mask, and forming a final mask from a union of said first mask and said second mask.","['G06V40/172', 'G01S17/89', 'G06T17/00', 'G06V20/653']"
CN111650220B,Vision-based image-text defect detection method,"The invention discloses a vision-based image-text defect detection method, and relates to the field of computer vision identification. The method adopts a color high-definition industrial camera and a lens to photograph prints within the size, firstly, a standard template is made on qualified products, characteristic information of the template is extracted, and then, the prints placed in random postures are photographed and compared; in the image comparison process, the algorithm automatically performs high-precision multi-scale registration correction on the sample image and the template image, so that the complex work that a jig needs to perform high-precision placement or a manual correction on the rotation angle and the like for a single printed product at present is omitted, the deformation generated by samples in different placement postures can be automatically corrected, and finally, the small image-text printing defect is identified. The method not only can process characters and symbols, but also can process various printing contents containing images, characters and tables, can better detect various defects, and meets the high-standard quality detection requirements of the fields of 3C, food, medicines and the like on printed matters.",['G01N21/95607']
EP4303820A1,Neural network based identification of areas of interest in digital pathology images,"A CNN is applied to a histological image to identify areas of interest. The CNN classifies pixels according to relevance classes including one or more classes indicating levels of interest and at least one class indicating lack of interest. The CNN is trained on a training data set including data which has recorded how pathologists have interacted with visualizations of histological images. In the trained CNN, the interest-based pixel classification is used to generate a segmentation mask that defines areas of interest. The mask can be used to indicate where in an image clinically relevant features may be located. Further, it can be used to guide variable data compression of the histological image. Moreover, it can be used to control loading of image data in either a client-server model or within a memory cache policy. Furthermore, a histological image of a tissue sample of a tissue type that has been treated with a test compound is image processed in order to detect areas where toxic reactions to the test compound may have occurred. An autoencoder is trained with a training data set comprising histological images of tissue samples which are of the given tissue type, but which have not been treated with the test compound. The trained autoencoder is applied to detect tissue areas by their deviation from the normal variation seen in that tissue type as learnt by the training process, and so build up a toxicity map of the image. The toxicity map can then be used to direct a toxicological pathologist to examine the areas identified by the autoencoder as lying outside the normal range of heterogeneity for the tissue type. This makes the pathologist's review quicker and more reliable. The toxicity map can also be overlayed with the segmentation mask indicating areas of interest. When an area of interest and an area identified as lying outside the normal range of heterogeneity for the tissue type, and increased confidence score is applied to the overlapping area.","['G06T7/0012', 'G06V10/454', 'G06V20/695', 'G06V10/26', 'G06F18/2415', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06V10/764', 'G06V10/82', 'G06V10/945', 'G16H80/00', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30096', 'G06V2201/03', 'G06V2201/10']"
CN110930416B,A U-shaped network-based MRI image prostate segmentation method,"A prostate segmentation method for an MRI image based on a U-shaped network comprises the following steps: step 1, selecting S from MRI image data set1The images form a training set and S is selected2Forming a test set by the MRI images which do not participate in training, wherein the test set and the test set both comprise manual labeling results of corresponding prostate areas; step 2, preprocessing all MRI images in the training set and the testing set respectively to obtain a first image and a second image; step 3, initializing parameters in the U-shaped network, sequentially inputting all the first images into the U-shaped network for training, and continuously updating the parameters in the U-shaped network to obtain the trained U-shaped network; and 4, inputting the second image into the trained U-shaped network to obtain a segmentation map of the prostate area of each MRI image. Has the advantages that: the feature extraction work is more efficient, the segmentation result at the edge and the detail is more accurate, the network model convergence speed is accelerated, and the overall segmentation precision is improved.","['G06T7/11', 'G06N3/045', 'G06N3/082', 'G06N3/084', 'G06T5/40', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20192', 'G06T2207/30004']"
WO2021164322A1,"Artificial intelligence-based object classification method and apparatus, and medical imaging device","An artificial intelligence-based object classification method and apparatus, a computer readable storage medium, and a computer device. The method comprises: obtaining an image to be processed, wherein said image comprises a target detection object (S202); segmenting a target detection object image of the target detection object from said image (S204); inputting the target detection object image into a feature object prediction model to obtain a feature object segmentation image of a feature object in the target detection object image (S206); obtaining quantitative feature information of the target detection object according to the target detection object image and the feature object segmentation image (S208); and according to the quantitative feature information, classifying the target detection object image to obtain category information of the target detection object in said image (S210).","['G06T7/0012', 'G06V10/273', 'G06V10/764', 'G06F18/241', 'G06T3/4038', 'G06T7/11', 'G06V10/26', 'G06V10/454', 'G06V10/82', 'G06N3/045', 'G06N3/08', 'G06T2207/10056', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024', 'G06T2207/30068', 'G06V2201/032']"
US8929602B2,Component based correspondence matching for reconstructing cables,"In a stereoscopic pair of images, global homography at the image level is applied to feature points extracted from connected components (CC) to identify corresponding CC's and feature points, and to discard any CC's that do not have a corresponding pair in the stereoscopic pair of images. Local homography at the CC level is then applied to individual footprint areas of the previously identified paired CC to further clean feature point correspondence. Any CC or feature point or pixel within a paired CC footprint not satisfying local homography constraint is discarded. A correspondence is also extrapolated between unknown pixels within a paired CC footprint using a weighing mechanism and the unknown pixel's surrounding pixels that do have a known correspondence. This provides a dense correspondence of pixels, or feature points, which is then used to create a dense 3D point cloud of identified objects within a 3D space.","['G06K9/6211', 'G06T7/73', 'G06T7/004', 'G06T7/70', 'G06V10/757', 'G06T2207/10012']"
US7756335B2,Handwriting recognition using a graph of segmentation candidates and dictionary search,"A method for determining at least one recognition candidate for a handwritten pattern comprises selecting possible segmentation points in the handwritten pattern for use in segmenting and recognizing the handwritten pattern. The method further may comprise comparing segments of the handwritten pattern to templates. The comparison may return segment candidates forming possible recognition results of the segments of the handwritten pattern. The method further comprises forming a representation of sequences of segment candidates, said representation comprising data blocks corresponding to segmentation points, wherein a data block comprises references to data blocks corresponding to subsequent segmentation points. The reference may comprise information of segment candidates. The method further may comprise comparing the representation of the sequences of segment candidates to a dictionary, finding sequences of segment candidates that correspond to allowed sequences of symbols in the dictionary, and returning at least one of these allowed sequences of symbols as a recognition candidate for the handwritten pattern.","['G06V30/347', 'G06V30/2272']"
CN111915609B,"Focus detection analysis method, apparatus, electronic device and computer storage medium","The invention relates to artificial intelligence technology, and discloses a focus detection and analysis method, which comprises the following steps: acquiring CT slice data, and carrying out normalization operation on the CT slice data to obtain a standard data set; selecting one slice of the standard data set, calculating a slice step length according to the standard data set, and obtaining an input slice set before selecting the first n Zhang Qiepian of the slice data and the second m Zhang Qiepian of the slice; inputting the input slice set into a focus segmentation model for cavity convolution, extracting characteristic data, and determining a focus area according to the characteristic data; and carrying out density analysis on the focus area, and feeding back the result of the density analysis to a user. The invention also provides a focus detection analysis device, equipment and a computer readable storage medium. Furthermore, the present invention also relates to blockchain techniques, the CT slice data may be stored in blockchain nodes. The invention can improve the accuracy of focus segmentation and can analyze focuses in multiple aspects.","['G06T7/0012', 'G06N3/045', 'G06N3/084', 'G06T7/11', 'G06T2207/10072', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30096', 'Y02A90/10']"
CN108895981B,"Three-dimensional measurement method, device, server and storage medium","The embodiment of the invention discloses a three-dimensional measurement method, a three-dimensional measurement device, a server and a storage medium. The method comprises the following steps: performing depth estimation according to a video of a target scene to obtain a depth image and a three-dimensional point cloud of the target scene; taking the depth image and the color image of the target scene as the input of a semantic segmentation model to obtain a semantic segmentation result of the target scene; and extracting a target object from the three-dimensional point cloud according to the semantic segmentation result, and measuring the target object. The technical scheme provided by the embodiment of the invention can improve the measurement precision and reduce the calculation amount.","['G01B11/24', 'G01B11/002']"
CN109785316B,A method for detecting apparent defects on a chip,"A chip apparent defect detection method comprises shooting an SOP chip image by a color CCD camera, extracting the outline and the centroid of a chip circular mark and a pin through a series of pre-treatments, calculating the improved environment characteristic vector of each centroid of the circular mark and the pin, matching and positioning with a template image, calculating an affine transformation matrix, affine transforming the image into a template image coordinate system, and finally, judging whether the pins are lack of the improved environment vectors of the circular marks, extracting printing pixels and edges of the printing areas of the pins to judge whether printing information is defective, calculating minimum circumscribed rectangles of the outlines of the pins to judge whether the pins are upwarped, downwarped and inclined, and extracting oxidation and desoldering pixels of the pin areas to judge whether the pins are oxidized and desoldered. The method can automatically, quickly, conveniently and accurately judge the problems of pin loss, upwarp, downwarp, skew, desoldering and oxidation of the SOP chip, can also judge whether a printed information area is clear and complete, can effectively detect the appearance of the SOP chip product and reduce the labor intensity of workers.",[]
US11783496B2,Scalable real-time hand tracking,"Example aspects of the present disclosure are directed to computing systems and methods for hand tracking using a machine-learned system for palm detection and key-point localization of hand landmarks. In particular, example aspects of the present disclosure are directed to a multi-model hand tracking system that performs both palm detection and hand landmark detection. Given a sequence of image frames, for example, the hand tracking system can detect one or more palms depicted in each image frame. For each palm detected within an image frame, the machine-learned system can determine a plurality of hand landmark positions of a hand associated with the palm. The system can perform key-point localization to determine precise three-dimensional coordinates for the hand landmark positions. In this manner, the machine-learned system can accurately track a hand depicted in the sequence of images using the precise three-dimensional coordinates for the hand landmark positions.","['G06T7/251', 'G06T7/75', 'G06V40/107', 'G06V40/28', 'G06T2207/20081', 'G06T2207/30196']"
US10346035B2,Managing real-time handwriting recognition,"Methods, systems, and computer-readable media related to a technique for providing handwriting input functionality on a user device. A handwriting recognition module is trained to have a repertoire comprising multiple non-overlapping scripts and capable of recognizing tens of thousands of characters using a single handwriting recognition model. The handwriting input module provides real-time, stroke-order and stroke-direction independent handwriting recognition for multi-character handwriting input. In particular, real-time, stroke-order and stroke-direction independent handwriting recognition is provided for multi-character, or sentence level Chinese handwriting recognition. User interfaces for providing the handwriting input functionality are also disclosed.","['G06F3/04883', 'G06F17/24', 'G06F17/276', 'G06F3/018', 'G06F3/0482', 'G06F40/166', 'G06F40/274', 'G06K9/00416', 'G06K9/00422', 'G06K9/00436', 'G06V30/347', 'G06V30/36', 'G06V30/387']"
CN114910480B,Wafer surface defect detection method based on machine vision,"A wafer surface defect detection method based on machine vision belongs to the field of defect detection in the wafer production and manufacturing process. The method comprises the steps of firstly, scanning the whole wafer by using an industrial linear array camera to obtain an original image of the whole wafer; secondly, extracting gray features of an original image of the wafer, and correcting the pose of the image of the wafer; carrying out multi-template matching by using the single-sample grain image and the complete wafer image again, and screening out the best matching coordinates by using a Kmeans and a nonlinear difference method to finish the segmentation of the grain sample; and finally, carrying out image enhancement on the segmented samples, carrying out traversal extraction on the outer contour features and the gray scale features in the contours on the enhanced images to respectively generate one-dimensional arrays, carrying out Kmeans and hierarchical clustering analysis, screening out discrete samples, marking the discrete samples as defect samples, generating a defect wafer map, and finishing wafer defect detection. The method can be applied to the wafer production and manufacturing process to replace the existing manual visual inspection mode.","['G01N21/8851', 'G01N21/9501', 'Y02P90/30']"
CN106875381B,Mobile phone shell defect detection method based on deep learning,"The invention relates to a mobile phone shell defect detection method based on deep learning, which comprises the following steps: (1) acquiring and preprocessing a mobile phone shell image to be detected; (2) inputting the preprocessed image into a defect detection model trained in advance to perform defect detection to obtain a position with a defect on the mobile phone shell, and giving a confidence coefficient that the position is the defect; the defect detection model is a deep network based on deep learning and comprises a feature extraction network, a classifier and a regressor network which are sequentially cascaded, the feature extraction network performs feature extraction on the preprocessed image to obtain a feature image, and the classifier and the regressor network perform classification regression on the feature image to obtain the defect position and the confidence coefficient of the mobile phone shell. Compared with the prior art, the invention has high detection precision and accurate and reliable detection result.","['G06T7/0004', 'G06F18/214', 'G06F18/2163', 'G06F18/24', 'G06T3/147', 'G06V10/44', 'G06V10/48']"
USRE47889E1,System and method for segmenting text lines in documents,Methods and systems of the present embodiment provide segmenting of connected components of markings found in document images. Segmenting includes detecting aligned text. From this detected material an aligned text mask is generated and used in processing of the images. The processing includes breaking connected components in the document images into smaller pieces or fragments by detecting and segregating the connected components and fragments thereof likely to belong to aligned text.,"['G06K9/00456', 'G06V30/155', 'G06V30/412', 'G06V30/413', 'G06K9/00442', 'G06K9/00449', 'G06K9/346', 'G06V30/10']"
US9224060B1,Object tracking using depth information,"Systems and approaches are provided for tracking an object of interest using depth or disparity information, such as obtained by calculating stereo disparity between a pair of images. The depth or disparity information can be used as an additional signature for a template of the object of interest for tracking the object. A template that includes depth, distance, or disparity information for an object of interest may be invariant to the effects of lighting, such as shadows and changes in illumination conditions. Depth, distance, or disparity information can also provide information regarding shape and size that can be used to differentiate foreground objects. Depth, distance, or disparity information can also better handle occlusion. Depth, distance, or disparity information can also provide an additional disambiguating dimension for tracking an object.","['G06K9/32', 'G06V40/161', 'G06V10/255', 'G06V40/113', 'G06V40/67', 'G06V10/62']"
CN107180239B,Text line identification method and system,"The invention discloses a text line identification method and a system, wherein the method comprises the following steps: receiving a text image to be recognized; acquiring a binary image of the text image to be recognized; obtaining a layout type image corresponding to the text image to be recognized by utilizing the binary image and a pre-constructed layout type recognition model; and obtaining text line information according to the upper and lower category structural relationship of the text lines in the layout category image. The invention can not only improve the accuracy of text line identification, but also has strong universality and can adapt to the change of various scenes.",['G06V30/158']
CN108958487B,Gesture pre-processing of video streams using tagged regions,"Techniques are disclosed for processing video streams to reduce platform power by employing stepped and distributed pipeline steps, where CPU-intensive processing is selectively performed. The techniques are particularly applicable to hand-based navigation gesture processing. In one exemplary instance, for example, the techniques are implemented in a computer system where an initial threshold detection (image disturbance) and optionally user presence (hand image) processing component is proximate to or within a camera of the system and the camera is located within or proximate to a main display of the system. In some instances, image processing and pixel information transfer between multiple processing stages outside of the marked area is suppressed. In some embodiments, the marker region is aligned with a mouse pad or designated desktop area or a user input device (e.g., keyboard). The pixels evaluated by the system may be limited to a subset of the marker region.","['G06F3/017', 'G06F3/0304', 'G06V10/96', 'G06V40/161', 'G06V40/162', 'G06V40/28']"
US10572725B1,Form image field extraction,"Field extraction from a form image includes identifying a target field of the form image, defining a patch from the form image based on the target field, and encoding the patch using a color encoding scheme to obtain an encoded patch. Field extraction further includes applying a trained classifier to the encoded patch to identify a relationship between a field value and a field identifier, and extracting the field value from the form image according to the relationship.","['G06K9/00449', 'G06V30/40', 'G06F18/2411', 'G06F18/2413', 'G06K9/00463', 'G06K9/4652', 'G06K9/6269', 'G06T3/40', 'G06V10/764', 'G06V10/82', 'G06V20/63', 'G06V30/148', 'G06V30/412', 'G06V30/414', 'G06K2209/015', 'G06V2201/01', 'G06V30/10']"
US11823392B2,Segmenting generic foreground objects in images and videos,"A method, system and computer program product for segmenting generic foreground objects in images and videos. For segmenting generic foreground objects in videos, an appearance stream of an image in a video frame is processed using a first deep neural network. Furthermore, a motion stream of an optical flow image in the video frame is processed using a second deep neural network. The appearance and motion streams are then joined to combine complementary appearance and motion information to perform segmentation of generic objects in the video frame. Generic foreground objects are segmented in images by training a convolutional deep neural network to estimate a likelihood that a pixel in an image belongs to a foreground object. After receiving the image, the likelihood that the pixel in the image is part of the foreground object as opposed to background is then determined using the trained convolutional deep neural network.","['G06T7/194', 'G06N3/045', 'G06N3/0464', 'G06N3/0895', 'G06N3/09', 'G06N3/096', 'G06T7/11', 'G06T7/143', 'G06T7/215', 'G06V10/82', 'G06V20/49', 'G06T2207/10016', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'H04N19/21', 'H04N19/25']"
US9135503B2,Fingertip tracking for touchless user interface,"In general, this disclosure describes techniques for providing a gesture-based user interface. For example, according to some aspects of the disclosure, a user interface generally includes a camera and a computing device that identifies and tracks the motion of one or more fingertips of a user. In some examples, the user interface is configured to identify predefined gestures (e.g., patterns of motion) associated with certain motions of the user's fingertips. In another example, the user interface is configured to identify hand postures (e.g., patterns of showing up of fingertips). Accordingly, the user can interact with the computing device by performing the gestures.","['G06V40/107', 'G06K9/00375', 'G06F3/017', 'G06K9/00006', 'G06K9/00355', 'G06T7/2046', 'G06T7/251', 'G06V40/28', 'G06T2207/30196']"
CN108154502B,Through hole welding spot identification method based on convolutional neural network,"The invention provides a through hole welding spot identification method based on a convolutional neural network, which comprises the following four steps: (1) performing segmentation operation based on the region pixel similarity on the input image; (2) carrying out histogram equalization on the segmented region picture; (3) serializing and normalizing the input area pictures; (4) inputting the equalized area image into multilayer convolution and pooling operation, and extracting convolution characteristics; and inputting the convolution characteristics into a multilayer full-connection layer to classify the through hole welding points, and outputting whether the area image is the through hole welding point or not. The method can realize the recognition of the through hole welding points in the PCB image, has the characteristics of high speed and high accuracy, and is helpful for helping automatic point picking of PCB welding equipment such as an automatic tin soldering machine and the like.","['G06T7/0004', 'G06N3/045', 'G06T5/40', 'G06T7/11']"
WO2021093432A1,"System, device and method for video frame interpolation using structured neural network","A system, device and method of video frame interpolation. In one aspect, a frame interpolation system receive candidate interpolated frame and difference data based on a first frame and second frame. It identifies a high-error region and one or more increasingly lower-error regions of the candidate interpolated frame based on the difference data and applies multiple successive stages of a refinement neural network to the regions of the candidate interpolated frame, from highest-to lowest-error. At each stage, the results of the previous stage are combined with the next lower-error region before being refined by the current stage. In some aspects, the system also generates the candidate interpolated frame and the difference data using a flow estimation neural network.","['G06N3/084', 'G06N3/045', 'G06N3/08', 'H04N11/02', 'H04N19/587', 'H04N7/0127', 'H04N7/014']"
US8913783B2,3-D model based method for detecting and classifying vehicles in aerial imagery,A computer implemented method for determining a vehicle type of a vehicle detected in an image is disclosed. An image having a detected vehicle is received. A number of vehicle models having salient feature points is projected on the detected vehicle. A first set of features derived from each of the salient feature locations of the vehicle models is compared to a second set of features derived from corresponding salient feature locations of the detected vehicle to form a set of positive match scores (p-scores) and a set of negative match scores (n-scores). The detected vehicle is classified as one of the vehicle models models based at least in part on the set of p-scores and the set of n-scores.,"['G06K9/00651', 'G06V20/182', 'G06F18/2411', 'G06F18/24323', 'G06K9/00785', 'G06K9/4671', 'G06T7/11', 'G06T7/20', 'G06V10/462', 'G06V20/54', 'B64U2101/30', 'G06T2207/10012']"
CN113947598B,"Plastic lunch box defect detection method, device and system based on image processing","The invention provides a method, a device and a system for detecting defects of a plastic meal box based on image processing, wherein the method comprises the following steps: acquiring a to-be-detected image of the plastic lunch box to be detected; sequentially carrying out denoising processing and gray level processing on an image to be detected to obtain a gray level image; determining a first probability that any pixel point in the gray level image belongs to a background area and a second probability that any pixel point belongs to a target area; calculating the background region entropy according to the first probability, and calculating the target region entropy according to the second probability; determining a threshold to be measured according to the background region entropy and the target region entropy; comparing the gray value of each pixel point in the gray image with a threshold value to be detected, and segmenting the gray image according to a comparison result to obtain a target area image; determining the similarity between the target area image and the standard plastic lunch box image, and comparing the similarity with a preset threshold value; and determining whether the plastic lunch box to be detected has defects according to the comparison result. The technical scheme of the invention improves the accuracy of the detection of the defects of the plastic lunch box.","['G06T7/0002', 'G06F18/22', 'G06F18/2415']"
US11763552B2,"Method for detecting surface defect, method for training model, apparatus, device, and media","A method for detecting a surface defect, a method for training model, an apparatus, a device, and a medium, are provided. The method includes: inputting a surface image of the article for detection into a defect detection model to perform a defect detection, and acquiring a defect detection result output by the defect detection model; inputting a surface image of a defective article determined to be defective into an image discrimination model based on the defect detection result to determine whether the surface image of the defective article is defective, wherein the image discrimination model is a trained generative adversarial networks model, and the generative adversarial networks model is obtained by training using a surface image of a defect-free good article; and adjusting the defect detection result of the surface image of the defective article according to a determination result of the image discrimination model.","['G06V10/82', 'G01N21/8851', 'G06F18/2148', 'G06N3/045', 'G06N3/0464', 'G06N3/0475', 'G06N3/08', 'G06N3/088', 'G06N3/09', 'G06N3/094', 'G06T7/0004', 'G06V10/776', 'G06V20/60', 'G01N2021/8883', 'G01N2021/8887', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30108', 'G06T2207/30124', 'Y02P90/30']"
CN107239777A,A tableware detection and recognition method based on multi-view graph model,"The invention discloses a tableware detection and identification method based on a multi-view graph model, which combines tableware detection and identification into a unified frame by utilizing a learning frame of the multi-view graph model. The method comprises the steps of adopting a multi-view graph model to detect tableware of an image, constructing the graph model by utilizing the characteristics of super pixel points under multiple views, and then learning the confidence coefficient that each super pixel point is the position of the tableware, so that the tableware can be detected more accurately. And a multi-view fusion algorithm is adopted for feature fusion, so that more distinctive features are constructed, and the identification rate is improved. In the construction of the multi-view graph model, the exponential weight parameters are utilized to avoid the occurrence of zero weight coefficient of the multi-view, so that the characteristics of all the view angles can be mutually complemented.","['G06V10/255', 'G06F18/214', 'G06F18/2411']"
US7899253B2,Detecting moving objects in video by classifying on riemannian manifolds,"A method constructs a classifier from training data and detects moving objects in test data using the trained classifier. High-level features are generated from low-level features extracted from training data. The high level features are positive definite matrices on an analytical manifold. A subset of the high-level features is selected, and an intrinsic mean matrix is determined. Each high-level feature is mapped to a feature vector on a tangent space of the analytical manifold using the intrinsic mean matrix. An untrained classifier is trained with the feature vectors to obtain a trained classifier. Test high-level features are similarly generated from test low-level features. The test high-level features are classified using the trained classifier to detect moving objects in the test data.","['G06V10/50', 'G06F18/2148', 'G06V10/7747', 'G06V40/103']"
US11270158B2,"Instance segmentation methods and apparatuses, electronic devices, programs, and media","An instance segmentation method includes: performing feature extraction on an image via a neural network to output features at at least two different hierarchies; extracting region features corresponding to at least one instance candidate region in the image from the features at the at least two different hierarchies, and fusing region features corresponding to a same instance candidate region, to obtain a first fusion feature of each instance candidate region; and performing instance segmentation based on each first fusion feature, to obtain at least one of an instance segmentation result of the corresponding instance candidate region or an instance segmentation result of the image.","['G06T7/246', 'G06V10/25', 'G06K9/6232', 'G06V10/7715', 'G06F18/213', 'G06F18/231', 'G06F18/2413', 'G06F18/253', 'G06K9/3233', 'G06K9/46', 'G06K9/629', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/11', 'G06V10/40', 'G06V10/764', 'G06V10/806', 'G06V10/82', 'G06T2207/20084']"
US20230237771A1,"Self-supervised learning method and apparatus for image features, device, and storage medium","The present application provides a self-supervised learning method performed by a computer device. The method includes: performing a data enhancement on an original medical image to obtain a first enhanced image and a second enhanced image, the first enhanced image and the second enhanced image being positive samples of each other; performing feature extractions on the first enhanced image and the second enhanced image by a feature extraction model to obtain a first image feature of the first enhanced image and a second image feature of the second enhanced image; determining a model loss of the feature extraction model based on the first image feature, the second image feature, and a negative sample image feature, the negative sample image feature being an image feature corresponding to other original medical images; and training the feature extraction model based on the model loss.","['G06V10/7715', 'G06V10/82', 'G06F18/217', 'G06F18/214', 'G06F18/23213', 'G06F18/24', 'G06N3/084', 'G06T7/0012', 'G06V10/764', 'G06V10/771', 'G06V10/774', 'G06V20/698', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024', 'G06V10/761', 'G06V10/762', 'G06V2201/03', 'G06V2201/031']"
US11468693B2,Digital image classification method for cervical fluid-based cells based on a deep learning detection model,"The present invention relates to the field of medical technology, and more particularly, to a digital image classification method for cervical fluid-based cells based on a deep learning detection model. The method comprises the following steps: selecting and labeling positions and categories of abnormal cells or biological pathogens in a digital image of cervical liquid-based smears; performing data normalization processing on the digital image of the cervical liquid-based smears; performing model training to obtain a trained Faster-RCNN model by taking the normalized digital image of the cervical liquid-based smears as an input, and the labeled position and category of each abnormal cell or biological pathogen as an output; and inputting an image to be recognized into the trained model and outputting a classification result. The method provided by the embodiment of the present invention can achieve the following advantages: abnormal cells or biological pathogens in a cervical cytological image are positioned; the abnormal cells or biological pathogens in the cervical cytological image are classified; and slice-level diagnostic recommendations are derived by recognizing the positioned abnormal cells or biological pathogens.","['G06V10/245', 'G06V20/698', 'G06N3/045', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06T3/60', 'G06T7/0012', 'G06T7/11', 'G06T7/136', 'G06V10/44', 'G06V10/774', 'G06V10/82', 'G06V20/695', 'G16H10/40', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'G16H50/70', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024']"
CN106023185B,A kind of transmission facility method for diagnosing faults,"The invention discloses a kind of transmission facility method for diagnosing faults, comprising the following steps: A: establishing the normal spectrum library of power transmission network equipment, power transmission network equipment includes transmission pressure, fitting and insulator；B: acquisition power transmission network equipment operation image pre-processes collected power transmission network equipment operation image and carries out image characteristics extraction；C: pass through image comparison module, utilize the image in collected power transmission network equipment operation image and the normal spectrum library of power transmission network equipment, the variation and defect characteristic that power transmission network equipment is occurred in analysis power transmission network equipment operation image, with the map difference under equipment normal condition in the normal spectrum library of power transmission network equipment, fault identification diagnosis is carried out to power transmission network equipment.The present invention can analyze the method combined with defect characteristic pattern match by stay in place form, find transmission line of electricity potential faults that may be present in time, failure decision error is greatly reduced.","['G06T7/001', 'G06T2207/10032', 'G06T2207/20182', 'G06T2207/20201']"
US8233676B2,Real-time body segmentation system,"In a human feature recognition system that is intended to provide substantially real-time recognition of body segments, various methods and structures are provided to facilitate real-time recognition with reduced computation requirements, including a face detection module employing an active boosting procedure and a lazy boosting procedure on a hybrid cascade structure, a human body segmentation module and a boundary matting module. The hybrid cascade structure is in the form of a tree where one type of node represents a strong classifier learned from active boosting, another type of classifier is obtained by low-computation-load lazy boosting, and weak classifiers are obtained from the previous layers.","['G06V10/446', 'G06F18/2148', 'G06V10/7747', 'G06V40/103', 'G06V40/162', 'G06V40/165']"
US7898522B2,Video-based image control system,"A method of using stereo vision to interface with computer is provided. The method includes capturing a stereo image, and processing the stereo image to determine position information of an object in the stereo image. The object is controlled by a user. The method also includes communicating the position information to the computer to allow the user to interact with a computer application.","['G06F3/011', 'G06F3/017', 'G06T19/006', 'G06T7/593', 'G06T7/74', 'G06V40/107', 'H04N13/239', 'A63F2300/1093', 'A63F2300/69', 'A63F2300/8082', 'G06T2207/10012', 'G06T2207/10021', 'G06T2207/20021', 'G06T2207/30196', 'H04N13/246', 'H04N2013/0081']"
JP2019086979A,"INFORMATION PROCESSING APPARATUS, INFORMATION PROCESSING METHOD, AND PROGRAM","To provide an information processing device, an information processing method, and a program that learn a feature quantity extractor more appropriately and with less burden.SOLUTION: An information processing device acquires an image, in which an area included in an area set in an input image is changed to another image different from the input image for each area set included in a predetermined area set group, as a learning image used for learning of a feature quantity extractor that extracts a feature quantity from the image; and learns the feature quantity extractor, based on the learning image acquired for each area set included in the area set group and a predetermined evaluation function used for learning of the feature quantity extractor that extracts feature quantities that are close values as the input image is similar, where each of the area sets included in the area set group is an area set created in advance in the input image and is ordered by an inclusion relation.SELECTED DRAWING: Figure 3",[]
US10646999B2,Systems and methods for detecting grasp poses for handling target objects,"Systems and methods for detecting grasping poses for handling target objects is disclosed. The system solves problem of grasp pose detection and finding suitable graspable affordance for picking objects from a confined and cluttered space, such as the bins of a rack in a retail warehouse by creating multiple surface segments within bounding box obtained from a neural network based object recognition module. Surface patches are created using a region growing technique in depth space based on surface normals directions. A Gaussian Mixture Model based on color and depth curvature is used to segment surfaces belonging to target object from background, thereby overcoming inaccuracy of object recognition module trained on a smaller dataset resulting in larger bounding boxes for target objects. Target object shape is identified by using empirical rules on surface attributes thereby detecting graspable affordances and poses thus avoiding collision with neighboring objects and grasping objects more successfully.","['B25J9/1669', 'B25J19/021', 'B25J9/1697', 'G06N3/008', 'G06T7/187', 'G06T7/564', 'G06T7/70', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30108']"
CN112990203B,"Target detection method and device, electronic equipment and storage medium","The application discloses a target detection method, a target detection device, electronic equipment and a storage medium, and the specific implementation scheme is as follows: performing feature extraction on the first text image based on a feature extraction module to obtain a feature image; inputting the characteristic image into a detection module to obtain a probability map of a text region, a probability map of an inner contracted text region, a probability map of a text region boundary, a probability map of a central region and a plurality of characteristic vectors for representing the upper, lower, left and right positions of the text region; taking a detection network obtained by training based on the probability map of the text region, the probability map of the contracted text region, the probability map of the text region boundary, the probability map of the central region and the plurality of feature vectors as a target detection network; and detecting a corresponding text region in the second text image according to the target detection network, and positioning the text region. By the method and the device, the accuracy of target detection can be improved.","['G06V20/62', 'G06F18/214', 'G06F18/2415', 'G06N3/045']"
US7991228B2,Stereo image segmentation,"Real-time segmentation of foreground from background layers in binocular video sequences may be provided by a segmentation process which may be based on one or more factors including likelihoods for stereo-matching, color, and optionally contrast, which may be fused to infer foreground and/or background layers accurately and efficiently. In one example, the stereo image may be segmented into foreground, background, and/or occluded regions using stereo disparities. The stereo-match likelihood may be fused with a contrast sensitive color model that is initialized or learned from training data. Segmentation may then be solved by an optimization algorithm such as dynamic programming or graph cut. In a second example, the stereo-match likelihood may be marginalized over foreground and background hypotheses, and fused with a contrast-sensitive color model that is initialized or learned from training data. Segmentation may then be solved by an optimization algorithm such as a binary graph cut.","['G06T7/11', 'G06T7/162', 'G06T7/194', 'G06V10/267', 'G06V10/28', 'G06V10/56', 'G06V40/162', 'G06T2207/10021', 'G06T2207/10024', 'G06T2207/20072']"
US8774504B1,System for three-dimensional object recognition and foreground extraction,"The present invention describes a system for recognizing objects from color images by detecting features of interest, classifying them according to previous objects' features that the system has been trained on, and finally drawing a boundary around them to separate each object from others in the image. Furthermore, local feature detection algorithms are applied to color images, outliers are removed, and resulting feature descriptors are clustered to achieve effective object recognition. Additionally, the present invention describes a system for extracting foreground objects and the correct rejection of the background from an image of a scene. Importantly, the present invention allows for changes to the camera viewpoint or lighting between training and test time. The system uses a supervised-learning algorithm and produces blobs of foreground objects that a recognition algorithm can then use for object detection/recognition.","['G06V20/64', 'G06F18/24', 'G06T7/11', 'G06T7/194', 'G06V10/56', 'G06T2207/20081', 'G06T2207/20164', 'G06V10/462']"
US11182899B2,Systems and methods for processing electronic images to detect contamination,"Systems and methods are disclosed for receiving one or more digital images associated with a tissue specimen, detecting one or more image regions from a background of the one or more digital images, determining a prediction, using a machine learning system, of whether at least one first image region of the one or more image regions comprises at least one external contaminant, the machine learning system having been trained using a plurality of training images to predict a presence of external contaminants and/or a location of any external contaminants present in the tissue specimen, and determining, based on the prediction of whether a first image region comprises an external contaminant, whether to process the image region using an processing algorithm.","['G06T7/0012', 'G06K9/4671', 'G06T7/11', 'G06T7/194', 'G06V10/764', 'G06V10/776', 'G06V10/82', 'G06V10/993', 'G06V20/69', 'G06K2209/05', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024', 'G06V2201/03']"
US9495620B2,Multi-script handwriting recognition using a universal recognizer,"Methods, systems, and computer-readable media related to a technique for providing handwriting input functionality on a user device. A handwriting recognition module is trained to have a repertoire comprising multiple non-overlapping scripts and capable of recognizing tens of thousands of characters using a single handwriting recognition model. The handwriting input module provides real-time, stroke-order and stroke-direction independent handwriting recognition. User interfaces for providing the handwriting input functionality are also disclosed.","['G06K9/6842', 'G06V30/36', 'G06F17/2223', 'G06F18/217', 'G06F3/04883', 'G06F40/129', 'G06K9/00422', 'G06K9/00979', 'G06K9/6821', 'G06N20/00', 'G06N3/088', 'G06V10/95', 'G06V30/2445', 'G06V30/246', 'G06K2209/01', 'G06V30/293']"
US8645216B2,"Enrollment apparatus, system, and method","An apparatus for enrolling a package is disclosed including: a receiving surface for receiving the package; at least one weight sensor in communication with the receiving surface which generates a weight signal indicative of the weight of the package; at least one video camera which generates a video signal indicative of an image of the package on the receiving surface; and a processor in communication with the at least one weight sensor and the at least one video camera. The processor includes: a weight module which produces, in response to the weight signal, weight data indicative of the weight of the package; and a dimension capture module which produces, in response to the video signal, dimension data indicative of the size of the package. In some embodiments, the processor further includes a recognition module which produces, in response to the video signal, character data indicative of one or more characters present on the package.","['G06V30/424', 'G06Q10/087', 'G06Q20/202', 'G06Q20/208', 'G06T7/62', 'G06V20/20', 'G06V20/64', 'G06V30/1456', 'G07B17/00193', 'G07B17/00661', 'G06V30/10', 'G07B17/00508', 'G07B2017/00225', 'G07B2017/00685', 'G07B2017/00701', 'G07B2017/00725']"
US7684592B2,Realtime object tracking system,"A real-time computer vision system tracks one or more objects moving in a scene using a target location technique which does not involve searching. The imaging hardware includes a color camera, frame grabber and processor. The software consists of the low-level image grabbing software and a tracking algorithm. The system tracks objects based on the color, motion and/or shape of the object in the image. A color matching function is used to compute three measures of the target's probable location based on the target color, shape and motion. The method then computes the most probable location of the target using a weighting technique. Once the system is running, a graphical user interface displays the live image from the color camera on the computer screen. The operator can then use the mouse to select a target for tracking. The system will then keep track of the moving target in the scene in real-time.","['G06F3/017', 'G06T7/246', 'G06V40/20', 'G06V40/28', 'A63F2300/1093', 'A63F2300/6045', 'A63F2300/69']"
CN116433701B,"Workpiece hole profile extraction method, device, equipment and storage medium","The application relates to a method, a device, equipment and a storage medium for extracting a hole outline of a workpiece, wherein the method comprises the following steps: performing binarization processing on the hole image of the workpiece to obtain a hole binarization image, and determining the pixel positions of all holes; setting preset hole boundaries for pixel positions of all holes, and performing image segmentation based on the preset hole boundaries and the hole binarization image to determine initial hole contours; and performing curve fitting on the pixel points of the initial hole outline to determine sub-pixel edge points of the hole outline, and obtaining the target hole outline. According to the method, the device, the equipment and the storage medium for extracting the hole outline of the workpiece, the hole outline of the workpiece is determined through the hole binarized image by performing binarization processing on the hole image of the workpiece, so that the problems of deformation and shadow of the shot hole image caused by the influence of illumination and materials of the hole image of the workpiece are avoided, and the accurate hole outline in the workpiece is successfully extracted.","['G06T7/13', 'G06T7/11', 'G06V10/28', 'G06V10/44', 'G06T2207/30164', 'Y02P90/30']"
WO2020199478A1,"Method for training image generation model, image generation method, device and apparatus, and storage medium","A method for training an image generation model, an image generation method, device and apparatus, and a storage medium. The method for training an image generation model comprises: acquiring a first image set and a second image set, the first image set comprising multiple captured images, and the second image set comprising multiple cartoon images; performing pre-processing of the captured images according to a preset cartoon generation algorithm to obtain corresponding target cartoon images; and iteratively training a generative network and a discriminative network in an alternating manner to obtain an image generation model.","['G06F18/214', 'G06T3/04', 'G06T7/11', 'G06T7/13', 'G06T2207/20081', 'G06T2207/20084', 'Y02T10/40']"
US12008796B2,Systems and methods for pose detection and measurement,"A method for estimating a pose of an object includes: receiving a plurality of images of the object captured from multiple viewpoints with respect to the object; initializing a current pose of the object based on computing an initial estimated pose of the object from at least one of the plurality of images; predicting a plurality of 2-D keypoints associated with the object from each of the plurality of images; and computing an updated pose that minimizes a cost function based on a plurality of differences between the 2-D keypoints and a plurality of 3-D keypoints associated with a 3-D model of the object as arranged in accordance with the current pose, and as projected to each of the viewpoints.","['G06T7/579', 'G06T7/75', 'G06V10/454', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06V20/647', 'G06T2207/10012', 'G06T2207/20081', 'G06T2207/20084', 'G06T2219/2004']"
CN113033465B,"Living body detection model training method, device, equipment and storage medium","The disclosure provides a living body detection model training method, a living body detection model training device, living body detection model training equipment, a living body detection model training storage medium and a living body detection model training program product, relates to the field of artificial intelligence, and particularly relates to a computer vision and deep learning technology, and can be used in a smart city scene. One embodiment of the method comprises the following steps: acquiring a sample image set, wherein a living body label or an attack label is marked on a sample image in the sample image set; cutting to obtain a plurality of sample area blocks based on the sample image; disturbing a plurality of sample area blocks to carry out superposition to obtain a sample tensor; and taking the sample tensor as input, taking the superposition sequence of the labels of the sample image and the plurality of sample area blocks as supervision information, and training the convolutional neural network to obtain a living body detection model. According to the embodiment, self-supervision learning is added on the basis of the region blocks, supervision information is added, and accuracy and generalization of a living body detection algorithm are improved.","['G06N3/08', 'G06F18/214', 'G06F18/24', 'G06V40/161', 'G06V40/45']"
CN110659647B,"Seal image identification method and device, intelligent invoice identification equipment and storage medium","A stamp image recognition method, a stamp image recognition apparatus, an intelligent invoice recognition device, and a storage medium are provided. The stamp image identification method comprises the following steps: acquiring an input image, wherein the input image comprises a seal image; and performing character recognition on a target character area in the stamp image based on an attention model and a character recognition model to obtain and output target character information in the target character area. The stamp image identification method can well realize identification of irregularly arranged characters in the stamp image so as to obtain an accurate identification result.","['G06V30/153', 'G06N3/045', 'G06V30/287']"
US10136116B2,Object segmentation from light field data,"A scene is segmented into objects based on light field data for the scene, including based on image pixel values (e.g., intensity) and disparity map(s). In one aspect, the light field data is used to estimate one or more disparity maps for the scene taken from different viewpoints. The scene is then segmented into a plurality of regions that correspond to objects in the scene. Unlike other approaches, the regions can be variable-depth. In one approach, the regions are defined by boundaries. The boundaries are determined by varying the boundary to optimize an objective function for the region defined by the boundary. The objective function is based in part on a similarity function that measures a similarity of image pixel values for pixels within the boundary and also measures a similarity of disparities for pixels within the boundary.","['H04N13/128', 'G06T7/11', 'G06T7/12', 'G06T7/149', 'G06T7/557', 'G06T2200/04', 'G06T2200/21', 'G06T2207/10052', 'G06T2207/20016', 'G06T7/162', 'H04N2013/0081']"
US10242255B2,Gesture recognition system using depth perceptive sensors,"Acquired three-dimensional positional information is used to identify user created gesture(s), which gesture(s) are classified to determine appropriate input(s) to an associated electronic device or devices. Preferably at at least one instance of a time interval, the posture of a portion of a user is recognized, based at least one factor such as shape, position, orientation, velocity. Posture over each of the instance(s) is recognized as a combined gesture. Because acquired information is three-dimensional, two gestures may occur simultaneously.","['G06K9/00389', 'G06F3/017', 'G06K9/00382', 'G06V40/11', 'G06V40/113', 'H04N21/4223', 'H04N21/44218', 'A63F2300/1093']"
AU2020220153B2,Robust warping via multi-scale patch adversarial loss for virtual try-on,"Generating a synthesized image of a person wearing clothing is described. A two-dimensional reference image depicting a person wearing an article of clothing and a two-dimensional image of target clothing in which the person is to be depicted as wearing are received. To generate the synthesized image, a warped image of the target clothing is generated via a geometric matching module, which implements a machine learning model trained to recognize similarities between warped and non-warped clothing images using multi-scale patch adversarial loss. The multi-scale patch adversarial loss is determined by sampling patches of different sizes from corresponding locations of warped and non-warped clothing images. The synthesized image is generated on a per-person basis, such that the target clothing fits the particular body shape, pose, and unique characteristics of the person.","['G06T3/18', 'G06F18/22', 'G06F18/24', 'G06F18/25', 'G06N3/045', 'G06N3/08', 'G06T11/60', 'G06T7/40', 'G06T2207/20081', 'G06T2207/30196', 'G06T2210/16']"
US9064147B2,Sketch recognition system,"Handwriting interpretation tools, such as optical character recognition (OCR), have improved over the years such that OCR is a common tool in business for interpreting typed text and sometimes handwritten text. OCR does not apply well to non-text-only diagrams, such as chemical structure diagrams. A method according to an embodiment of the present invention of interpreting a human-drawn sketch includes determining a local metric indicating whether a candidate symbol belongs to a certain classification based on a set of features. The set of features includes, as a feature, scores generated from feature images of the candidate symbol. Also included is determining a joint metric of multiple candidate symbols based on their respective classifications and interpreting the sketch as a function of the local and joint metrics. Sketches can be chemical composition, biological composition, electrical schematic, mechanical, or any other science- or engineering-based diagrams for which human-drawn symbols have well-known counterparts.","['G06V30/347', 'G06V30/36', 'G06K9/00416', 'G06K9/00422', 'G06K9/00476', 'G06V30/422']"
US8908996B2,Methods and apparatus for automated true object-based image analysis and retrieval,An automated and extensible system is provided for the analysis and retrieval of images based on region-of-interest (ROI) analysis of one or more true objects depicted by an image. The system uses an ROI database that is a relational or analytical database containing searchable vectors representing images stored in a repository. Entries in the ROI database are created by an image locator and ROI classifier that locate images within the repository and extract relevant information to be stored in the ROI database. The ROI classifier analyzes objects in an image to arrive at actual features of the true object. Graphical searches may also be performed.,"['G06F16/5838', 'G06K9/3233', 'G06F16/5854', 'G06F16/5862', 'G06V10/25', 'G06F2218/12', 'Y10S707/99933']"
US11928873B2,Systems and methods for efficient floorplan generation from 3D scans of indoor scenes,"Methods, systems, and wearable extended reality devices for generating a floorplan of an indoor scene are provided. A room classification of a room and a wall classification of a wall for the room may be determined from an input image of the indoor scene. A floorplan may be determined based at least in part upon the room classification and the wall classification without constraining a total number of rooms in the indoor scene or a size of the room.","['G06V20/64', 'G06F18/23', 'G06F18/231', 'G06F18/2431', 'G06T17/00', 'G06T7/55', 'G06V10/7625', 'G06V20/20', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2210/04']"
CN109389129B,"Image processing method, electronic device and storage medium","The embodiment of the application discloses an image processing method, electronic equipment and a storage medium, wherein the method comprises the following steps: processing a first image to obtain prediction results of a plurality of pixel points in the first image, wherein the prediction results comprise semantic prediction results and center relative position prediction results, the semantic prediction results indicate that the pixel points are located in an example area or a background area, and the center relative position prediction results indicate the relative positions of the pixel points and the example centers; and determining an example segmentation result of the first image based on the semantic prediction result and the central relative position prediction result of each pixel point in the plurality of pixel points, so that the example segmentation in the image processing has the advantages of high speed and high precision.",['G06V10/462']
US7440615B2,Video foreground segmentation method,"A fully automatic, computationally efficient segmentation method of video employing sequential clustering of sparse image features. Both edge and corner features of a video scene are employed to capture an outline of foreground objects and the feature clustering is built on motion models which work on any type of object and moving/static camera in which two motion layers are assumed due to camera and/or foreground and the depth difference between the foreground and background. Sequential linear regression is applied to the sequences and the instantaneous replacements of image features in order to compute affine motion parameters for foreground and background layers and consider temporal smoothness simultaneously. The Foreground layer is then extracted based upon sparse feature clustering which is time efficient and refined incrementally using Kalman filtering.","['G06T7/12', 'G06T7/181', 'G06T7/194', 'G06T7/215', 'G06T7/277', 'G06V10/28', 'G06T2207/10016', 'H04N7/141']"
EP4116867A1,"Vehicle tracking method and apparatus, and electronic device","A vehicle tracking method tracking vehiclesand apparatus, and an electronic device, relating to the technical field of artificial intelligence computer vision and intelligent transportation. The method comprises: extracting a target image at a current moment from a video stream obtained during traveling of vehicles (101); performing instance segmentation on the target image to obtain detection boxes corresponding to individual vehicles in the target image (102); extracting, from the detection box for each vehicle, a set of pixel points corresponding to each vehicle (103); processing image features of each pixel point in the set of pixel points corresponding to each vehicle to determine features of each vehicle in the target image (104); and determining, according to the features of each vehicle in the target image and the degree of matching between the features of each vehicle in past images, movement trajectory of each vehicle in the target image, wherein the past images are the first n image frames adjacent to the target image in the video stream, and n is a positive integer (105). According to the present method for tracking vehicles, the vehicle tracking efficiency is increased, and the real-time capability is high.","['G06V20/41', 'G06V20/58', 'G06T7/20', 'G06F18/22', 'G06T9/00', 'G06V10/25', 'G06V10/26', 'G06V10/44', 'G06V10/469', 'G06V10/751', 'G06V10/761', 'G06V10/762', 'G06V20/46', 'G06V20/49', 'G06T2207/10028', 'G06T2207/30241', 'G06T2207/30252', 'G06V2201/08', 'Y02T10/40']"
US20200005468A1,Method and system of event-driven object segmentation for image processing,"Methods, systems, and articles herein are directed to event-driven object segmentation to track events rather than tracking all pixel locations in an image.","['G06T7/254', 'G06F18/24143', 'G06K9/00711', 'G06K9/00986', 'G06T7/215', 'G06V10/25', 'G06V10/30', 'G06V10/764', 'G06V10/955', 'G06V20/40', 'G06V20/52', 'G06V40/103', 'H04N25/47', 'G06K2009/00738', 'G06T2207/10016', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196', 'G06T2207/30232', 'G06V20/44']"
CN113658206B,Plant leaf segmentation method,"The invention discloses a plant leaf segmentation method, and relates to the field of image processing. The method comprises the following steps: the method comprises the steps of constructing a sample data set, inputting a sample image in the sample data set into a convolutional neural network, wherein the convolutional neural network comprises a Backbone network, an RPN network and a plurality of cascaded blade segmentation modules, each blade segmentation module comprises a ROIAlign network and a Head network, each Head network comprises a classification branch, a segmentation branch and a detection branch, training the sample data set based on the convolutional neural network to obtain a plant blade segmentation model, inputting an image to be segmented into the plant blade segmentation model to obtain a blade segmentation result of the image to be segmented, and the image to be segmented can adopt a multi-scale segmentation strategy. The method provided by the invention can effectively divide the blades with shielding, the blades with unclear edges and the small-scale blades, and promote the application of deep learning in the field of plant blade division.","['G06T7/136', 'G06F18/241', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T7/12', 'G06T7/13', 'G06T2207/20104', 'G06T2207/20192', 'G06T2207/20221']"
CN112784763B,Expression recognition method and system based on local and overall feature adaptive fusion,"The invention discloses an expression recognition method and system based on local and overall feature adaptive fusion. Firstly, acquiring an expression image set, and carrying out face detection, cutting and alignment processing on images in the expression image set; then, constructing an expression recognition model, wherein the model comprises a data processing module, a feature extraction module, a feature fusion module and a classification layer, wherein the feature fusion module learns the attention weights of the whole face image and the image features of a plurality of local areas of the whole face image by using an attention mechanism, and adaptively selects important features for weighted fusion based on the attention weights; then, training the constructed expression recognition model by using the images in the expression image set as training samples; and finally, carrying out expression classification and identification on the newly input test image by using the trained expression identification model. The invention can adaptively fuse the expression characteristics from the whole face and the non-shielded important area, and effectively solve the expression recognition problem under the shielding and posture changing environment.","['G06V40/174', 'G06F18/214', 'G06F18/2415', 'G06F18/253', 'G06N3/084', 'G06V40/169', 'G06V40/171', 'G06V40/172']"
US11182885B2,"Method and apparatus for implementing image enhancement, and electronic device","A method for implementing image enhancement includes: performing filtering processing on a to-be-processed image to obtain an image subjected to the filtering processing; determining similarity degrees between pixel points in the to-be-processed image and a target region of a target object in the to-be-processed image; and fusing the similarity degrees, the to-be-processed image and the image subjected to the filtering processing, so that the higher a similarity degree between a pixel point and the target object in the to-be-processed image, the stronger a filtering effect of the pixel point, and the lower a similarity degree between the pixel point and the target object in the to-be-processed image, the weaker a filtering effect of the pixel point.","['G06T5/77', 'G06T5/00', 'G06F18/22', 'G06K9/6215', 'G06T3/14', 'G06T5/005', 'G06T5/20', 'G06T5/50', 'G06T5/70', 'G06T7/11', 'G06V10/761', 'G06V40/10', 'G06V40/161', 'G06T2207/10004', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20012', 'G06T2207/20021', 'G06T2207/20221', 'G06T2207/30196', 'G06T2207/30201']"
US10043308B2,Image processing method and apparatus for three-dimensional reconstruction,"An image processing method and apparatus are disclosed. The method includes obtaining a two-dimensional target face image, receiving an identification curve marked by a user in the target face image, locating a facial contour curve of a face from the target face image according to the identification curve and by using an image segmentation technology, determining a three-dimensional posture and a feature point position of the face in the target face image, and constructing a three-dimensional shape of the face in the target face image according to the facial contour curve, the three-dimensional posture, and the feature point position of the face in the target face image by using a preset empirical model of a three-dimensional face shape and a target function matching the empirical model of the three-dimensional face shape.","['G06T17/10', 'G06T17/00', 'G06T7/12', 'G06T7/13', 'G06T7/149', 'G06T2207/10004', 'G06T2207/20096', 'G06T2207/20124', 'G06T2207/30201']"
US8265393B2,Photo-document segmentation method and system,The present application provides an improved segmentation method and system for processing digital images that include an imaged document and surrounding image. A plurality of edge detection techniques are used to determine the edges of the imaged document and then segment the imaged document from the surrounding image.,"['G06T7/12', 'G06T7/194', 'G06V10/25', 'G06V30/414', 'G06T2207/10024', 'G06T2207/20084']"
US10706556B2,Skeleton-based supplementation for foreground image segmentation,"Systems and methods for foreground image segmentation, which receive image data captured at a first time, generate a skeleton model of a foreground subject captured in the image data, generate an initial foreground mask for a foreground segment including the foreground subject, generate secondary points corresponding to the foreground subject, identify supplemental points for the foreground segment based on at least the secondary points and the skeleton model, combine the supplemental points with the initial foreground mask to obtain a combined mask, obtain a foreground segment mask for the foreground subject based on the combined mask, and apply the foreground segment mask to the image data to obtain a foreground image portion for the foreground subject.","['G06T7/11', 'G06T5/002', 'G06T5/70', 'G06T7/12', 'G06T7/13', 'G06T7/181', 'G06T7/194', 'G06T7/215', 'G06T7/50', 'G06T2207/10016', 'G06T2207/20044', 'G06T2207/30196', 'G06T2207/30232']"
CN110163076B,Image data processing method and related device,"The embodiment of the invention discloses an image data processing method and a related device, wherein the method comprises the following steps: acquiring a plurality of first objects with outline information in a target image frame, and determining a positioning area of each first object in the target image frame; determining a main body region corresponding to the target image frame according to pixel information associated with all pixel points in the target image frame, and respectively determining the contact ratio information between the main body region and the positioning region of each first object; screening a plurality of candidate objects from the plurality of first objects, and acquiring average depth information corresponding to the positioning area of each candidate object; and determining a second object for classifying attribute identification from each candidate object according to the positioning area of each candidate object, the coincidence degree information associated with the positioning area of each candidate object and the average depth information. By adopting the method and the device, the waste of computing resources can be avoided, and the accuracy of acquiring the target object can be improved.","['G06V10/25', 'G06V10/267', 'G06V20/20']"
US9704230B2,Feedback to user for indicating augmentability of an image,"Methods and systems for determining augmentability information associated with an image frame captured by a digital imaging part of a user device. The determined augmentability score may then be used in the generation of feedback to the user. For example, a graphical user interface may be generated and rendered having a substantially continuous visual output corresponding to the augmentability information.","['G06T7/0002', 'G06T19/006', 'G06T2200/24', 'G06T2207/10004', 'G06T2207/30168']"
CN112967283B,"Target identification method, system, equipment and storage medium based on binocular camera","The invention provides a target identification method, a target identification system, target identification equipment and a target identification storage medium based on a binocular camera, wherein the target identification method comprises the following steps: calculating a parallax matrix according to a left image and a right image shot by a binocular shooting device, obtaining a distance value and a distance confidence coefficient of each pixel point, and generating point cloud information and a top view based on the left image; performing image segmentation based on the left image to obtain an obstacle category label, an obstacle code and a label confidence corresponding to the image area; generating a filtering mask image to filter composite image information of the left image according to the label confidence and the distance confidence of each pixel of the left image; sequentially extracting point clouds according to obstacle codes in the filtered composite image information to obtain point cloud information corresponding to object sub-labels; the positions, distances, and obstacle category labels of all obstacles are indicated in the top view. The invention can greatly reduce the cost of obstacle detection and improve the accuracy of obstacle detection.","['G06T7/0002', 'G06F18/23', 'G06N3/045', 'G06N3/08', 'G06T5/70', 'G06T7/11', 'G06T2207/10028', 'Y02T10/40']"
CN107680120B,Infrared small target tracking method based on sparse representation and transfer restricted particle filter,"The invention discloses an infrared small target tracking method based on sparse representation and transfer limited particle filtering, which comprises the steps of firstly introducing a shielding binary indication vector representing a normal target region and a noise background outlier region, and establishing a sparse representation model of an infrared small target by combining a target dictionary template; then, extracting an infrared image high-frequency region through saliency detection, using the infrared image high-frequency region as prior information in the state transition process of the particle filter model, and performing target state prediction and particle sampling under the limitation; and finally, establishing a particle filter observation model based on the target sample particle sparse reconstruction error difference, and realizing the estimation and tracking of the target state by combining an online template updating strategy. The method enhances the state estimation capability of the random particles, and improves the adaptability of the particle sparse representation to small and weak moving targets and the tracking accuracy.","['G06T7/251', 'G06F18/21345', 'G06F18/28', 'G06T2207/10048']"
US6614930B1,Video stream classifiable symbol isolation method and system,"An image processing device and method for classifying symbols relies on a connected-component technique for isolating symbol regions. The device and method form connected components from an image derived by the application of an edge detection filter. The formation of connected components from this filtered image defines the edges of character shapes. As a result, the number of pixels that must be connected to define each connected component is substantially reduced and the speed of processing thereby increased. The application of the method is discussed primarily with respect to text in a video stream.","['G06V10/20', 'G06V20/62', 'G06V30/10']"
CN111461110B,A Small Object Detection Method Based on Multi-Scale Images and Weighted Fusion Loss,"The invention belongs to the field of image and video processing, and relates to a small target detection method based on multi-scale images and weighted fusion loss, which comprises the following steps: extracting a plurality of groups of feature vectors from a plurality of images with different scales based on an improved Mask RCNN model, fusing the plurality of groups of feature vectors, and constructing a feature pyramid; generating candidate detection frames based on the feature pyramid and screening to obtain suggested detection frames; the suggested detection frames are correspondingly returned to the feature pyramid to generate the feature graphs of the suggested detection frames, and the suggested detection frames are aligned and intercepted on the feature graphs; inputting the aligned suggested detection frames into a classifier layer to obtain the category confidence coefficient and the position offset of the suggested detection frames; in the test stage, a certain suggested detection frame is screened according to the category confidence score of the suggested detection frame, and non-maximum suppression is carried out; in the training stage, the loss function calculated by the small target feature layer is weighted and fused with the loss function of the large and middle target layers, so that the sensitivity of the model to small target objects is enhanced.","['G06V10/25', 'G06F18/24', 'G06F18/253', 'G06N3/045', 'G06V10/267', 'G06V2201/07', 'Y02T10/40']"
CN108229456B,"Target tracking method and device, electronic equipment and computer storage medium","The embodiment of the invention discloses a target tracking method and device, electronic equipment and a computer storage medium, wherein the method comprises the following steps: establishing a target object track of each target object in the video according to a first detection frame in a first image frame of the video and a second detection frame in a second image frame of the video; the second image frame is a frame image of the first image frame before the video; respectively extracting the characteristics of the first detection frame corresponding to each target object; according to the characteristics of the first detection frames corresponding to the target objects and the target object tracks of the target objects in the first detection frames and the second detection frames respectively, performing track segmentation on each first detection frame to obtain segmented first detection frame information; and tracking the target according to the segmented first detection frame information. The above embodiments of the present invention eliminate the possibility of detecting tracking errors while filtering out noise.","['G06V10/22', 'G06V10/30', 'G06V10/40']"
CA3138959C,"Image diagnostic system, and methods of operating thereof","Various image diagnostic systems, and methods of operating thereof, are disclosed herein. Example embodiments relate to operating the image diagnostic system to identify one or more tissue types within an image patch according to a hierarchical histological taxonomy, identifying an image patch associated with normal tissue, generating a pixel-level segmented image patch for an image patch, generating an encoded image patch for an image patch of at least one tissue, searching for one or more histopathological images, and assigning an image patch to one or more pathological cases.","['G16H30/40', 'G06F18/24323', 'G06T7/0012', 'G06T7/0014', 'G06T7/11', 'G06V10/25', 'G06V10/26', 'G06V10/454', 'G06V10/764', 'G06V10/7788', 'G06V10/82', 'G16H40/67', 'G16H50/20', 'G06T2200/24', 'G06T2207/10056', 'G06T2207/20016', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024', 'G06V2201/03']"
US20180322623A1,Systems and methods for inspection and defect detection using 3-d scanning,"A method for detecting defects in objects includes: controlling, by a processor, one or more depth cameras to capture a plurality of depth images of a target object; computing, by the processor, a three-dimensional (3-D) model of the target object using the depth images; rendering, by the processor, one or more views of the 3-D model; computing, by the processor, a descriptor by supplying the one or more views of the 3-D model to a convolutional stage of a convolutional neural network; supplying, by the processor, the descriptor to a defect detector to compute one or more defect classifications of the target object; and outputting the one or more defect classifications of the target object.","['G06T7/0004', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'G06N3/084', 'G06N5/046', 'G06T15/205', 'G06T17/20', 'G06T7/55', 'G06N20/10', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30124']"
US20210081698A1,Systems and methods for physical object analysis,"Disclosed are devices, systems, apparatus, methods, products, and other implementations, including a method that includes obtaining physical object data for a physical object, determining a physical object type based on the obtained physical object data, and determining based on the obtained physical object data, using at least one processor-implemented learning engine, findings data comprising structural deviation data representative of deviation between the obtained physical object data and normal physical object data representative of normal structural conditions for the determined physical object type.","['G06K9/3241', 'G06Q40/08', 'G01M17/007', 'G06F18/2433', 'G06N5/025', 'G06Q10/20', 'G06Q30/0278', 'G06Q30/0283', 'G06Q50/40', 'G06T3/403', 'G06T7/0006', 'G06T7/001', 'G06T7/11', 'G06T7/70', 'G06V10/255', 'G06V10/764', 'G06V10/82', 'G01N2021/8883', 'G06K2209/23', 'G06T2207/20081', 'G06T2207/30108', 'G06T2207/30248', 'G06V2201/08']"
CN111652217B,"Text detection method and device, electronic equipment and computer storage medium","The embodiment of the application provides a text detection method, a text detection device, an electronic device and a computer storage medium, wherein the text detection method comprises the following steps: performing feature extraction and segmentation on a text image to be detected to obtain a text region threshold map and a text region central point probability map of the text image to be detected; acquiring a text region frame binary image corresponding to the text region threshold image and a text region center point binary image corresponding to the text region center point probability image; and performing connected domain detection on the binary image of the center point of the text region, determining a clustering center of the text region, and determining a text detection result in the text image to be detected according to the similarity between pixel points of the text region border in the binary image of the text region border and the clustering center. By the embodiment of the invention, the speed of text detection, particularly intensive text detection, is improved.","['G06V20/62', 'G06N3/045', 'G06V30/153']"
EP1960970B1,Stereo video for gaming,"A real-time stereo video signal of a captured scene with a physical foreground object and a physical background is received. In real-time, a foreground/background separation algorithm is used on the real-time stereo video signal to identify pixels from the stereo video signal that represent the physical foreground object. A video sequence may be produced by rendering a 3D virtual reality based on the identified pixels of the physical foreground object.","['A63F13/213', 'G06T15/00', 'G06T5/50', 'G06T7/593', 'G06V40/28', 'H04N13/207', 'H04N13/239', 'A63F2300/1093', 'G06T2207/10012']"
CN114581742B,"Linearity-based connected domain clustering fusion method, device, system and medium","The application provides a connected domain clustering fusion method, a device, an identification system and a storage medium based on linearity, wherein the method comprises the following steps: extracting a defect connected domain set based on a target image to be detected; screening defect connected domains in the defect connected domain set according to the linearity to obtain linear connected domains and dotted connected domains; selecting a plurality of groups of linear connected domains from the linear connected domains, and clustering according to the angle and the distance between the central axes of the linear connected domains in each group of linear connected domains to obtain a linear connected domain set; each set of linear connected domains comprises two linear connected domains; the distance comprises at least one of a vertical distance, a translational distance, a relative distance, an offset distance, and a linear distance; and scanning the second minimum circumscribed rectangle by using a dense point search area containing a search window to obtain dense point defects and isolated point defects, and the method has great value for judging the quality of subsequent products.","['G06F18/254', 'G06F18/23', 'G06F18/24', 'Y02P90/30']"
CN110874594B,Human body appearance damage detection method and related equipment based on semantic segmentation network,"A human body appearance impairment detection method based on a semantic segmentation network, the method comprising: acquiring a picture to be detected, which is required to be subjected to human body appearance damage detection; inputting the picture to be detected into a human body injury detection model based on semantic segmentation; sequentially extracting features through a region candidate network and an output network in the human body injury detection model; carrying out feature fusion on the first feature map extracted by the area candidate network and the second feature map extracted by the output network to obtain a final feature map; and performing feature detection on the final feature map to obtain a human body appearance damage detection result of the picture to be detected, wherein the human body appearance damage detection result comprises a damage area boundary box, a damage type and a picture mask. The invention also provides a human body appearance damage detection device, electronic equipment and a storage medium based on the semantic segmentation network. The invention can more accurately detect the damage of the appearance of the human body.","['G06V10/267', 'G06F18/213', 'G06F18/214', 'G06F18/253', 'G06N3/045', 'G06T7/0012', 'G06V10/44']"
US20220270289A1,Method and apparatus for detecting vehicle pose,"A method and device for detecting a vehicle pose, relating to the fields of computer vision and automatic driving. The specific implementation solution comprises: inputting a vehicle left view point image and a vehicle right view point image into a part prediction and mask segmentation network model, and determining foreground pixel points and part coordinates thereof in a reference image; converting coordinates of the foreground pixels in the reference image into coordinates of the foreground pixels in a camera coordinate system so as to obtain a pseudo-point cloud, and fusing part coordinate of the foreground pixels and the pseudo-point cloud to obtain fused pseudo-point cloud; and inputting the fused pseudo-point cloud into a pre-trained pose prediction model to obtain a pose information of the vehicle to be detected.","['G06T7/194', 'G06T7/593', 'G06T7/70', 'G06T7/74', 'G06T7/80', 'H04N13/128', 'G06T2207/10004', 'G06T2207/10012', 'G06T2207/10028', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20228', 'G06T2207/30244', 'G06T2207/30252', 'H04N2013/0081', 'H04N2013/0092']"
US12354295B2,Eye tracking latency enhancements,Systems and methods for eye tracking latency enhancements. An example head-mounted system obtains a first image of an eye of a user. The first image is provided as input to a machine learning model which has been trained to generate iris and pupil segmentation data given an image of an eye. A second image of the eye is obtained. A set of locations in the second image at which one or more glints are shown is detected based on iris segmentation data generated for the first image. A region of the second image at which the pupil of the eye of the user is shown is identified based on pupil segmentation data generated for the first image. A pose of the eye of the user is determined based on the detected set of glint locations in the second image and the identified region of the second image.,"['G06T7/70', 'G06F21/32', 'G06F3/013', 'G06T7/11', 'G06T7/13', 'G06T7/60', 'G06V10/22', 'G06V10/766', 'G06V40/193', 'G06V40/197', 'G06F1/163', 'G06T2207/10052', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30201']"
CN114359274B,"Ventilation equipment blade quality detection method, device and system based on image processing","The invention discloses a method, a device and a system for detecting the quality of a ventilation equipment blade based on image processing, and relates to the field of artificial intelligence. The method mainly comprises the following steps: acquiring a blade surface image of a to-be-detected ventilation device and preprocessing the blade surface image to obtain a gray image of the blade; acquiring abnormal points in the gray level image according to the gradient amplitude of the pixel points in the gray level image and the gradient amplitude of the pixel points in the neighborhood of the pixel points; dividing the abnormal points in the dividing window with the same preset size into the same category to obtain a plurality of categories after all the abnormal points are divided; performing mean shift clustering on the intersection points of the gradient directions of the abnormal points contained in the category, and obtaining the defect points in the gray level image according to the proportion of the pixel points contained in the clustering result in the category; and obtaining the severity of the defects according to the dispersion degree of the defect points and the number of the defect points, and detecting the quality of the ventilation equipment blade by using the severity of the defects.",[]
CN106650721B,A kind of industrial character identifying method based on convolutional neural networks,"The invention proposes a kind of industrial character identifying method based on convolutional neural networks, including establishing character data collection, data enhancing and pretreatment are carried out to character data collection, establish CNN integrated model, the model includes 3 different component classifiers, then, it is trained using model, training is divided into the completion of two steps, and the first step is off-line training, obtains off-line training model, second step is on-line training, off-line training model is used as initialization, carries out the training of specific production line character data collection, obtains on-line training model；And targeted graphical is pre-processed, the segmentation of character locating and single character picture；The character picture divided is sent into trained on-line training model, probability value of three classifiers by single target image classification for each classification in CNN integrated model is obtained；Final decision is carried out by the way of ballot, obtains the category result of test data.The present invention can fast and efficiently identify the character on different production lines.","['G06V10/22', 'G06F18/2415', 'G06N3/04', 'G06V30/10']"
WO2020253629A1,"Detection model training method and apparatus, computer device, and storage medium","The present application relates to a detection model training method and apparatus, a computer device, and a storage medium, relating to machine learning in artificial intelligence. The method comprises: obtaining a candidate image area set obtained by segmenting a first sample image; obtaining a first relation degree corresponding to each candidate image area, the first relation degree being a relation degree, about the circumstance that the candidate image area comprises a target object, output after the candidate image area is input into a first detection model; obtaining a second relation degree corresponding to each candidate image area, the second relation degree being a relation degree, about the circumstance that the candidate image area comprises the target object, output after the candidate image area is inputted into a second detection model; obtaining a relation degree change value according to the first relation degree and the second relation degree, and screening out a target image area comprising the target object from the candidate image area set according to the relation degree change value; and performing model training according to the target image area to obtain a target detection model.","['G06F18/214', 'G06T7/0012', 'G06N20/20', 'G06N3/08', 'G06T7/11', 'G06T7/174', 'G06T7/194', 'G06T7/70', 'G06V10/25', 'G06V10/26', 'G06V10/774', 'G06V10/82', 'G06T2207/10116', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30068', 'G06V2201/03']"
US8896682B2,System and method for automated detection of age related macular degeneration and other retinal abnormalities,"A system and method for automated detection of age related macular degeneration and other retinal abnormalities which may have a retinal scanner capable of obtaining retinal data from a subject. The retinal scanner may be coupled to a central processing unit (CPU) which may have memory storing CPU-executable instructions which may detect retinal abnormalities. When the CPU receives retinal data from the retinal scanner, the system may perform CPU-executable instructions for detecting retinal abnormalities. The system may analyze the retinal data to determine one or more healthy areas and, based on the analyzed healthy area, the system may detect abnormalities in the retinal data.","['A61B3/12', 'A61B3/0025']"
CN110300977B,Method for image processing and video compression,"A method for video compression by image processing and object detection, the method being performed by an electronic processing unit based on a stream of images or digital video images, the images being defined by individual frames or sequences of frames of the video stream, with the aim of enhancing and then isolating frequency domain signals representing content to be identified and reducing or ignoring frequency domain noise with respect to the content within the images or video streams, comprising the steps of: obtaining a digital image or sequence of digital images from a corresponding single frame or corresponding sequence of frames of the video stream, all digital images being defined in the spatial domain; selecting one or more pairs of sparse regions, each pair of sparse regions covering at least a portion of the single frame or at least two frames of the sequence of frames, each pair of sparse regions generating selected features, each region defined by two spatial data sequences; the selected features are transformed into frequency domain data by: combining the two spatial data sequences by 2D modification of the L-transform for each region, changing the transfer function, shape and direction of the frequency domain data of each region, thus generating normalized complex vectors for each of the selected features; combining all of the normalized complex vectors to define a model of the content to be identified; and inputting the model from the selected feature into a classifier, thereby obtaining data for object detection or visual saliency for use in video compression.","['H04N19/60', 'G06T5/10', 'G06V10/431', 'G06T5/70', 'G06V10/764', 'H04N19/139', 'H04N19/149', 'H04N19/70', 'H04N19/80', 'H04N19/85', 'G06T2207/20212']"
CN106599883B,CNN-based multilayer image semantic face recognition method,"The invention provides a CNN-based face recognition method for multilevel image semantics, which is further optimized on the basis of VGGNet and provides a new network structure for performing cross-layer splicing on multilayer features, so that finally extracted image features have multilevel image semantics. Meanwhile, in the training of the convolutional neural network, the extracted traditional features are added as additional features, so that the CNN feature information is more complete. And then, the structure optimization is carried out on the shallow layer convolution layer, so that the redundant calculation is reduced, and the calculation amount of the model is greatly reduced. And finally, accelerating the convolution layer by using an improved matrix decomposition algorithm, accelerating the network within 1 second, and not reducing the accuracy of the model under the condition that the acceleration ratio reaches 4 times. The face comparison algorithm realized by the invention has the characteristics of high accuracy and high real-time performance, and compared with the existing algorithm, the face comparison algorithm has higher accuracy and higher calculation efficiency.","['G06V40/161', 'G06F18/24', 'G06N3/02', 'G06V40/168', 'G06V40/172']"
US5828776A,Apparatus for identification and integration of multiple cell patterns,A biological specimen classification strategy employs identification and integration of multiple cell patterns. An automated microscope acquires an image of a biological specimen such as a Pap smear and provides an image output to biological classifiers. The classifiers independently detect and classify a number of specimen types and provide classifications to an output field of view integrator. The integrator integrates the classifications. The integrated output then determines whether the classifiers should be reapplied to the image.,"['G06V20/69', 'G06V30/248']"
CN111292337B,"Image background replacement method, device, equipment and storage medium","The embodiment of the invention discloses an image background replacement method, an image background replacement device, image background replacement equipment and a storage medium. The method comprises the following steps: acquiring a current video frame, and selecting a target portrait area in the current video frame; acquiring an initial mask corresponding to a target portrait area of the current video frame; performing segmentation optimization treatment and inter-frame smoothing treatment on the initial mask to obtain a target mask; and replacing the background of the current video frame with a new background according to the target mask, and generating a synthesized frame corresponding to the current video frame. According to the technical scheme, the calculated amount of the portrait segmentation of the real-time video is reduced, the portrait segmentation precision is improved, and the portrait background replacement of the real-time video is realized.","['G06T7/11', 'G06T5/50', 'G06T7/194', 'G06T2207/10016', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30196']"
WO2023160075A1,"Image inpainting method and apparatus, and device and medium","Provided in the embodiments of the present disclosure are an image inpainting method and apparatus, and a device and a medium. The method comprises: acquiring a facial image of a target object and an environment image, which comprises the surrounding environment of the target object, wherein the facial image includes a spectacle lens area image of glasses worn by the target object; according to a matching result of the spectacle lens area image and the environment image, determining a light reflection area in the spectacle lens area image; and according to the light reflection area, inpainting the spectacle lens area image, so as to obtain an inpainted target image. By means of the present method, a spectacle lens area image can be inpainted on the basis of a light reflection area, thereby achieving a better light reflection cancellation effect.","['G06T5/77', 'G06T7/12', 'G06T7/13', 'G06T2207/30201']"
US9898677B1,Object-level grouping and identification for tracking objects in a video,"In one embodiment, a method of determine and track moving objects in a video, including detecting and extracting regions from accepted frames of a video, matching parts including using the extracted parts of a current frame and matching each part from a previous frame to a region in the current frame, tracking the matched parts to form part tracks, and determining a set of path features for each tracked part path. The determined path features are used to classify each path as that of mover or a static. The method includes clustering the paths of movers, including grouping parts of movers that likely belong to a single object, in order to generate one or more single moving objects and track moving objects. Also a system to carry out the method and a non-transitory computer-readable medium that when executed in a processing system causes carrying out the method.","['G06K9/3241', 'G06T7/215', 'G06F18/23', 'G06K9/00718', 'G06K9/00744', 'G06K9/4609', 'G06K9/6218', 'G06T7/0022', 'G06T7/0081', 'G06T7/0097', 'G06T7/2006', 'G06T7/2033', 'G06T7/246', 'G06V10/255', 'G06V10/44', 'G06V10/443', 'G06V10/762', 'G06V20/52', 'G06T2207/10016']"
US11256961B2,Training a neural network to predict superpixels using segmentation-aware affinity loss,"Segmentation is the identification of separate objects within an image. An example is identification of a pedestrian passing in front of a car, where the pedestrian is a first object and the car is a second object. Superpixel segmentation is the identification of regions of pixels within an object that have similar properties. An example is identification of pixel regions having a similar color, such as different articles of clothing worn by the pedestrian and different components of the car. A pixel affinity neural network (PAN) model is trained to generate pixel affinity maps for superpixel segmentation. The pixel affinity map defines the similarity of two points in space. In an embodiment, the pixel affinity map indicates a horizontal affinity and vertical affinity for each pixel in the image. The pixel affinity map is processed to identify the superpixels.","['G06K9/6262', 'G06V10/82', 'G06F18/217', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06T7/11', 'G06T2200/28', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084']"
EP3916612A1,"Method and apparatus for training language model based on various word vectors, device, medium and computer program product","A method and apparatus for training a language model based on various word vectors, a device, a medium and a computer program product, which relate to the field of natural language processing technologies in artificial intelligence, are disclosed. An implementation includes inputting a first sample text language material including a first word mask into the language model, and outputting a context vector of the first word mask via the language model; acquiring a first probability distribution matrix of the first word mask based on the context vector of the first word mask and a first word vector parameter matrix, and a second probability distribution matrix of the first word mask based on the context vector of the first word mask and a second word vector parameter matrix; and training the language model based on a word vector corresponding to the first word mask. The language model is trained by combining various high-quality word vectors, such that the language model learns multi-source high-quality word meaning information, the capacity of the language model to learn the word meaning information is enhanced, the prediction performance of the language model is improved, and an information leakage risk caused by a learning process based on character granularity is avoided.","['G06N3/08', 'G06F18/2415', 'G06F40/279', 'G06F40/289', 'G06F40/30', 'G06N20/00', 'G06N3/045', 'G06F40/284']"
US10453200B2,Automated segmentation using deep learned priors,"Embodiments described herein provide a hybrid technique which incorporates learned pulmonary nodule features in a model based energy minimization segmentation using graph cuts. Features are extracted from training samples using a convolutional neural network, and the segmentation cost function is augmented via the deep learned energy. The system and method improves segmentation performance and more robust initialization.","['G06T7/143', 'G06T7/11', 'G06N3/04', 'G06N3/045', 'G06N3/0454', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/0472', 'G06N3/08', 'G06N3/09', 'G06N7/005', 'G06N7/01', 'G06T5/002', 'G06T5/70', 'G06T7/12', 'G06T7/136', 'G06T7/162', 'G06T7/194', 'G06T2207/10072', 'G06T2207/10081', 'G06T2207/10136', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20161', 'G06T2207/30061', 'G06T2207/30064', 'G06T2207/30096', 'G06T2207/30101']"
CN107563446B,Target detection method for micro-operation system,"The invention discloses a target detection method of a micro-operation system, which comprises the following steps: performing feature extraction on the sample image by using a depth residual convolution neural network to obtain a sample feature map; performing convolution operation on the sample characteristic graph by using the regional suggestion network to obtain a sample target candidate frame; screening the sample target candidate frame by using an online difficult sample mining method to obtain a new sample target candidate frame, and finishing training of the fully-connected classification network by taking the sample feature map and the new sample target candidate frame as training samples of the fully-connected classification network; and obtaining a characteristic graph of the image to be recognized by using a depth residual convolution neural network, combining a target candidate frame obtained by the region suggestion network, and obtaining a target recognition result through the region-of-interest pooling layer and the trained fully-connected classification network. The method is applied to target detection in the micro-operation system, can effectively position and identify each object, and simultaneously meets the requirements of accuracy and real-time performance.",[]
CN109544677B,Indoor scene main structure reconstruction method and system based on depth image key frame,"The method and the system for reconstructing the main structure of the indoor scene based on the depth image key frame acquire the depth image: acquiring a depth image from a depth camera, processing the depth image to obtain corresponding point cloud data, and obtaining a normal vector according to the point cloud data; calculating a camera pose matrix of the current frame; judging whether the current frame depth image is a key frame depth image or not, and if so, adding the current frame depth image into the key frame sequence; calculating a main structure plane equation set for each frame of depth image added into the key frame sequence; converting the main structure plane equation from a camera coordinate system to a world coordinate system based on the camera pose matrix; adding the primary structure plane equation set into the secondary structure plane equation set, and adding the primary structure plane equation set into the primary structure plane equation set for registration and fusion; and reconstructing the main structure of the indoor scene according to the finally fused main structure plane equation set until all frames in the key frame sequence are processed.","['G06T17/00', 'G06T7/30', 'G06T2207/10028', 'G06T2207/20028']"
WO2022120665A1,Capacitance defect intelligent detection method based on deep learning,"The present invention relates to a capacitance defect intelligent detection method based on deep learning. The method comprises: data set cropping, performing data augmentation, label smoothing, constructing a network and parameters, training a network model and the parameters, applying the network model to test data, and carrying out post-processing. By means of the method, a deep learning framework is used to extract features, and the extracted features are further processed, such that the detection of multi-scale defect capacitance is realized, thereby improving the accuracy of detection, increasing the speed of detection, and also avoiding the problems of there being great difficulty and a high cost in the usage of human eyes for recognition, and the accuracy of traditional image processing and computer vision methods, etc., being low.",['G06T7/00']
EP3926526A2,"Optical character recognition method and apparatus, electronic device and storage medium","The present application discloses a method and an apparatus for optical character recognition, an electronic device, a storage medium and a computer program product, which relates to the fields of artificial intelligence and deep learning. The method may include: determining, for a to-be-recognized image, a text bounding box of a text area therein, and extracting a text area image from the to-be-recognized image according to the text bounding box; determining a bounding box of text lines in the text area image, and extracting a text-line image from the text area image according to the bounding box; and performing text sequence recognition on the text-line image, and obtaining a recognition result. The application of the solution in the present application can improve a recognition speed and the like.","['G06V30/414', 'G06V10/22', 'G06F18/24', 'G06F40/279', 'G06N20/00', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/082', 'G06N3/09', 'G06N3/092', 'G06N3/0985', 'G06V10/267', 'G06V10/462', 'G06V10/82', 'G06V20/62', 'G06V30/148', 'G06V30/19173', 'G06V30/262', 'G06V30/413', 'G06T2210/12', 'G06V30/10']"
CN106960202B,Smiling face identification method based on visible light and infrared image fusion,"The invention provides a smiling face recognition method based on fusion of visible light and infrared images, belongs to the field of image processing, utilizes the characteristic that the infrared images have strong anti-interference performance to illumination, can make up the defect that the visible light images are sensitive to the illumination after the visible light and the infrared images are fused, and has higher practical value. The technical scheme adopted by the invention is as follows: fusing the visible light expression images and the infrared expression images by adopting an image fusion method based on contrast pyramid decomposition to obtain fused images, extracting expression characteristics from the fused images, and finally performing classification and identification by using a Support Vector Machine (SVM). The invention fuses the visible light and the infrared image, makes up the defect that the visible light image is sensitive to the illumination and solves the problem that the illumination influences the recognition rate.","['G06V40/175', 'G06F18/2411', 'G06T3/40', 'G06T3/60', 'G06T5/30', 'G06T5/50', 'G06V10/751', 'G06V40/161', 'G06V40/168', 'G06T2207/10048', 'G06T2207/20221', 'G06T2207/30201']"
CN117809122B,"A method, system, electronic device and medium for processing intracranial large blood vessel images","The invention discloses a processing method, a system, electronic equipment and a medium of intracranial large blood vessel images, relating to the field of image processing, wherein the method comprises the following steps: acquiring an intracranial macrovascular original image of a target to be identified and a sample subject; according to the intracranial large blood vessel original image, a cerebral blood vessel segmentation model is applied to obtain a cerebral blood vessel mask image; calculating a region of interest and a bounding box of the cerebrovascular mask image; selecting a corresponding region of interest from the intracranial macrovascular original image according to the bounding box of the region of interest; preprocessing the mask image region of interest and the original image region of interest respectively to obtain an image to be processed; labeling a target area in an image to be processed of a sample subject; training the convolutional neural network model by using a training set to obtain a cerebral vessel occlusion classification model; and inputting the image to be processed of the target to be identified into the cerebral vascular occlusion classification model to obtain a target area identification result. The method and the device can improve the accuracy of target area identification.","['G06V10/764', 'G06N3/045', 'G06N3/0464', 'G06V10/25', 'G06V10/26', 'G06V10/454', 'G06V10/7715', 'G06V10/82', 'G06V40/14', 'G06N3/048', 'G06V2201/03']"
US11210865B2,Visually interacting with three dimensional data in augmented or virtual reality,"An apparatus includes: a memory embodying three-dimensional data and computer executable instructions; sensors detecting a movable controller in a real space; at least one processor, operative by the computer executable instructions to facilitate: mapping the three dimensional data to a virtual space; tracking a real position and orientation of the movable controller using data from the sensors; converting the real position and orientation into a virtual position and orientation relative to the three dimensional data; identifying a selected region of the three dimensional data, which is tethered to the virtual position and orientation; and rendering the three dimensional data in the virtual space, with the selected region being rendered differently from the remainder of the three dimensional data; and showing a user the rendering of the three dimensional data in the virtual space by activating a virtual or augmented reality display that is activated by the at least one processor.","['G06T19/20', 'G06T15/30', 'G06F3/011', 'G06F3/0346', 'G06F3/038', 'G06T15/005', 'G06T15/40', 'G06T19/006', 'A61B2034/105', 'A61B2090/365', 'G06F2203/0384', 'G06T2200/24', 'G06T2210/41', 'G06T2219/2021']"
CN110390033B,"Training method and device for image classification model, electronic equipment and storage medium","The invention provides a training method and device of an image classification model, electronic equipment and a storage medium; the method comprises the following steps: acquiring video sets of video playing by different user groups of a video client; based on click data of videos, video classification is carried out on video sets of all user groups respectively, and a plurality of video subsets corresponding to all user groups are obtained; acquiring cover images of videos in each video subset, and determining the category to which the cover images belong; taking the cover image with the marked category as a sample image, and predicting the corresponding image category based on the image characteristics of the sample image through an image classification model; and updating model parameters of the image classification model according to the difference between the predicted image category and the marked category. Thus, the accuracy and the classification efficiency of model classification can be improved.","['G06F16/735', 'G06F16/74', 'G06F16/75', 'G06F16/784']"
USRE50286E1,Intra-perinodular textural transition (ipris): a three dimenisonal (3D) descriptor for nodule diagnosis on lung computed tomography (CT) images,"Embodiments classify lung nodules by accessing a 3D radiological image of a region of tissue, the 3D image including a plurality of voxels and slices, a slice having a thickness; segmenting the nodule represented in the 3D image across contiguous slices, the nodule having a 3D volume and 3D interface, where the 3D interface includes an interface voxel; partitioning the 3D interface into a plurality of nested shells, a nested shell including a plurality of 2D slices, a 2D slice including a boundary pixel; extracting a set of intra-perinodular textural transition (Ipris) features from the 2D slices based on a normal of a boundary pixel of the 2D slices; providing the Ipris features to a machine learning classifier which computes a probability that the nodule is malignant, based, at least in part, on the set of Ipris features; and generating a classification of the nodule based on the probability.","['G06T7/0012', 'G06F18/2163', 'G06F18/2411', 'G06T15/08', 'G06T7/11', 'G06T7/174', 'G06T7/187', 'G06T7/194', 'G06T7/40', 'G06V10/764', 'G16H30/20', 'G06T2207/10081', 'G06T2207/10104', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20152', 'G06T2207/30064', 'G06T2207/30096', 'G06V2201/032', 'G16H20/17', 'G16H20/40', 'G16H30/40', 'G16H50/20']"
US5524065A,Method and apparatus for pattern recognition,"Pattern recognition system which provides an indication of the confidence with which a candidate is selected for an unknown pattern. The pattern recognition apparatus includes an image data input device, a host for segmenting the image data into unknown patterns, and a character recognition device for providing a candidate for each unknown pattern. The character recognition device includes a confidence level indicator for providing a confidence level indication. In one aspect, the confidence level indication is determined based on the proximity of an unknown pattern to a known pattern. In another aspect, the confidence level indication is determined based on the consistency with which the unknown pattern is recognized using different recognition functions. In yet another aspect, the confidence level indication is determined by ensuring that a candidate is not provided unless the candidate is closer than a predetermined distance from a known pattern. The pattern recognition apparatus may be provided in a stand-alone device including the image data input device, the host and the character recognition device in one integrated device, and this device may interface to a network bus. Alternatively, the pattern recognition apparatus may be distributed over a network bus. A preview function may be provided so as to preview results of recognition processing.",['G06V30/2504']
US7274803B1,Method and system for detecting conscious hand movement patterns and computer-generated visual feedback for facilitating human-computer interaction,"The present invention is a system and method for detecting and analyzing motion patterns of individuals present at a multimedia computer terminal from a stream of video frames generated by a video camera and the method of providing visual feedback of the extracted information to aid the interaction process between a user and the system. The method allows multiple people to be present in front of the computer terminal and yet allow one active user to make selections on the computer display. Thus the invention can be used as method for contact-free human-computer interaction in a public place, where the computer terminal can be positioned in a variety of configurations including behind a transparent glass window or at a height or location where the user cannot touch the terminal physically.",['G06V40/107']
US12175741B2,Systems and methods for a vision guided end effector,"Systems and method for an object from a plurality of objects are disclosed. An image of a scene containing the plurality of objects is obtained, and a segmentation map is generated for the objects in the scene. The shapes of the objects are determined based on the segmentation map. An end effector is adjusted in response to determining the shapes of the objects. The adjusting the end effector includes shaping the end effector according to at least one of the shapes of the objects. The plurality of objects is approached in response to the shaping of the end effector, and one of the plurality of objects is picked with the end effector.","['G06V20/10', 'B25J15/0061', 'B25J15/0608', 'B25J9/1612', 'B25J9/1679', 'B25J9/1697', 'G06T7/11', 'G06T7/50', 'G06T7/70', 'G05B2219/39489', 'G05B2219/39497', 'G05B2219/39543', 'G05B2219/39554', 'G05B2219/40532', 'G05B2219/40564']"
AU2020102091A4,Intelligent steel slag detection method and system based on convolutional neural network,"The invention discloses an intelligent steel slag detection method and system based on a convolutional neural network. The intelligent steel slag detection method includes 5 the following steps: steel slag image recognition: by taking a color steel slag image in a video frame image as an object, recognizing the color steel slag image in the video frame image by adopting an image recognition method based on an improved AlexNet convolutional neural network; steel flow target detection: detecting steel flow information in the color steel slag image, detecting a steel flow from a complex 10 background by using a target detection method based on a YOLOv3 convolutional neural network model, thereby accurately detecting a slag inclusion condition of the steel flow; and color steel slag image segmentation: preprocessing the color steel slag image by using a K-means clustering algorithm based on a Lab color space, and completely separating steel slag from molten steel by adopting an improved Otsu 15 image segmentation algorithm. Visual detection is performed on the steel slag by using a visual user interface system. The intelligent steel slag detection method is simple, practicable, low in cost and capable of distinguishing the steel slag from the molten steel, avoiding false detection and improving the real-time recognition precision of the steel slag image and the purity of the molten steel. 1/8 Digital steel flow Steel fowanalog video transmission data tran mission SDrive the imag Infrared camera | mag acquisition i acquisition card to 1 Remeive steel cad capture avideo flow video data Imag processing technology computer 4 Steel slag image Steel slag image Steel slag image recognition module detection module segmentation module Display a g Proportion of steel slag in real time Alann Fig. 1","['G06V20/40', 'G06F18/23213', 'G06N3/045', 'G06N3/08', 'G06V10/28', 'G06V10/56']"
US11069036B1,Method and system for real-time and offline de-identification of facial regions from regular and occluded color video streams obtained during diagnostic medical procedures,"Systems and techniques that facilitate real-time and/or offline de-identification of facial regions from regular and/or occluded color video streams obtained during diagnostic medical procedures are provided. A detection component can generate a bounding box substantially around a person in a frame of a video stream, can generate a heatmap showing key points or anatomical masks of the person based on the bounding box, and can localize a face or facial region of the person based on the key points or anatomical masks. An anonymization component can anonymize pixels in the frame that correspond to the face or facial region. A tracking component can track the face or facial region in a subsequent frame based on a structural similarity index between the frame and the subsequent frame being above a threshold. If the structural similarity index between the frame and the subsequent frame is above the threshold, the tracking component can track the face or facial region in the subsequent frame without having the detection component generate a bounding box or a heatmap in the subsequent frame, and the anonymization component can anonymize pixels in the subsequent frame corresponding to the tracked face or facial region.","['G06T5/002', 'G16H30/40', 'G06K9/00241', 'G06N20/20', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/096', 'G06T5/70', 'G06T7/246', 'G06T7/248', 'G06V10/7715', 'G06V40/164', 'G06V40/165', 'G16H50/20', 'G16H50/70', 'G06T2207/10016', 'G06T2207/20012', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30201', 'G16H30/20']"
US9852513B2,Tracking regions of interest across video frames with corresponding depth maps,"Techniques related to tracking regions of interest across video frames with corresponding depth maps are discussed. Such techniques may include motion tracking feature points of a region of interest, filtering the tracked feature points, determining a center of the region of interest, verifying a depth consistency of the center, and, if the number of tracked feature points falls below a threshold or the center fails depth consistency for a current frame, re-initializing the region of interest in a prior frame.","['G06T7/246', 'G06T7/2006', 'G06T7/215', 'G06T2200/04', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20104']"
US10303986B2,"Automated measurement of brain injury indices using brain CT images, injury data, and machine learning","A decision-support system and computer implemented method automatically measures the midline shift in a patient's brain using Computed Tomography (CT) images. The decision-support system and computer implemented method applies machine learning methods to features extracted from multiple sources, including midline shift, blood amount, texture pattern and other injury data, to provide a physician an estimate of intracranial pressure (ICP) levels. A hierarchical segmentation method, based on Gaussian Mixture Model (GMM), is used. In this approach, first an Magnetic Resonance Image (MRI) ventricle template, as prior knowledge, is used to estimate the region for each ventricle. Then, by matching the ventricle shape in CT images to the MRI ventricle template set, the corresponding MRI slice is selected. From the shape matching result, the feature points for midline estimation in CT slices, such as the center edge points of the lateral ventricles, are detected. The amount of shift, along with other information such as brain tissue texture features, volume of blood accumulated in the brain, patient demographics, injury information, and features extracted from physiological signals, are used to train a machine learning method to predict a variety of important clinical factors, such as intracranial pressure (ICP), likelihood of success a particular treatment, and the need and/or dosage of particular drugs.","['G06K9/629', 'G06T7/0012', 'G06F18/253', 'G06T7/42', 'G06T7/68', 'G06V10/806', 'G06F19/321', 'G06K2209/055', 'G06T2207/10081', 'G06T2207/20056', 'G06T2207/20064', 'G06T2207/30016', 'G06V2201/033', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'G16H50/30', 'Y02A90/10', 'Y02A90/26']"
CN113095176B,A method and device for background restoration of video data,"The invention discloses a method and a device for carrying out background reduction on video data, and relates to the technical field of artificial intelligence. The method comprises the steps of conducting key frame sampling on a video to be processed based on a sampling threshold value to obtain a key frame set, conducting foreground detection on each key frame in the key frame set to determine foreground target information in each key frame, obtaining a background template and a background library based on the foreground detection, and conducting background filling on the background template according to second frame data and foreground target information in the background library to obtain background reduction data of the video to be processed. According to the method, partial frame data of the video are extracted, foreground and background are distinguished by utilizing target detection, the foreground and the background on a single frame are rapidly and effectively separated, and filling is carried out through a plurality of background images, so that a restored background environment is obtained efficiently.","['G06V20/40', 'G06N3/045', 'G06V10/25']"
US11775056B2,"System and method using machine learning for iris tracking, measurement, and simulation","This document relates to hybrid eye center localization using machine learning, namely cascaded regression and hand-crafted model fitting to improve a computer. There are proposed systems and methods of eye center (iris) detection using a cascade regressor (cascade of regression forests) as well as systems and methods for training a cascaded regressor. For detection, the eyes are detected using a facial feature alignment method. The robustness of localization is improved by using both advanced features and powerful regression machinery. Localization is made more accurate by adding a robust circle fitting post-processing step. Finally, using a simple hand-crafted method for eye center localization, there is provided a method to train the cascaded regressor without the need for manually annotated training data. Evaluation of the approach shows that it achieves state-of-the-art performance.","['G06F3/012', 'G06F18/217', 'G06F18/24323', 'G06F3/013', 'G06T7/73', 'G06V10/764', 'G06V10/766', 'G06V40/168', 'G06V40/19', 'G06V40/193', 'G06V40/197', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/30201']"
CN105913456B,Saliency detection method based on region segmentation,"The invention discloses a kind of saliency detection methods of region segmentation, mainly solve the problems, such as that existing saliency detection method Detection accuracy is low, it the steps include: that 1. pairs of video frames carry out linear iteraction cluster, obtain super-pixel block, extract super-pixel block static nature；2. utilizing variation optical flow method, the behavioral characteristics of super-pixel block are obtained；3. static nature and behavioral characteristics are merged to obtain eigenmatrix, then K-means cluster is carried out to eigenmatrix；4. pair every a kind of progress linear regression model (LRM) training respectively, obtains regression model, 5. obtain the saliency value of test set super-pixel block with the mapping relations that regression model rebuilds test set sample and conspicuousness degree, and then obtain the notable figure of cycle tests.The present invention enhances feature space and time representation ability compared to traditional saliency algorithm, reduces influence of the illumination to detection effect, can be used for pretreatment early period of video frequency object tracking, Video segmentation.",['G06T2207/10016']
US11657631B2,"Scalable, flexible and robust template-based data extraction pipeline","A computer-implemented method for extracting information from a document, for example an official document, is disclosed. The method comprises acquiring an input image comprising a document portion; performing image segmentation on the input image to form a binary input image that distinguishes the document portion from the remaining portion of the input image; estimating a first image transform to align the binary input image to a binary template image, using the first image transform on the input image to form an intermediate image; estimating a second image transform to align the intermediate image to a template image; using the second image transform on the intermediate image to form an output image; and extracting a field from the output image using a predetermined field of the template image.","['G06V30/40', 'G06F18/214', 'G06F18/24', 'G06K9/6256', 'G06K9/6267', 'G06T3/40', 'G06T7/11', 'G06T7/136', 'G06T7/30', 'G06T7/70', 'G06V10/22', 'G06V10/7515', 'G06V30/414', 'G06T2207/20132', 'G06T2207/30176', 'G06V30/10']"
US6608930B1,Method and system for analyzing video content using detected text in video frames,"There is disclosed, for use in video text analysis system, a video processing device for searching video streams for one or more user-selected image text attributes. The video processing device comprises an image processor capable detecting and extracting image text from video frames, determining attributes of the extracted image text, comparing the extracted image text attributes and the user-selected image text attributes, and, if a match occurs, modifying, transferring, and/or labeling at least a portion of the video stream in accordance with user commands. The invention uses the user-selected image text attributes to search through an archive of video clips to 1) locate particular types of events, such as news programs or sports events; 2) locate programs featuring particular persons or groups; 3) locate programs by name; 4) save or remove all or some commercials, and to otherwise sort, edit, and save all of, or portions of, video clips according to image text that appears in the frames of the video clips.","['G06V20/635', 'G06F16/738', 'G06F16/78', 'G06F16/7844', 'G06F16/7854']"
US9741125B2,Method and system of background-foreground segmentation for image processing,"Techniques for a system, article, and method of background-foreground segmentation for image processing may include obtaining pixel data including both non-depth data and depth data for at least one image, where the non-depth data includes color data or luminance data or both and associated with the pixels; determining whether a portion of the image is part of a background or foreground of the image based on the depth data and without using the non-depth data; and determining whether a border area between the background and foreground formed by using the depth data are part of the background or foreground depending on the non-depth data without using the depth data.","['G06T7/136', 'G06T7/11', 'G06T7/0081', 'G06T7/0051', 'G06T7/0085', 'G06T7/12', 'G06T7/174', 'G06T7/187', 'G06T7/194', 'G06T7/408', 'G06T7/50', 'G06T7/507', 'G06T7/90', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20116', 'G06T2207/20144', 'G06T2207/20148', 'G06T2207/30196']"
US9848186B2,Graphical system with enhanced stereopsis,"A computer system that provides stereoscopic images is described. During operation, the computer system generates the stereoscopic images at a location corresponding to a viewing plane based on data having a discrete spatial resolution, where the stereoscopic images include image parallax. Then, the computer system scales objects depicted in the stereoscopic images so that depth acuity associated with the image parallax is increased, where the scaling is based on the spatial resolution and a viewing geometry associated with a display. For example, the viewing geometry may include a distance from an individual that views the stereoscopic images on the display and the display. Alternatively, the viewing geometry may include a focal point of the individual. Next, the computer system provides the resulting stereoscopic images to the display. In this way, the computer system may optimize the depth acuity for data having discrete sampling.","['H04N13/383', 'H04N13/128', 'H04N13/0484', 'G06T15/20', 'G06T3/40', 'H04N13/0022', 'H04N13/0278', 'H04N13/279', 'G06T2200/04', 'H04N2213/006']"
CN113168510B,Refining shape prior segmentation objects,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for performing instance segmentation by detecting and segmenting individual objects in an image. In one aspect, a method includes: processing the image to generate data identifying an area of the image depicting the particular object; obtaining data defining a plurality of example object partitions; generating a respective weight value for each of the example object partitions; for each pixel of a plurality of pixels in a region of the image, determining a score characterizing a likelihood that the pixel is included in a particular object depicted in the region of the image using (i) an example object segmentation and (ii) a weight value of the example object segmentation; and generating a segmentation of the particular object depicted in the region of the image using the fraction of pixels in the region of the image.","['G06V10/82', 'G06F18/2413', 'G06T7/10', 'G06V10/25', 'G06V10/26', 'G06V10/454', 'G06V10/764', 'G06V10/7715', 'G06V10/774', 'G06V20/10', 'G06F18/23', 'G06T2207/20081']"
US7274800B2,Dynamic gesture recognition from stereo sequences,"According to an embodiment, an apparatus and method are disclosed for dynamic gesture recognition from stereo sequences. In an embodiment, a stereo sequence of images of a subject is obtained and a depth disparity map is generated from the stereo sequence. The system is initiated automatically based upon a statistical model of the upper body of the subject. The upper body of the subject is modeled as three planes, representing the torso and arms of the subject, and three Gaussian components, representing the head and hands of the subject. The system tracks the upper body of the subject using the statistical upper body model and extracts three-dimensional features of the gestures performed. The system recognizes the gestures using recognition units, which, under a particular embodiment, utilizes hidden Markov models for the three-dimensional gestures.","['G06F3/017', 'G06V40/20']"
WO2020211499A1,Self-checkout method and device for commodities,"A self-checkout method and device for commodities. The method comprises: acquiring an image obtained by a camera photographing a commodity placed on a checkout table (501), and then dividing the image into image regions (502); detecting a code region of a commodity code in any image region (503); if a code region is detected, then identifying a code in the code region, and determining the category of a commodity comprised in the image region according to the identified code (504); if the code region is not detected or the code cannot be identified, identifying the category of the commodity on the basis of the image region by means of visual identification (505); and finally, determining a pricing result of the commodity according to the category of the commodity (506). The scanning speed and accuracy of the commodity barcode scheme, as well as the user experience of the visual identification scheme may be taken into account.","['G06K7/1413', 'G06K7/1417', 'G06K7/1443', 'G06V10/25', 'G06V10/267', 'G07G1/0081']"
EP4182895A1,Methods and systems for performing on-device image to text conversion,"A method and system for performing on-device image to text conversion are provided. Embodiments herein relates to the field of performing image to text conversion and more particularly to performing on-device image to text conversion with an improved accuracy. A method performing on-device image to text conversion is provided. The method includes language detection from an image, understanding of text in an edited image and using a contextual and localized lexicon set for post optical character recognition (OCR) correction.","['G06V30/148', 'G06V30/153', 'G06V10/82', 'G06V30/1444', 'G06V30/147', 'G06V30/18076', 'G06V30/22', 'G06V30/226', 'G06V30/2455', 'G06V30/268']"
CN113498530B,Object size marking system and method based on local visual information,"A method for estimating, by a computing system, a tight bounding box includes controlling a scanning system including one or more depth cameras to capture visual information of a scene including one or more objects, detecting the one or more objects of the scene based on the visual information, separating each of the one or more objects from a frame of the scene to generate one or more 3D models corresponding to the one or more objects, the one or more 3D models including a local 3D model of a corresponding one of the one or more objects, extrapolating a more complete 3D model of the corresponding one of the one or more objects based on the local 3D model, and estimating a tight bounding box of the corresponding one of the one or more objects based on the more complete 3D model.","['G06T7/0002', 'G06N3/084', 'G06T15/00', 'G06T17/00', 'G06T3/4046', 'G06T7/10', 'H04N13/20', 'G06T2207/10012', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2210/12']"
US10168794B2,Motion-assisted visual language for human computer interfaces,"Embodiments of the invention recognize human visual gestures, as captured by image and video sensors, to develop a visual language for a variety of human computer interfaces. One embodiment of the invention provides a computer-implement method for recognizing a visual gesture portrayed by a part of human body such as a human hand, face or body. The method includes steps of receiving the visual signature captured in a video having multiple video frames, determining a gesture recognition type from multiple gesture recognition types including shaped-based gesture, position-based gesture, motion-assisted and mixed gesture that combining two different gesture types. The method further includes steps of selecting a visual gesture recognition process based on the determined gesture type and applying the selected visual gesture recognition process to the multiple video frames capturing the visual gesture to recognize the visual gesture.","['G06F3/017', 'G06F3/0482', 'G06F3/04842', 'G06K9/00355', 'G06V40/28']"
US11727596B1,Controllable video characters with natural motions extracted from real-world videos,"A video generation system is described that extracts one or more characters or other objects from a video, re-animates the character, and generates a new video in which the extracted characters. The system enables the extracted character(s) to be positioned and controlled within a new background scene different from the original background scene of the source video. In one example, the video generation system comprises a pose prediction neural network having a pose model trained with (i) a set of character pose training images extracted from an input video of the character and (ii) a simulated motion control signal generated from the input video. In operation, the pose prediction neural network generates, in response to a motion control input from a user, a sequence of images representing poses of a character. A frame generation neural network generates output video frames that render the character within a scene.","['G06T7/73', 'G06T7/75', 'G06N20/20', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/0475', 'G06N3/08', 'G06N3/09', 'G06N3/094', 'G06T7/194', 'G06T7/20', 'G06N3/048', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30221']"
US9747498B2,Biometric recognition,"A fused image of the person's hand is accessed, the fused image having been generated using a segmented graylevel image and a segmented color image. The hand in the fused image is identified. One or more finger tips and one or more finger valleys in the fused image are identified. One or more fingers of the hand are segmented, based on the identified finger tips and finger valleys. The one or more fingers of the hand are labeled. One or more features for each finger of the hand are determined.","['G06K9/00382', 'G06V40/11', 'G06F18/22', 'G06F18/251', 'G06K9/4652', 'G06K9/6201', 'G06K9/6289', 'G06K9/78', 'G06V10/803', 'G06T2207/10004', 'G06T2207/10024', 'G06T2207/30196']"
US10395147B2,Method and apparatus for improved segmentation and recognition of images,"A system for and method for improved computer vision image processing and image segmentation and recognition is disclosed. The system and method utilizes a cost function approach for improved image segmentation and recognition. In particular, a global cost function is defined and then the global cost function is minimized. This global cost function takes into account two processing pipelines of data determined by the operation of two different processing technologies upon the target input image. Constraints are utilized to ensure data consistency across the model and between the data pipelines. The system and method are useful for processing low quality images containing alphanumeric data such as floorplan images.","['G06K9/726', 'G06V30/422', 'G06K9/00476', 'G06K9/6212', 'G06T7/143', 'G06V30/274', 'G06V30/414', 'G06K2209/01', 'G06T2207/20084', 'G06V30/10']"
CN109325952B,Fashionable garment image segmentation method based on deep learning,"The invention relates to a fashionable garment image segmentation method based on deep learning, which comprises the following steps of: constructing a deep neural network clothing model, designing a loss function of reverse error propagation and training a model; the deep neural network clothing segmentation model comprises a feature extraction module, a clothing semantic information extraction module and a clothing segmentation prediction module; the loss function comprises a regression function of key point positions, a key visibility loss function, a clothing prediction category cross entropy loss function with weight and a regression loss function of clothing positions; the model training strategy comprises a weight parameter initialization method, data preprocessing, an optimization algorithm and a training step. Its advantages are: the method can automatically divide and recognize the clothes of the upper half body and the clothes of the lower half body of the person in the complex image and the matching of the clothes of the whole body, and is favorable for deep learning and network training designed aiming at the field of fashion clothes.","['G06T7/10', 'G06T2207/20081', 'G06T2207/20084']"
EP4099220A1,"Processing apparatus, method and storage medium","This application discloses a processing apparatus, and relates to the field of artificial intelligence, and in particular, to the field of computer vision. The processing apparatus includes a collection module and a training module, the training module includes a backbone network and a region proposal network RPN layer, the backbone network is connected to the RPN layer, and the RPN layer includes a class activation map CAM unit. The collection module is configured to obtain an image, where the image includes an image with an instance-level label and an image with an image-level label. The backbone network is used to output a feature map of the image based on the image obtained by the collection module. The RPN layer is used to determine a proposal region of the image based on the feature map. The RPN layer is further used to determine, by using the CAM unit, a CAM corresponding to the proposal region, and determine, based on the CAM, a probability that the proposal region belongs to a foreground. In the solution provided in this application, both the image with an instance-level label and the image with an image-level label may be used for training, to obtain a high-performance object detection model.","['G06V10/82', 'G06F18/241', 'G06N3/042', 'G06N3/045', 'G06N3/0464', 'G06N3/063', 'G06N3/084', 'G06N3/09', 'G06V10/25', 'G06V10/28', 'G06V10/454', 'G06V10/7715', 'G06V10/774', 'G06V10/7753', 'G06V10/84', 'G06V20/00', 'G06V10/95']"
US9372546B2,Hand pointing estimation for human computer interaction,"Hand pointing has been an intuitive gesture for human interaction with computers. A hand pointing estimation system is provided, based on two regular cameras, which includes hand region detection, hand finger estimation, two views' feature detection, and 3D pointing direction estimation. The technique may employ a polar coordinate system to represent the hand region, and tests show a good result in terms of the robustness to hand orientation variation. To estimate the pointing direction, Active Appearance Models are employed to detect and track, e.g., 14 feature points along the hand contour from a top view and a side view. Combining two views of the hand features, the 3D pointing direction is estimated.","['G06F3/017', 'G06F3/0304', 'G06K9/00335', 'G06K9/00355', 'G06K9/00375', 'G06K9/00624', 'G06K9/4652', 'G06K9/52', 'G06K9/621', 'G06T7/0042', 'G06T7/0097', 'G06T7/174', 'G06T7/194', 'G06T7/73', 'G06V10/7557', 'G06V40/107', 'G06V40/20', 'G06V40/28', 'G06K2009/4666', 'G06K9/4614', 'G06T2207/20144', 'G06V10/446', 'G06V10/467']"
US9411849B2,"Method, system and computer storage medium for visual searching based on cloud service","A method, system and computer storage medium for visual searching based on cloud service is disclosed. The method includes: receiving, from a client, an image recognition request of cloud service, the request containing image data; forwarding, according to a set classified forwarding rule, the image data to a corresponding classified visual search service; recognizing, by the respective corresponding classified visual search services, corresponding classified type information in the image data, and determining a corresponding name of the image data in accordance with the respective classified type information, and obtaining a classified visual search result; summarizing and sending, to a client, the classified visual search result of the corresponding classified visual search service. By detection and recognition of the classified type information of the image data, the comprehensive feature information of a picture is obtained, based on which further applications are allowed, and thus the user experience is improved.","['G06F16/24', 'G06F17/30386', 'G06F16/434', 'G06F16/583', 'G06F17/30047', 'G06F17/30247', 'G06F18/22', 'G06K9/4671', 'G06K9/6215', 'G06V10/462']"
WO2022063199A1,"Pulmonary nodule automatic detection method, apparatus and computer system","A pulmonary nodule automatic detection method, apparatus and computer system. The method comprises the following steps: acquiring a CT image to be subjected to detection; performing filtering enhancement processing on the CT image to be subjected to detection, so as to obtain a lung enhanced CT image sequence; performing segmentation processing on the CT image sequence by using a threshold method to obtain an image which only includes a lung parenchyma region; cutting an image obtained by a lung parenchyma segmentation module into several image blocks, and obtaining a region of interest by means of a multi-scale feature fusion U-Net network model; and performing automatic detection and recognition on the region of interest by using a 3D CNN model, so as to obtain a pulmonary nodule detection result. The method has the advantages of a high detection sensitivity and precision, etc.","['G06T7/0012', 'G06F18/253', 'G06N3/045', 'G06T7/11', 'G06T7/136', 'G06V10/25', 'G06T2207/10081', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30064']"
CN111428726B,"Panoptic segmentation method, system, device and storage medium based on graph neural network","The invention discloses a panorama segmentation method based on a graph neural network, which comprises the following steps: extracting a plurality of target features from the picture; segmenting the head network through an example to obtain the foreground category probability, the background category probability and a mask result of the picture, and segmenting the head network through semantics to obtain a preliminary semantic segmentation result of the picture; processing the new foreground image through the foreground class probability to generate an example classification result, and extracting a target example segmentation mask from the example classification result according to the mask result; processing the new background image through the background category probability and the preliminary semantic segmentation result to generate a target semantic segmentation result; and fusing the target instance segmentation mask and the target semantic segmentation result by adopting a heuristic algorithm to generate a panoramic segmentation result. The invention also discloses a panoramic segmentation system based on the graph neural network, computer equipment and a computer readable storage medium. By adopting the method and the device, the panoramic segmentation effect of the picture can be optimized by utilizing the mutual relation between the objects.","['G06V10/267', 'G06N3/045', 'G06T7/194', 'G06V10/25']"
CN111415363B,Image edge identification method,"The invention discloses an image edge identification method, which comprises the steps of firstly creating a canvas, obtaining a two-dimensional context object of the canvas, rendering an image, obtaining pixel data of an image to be subjected to edge identification, carrying out graying processing on the image, carrying out re-rendering on the image subjected to the graying processing to obtain an image subjected to the edge identification, then obtaining the pixel data of the image subjected to the edge identification, carrying out binarization processing on the image subjected to the edge identification by using a maximum inter-class variance method, obtaining the pixel data of the image subjected to the binarization processing, comparing the pixel value of a certain pixel point with the pixel values of the pixel point distributed along four directions, judging whether the pixel value of the pixel point is the same as the pixel values of the pixel point in the four directions, realizing the edge identification of the image, saving the image positioning time, obtaining a binarization threshold value of a target by adopting a self-adaptive method, being more accurate and effective, reducing the influence of external factors by adopting an improved first-order gradient transformation and a local gradient integration method, and being capable of more accurately realizing the positioning of the external boundary.","['G06T7/13', 'G06T7/136']"
US10789462B2,Weakly and fully labeled mammogram classification and localization with a dual branch deep neural network,"Embodiments may classify medical images, such as mammograms, using weakly labeled data sets, fully labeled data sets, or a combination of both. For example, a method may comprise receiving a whole medical image, extracting a plurality of image patches from the whole medical image, each image patch including a portion of the whole image, generating a representation of features found in the plurality of image patches, classifying each image patch as including a malignant abnormality, a benign abnormality or not including an abnormality to form a classification for each patch, in parallel, the detection branch computes a malignant distribution over patches and a benign distribution over patches resulting in ranking of patches compare to one another for malignancy, and ranking of patches compare to one another for benign. Patches classification probabilities and ranking are multiplied and summed for malignant and benign, resulting in global malignant probability and global benign probability.","['G06T7/0012', 'G06K9/00536', 'G06K9/00523', 'G06K9/00671', 'G06N3/045', 'G06N3/0464', 'G06N3/048', 'G06N3/08', 'G06N3/0895', 'G06N3/09', 'G06N3/096', 'G06V10/50', 'G06V10/764', 'G06V10/82', 'G06V20/20', 'G06F2218/08', 'G06F2218/12', 'G06T2207/10088', 'G06T2207/20021', 'G06T2207/20084', 'G06T2207/30068', 'G06T2207/30096', 'G06V2201/03']"
CA2837470C,System for finger recognition and tracking,"A system and method are disclosed relating to a pipeline for generating a computer model of a target user, including a hand model of the user's hands and fingers, captured by an image sensor in a NUI system. The computer model represents a best estimate of the position and orientation of a user's hand or hands. The generated hand model may be used by a gaming or other application to determine such things as user gestures and control actions.","['A63F13/428', 'G06T7/20', 'G06F3/01', 'A63F13/213', 'A63F13/44', 'G06F3/0425', 'G06F3/04883', 'G06T11/20', 'G06T7/251', 'G06V40/113', 'G06V40/28', 'A63F13/812', 'A63F2300/1087', 'A63F2300/6045', 'A63F2300/6607', 'A63F2300/6615', 'G06T2207/10028', 'G06T2207/30196']"
US6188777B1,Method and apparatus for personnel detection and tracking,"Techniques from computer vision and computer graphics are combined to robustly track a target (e.g., a user) and perform a function based upon the image and/or the identity attributed to the target's face. Three primary modules are used to track a user's head: depth estimation, color segmentation, and pattern classification. The combination of these three techniques allows for robust performance despite unknown background, crowded conditions, and rapidly changing pose or expression of the user. Each of the modules can also provide an identity classification module with valuable information so that the identity of a user can be estimated. With an estimate of the position of a target in 3-D and the target's identity, applications such as individualized computer programs or graphics techniques to distort and/or morph the shape or apparent material properties of the user's face can be performed. The system can track and respond to a user's face in real-time using completely passive and non-invasive techniques.",['G06V40/10']
US10824862B2,Three-dimensional object detection for autonomous robotic systems using image proposals,"Provided herein are methods and systems for implementing three-dimensional perception in an autonomous robotic system comprising an end-to-end neural network architecture that directly consumes large-scale raw sparse point cloud data and performs such tasks as object localization, boundary estimation, object classification, and segmentation of individual shapes or fused complete point cloud shapes.","['G06K9/00664', 'G06V10/82', 'G06F18/241', 'G06F18/2414', 'G06K9/00208', 'G06K9/00791', 'G06K9/3233', 'G06K9/6268', 'G06K9/6273', 'G06K9/66', 'G06V20/10', 'G06V20/56', 'G06V20/647', 'G06V30/19173', 'G06V30/194']"
US8787663B2,Tracking body parts by combined color image and depth processing,"A method for image processing includes receiving a depth image of a scene containing a human subject and receiving a color image of the scene containing the human subject. A part of a body of the subject is identified in at least one of the images. A quality of both the depth image and the color image is evaluated, and responsively to the quality, one of the images is selected to be dominant in processing of the part of the body in the images. The identified part is localized in the dominant one of the images, while using supporting data from the other one of the images.","['G06T7/521', 'G06F18/251', 'G06T7/248', 'G06T7/277', 'G06T7/73', 'G06T7/90', 'G06V10/803', 'G06V40/107', 'G06V40/28', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20076', 'G06T2207/30196', 'G06T2207/30241']"
CN109816011B,Video key frame extraction method,"The invention discloses a method for generating a portrait segmentation model and a video key frame extraction method, wherein the method for generating the portrait segmentation model comprises the following steps: acquiring a training set, wherein the training set comprises a plurality of image groups consisting of a current frame image, a previous frame mask image and an artificially marked current frame mask image; inputting an array consisting of a current frame image and a previous frame mask image into a pre-trained portrait segmentation model, wherein the portrait segmentation model is an end-to-end model and comprises a coding model and a decoding model, the coding model is suitable for extracting the characteristics of the current frame image from the array, and the decoding model is suitable for outputting the current frame mask image based on the extracted characteristics; and training a portrait segmentation model based on the manually marked current frame mask image and the output current frame mask image to obtain the generated portrait segmentation model. According to the scheme, the video frames with large human image motion difference can be extracted as key frames based on the human image mask image, so that the motion behaviors of people can be well expressed.",[]
US11120209B2,Extracting structured information from a document containing filled form images,"A system and process for extracting information from filled form images is described. In one example the claimed invention first extracts textual information and the hierarchy in a blank form. This information is then used to extract and understand the content of filled forms. In this way, the system does not have to analyze from the beginning each filled form. The system is designed so that it remains as generic as possible. The number of hard coded rules in the whole pipeline was minimized to offer an adaptive solution able to address the largest number of forms, with various structures and typography. The system is also created to be integrated as a built-in function in a larger pipeline. The form understanding pipeline could be the starting point of any advanced Natural Language Processing application.","['G06F40/137', 'G06F40/103', 'G06F40/174', 'G06F40/186', 'G06F40/253', 'G06F40/30', 'G06K9/00449', 'G06K9/00456', 'G06K9/00463', 'G06V10/267', 'G06V10/273', 'G06V30/412', 'G06V30/413', 'G06V30/414', 'G06V10/473']"
US20240061532A1,Method and Device for Detecting a Touch Between a First Object and a Second Object,"The present disclosure is related to a method and device for detecting a touch between at least part of a first object and at least part of a second object, wherein the at least part of the first object has a different temperature than the at least part of the second object. The method includes providing at least one thermal image of a portion of the second object, determining in at least part of the at least one thermal image a pattern which is indicative of a particular value or range of temperature or a particular value or range of temperature change, and using the determined pattern for detecting a touch between the at least part of the first object and the at least part of the second object.","['G06F3/0425', 'G02B27/0176', 'G06F3/011', 'G06F3/017', 'G06T19/006', 'G06V10/143', 'G06V40/28', 'H04N23/23', 'H04N5/33', 'G02B2027/0178', 'G06F3/005']"
US8861881B2,"Image processing apparatus, method thereof, program, and image capturing apparatus","An image processing apparatus stores model information representing a subject model belonging to a specific category, detects the subject from an input image by referring to the model information, determines a region for which an image correction is to be performed within a region occupied by the detected subject in the input image, stores, for a local region of the image, a plurality of correction data sets representing correspondence between a feature vector representing a feature before correction and a feature vector representing a feature after correction, selects at least one of the correction data sets to be used to correct a local region included in the region determined to undergo the image correction, and corrects the region determined to undergo the image correction using the selected correction data sets.","['G06T5/73', 'G06T5/003', 'G06T5/005', 'G06T5/77', 'G06T7/0046', 'G06T7/75', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20016', 'G06T2207/20064', 'G06T2207/20076']"
US7516005B2,Method and apparatus for locating an object of interest within an image,"Systems and methods are provided for identifying a position of a head of a vehicle occupant from a plurality of images of a vehicle interior. Image data is translated from the plurality of images to produce a virtual side view image of the vehicle interior (220). A largest grouping of pixels in the side view image having values within a desired range is identified (230). The grouping includes a plurality of pixel columns. A contour including uppermost pixels of each pixel column is produced and peaks are located within the contour (240, 250, and 260). The contour peaks are analyzed according to a heuristic voting algorithm to identify a peak location associated with the head of the vehicle occupant (270).","['B60R21/01538', 'G06T7/73', 'G06V40/103', 'G06T2207/10012', 'G06T2207/30268']"
US9363499B2,"Method, electronic device and medium for adjusting depth values","A depth processing method, an electronic device and a medium are provided. The depth processing method includes: obtaining a color image and a depth map corresponding to the color image; extracting a plurality of regions from at least one of the depth map and the color image; obtaining region information of the regions, and classifying the regions into at least one of a region-of-interest and a non-region-of-interest according to the region information, wherein the region information comprises area information and edge information; and adjusting a plurality of depth values in the depth map according to the region information.","['H04N13/0022', 'H04N13/128', 'G06K9/00362', 'G06K9/3233', 'G06V10/25', 'G06V40/10', 'H04N13/0018', 'H04N13/122', 'H04N23/62', 'H04N5/23216']"
US11227147B2,"Face image processing methods and apparatuses, and electronic devices","A face image processing method includes: segmenting a face in an image to be processed to obtain at least one organ image block; respectively inputting the at least one organ image block into at least one first neural network, where at least two different types of organs correspond to different first neural networks; and extracting key point information of an organ from the respective input organ image block by the at least one first neural network to respectively obtain key point information of at least one corresponding organ of the face.","['G06F18/214', 'G06V40/171', 'G06K9/00281', 'G06F18/217', 'G06F18/29', 'G06K9/34', 'G06K9/6256', 'G06K9/6262', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/0895', 'G06N3/09', 'G06V10/44', 'G06V10/764', 'G06V10/82', 'G06V10/84', 'G06V40/174']"
US10394334B2,Gesture-based control system,"A method and system for human computer interaction using hand gestures is presented. The system permits a person to precisely control a computer system without wearing an instrumented glove or any other tracking device. In one embodiment, two cameras observe and record images of a user's hands. The hand images are processed by querying a database relating hand image features to the 3D configuration of the hands and fingers (i.e. the 3D hand poses). The 3D hand poses are interpreted as gestures. Each gesture can be interpreted as a command by the computer system. Uses for such a system include, but are not limited to, computer aided design for architecture, mechanical engineering and scientific visualization. Computer-generated 3D virtual objects can be efficiently explored, modeled and assembled using direct 3D manipulation by the user's hands.","['G06F3/017', 'G06F3/011', 'G06K9/00355', 'G06K9/00389', 'G06T7/74', 'G06V40/113', 'G06V40/28', 'G06T2207/20216', 'G06T2207/30196']"
CN110472616B,"Image recognition method and device, computer equipment and storage medium","The invention discloses an image recognition method, an image recognition device, computer equipment and a storage medium, and belongs to the technical field of image processing. According to the invention, the original image is subjected to color space conversion to obtain the H channel image and the DAB channel image, the two-channel images of the H channel and the DAB channel are respectively subjected to preprocessing and fusion processing to obtain the first fusion image and the second fusion image, and the image recognition is carried out based on the first fusion image and the second fusion image.","['G06F18/24', 'G06F18/25', 'G06T5/50', 'G06T7/0012', 'G06T7/10', 'G06V10/56', 'G06V20/69', 'G06V20/695', 'G06T2207/20032', 'G06T2207/20221', 'G06T2207/30096']"
US10872416B2,Object oriented image editing,"Implementations generally relate to object based image editing. In some implementations, a method includes segmenting an image into object data by identifying one or more object classifications in the image and storing at least one locator for one or more regions of the image corresponding to each instance of the object classification. The method further includes receiving a selection of a representative portion of the segmented image from a user, and matching the representative portion with the object data to determine at least one matched object classification associated with the representative portion. The method further includes presenting the user with one or more of the matched object classifications for the user to instruct one or more edit operations to be applied to at least one object represented by the matched object classification.","['G06T11/60', 'G06F18/22', 'G06F18/24', 'G06K9/00221', 'G06K9/00664', 'G06K9/3233', 'G06K9/46', 'G06K9/6215', 'G06K9/6267', 'G06T5/00', 'G06T7/11', 'G06V20/10', 'G06V40/16', 'G06K2009/4666', 'G06T2207/20012', 'G06T2207/20104', 'G06T2207/30201']"
US10739849B2,Selective peripheral vision filtering in a foveated rendering system,Gaze tracking data representing a viewer's gaze with respect to one or more images presented to the viewer is used to generate foveated image data representing one or more foveated images characterized by a higher level of detail within one or more regions of interest and a lower level of detail outside the regions of interest. The image data for portions outside the one or more regions of interest is selectively filtered to reduce visual artifacts due to contrast resulting from the lower level of detail before compositing foveated images for presentation.,"['G06F3/012', 'G02B27/017', 'G02B27/0179', 'G06F1/163', 'G06F1/1694', 'G06F3/011', 'G06F3/013', 'G06F3/0304', 'G06T17/20', 'G02B2027/0118', 'G02B2027/014', 'G02B2027/0147', 'G02B2027/0187', 'G06K9/00597', 'G06T2210/36', 'G06V40/18']"
CN111695483B,"Vehicle violation detection method, device and equipment and computer storage medium","The application provides a vehicle violation detection method, a device, equipment and a computer storage medium, relates to the technical field of computers, and aims to optimize a process of detecting a vehicle violating the regulations based on a machine vision technology. The method comprises the following steps: acquiring a target area image acquired by vehicle-mounted image acquisition equipment; carrying out image recognition on the target area image by using the trained lightweight convolutional neural network, and determining road elements and vehicle information in the target area image; and when determining that the target area image has a vehicle meeting the violation conditions based on the road elements and the vehicle information in the target area image, sending violation report information at least comprising the target area image to an application server, wherein the lightweight convolutional neural network is obtained by training image sample data subjected to data annotation and data enhancement, and the data annotation comprises annotated road elements and vehicle information. The method directly detects the violation of the vehicle on the vehicle-mounted equipment, and saves the flow consumption in the violation of the vehicle detection process.","['G06V20/56', 'G06N3/045', 'G06N3/08', 'G06V20/41']"
US10127437B2,Unified face representation for individual recognition in surveillance videos and vehicle logo super-resolution system,A method is disclosed of recognizing a logo of a vehicle. The method including obtaining a limited number of high resolution logos; populating a training dataset for each of the limited number of high resolution logos using gamma transformations; obtaining a low resolution image of a vehicle; and matching the low resolution image of the vehicle with the training dataset.,"['G06V40/171', 'G06K9/00288', 'G06K9/00281', 'G06K9/00315', 'G06K9/6206', 'G06V10/754', 'G06V40/172', 'G06V40/176', 'G06K2009/4666', 'G06K2209/23', 'G06K9/00241', 'G06K9/00785', 'G06K9/6212', 'G06V10/467', 'G06V10/758', 'G06V20/54', 'G06V2201/08', 'G06V40/164']"
US11436437B2,Three-dimension (3D) assisted personalized home object detection,The disclosure relates to technology for object detection in which a vision system receives training datasets including a set of two-dimensional (2D) images of the object from multiple views. A set of 3D models is reconstructed from the set of 2D images based on salient points of the object selected during reconstruction to generate one or more salient 3D models of the object that is an aggregation of the salient points of the object in the set of 3D models. A set of training 2D-3D correspondence data are generated between the set of 2D images in a first training dataset of the training datasets and the salient 3D model of the object generated using the first training dataset. A deep neural network is trained using the set of training 2D-3D correspondence data generated using the first training dataset for object detection and segmentation.,"['G06T15/20', 'G06K9/6257', 'G06F18/2148', 'G06F18/2193', 'G06F18/254', 'G06K9/6232', 'G06K9/6265', 'G06K9/6292', 'G06T19/20', 'G06T7/11', 'G06V10/22', 'G06V10/454', 'G06V10/464', 'G06V10/764', 'G06V10/82', 'G06V20/647', 'G06T2207/20081', 'G06T2207/20084', 'G06T2219/2016']"
US11004129B2,Image processing,"An image processing method includes partitioning an image under test to form a plurality of contiguous image segments having similar image properties, deriving feature data from a subset including one or more of the image segments, and comparing the feature data from the subset of image segments with feature data derived from respective image segments of one or more other images so as to detect a similarity between the image under test and the one or more other images.","['G06T7/00', 'G06Q30/0621', 'G06F16/5838', 'G06F16/7335', 'G06F16/7847', 'G06F3/048', 'G06K9/00536', 'G06K9/4638', 'G06K9/4652', 'G06Q30/0631', 'G06Q30/0633', 'G06Q30/0643', 'G06T7/0002', 'G06T7/10', 'G06T7/12', 'G06T7/143', 'G06T7/45', 'G06V10/40', 'G06V10/44', 'G06V10/443', 'G06V10/457', 'G06V10/56', 'G06F2218/12', 'G06T2207/10016']"
US10580137B2,Systems and methods for detecting an indication of malignancy in a sequence of anatomical images,"A method for detecting an indication of likelihood of malignancy, comprising: receiving a sequence of anatomical images of a breast of a target individual acquired over a time interval during which contrast is administered, analyzing the sequence of anatomical images to identify: a baseline pre-contrast image denoting lack of contrast, a peak contrast image denoting a peak contrast enhancement, an initial uptake image denoting initial contrast enhancement, and a delayed response image denoting final contrast enhancement, creating a multi-channel image representation comprising: intensity channel including the peak contrast enhanced image, contrast-update channel including the computed difference between the peak contrast enhanced image and the pre-contrast image, and contrast-washout channel including the computed difference between the initial uptake image and the delayed response image, and computing by a trained deep convolutional neural network, a classification category indicative of likelihood of malignancy for the sequence according to the multi-channel image representation.","['A61B10/0041', 'A61B5/055', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/0016', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'G16H50/30', 'A61B5/4312', 'A61B5/7264', 'A61B5/7275', 'G06F17/18', 'G06T2207/10096', 'G06T2207/20016', 'G06T2207/20076', 'G06T2207/20084', 'G06T2207/30068']"
US20250103829A1,Natural language understanding for visual tagging,A tag characterizing a portion of a multi-view interactive digital media representation (MVIDMR) may be determined by applying a grammar to natural language data. The MVIDMR may include images of an object and may be navigable in one or more dimensions. An object model location for the tag identifying a location within a three-dimensional object model may be determined by applying the grammar to the natural language data. The tag may then be applied to the MVIDMR by associating it with two or more of the images at positions determined based on the object model location.,"['G06F40/211', 'G06F40/40', 'G06F40/284', 'G06F40/30', 'G06T7/77']"
CN114820635B,Polyp segmentation method based on joint attention U-shaped network and multi-scale feature fusion,"The invention provides a polyp segmentation method combining attention U-shaped network and multi-scale feature fusion, which comprises the steps of selecting U-Net as a main network, adding attention gates at jump connection ends of corresponding layers at the stages of an encoder and a decoder of the U-shaped main network structure to restrain unimportant features and strengthen important information at the same time, fusing different scale features to obtain rich global semantic information feature images, decoding the feature images to obtain a global map serving as an initial guiding area of a subsequent step, transmitting parallel high-level features into a receptive field module to strengthen network depth representation, transmitting the parallel high-level features into a plurality of reverse attention modules constructed in a cascading mode under the guidance of the global map to better mine target area features and boundary clues, and finally refining polyp target areas and boundary information through a refined residual error module to obtain polyp segmentation results with higher performance. The polyp image data set segmentation performance is more accurate and excellent.","['G06T7/10', 'G06F18/253', 'G06N3/045', 'G06N3/084', 'G06T7/194', 'G06T2207/30096']"
US8175376B2,Framework for image thumbnailing based on visual similarity,"An apparatus and method for detecting a region of interest in an image are disclosed. Image representations for a set of images that have been manually annotated with regions of interest are stored, along with positive and negative representations of each image which are similarly derived to the image representations except that they are based on features extracted from patches within the region of interest and outside it, respectively. For an original image for which a region of interest is desired, the stored information for K similar images is automatically retrieved and used to train a classifier. The trained classifier provides, for each patch of the original image, a probability of being in a region of interest, based extracted features of the patch (represented, for example, as a Fisher vector), which can be used to determine a region of interest in the original image.","['G06F16/5838', 'G06F16/51', 'G06F18/24155', 'G06V10/426', 'G06V10/462', 'G06V10/764']"
US10540771B2,System and method for image segmentation,"An image segmentation method is disclosed that allows a user to select image component types, for example tissue types and or background, and have the method of the present invention segment the image according to the user's input utilizing the superpixel image feature data and spatial relationships.","['G06T7/11', 'G06T7/187', 'G06T7/0008', 'G06T7/12', 'G06T7/162', 'G06T2207/10056', 'G06T2207/20072', 'G06T2207/20092', 'G06T2207/20156', 'G06T2207/30024', 'G06T7/168']"
CN114072850B,"Method, system and computer readable medium for obtaining foreground video","Embodiments described herein relate to methods, systems, and computer-readable media for obtaining foreground video. In some implementations, a method includes receiving a plurality of video frames including depth data and color data. The method further includes downsampling frames of the video. The method further includes, for each frame, generating an initial segmentation mask that classifies each pixel of the frame as either a foreground pixel or a background pixel. The method further includes determining a trimap image classifying each pixel of the frame as either a known background, a known foreground, or an unknown. The method further includes, for each pixel classified as unknown, calculating a weight and storing the weight in a weight map. The method further includes performing a fine segmentation to obtain a binary mask for each frame. The method further includes upsampling the plurality of frames based on the binary mask for each frame to obtain a foreground video.","['G06T7/194', 'G06T7/11', 'G06T7/143', 'G06T7/155', 'G06T7/162', 'G06V10/34', 'G06V10/446', 'G06V10/56', 'G06V40/161', 'G06V40/162', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20036', 'G06T2207/20072', 'G06T2207/20076', 'G06T2207/20224', 'G06T2207/30201']"
CN112116605B,Pancreas CT image segmentation method based on integrated depth convolution neural network,"The invention discloses a pancreas CT image segmentation method based on an integrated depth convolution neural network, which adopts a two-stage segmentation frame from coarse to fine to accurately segment pancreas in a CT image. Firstly, a CNN network of a three-dimensional U-shaped coding-decoding structure with an attention module and cross-layer dense connection is constructed, namely a Unet model is used as an identification network and applied to two stages of pancreas image segmentation; in the rough segmentation stage, the original image is subjected to down-sampling normalization pretreatment, and then a plurality of data blocks are randomly taken as the input of a network for training to obtain the rough segmentation result of the pancreas; in a segmentation stage, a boundary frame is used for containing a pancreas region, and image blocks are taken from the boundary frame region for training; during identification, the area where the pancreas is located is determined by using the rough segmentation result, and then the segmentation result is obtained by predicting through segmentation. And finally, voting the results of the two stages to obtain a segmentation result. The invention overcomes the problem of manual marking and obtains more ideal segmentation results.","['G06T7/11', 'G06N3/045', 'G06N3/084', 'G06T7/0012', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20084']"
CN111462036B,"Pathological image processing method, model training method and device based on deep learning","The application discloses a pathology image processing method based on deep learning, which comprises the steps of obtaining pathology images of a target object to be processed, conducting blocking processing on the pathology images of the target object to be processed to obtain a blocking image set, conducting feature classification extraction processing on the blocking image set to obtain N first feature sets, N second feature sets and N third feature sets, generating target fusion features corresponding to the target object, and calling a pathology image analysis model to analyze the target fusion features to output pathology analysis results of the target object to be processed. The application also discloses a model training method. Compared with biological detection technology, the method can reduce the time for waiting for the detection result, improve the detection efficiency, and avoid errors caused by uncontrollable factors such as experimental errors, operation errors and the like, thereby providing more accurate detection results.","['G06T7/0012', 'G06F18/214', 'G06F18/253', 'G06T7/66', 'G06V10/25', 'G06V10/50', 'G06V20/693', 'G06T2207/10056', 'G06T2207/30028']"
CN111967301B,"Positioning and navigation method, device, electronic device and storage medium","The application discloses a positioning navigation method, a positioning navigation device, electronic equipment and a storage medium, and relates to the fields of artificial intelligence, deep learning and intelligent transportation. The specific implementation scheme is as follows: judging whether the vehicle is about to drive into a preset traffic scene or not according to the real-time position of the vehicle; if the vehicle is about to drive into the traffic scene, acquiring a front road image of the vehicle; generating positioning auxiliary information of a traffic scene according to the front road image; determining a target road on which the vehicle is currently running according to the positioning auxiliary information; and obtaining navigation information according to the preset destination and the target road to carry out navigation prompt. The method and the device can accurately position the vehicle driving road under the preset traffic scene, thereby providing an accurate navigation route for the user.","['G06V20/56', 'G01C21/3602', 'G01C21/3629', 'G01C21/3647', 'G06V20/588']"
WO2021017361A1,Template matching algorithm based on edge and gradient feature,"A template matching algorithm based on edge and gradient features. A matching process comprises an offline stage and an online stage. The offline stage corresponds to processing, comprising at least edge extraction and gradient computation, of a template image, and generation of an R-table of generalized Hough transform; and the online stage corresponds to edge extraction, gradient computation and coarse-to-fine matching processing of an input target image, acquisition, according to an angle and a position and by means of the R-table, of a value in a voting space of generalized Hough transform, and acquisition of a target position. The template matching algorithm combines a pyramid model with an optimized search strategy, has an anti-interference capability, can achieve rotational invariance, and has a certain capability to resist scaling and occlusion; moreover, the computation amount of generalized Hough transform is greatly reduced, and the matching rate is increased manyfold.","['G06V10/44', 'G06V10/751']"
US9384556B2,Image processor configured for efficient estimation and elimination of foreground information in images,"An image processing system comprises an image processor having image processing circuitry and an associated memory. The image processor is configured to implement a foreground processing module utilizing the image processing circuitry and the memory. The foreground processing module is configured to obtain one or more images, to estimate a foreground region of interest from the one or more images, to determine a plurality of segments of the foreground region of interest, to calculate amplitude statistics for respective ones of the plurality of segments, to classify respective segments as being respective portions of static foreground objects or as being respective portions of dynamic foreground objects based at least in part on the calculated amplitude statistics and one or more defined patterns for known static and dynamic objects, and to remove one or more segments classified as static foreground objects from the foreground region of interest.","['G06T7/215', 'G06T7/0081', 'G06K9/00355', 'G06K9/00375', 'G06K9/469', 'G06T7/11', 'G06T7/194', 'G06V10/426', 'G06V40/107', 'G06V40/28', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20076', 'G06T2207/20084']"
CN109461167B,"Image processing model training method, mapping method, device, medium and terminal","The embodiment of the application discloses a training method, a matting method, a device, a medium and a terminal of an image processing model. The training method of the image processing model comprises the steps of obtaining a trimap image of an original image; generating a training sample set according to the original image and the trimap image; and training a preset deep learning network based on the training sample set to obtain an image processing model, wherein the image processing model is used for performing labeling processing on an original image to obtain a trisection image. By adopting the technical scheme, the input original image can be automatically labeled to obtain the three-part graph, a large amount of hair-level data labeling is not required to be carried out in a manual labeling mode, the labeling workload can be reduced, and the image labeling efficiency is improved. In addition, the original image is marked by adopting the image processing model, so that errors possibly introduced by manual marking are avoided, and the matting effect can be optimized.","['G06T7/136', 'G06T7/187', 'G06T7/194', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196', 'G06T2207/30204']"
US11704922B2,"Systems, methods and computer program products for automatically extracting information from a flowchart image","A method of extracting information from a flowchart image comprising a plurality of closed-shaped data nodes having text enclosed within, connecting lines connecting the plurality of closed-shaped data nodes and free text adjacent to the connecting lines includes receiving the flowchart image, detecting the closed-shaped data nodes, localizing the text enclosed within the closed-shaped data nodes, and masking the localized text.to generate an annotated image. Lines in the annotated image are the detected to reconstruct them as closed-shaped data nodes and connecting lines. A tree frame with the plurality of closed-shaped data nodes and the connecting lines is extracted. The free text is then localized. Chunks of the free text oriented and positioned proximally together are assembled into text blocks using an orientation-based two-dimensional clustering.","['G06V30/412', 'G06V30/224', 'G06V30/413', 'G06V30/414', 'G06V30/416', 'H04N13/261', 'G06V30/10']"
US8175384B1,Method and apparatus for discriminative alpha matting,"Method and apparatus for creating foreground masks, or mattes, in images including complex images. A discriminative matting technique may generate accurate alpha mattes for textured images or objects with spatial-varying color distributions. Given an input image and a trimap defining an unknown region, a discriminative color analysis is applied to the unknown region, yielding estimated alpha values, estimated binary segmentation values, and a mixture probability map for the region. The map may be adaptively smoothed. The pixels in the unknown region are classified into boundary pixels and non-boundary pixels according to the probability map. The non-boundary pixels are classified as either foreground or background pixels using a differencing technique that compares multiple pixel features. The estimated alpha values for the boundary pixels are refined. An alpha matte for the image is output. The process may be repeated until convergence of the alpha matte.","['G06T7/12', 'G06T7/11', 'G06T7/194', 'G06T2207/10024']"
US9922400B2,Image display apparatus and image display method,"An image display apparatus for displaying an image containing a plurality of objects includes a setting unit configured to set a display magnification and a display position according to an attribute of a display target object when a first display mode for displaying each object included in the image is specified, and a display control unit configured to perform control to display on a screen the image containing the display target object based on the display magnification and the display position set by the setting unit.","['G06F3/14', 'G06V30/413', 'G06T3/40', 'G06F3/0481', 'G06F3/0484', 'G06K9/00456', 'G06K9/00463', 'G06T11/60', 'G06T7/60', 'G06V30/414']"
CN117333929B,Method and system for identifying abnormal personnel under road construction based on deep learning,"The invention relates to the field of abnormal person identification, and discloses a method and a system for identifying abnormal persons under road construction based on deep learning, wherein the method comprises the following steps: extracting a scene image and a scene video of construction scene data; performing equalization processing on the scene image to obtain an equalized image, marking a personnel main body in the equalized image, identifying the face information of the personnel main body, comparing the face information with a preset face library to obtain abnormal face information, and identifying the person with abnormal face in the personnel main body; identifying a person with a position abnormality in the person's body; analyzing the behavior action of the personnel main body, calculating the acceleration vector amplitude of the behavior action, predicting the predicted behavior action of the personnel main body, analyzing the action abnormal value of the predicted behavior action, and identifying the abnormal behavior personnel in the personnel main body; abnormal personnel in the personnel body are analyzed. The method and the device can improve the accuracy of identifying the abnormal personnel in the road construction scene.","['G06V40/161', 'G06T5/40', 'G06V10/50', 'G06V10/761', 'G06V10/764', 'G06V10/82', 'G06V40/168', 'G06V40/172', 'G06V40/20']"
US12073621B2,"Method and apparatus for detecting information insertion region, electronic device, and storage medium","A method for detecting an information insertion region is provided. In the method, a video is obtained. The video is segmented to obtain video fragments, each of the video fragments including a subset of image frames in the video. A target frame is obtained in the video fragments. Objects in the target frame are identified and segmented, to obtain labeling information corresponding to the objects. A target object is determined according to the labeling information. Clustering is performed on the target object, to obtain a plurality of candidate to-be-inserted regions. A target candidate to-be-inserted region is determined from the candidate to-be-inserted regions. Further, maximum rectangle searching is performed in the target candidate to-be-inserted region to obtain a target to-be-inserted region in which an image is to be inserted.","['G06V20/41', 'H04N21/23418', 'G06F18/23', 'G06V10/25', 'G06V10/255', 'G06V10/462', 'G06V20/49', 'G06V20/70', 'H04N21/26241', 'H04N21/4316', 'H04N21/44008', 'H04N21/812', 'H04N21/8456']"
US10740603B2,Extracting data from electronic documents,A structured data processing system includes hardware processors and a memory in communication with the hardware processors. The memory stores a data structure and an execution environment. The data structure includes an electronic document. The execution environment includes a data extraction solver configured to perform operations including identifying a particular page of the electronic document; performing an optical character recognition (OCR) on the page to determine a plurality of alphanumeric text strings on the page; determining a type of the page; determining a layout of the page; determining at least one table on the page based at least in part on the determined type of the page and the determined layout of the page; and extracting a plurality of data from the determined table on the page. The execution environment also includes a user interface module that generates a user interface that renders graphical representations of the extracted data; and a transmission module that transmits data that represents the graphical representations.,"['G06F16/93', 'G06K9/00469', 'G06F18/214', 'G06F18/2411', 'G06K9/00449', 'G06K9/3275', 'G06K9/342', 'G06K9/346', 'G06K9/40', 'G06K9/6256', 'G06K9/6269', 'G06K9/72', 'G06V30/1475', 'G06V30/15', 'G06V30/155', 'G06V30/164', 'G06V30/19147', 'G06V30/19173', 'G06V30/412', 'G06V30/416', 'G06K2209/01', 'G06V30/10']"
CN111754414B,Image processing method and device for image processing,"The embodiment of the invention provides an image processing method, an image processing device and an image processing device. The method specifically comprises the following steps: determining a text line area in the image to be processed according to the text detection model; the text detection model is a deep neural network model which is obtained through training according to a sample text image and text annotation information corresponding to the sample text image; performing text segmentation processing on the text line area to obtain a text mask area in the text line area; performing image restoration on the image to be processed according to the text mask area to obtain a restored background image; and the repaired background image does not contain text contents in the text line area. The embodiment of the invention can improve the accuracy of positioning the text region, thereby improving the accuracy of image restoration.","['G06T5/77', 'G06F18/24', 'G06N3/045', 'G06T7/11', 'G06T7/194', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30176']"
CN113340909B,Glue line defect detection method based on machine vision,"The invention discloses a glue line defect detection method based on machine vision, which comprises the steps of dispensing in advance, selecting qualified glue line products as templates, sending a notification signal by an upper computer after a camera moves to the position of the templates, receiving the notification signal by the camera and acquiring image information, and photographing the templates by the camera to obtain images, wherein the images are template images; extracting a glue line area through threshold segmentation, solving an intersection of the glue line area and the drawn detection angular point area to obtain a glue line angular point part, and performing difference between the glue line angular point part and the glue line area to obtain a linear detection rectangular part of the glue line; fitting the minimum external rectangle of the straight line detection rectangular part, traversing to generate a measurement straight line set perpendicular to the long side of the rectangle, solving the intersection point of the measurement straight line set and the outline of the glue line, calculating to obtain the glue width, and comparing the glue width with the standard width; and (4) sequencing the corner point parts of the glue lines, calculating the corner point area, and comparing the corner point area with the standard area. The method has the advantages of strong universality, high accuracy and high efficiency.","['G01N21/8851', 'G01B11/02', 'G01B11/28', 'G06T7/0002', 'G06T7/62', 'G01N2021/8887']"
CN105229669B,Image processing apparatus and image processing method,"The line recognition unit compares a case where the 3 rd character region is added to a line starting from the 1 st character region and a case where the 3 rd character region is added to a line starting from the 2 nd character region when the same 3 rd character region can be repeatedly selected in a case where the line extraction processing is performed starting from the 1 st character region and a case where the line extraction processing is performed starting from the 2 nd character region located in a line different from the 1 st character region, and determines to which line the 3 rd character region should be added.","['G06V30/1478', 'G06F18/22', 'G06T11/203', 'G06V30/153', 'G06V30/10']"
CN110097044B,One-stage license plate detection and recognition method based on deep learning,"The invention discloses a one-stage license plate detection and identification method based on deep learning. The invention relates to a one-stage license plate detection and identification method based on deep learning, which comprises the following steps: s1, inputting a color image to be detected and identified, and zooming the color image to an image with the size of 300 x 300 through an image preprocessor; s2, inputting the zoomed image into a detection module and an identification module to carry out license plate detection and license plate character identification; and S3, the output result of the combined detection module and the recognition module is the output result of the whole system for detecting and recognizing the license plate. The invention has the beneficial effects that: the one-stage license plate detection and recognition system integrating license plate detection and license plate character recognition into one model has the following advantages: the license plate detection and character recognition algorithm based on deep learning provided by the invention can keep higher detection accuracy and character recognition accuracy in a complex natural scene, thereby showing the stability and accuracy exceeding other related systems.","['G06F18/214', 'G06N3/045', 'G06N3/084', 'G06V10/22', 'G06V10/464', 'G06V30/153']"
CN112613392B,"Lane line detection method, device and system based on semantic segmentation and storage medium","The invention relates to the field of lane line detection, in particular to a method, a system and a storage medium for lane line detection based on semantic segmentation, wherein the method comprises the steps of carrying out image binarization semantic segmentation on characteristic data of an input image to obtain pixel points of lane lines, predicting the output category of the lane lines by combining a data set, and calculating classification probability; and dynamically adjusting the weight coefficient and the sphere radius of a clustering algorithm according to the classification probability and the change of the lane line direction, classifying the pixel points through the clustering algorithm, and parameterizing each lane line by adopting a curve fitting algorithm. Aiming at a complex traffic environment, the embodiment combines semantic segmentation and a self-adaptive clustering method, realizes the pixel-level identification of the lane lines, can meet the urgent need of automatic driving, and has great significance for the practical application value and the safety value of the automatic driving by a high-performance detection algorithm.","['G06V20/588', 'G06F18/23', 'G06F18/241', 'G06N3/045', 'G06N3/08', 'G06T7/155', 'G06T7/187', 'G06T2207/10016']"
US11429805B2,System and method for deep machine learning for computer vision applications,"A computer vision (CV) training system, includes: a supervised learning system to estimate a supervision output from one or more input images according to a target CV application, and to determine a supervised loss according to the supervision output and a ground-truth of the supervision output; an unsupervised learning system to determine an unsupervised loss according to the supervision output and the one or more input images; a weakly supervised learning system to determine a weakly supervised loss according to the supervision output and a weak label corresponding to the one or more input images; and a joint optimizer to concurrently optimize the supervised loss, the unsupervised loss, and the weakly supervised loss.","['G06T7/593', 'G06T7/11', 'G06K9/6217', 'G06F18/21', 'G06N20/00', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/088', 'G06N3/0895', 'G06N3/09', 'G06N7/005', 'G06N7/01', 'G06T15/00', 'G06T3/4053', 'G06T3/4076', 'G06T7/215', 'G06T7/269', 'G06V20/00', 'G06V20/647', 'G06V30/19167', 'G06V30/248', 'G06T2200/08', 'G06T2207/10028', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06V2201/07']"
US9830700B2,Enhanced computed-tomography colonography,"A computer system that segments a colon for a computed tomography colonography (CTC) is described. During operation, the computer system accesses imaging data having a spatial resolution. Then, the computer system identifies the colon lumen based on probabilities for different tissue classes in the imaging data. Moreover, the computer system segments the colon into subsegments based on an articulated object model that fits a tortuosity of the colon along a centerline of the colon, where the articulated object model includes values of an orthonormal basis set, curvature and torsion along the centerline, and where boundaries between subsegments are based on the curvature and the torsion. For example, a given boundary between a pair of subsegments may corresponds to or may be related to a minimum value of the curvature and a maximum value of the torsion over a length of the colon.","['G06T7/0012', 'G06F3/016', 'A61B6/032', 'A61B6/50', 'G06F3/04845', 'G06T19/00', 'G06T7/11', 'G06T2207/10081', 'G06T2207/30004', 'G06T2207/30028', 'G06T2210/41', 'G06T2219/004']"
CN113569865B,Single sample image segmentation method based on class prototype learning,"The invention provides a single-sample image segmentation method based on class prototype learning, which effectively relieves the problem of lacking semantic information of features by introducing multi-class label information, prompts a network to generate class prototypes with rich semantic information for target classes, and guides the network to more accurately segment a target area in a query image through a more robust target class clue.","['G06F18/214', 'G06F18/253', 'G06N3/045']"
US6819782B1,"Device and method for recognizing hand shape and position, and recording medium having program for carrying out the method recorded thereon","An object of the present invention is to provide a device and a method for recognizing hand shape and position even if a hand image to be provided for recognition is rather complicated in shape, and a recording medium having a program for carrying out the method recorded thereon.A hand image normalization part 11 deletes a wrist region respectively from a plurality of images varied in hand shape and position before subjecting the images to normalization in hand orientation and size to generate hand shape images. An eigenspace calculation part 13 calculates an eigenvalue and an eigenvector respectively from the hand shape images under an analysis based on an eigenspace method. An eigenspace projection part 15 calculates eigenspace projection coordinates by projecting the hand shape images onto an eigenspace having the eigenvectors as a basis. A hand image normalization part 21 deletes a wrist region from an input hand image, and generates an input hand shape image by normalizing the input hand image to be equivalent to the hand shape images. An eigenspace projection part 22 calculates eigenspace projection coordinates for the input hand shape image by projecting the same onto the eigenspace having the eigenvectors as the basis. A hand shape image selection part 23 compares the eigenspace projection coordinates calculated for the input hand shape image with each of the eigenspace projection coordinates calculated for the hand shape images, and then determines which of the hand shape images is closest to the input hand shape image. A shape/position output part 24 outputs shape information and position information on the determined hand shape image.","['G06T7/74', 'G06V40/107', 'G06V40/28']"
CN112233038B,True image denoising method based on multi-scale fusion and edge enhancement,"The invention discloses a real image denoising method based on multi-scale fusion and edge enhancement, and belongs to the technical field of computer vision images. In the image input stage, in order to improve the generalization capability of the model, data enhancement is designed, and part of pixels randomly selected from the content of an input noise image are replaced by a corresponding noise-free image; carrying out multilevel smoothing processing on an input noise image by utilizing three convolution kernels with different receptive field sizes to obtain three preliminary smoothing results with different scales; carrying out self-adaptive expression on the multi-scale denoising result by utilizing a channel attention mechanism, and further fusing; edges are extracted through a Laplacian operator, the edge and texture information of the original noise image is introduced, and the fused smooth image is subjected to detail enhancement to improve the visual effect; the invention has reasonable design, keeps faster running speed while obtaining better denoising effect, and obtains better effect on denoising of real images integrally.","['G06T5/70', 'G06N3/045', 'G06N3/08', 'G06T5/50', 'G06T7/13', 'G06T2207/20048', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20192', 'G06T2207/20221']"
CN111860168B,"Pedestrian re-identification method and device, electronic equipment and storage medium","The application discloses a pedestrian re-identification method, belongs to the technical field of computer vision and pattern recognition, and can improve the efficiency of pedestrian re-identification. The method comprises the following steps: according to the action of the currently collected video image frame, caching the image frame to a first image information queue corresponding to a pedestrian detection network, or caching the image frame to a second image information queue corresponding to a pedestrian tracking network; carrying out pedestrian detection on the image frames cached in the first image information queue through at least one pedestrian detection network, and caching the detected pedestrian information into a preset pedestrian information queue; carrying out pedestrian tracking on the basis of the image frames cached in the second image information queue through at least one pedestrian tracking network, and updating pedestrian information in a preset pedestrian information queue according to a pedestrian tracking result; according to the pedestrian information in the preset pedestrian information queue, the pedestrian characteristics and/or the pedestrian track of the specified pedestrian are/is obtained, and the efficiency of pedestrian re-identification can be improved.","['G06V20/40', 'G06N3/045', 'G06N3/08', 'G06V20/52', 'G06V40/10', 'Y02T10/40']"
US10573018B2,Three dimensional scene reconstruction based on contextual analysis,"Techniques are provided for context-based 3D scene reconstruction employing fusion of multiple instances of an object within the scene. A methodology implementing the techniques according to an embodiment includes receiving 3D image frames of the scene, each frame associated with a pose of a depth camera, and creating a 3D reconstruction of the scene based on depth pixels that are projected and accumulated into a global coordinate system. The method may also include detecting objects, based on the 3D reconstruction, the camera pose and the image frames. The method may further include classifying the detected objects into one or more object classes; grouping two or more instances of objects in one of the object classes based on a measure of similarity of features between the object instances; and combining point clouds associated with each of the grouped object instances to generate a fused object.","['G06T7/60', 'G06K9/00671', 'G06T11/60', 'G06T17/00', 'G06T7/33', 'G06T7/50', 'G06T7/90', 'G06V20/20', 'H04N13/128', 'H04N13/257', 'G06T2207/20021', 'G06T2207/20048', 'G06T2207/20212', 'G06T2207/30244', 'G06T2210/56', 'H04N13/111', 'H04N13/271']"
CN108961304B,Method for identifying moving foreground in video and method for determining target position in video,"The application discloses a method for identifying a motion foreground in a video, which comprises the following steps: acquiring a video frame to be processed containing a video implantation target area; determining a corresponding region of interest according to the video implantation target region; for the region of interest, performing rough segmentation on the motion foreground of the region of interest by comparing the foreground with the background; and aiming at the rough segmentation result, finely segmenting the motion foreground of the region of interest through image edge processing to obtain a motion foreground segmentation result. The method can play a role in quickly and accurately identifying the motion foreground in the video frame in real time.","['G06T7/215', 'G06T7/11', 'G06T7/136', 'G06T7/194', 'G06T7/238', 'G06T7/246', 'G06T2207/10016']"
US12367705B2,Occlusion detection,"An occlusion detection model training method is provided. The training method includes the following steps: constructing a plurality of pieces of training sample data, where the training sample data includes a first face image added with an occlusion object, coordinate values of a first key point in the first face image, and occlusion information of the first key point; and using the first face image as input data, and using the coordinate values of the first key point and the occlusion information of the first key point as output data, to train an occlusion detection model, so that the occlusion detection model outputs, based on any input second face image, coordinate values of a second key point included in the second face image and an occlusion probability of the second key point.","['G06V40/171', 'G06T5/77', 'G06F18/241', 'G06F18/2415', 'G06N3/04', 'G06N3/084', 'G06V10/26', 'G06V10/44', 'G06V10/774', 'G06V10/82', 'G06V40/161', 'G06V40/168', 'G06V40/172', 'G06N3/045', 'G06N3/08', 'G06T2207/10004', 'G06T2207/10024', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30201', 'Y02T10/40']"
CN110555839B,"Defect detection and identification method, device, computer equipment and storage medium","The invention discloses a defect detection and identification method, a device, computer equipment and a medium, and belongs to the field of computer vision detection and identification. The method obtains a mask map by dividing the background and the foreground in the target product image, positions a defect target in the target product image according to the spatial position distribution and the number of connected domains in the mask map of the target product image, and further identifies a target product image block corresponding to a target positioning frame, the dividing method converts the prediction of the defect shape and the boundary into the division of the defect foreground and the background, the defect locating block comprises a defect foreground and an image background which meet target conditions, and meanwhile, the defect locating method provided by the invention can locate the defect position more accurately, is beneficial to extracting main defect characteristics, reduces influence of mask noise and target product image background on defect type identification, and improves accuracy of defect type identification.","['G01N21/8851', 'G06N3/045', 'G06T7/0004', 'G06T7/187', 'G06T7/194', 'G06V20/10', 'G01N2021/8887', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/30108', 'G06V2201/06', 'Y02P90/30']"
US9898866B2,Low latency stabilization for head-worn displays,"Methods, systems, and computer readable media for low latency stabilization for head-worn displays are disclosed. According to one aspect, the subject matter described herein includes a system for low latency stabilization of a head-worn display. The system includes a low latency pose tracker having one or more rolling-shutter cameras that capture a 2D image by exposing each row of a frame at a later point in time than the previous row and that output image data row by row, and a tracking module for receiving image data row by row and using that data to generate a local appearance manifold. The generated manifold is used to track camera movements, which are used to produce a pose estimate.","['G06F3/013', 'G01B11/002', 'G02B27/0093', 'G02B27/017', 'G06F3/012', 'G06K9/52', 'G06T19/006', 'G06T7/292', 'G06T7/60', 'G06T7/73', 'G06T7/77', 'H04N23/90', 'H04N5/247', 'H04N5/341', 'G02B2027/0138', 'G02B2027/014', 'G02B2027/0178', 'G06T2207/30244']"
US10872262B2,Information processing apparatus and information processing method for detecting position of object,"An information processing apparatus includes an acquiring unit to acquire an input image including an object, an extraction unit to extract a region that has been changed from a background image in the input image, as a changing region, a generating unit to generate a change region image in which a pixel value of each pixel represents whether or not a corresponding pixel in the input image belongs to the changing region, an extraction unit to extract a plurality of feature images from the input image as an output of a neural network by inputting a combination of the input image and the change region image to the neural network, and a detection unit to detect a position of the object from the plurality of feature images.","['G06K9/3241', 'G06V40/10', 'G06F9/3004', 'G06K9/00362', 'G06K9/4628', 'G06N3/02', 'G06T7/11', 'G06T7/194', 'G06T7/254', 'G06V10/454', 'G06T2207/20084']"
CN111898561A,"Face authentication method, device, equipment and medium","The application provides a face authentication method, a face authentication device, face authentication equipment and a face authentication medium, relates to the technical field of artificial intelligence, and is used for improving the accuracy of face authentication. The face authentication method comprises the following steps: when a target shielding object exists in a target face image to be authenticated, a target residual region except a region corresponding to the target shielding object is segmented from the target face image; according to the relative position information of the target shielding object in the target face image, removing a region corresponding to the relative position information from a reference face image to obtain a reference residual region; and determining the authentication result of the target face image according to the matching result of the target residual region and the reference residual region, wherein the method improves the accuracy of face authentication by carrying out similar processing on the reference face image to the target face image.","['G06V40/168', 'G06F18/214', 'G06F18/2411', 'G06N3/045', 'G06N3/08', 'G06V10/267', 'G06V40/172']"
US6453069B1,Method of extracting image from input image using reference image,"This invention relates to an image processing method for precisely and automatically recognizing a specific image extraction region to be divided from an input image, and extracting the specific image extraction region. Thresholding is done for intensity differences of edge data obtained from the input data to extract difference edge data. A main extraction region is estimated from the outermost contour line extracted based on the extraction result of the difference edge data. Thresholding or the like is done in units of pixels of the input image to extract an initial region. Extraction region determination processing is done for extracting an accurate target region by combining the main extraction region and initial region.","['G06T7/11', 'G06T7/12', 'G06T7/174', 'G06T7/181', 'G06T7/187', 'G06T7/194', 'G06V10/46', 'G06V10/7515', 'G06T2207/10016', 'G06T2207/20156', 'G06T2207/20224']"
US7853053B2,System and method providing improved head motion estimations for animation,"The computer-readable media provides improved procedures to estimate head motion between two images of a face. Locations of a number of distinct facial features are determined in two images. The locations are converted into as a set of physical face parameters based on the symmetry of the identified distinct facial features. An estimation objective function is determined by: (a) estimating each of the set of physical parameters, (b) estimating a first head pose transform corresponding to the first image, and (c) estimating a second head pose transform corresponding to the second image. The motion is estimated between the two images based on the set of physical face parameters by multiplying each term of the estimation objective function by a weighted contribution factor based on the confidence of data corresponding to the estimation objective function.","['G06T7/251', 'G06T7/70', 'G06V40/165', 'G06T2207/10016', 'G06T2207/30201']"
US8050481B2,Method and apparatus for small pulmonary nodule computer aided diagnosis from computed tomography scans,"The present invention is a multi-stage detection algorithm using a successive nodule candidate refinement approach. The detection algorithm involves four major steps. First, a lung region is segmented from a whole lung CT scan. This is followed by a hypothesis generation stage in which nodule candidate locations are identified from the lung region. In the third stage, nodule candidate sub-images or the lung region of the CT scan pass through a streaking artifact removal process. The nodule candidates are then successively refined using a sequence of filters of increasing complexity. A first filter uses attachment area information to remove vessels and large vessel bifurcation points from the nodule candidate list. A second filter removes small bifurcation points.","['G06T7/0012', 'G06T3/147', 'G06T7/44', 'G06T2207/10081', 'G06T2207/30061']"
WO2018205801A1,"Facial animation implementation method, computer device, and storage medium","The present application provides a facial animation implementation method. The method comprises: collecting a facial image; extracting a facial feature point in the facial image; comparing the facial feature point with a standard feature point, so as to obtain a first deformation coefficient corresponding to a geometrical feature; extracting a local region according to the facial feature point and performing processing, so as to obtain a second deformation coefficient corresponding to an appearance feature; and driving a three-dimensional virtual object to perform a corresponding expression according to the first deformation coefficient and the second deformation coefficient.","['G06T13/40', 'G06F18/214', 'G06F3/013', 'G06V40/171', 'G06V40/172', 'G06V40/175']"
US10510152B2,"Systems, methods, and devices for image matching and object recognition in images using textures",A computer-implemented method for determining whether a first image contains at least a portion of a second image includes: determining a first set of feature points associated with the first image; removing from said first set of feature points at least some feature points in the first set that correspond to one or more textures in the first image; and then attempting to match feature points in said first set of feature points with feature points in a second set of feature points associated with said second image to determine whether said first image contains at least a portion of said second image.,"['G06T7/73', 'G06K9/00', 'G06T7/337', 'G06V10/462', 'G06V10/757', 'G06T2207/20021', 'G06V2201/09']"
US8139900B2,System and method for providing objectified image renderings using recognition information from images,"An embodiment provides for enabling retrieval of a collection of captured images that form at least a portion of a library of images. For each image in the collection, a captured image may be analyzed to recognize information from image data contained in the captured image, and an index may be generated, where the index data is based on the recognized information. Using the index, functionality such as search and retrieval is enabled. Various recognition techniques, including those that use the face, clothing, apparel, and combinations of characteristics may be utilized. Recognition may be performed on, among other things, persons and text carried on objects.","['G06V20/70', 'G06F16/58', 'G06F16/583', 'G06F16/5838', 'G06F16/5846', 'G06F16/5854', 'G06F18/22', 'G06F18/23', 'G06F18/24', 'G06F3/0482', 'G06V20/20', 'G06V40/107', 'G06V40/161', 'G06V40/172']"
WO2019196130A1,Classifier training method and device for vehicle-mounted thermal imaging pedestrian detection,"A classifier training method and device for vehicle-mounted thermal imaging pedestrian detection. The classifier training method refers to a method for generating enhanced classifier training samples, and comprises: generating enhanced positive samples on the basis of positive sample labeling information and equalization technology, and analyzing information distribution of non-pedestrian background image blocks using a clustering method, so as to assist in screening enhanced negative samples of different categories; pre-processing the enhanced positive and negative samples by adjusting brightness and boundary information; and clustering the positive samples to obtain a sample scale division criteria for long, medium and near distances, and dividing the pre-processed enhanced positive and negative samples into three training sets, and respectively training three classifiers suitable for classifying pedestrian targets at long, medium and near distances. While ensuring the pedestrian detection accuracy, the present method can reduce the computation overhead of pedestrian detection and enhance the scene adaptability of the classifier. The classifier training device includes an enhanced positive and negative sample generation module, an enhanced positive and negative sample pre-processing module, and a training set division and classifier training module.","['G06V40/103', 'G06F18/214', 'G06F18/23213', 'G06V10/25', 'G06V20/56', 'G06V40/10']"
CN110427970B,"Image classification method, apparatus, computer device and storage medium","The application relates to an image classification method, an image classification device, a computer device and a storage medium. The method relates to an image classification technique, comprising: acquiring an image to be classified, and determining an interested region in the image to be classified; generating a plurality of scale sub-images according to the region of interest; respectively extracting features of the multiple scale sub-images to obtain feature sub-vectors corresponding to the scale sub-images; connecting the feature sub-vectors to obtain an image feature vector corresponding to the region of interest; and classifying the images to be classified according to the image feature vectors and a preset classifier to obtain image classification results. By adopting the method, the image classification accuracy can be improved.","['G06F18/24', 'G06V10/25']"
CN114972329B,Image enhancement method and system of surface defect detector based on image processing,"The invention relates to an image enhancement method and system of a surface defect detector based on image processing, which comprises the following steps: acquiring a defect image and converting the defect image into a gray scale image; carrying out super-pixel segmentation on the gray image to obtain a plurality of background super-pixel regions and a plurality of defect super-pixel regions; acquiring the average gray value of pixel points in each background super pixel area and the gray value of pixel points in each defect super pixel area, and drawing a gray histogram of a gray map according to the average gray value and the gray value; according to the method, the frequency of the background gray value is reduced, the frequency of the defect gray value is increased, the details of the defect are retained while the image is enhanced, the contrast of the image is increased, the accuracy of defect identification is improved, the practicability is high, and the method is worthy of popularization.","['G06T7/0004', 'G06T5/40', 'G06T5/90', 'G06T7/11']"
CN113761999B,"Target detection method and device, electronic equipment and storage medium","The embodiment of the invention discloses a target detection method, a target detection device, electronic equipment and a storage medium, wherein the method comprises the following steps: determining the outline of a detection target candidate region based on three-dimensional point cloud data acquired for physical space scanning; projecting the outline to an initial two-dimensional image acquired by shooting in a physical space to obtain a first two-dimensional image corresponding to a detection target; a position and/or type of the detection target is determined based on the first two-dimensional image and the initial two-dimensional image. By the technical scheme provided by the embodiment of the invention, the operation amount during target detection is reduced, and the target detection speed is further improved.",['G06F18/2321']
CN113379734B,"Quality detection method, quality detection device, quality detection equipment and computer-readable storage medium","The invention provides a quality detection method, a quality detection device, quality detection equipment and a computer readable storage medium. According to the method, the device and the system, the image to be detected including the object to be detected is obtained, the first detection model obtained through pre-training is utilized, the region of interest of the image to be detected is detected, at least one region of interest is obtained, then the image of each region of interest in the at least one region of interest is cut out from the image to be detected, the second detection model obtained through pre-training is utilized, the image of each region of interest is subjected to target detection, the target detection result corresponding to each region of interest is obtained, and further, the quality detection result of the image to be detected is determined based on the target detection result corresponding to the at least one region of interest, so that the local defect of the appliance and the defect of the appliance mould can be accurately and effectively identified.","['G06T7/0012', 'A61C7/002', 'G06F18/214', 'G06T3/4038', 'G06T7/11', 'G06T2207/20081', 'G06T2207/20104', 'G06T2207/20132', 'G06T2207/30036']"
US12354387B2,Identifying regions of interest from whole slide images,"The present application relates generally to identifying regions of interest in images, including but not limited to whole slide image region of interest identification, prioritization, de-duplication, and normalization via interpretable rules, nuclear region counting, point set registration, and histogram specification color normalization. This disclosure describes systems and methods for analyzing and extracting regions of interest from images, for example biomedical images depicting a tissue sample from biopsy or ectomy. Techniques directed to quality control estimation, granular classification, and coarse classification of regions of biomedical images are described herein. Using the described techniques, patches of images corresponding to regions of interest can be extracted and analyzed individually or in parallel to determine pixels correspond to features of interest and pixels that do not. Patches that do not include features of interest, or include disqualifying features, can be disqualified from further analysis. Relevant patches can analyzed and stored with various feature parameters.","['G06V20/693', 'G06V10/25', 'G06V10/454', 'G06V10/462', 'G06V10/507', 'G06V10/82', 'G06V20/69', 'G06V20/695', 'G06V20/698', 'G06V30/2504']"
US11989822B2,Damage detection from multi-view visual data,"A plurality of images may be analyzed to determine an object model. The object model may have a plurality of components, and each of the images may correspond with one or more of the components. Component condition information may be determined for one or more of the components based on the images. The component condition information may indicate damage incurred by the object portion corresponding with the component.","['G06T15/205', 'G01C21/32', 'G01N21/9515', 'G06F16/29', 'G06F17/18', 'G06F3/011', 'G06F3/04815', 'G06F30/15', 'G06F9/453', 'G06N3/02', 'G06N3/0464', 'G06N3/09', 'G06Q10/0837', 'G06Q10/20', 'G06Q30/0278', 'G06T15/04', 'G06T15/10', 'G06T17/00', 'G06T19/003', 'G06T19/006', 'G06T19/20', 'G06T3/00', 'G06T3/06', 'G06T7/0002', 'G06T7/0004', 'G06T7/001', 'G06T7/30', 'G06T7/593', 'G06T7/70', 'H04N13/221', 'H04N13/243', 'H04N13/25', 'H04N13/271', 'H04N23/633', 'H04N23/64', 'G01N2021/8887', 'G06N3/08', 'G06Q50/40', 'G06T2200/08', 'G06T2200/24', 'G06T2207/10016', 'G06T2207/10028', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20224', 'G06T2207/30108', 'G06T2207/30156', 'G06T2207/30244', 'G06T2207/30248', 'G06T2219/2012', 'H04N2013/0081', 'H04N23/698']"
US20220261659A1,Method and Apparatus for Determining Neural Network,"This application provides a method and related apparatus for determining a neural network in the field of artificial intelligence. The method includes: obtaining a plurality of initial search spaces; determining M candidate neural networks based on the plurality of initial search spaces, where the candidate neural network includes a plurality of candidate subnetworks, the plurality of candidate subnetworks belong to the plurality of initial search spaces, and any two of the plurality of candidate subnetworks belong to different initial search spaces; evaluating the M candidate neural networks to obtain M evaluation results; and determining N candidate neural networks from the M candidate neural networks based on the M evaluation results, and determining N first target neural networks based on the N candidate neural networks. According to the method and the related apparatus provided in this application, a combined neural network with relatively high performance can be obtained.","['G06N3/10', 'G06N3/082', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'G06N3/047', 'G06N3/084']"
US8452108B2,Systems and methods for image recognition using graph-based pattern matching,"A method for creating a modeling structure for classifying objects in an image comprises converting an image into digital image data; using a processor, simplifying the digital image data; using the processor, isolating objects in the simplified digital image data; using the processor, creating graphs of the isolated objects, the graphs comprising vertices and edges; using the processor, converting the graphs into representative graph data structures, the graph data structures comprising a database key based on the vertices and edges.","['G06V30/2272', 'G06V30/18181', 'G06V30/10']"
US10452713B2,"Video analysis techniques for improved editing, navigation, and summarization","Systems and processes for improved video editing, summarization and navigation based on generation and analysis of metadata are described. The metadata may be content-based (e.g., differences between neighboring frames, exposure data, key frame identification data, motion data, or face detection data) or non-content-based (e.g., exposure, focus, location, time) and used to prioritize and/or classify portions of video. The metadata may be generated at the time of image capture or during post-processing. Prioritization information, such as a score for various portions of the image data may be based on the metadata and/or image data. Classification information such as the type or quality of a scene may be determined based on the metadata and/or image data. The classification and prioritization information may be metadata and may be used to automatically remove undesirable portions of the video, generate suggestions during editing or automatically generate summary video.","['G06F16/739', 'G06F16/745', 'G06F16/783', 'G06F16/7837', 'G06F16/7867', 'G06K9/00718', 'G06K9/00751', 'G06V20/41', 'G06V20/47', 'G11B27/031', 'G11B27/3081', 'G11B27/34']"
CN112136139B,Method and system for improved quality inspection,"A method of performing automated object inspection includes obtaining a plurality of test images. For each test image of the plurality of test images, the method includes quantitatively determining a respective transformation from a predetermined contour of a reference mark in a predetermined generic layout to a respective anchor point contour corresponding to the reference mark captured in the test image; and applying an inverse transformation of the respective transformation to at least a portion of the test image to obtain a respective regularized version of the test image such that the reference marks captured in the respective regularized versions of the plurality of test images share image-independent positions and orientations. The method further includes performing a separate automated inspection on each of two or more sub-portions of the respective regularized version of the test image.","['G06T7/001', 'G06T7/10', 'G06V10/225', 'G06V10/44', 'G06V10/50', 'G06V10/764', 'G06V10/774', 'G06V10/7788', 'G06T2207/20132', 'G06T2207/30108', 'G06V10/95', 'G06V2201/06']"
US9536354B2,Object outlining to initiate a visual search,"Methods and devices for initiating a search of an object are disclosed. In one embodiment, a method is disclosed that includes receiving sensor data from a sensor on a wearable computing device and, based on the sensor data, detecting a movement that defines an outline of an area in the sensor data. The method further includes identifying an object that is located in the area and initiating a search on the object. In another embodiment, a server is disclosed that includes an interface configured to receive sensor data from a sensor on a wearable computing device, at least one processor, and data storage comprising instructions executable by the at least one processor to detect, based on the sensor data, a movement that defines an outline of an area in the sensor data, identify an object that is located in the area, and initiate a search on the object.","['G06V20/20', 'G02B27/017', 'G06F16/583', 'G06F17/30247', 'G06F3/005', 'G06F3/017', 'G06K9/00671', 'G06K9/4604', 'G06T19/006', 'G06T7/0083', 'G06T7/0085', 'G06T7/12', 'G06T7/13', 'G06T7/2006', 'G06T7/2053', 'G06T7/215', 'G06T7/254', 'G02B2027/0178', 'G06K2009/4666', 'G06T2207/10016', 'G06T2207/10021', 'G06T2207/10024', 'G06T2207/20076', 'G06T2207/20096', 'G06T2207/20112']"
CN109859296B,"Training method of SMPL parameter prediction model, server and storage medium","The application discloses a training method of an SMPL parameter prediction model, a server and a storage medium. The method comprises the following steps: acquiring a sample picture; inputting the sample picture into a posture parameter prediction model to obtain a posture prediction parameter; inputting the sample picture into a morphological parameter prediction model to obtain morphological prediction parameters; constructing a human body three-dimensional model through an SMPL (simple Markov chain Package) model according to the posture prediction parameters and the form prediction parameters; calculating model prediction loss according to the SMPL prediction parameters and/or the human body three-dimensional model and by combining the marking information of the sample picture; and (4) according to the model prediction loss reverse training attitude parameter prediction model and the morphological parameter prediction model. In the embodiment of the application, the sample picture is directly used as the model input for model training, and the model of the human body information in the picture is extracted without independent training, so that the complexity of the model training is reduced, and the efficiency of the model training is improved.","['G06T7/74', 'G06F18/22', 'G06F18/254', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T17/00', 'G06T7/70', 'G06V10/809', 'G06V10/82', 'G06V40/11', 'G06N3/045', 'G06T2207/20044', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196']"
CN116030454A,Text recognition method and system based on capsule network and multi-language model,"The invention discloses a character recognition method and a system based on a capsule network and a multi-language model, wherein the method has higher accuracy in character recognition. The invention relates to the following contents: (1) collecting pictures needing character recognition; (2) preprocessing an image and dividing the picture; (3) And inputting the pictures into a character recognition network based on a capsule network and a multi-language model to obtain character sequence information contained in the pictures. Wherein the capsule network comprises a convolutional layer, a main capsule layer, a digital capsule layer, and a CTC loss function. Dividing the pictures and sequentially inputting the pictures into the capsule network to realize the recognition of the text sequence by the capsule network; the multi-language model recognizes language characteristics in the text sequence recognition result generated by the capsule network and generates a recognition result of the multi-language model so that the recognition result of the capsule network is optimized by the text recognition method.",[]
US20250117917A1,Automated machine vision-based defect detection,"Provided are various mechanisms and processes for automatic computer vision-based defect detection using a neural network. A system is configured for receiving historical datasets that include training images corresponding to one or more known defects. Each training image is converted into a corresponding matrix representation for training the neural network to adjust weighted parameters based on the known defects. Once sufficiently trained, a test image of an object that is not part of the historical dataset is obtained. Portions of the test image are extracted as input patches for input into the neural network as respective matrix representations. A probability score indicating the likelihood that the input patch includes a defect is automatically generated for each input patch using the weighted parameters. An overall defect score for the test image is then generated based on the probability scores to indicate the condition of the object.","['G06N3/045', 'G06N3/08', 'G06T7/0004', 'G06T7/11', 'G06T7/194', 'G06N3/048', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30164']"
US7254256B2,Method and computer program product for locating facial features,"A digital image processing method detects facial features in a digital image. This method includes the steps of detecting iris pixels in the image, clustering the iris pixels, and selecting at least one of the following schemes to identify eye positions: applying geometric reasoning to detect eye positions using the iris pixel clusters; applying a summation of squared difference method using the iris pixel clusters to detect eye positions; and applying a summation of squared difference method to detect eye positions from the pixels in the image. The method applied to identify eye positions is selected on the basis of the number of iris pixel clusters, and the facial features are located using the identified eye positions.","['G06T7/74', 'G06T7/75', 'G06V40/171', 'G06V40/19', 'G06T2207/30216']"
CN112233125B,"Image segmentation method, device, electronic equipment and computer readable storage medium","The invention relates to an image processing technology, and discloses an image segmentation method, which comprises the following steps: the method comprises the steps of segmenting a medical image set to obtain a training image set and a test image set, marking the training image set and the test image set with a label to generate a label image set, performing up-sampling, down-sampling and binarization processing on the training image set by using an image segmentation model to obtain a standard characteristic image set, adjusting internal parameters of the image segmentation model according to error values between the standard characteristic image set and the label image set to obtain an initial image segmentation model, performing verification adjustment on the initial image segmentation model by using the test image set to obtain a standard image segmentation model, and performing segmentation processing on an image to be segmented by using the standard image segmentation model to obtain an image segmentation result. The present invention also relates to blockchain techniques, the set of tag images may be stored in a blockchain. The invention also discloses an image segmentation device, electronic equipment and a storage medium. The invention can improve the accuracy of image segmentation.","['G06T7/11', 'G06F18/214', 'G06T5/50', 'G06V10/25', 'G06T2207/20104', 'G06T2207/20221', 'Y02T10/40']"
CN113989582B,Self-supervision visual model pre-training method based on dense semantic comparison,"The invention discloses a self-supervision visual model pre-training method based on dense semantic comparison, which comprises the following steps: 1) For the sample image x i, the data enhancement methods a and b are used for respectively enhancing the sample image x i and then carrying out feature extraction and mapping to obtain the feature of each pixel point p i in the sample image x i, namely the pixel level featureExample level features2) Based onAndPerforming contrast learning to obtain an instance discrimination loss L ins; based onAndPerforming contrast learning to obtain pixel discrimination loss L pix; 3) Calculating neighbor discrimination loss L nei according to a positive sample set corresponding to the pixel point p i; 4) Clustering is respectively carried out on the pixel-level feature sets v a、vb, and K clustering clusters are respectively obtained; then, each cluster is subjected to contrast learning, and cluster contrast loss L KM is calculated; 5) The self-supervising visual model is trained end-to-end according to the loss function l=l ins+Lpix+Lsem.","['G06F18/2155', 'G06F18/23213', 'G06F18/24', 'G06N3/045', 'G06N3/088']"
CN108596940B,Video segmentation method and device,"The invention discloses a video segmentation method and a video segmentation device, and relates to the technical field of computers. One embodiment of the method comprises: acquiring a video file, and decoding the video file to acquire a frame sequence of the video file; extracting a first frame in the frame sequence to obtain a mask image of the first frame image; and calculating a boundary frame in the sequence of frames; and segmenting the current frame through a preset segmentation model according to the mask image of the first frame and the boundary frame in front of the current frame. The embodiment can more accurately extract the target in the video file.","['G06T7/194', 'H04N21/44008', 'G06T2207/10016']"
US11586863B2,Image classification method and device,"Provided are an image fusion classification method and device. The method includes that: a three-dimensional weight matrix of a hyperspectral image is obtained by use of a Support Vector Machine (SVM) classifier (101); superpixel segmentation is performed on the hyperspectral image to obtain K superpixel images, K being a positive integer (102); the three-dimensional weight matrix is regularized by use of a superpixel-image-based segmentation method to obtain a regular matrix (103); and a class that a sample belongs to is determined according to the regular matrix (104).","['G06K9/6277', 'G06T7/11', 'G06V20/13', 'G06F18/21', 'G06F18/2411', 'G06F18/2415', 'G06F18/254', 'G06K9/6217', 'G06K9/6292', 'G06V10/267', 'G06V20/194']"
CN111833303B,"Product detection method, device, electronic equipment and storage medium","The application discloses a detection method and device for products, electronic equipment and a storage medium, relates to the field of cloud computing, and relates to electronic component defect detection. The specific implementation scheme is as follows: by acquiring the product image of at least one projection surface of the product and acquiring the image quality index of the product image, when the image quality index is larger than a preset quality threshold, the foreground image of the product image is extracted for defect detection, and when the image quality index is smaller than or equal to the preset quality threshold, the product image is input into at least one defect identification model for defect detection, so that automatic detection of the product defect is realized, compared with a manual detection mode, the labor cost and the time cost are saved, the detection efficiency is improved, and the detection of the product defect does not depend on subjective experience of a quality inspector, and the accuracy of a detection result can be improved.","['G06T7/0004', 'G01N21/8851', 'G01N2021/8883', 'G01N2021/8887', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30108', 'Y02P90/30']"
CN109934283A,An adaptive moving object detection method integrating CNN and SIFT optical flow,"The invention discloses a kind of adaptive motion object detection methods for merging CNN and SIFT light stream, comprising the following steps: to the SIFT feature of image and CNN feature extraction and merges first；It is then based on Gaussian process and returns super-resolution rebuilding CNN light stream；And processing is weighted and averaged to light stream；Last combining adaptive threshold value carries out high-precision motion target detection.Figure SIFT feature and CNN structure feature are blended carry out optical flow computation by the present invention, and it proposes to rebuild super-resolution rebuilding technical application to convolutional neural networks CNN light stream in calculating, the accuracy and robustness of existing calculation method can be improved, improve stability of the system in operation.",[]
CN108288088B,A scene text detection method based on end-to-end fully convolutional neural network,"The invention discloses a scene text detection method based on an end-to-end full convolution neural network, which is used for finding the position of a multi-direction text in an image of a natural scene. The method specifically comprises the following steps: acquiring a plurality of image data sets for training scene text detection, and defining an algorithm target; performing feature learning on the image by using a full convolution feature extraction network; predicting an affine transformation matrix of an example level for each sample point on the feature map, and performing feature expression on the text according to the predicted affine transformation deformation sampling grid; classifying the feature vectors of the candidate texts, and simultaneously performing coordinate regression and affine transformation regression to jointly optimize the model; detecting a precise location of text using the learning framework; and carrying out non-maximum suppression on the bounding box set output by the network to obtain a final text detection result. The method is used for scene text detection of real image data, and has good effect and robustness on various complex conditions such as multiple directions, multiple scales, multiple languages, shape distortion and the like.","['G06N3/045', 'G06F16/355']"
CN112162930B,"Control identification method, related device, equipment and storage medium","The application discloses a control identification method based on artificial intelligence, a related device, equipment and a storage medium, comprising the following steps: acquiring an image to be identified; acquiring an original control image from an image to be identified; zooming the original control image to obtain a control image set; matching each control image in the control image set with a control template image to obtain N confidence scores; determining a target control image according to the N confidence scores; and if the confidence score corresponding to the target control image is greater than or equal to the confidence threshold value and the original control image meets the control image output condition, generating a control identification result according to the original control image. The method and the device avoid the process of training the deep learning model in advance, save training cost, and can weaken the sensitivity of the template matching algorithm to the size, thereby effectively improving the identification accuracy.","['G06F11/3668', 'A63F13/52', 'G06V10/50', 'G06V10/56']"
US10929987B2,Learning rigidity of dynamic scenes for three-dimensional scene flow estimation,"A neural network model receives color data for a sequence of images corresponding to a dynamic scene in three-dimensional (3D) space. Motion of objects in the image sequence results from a combination of a dynamic camera orientation and motion or a change in the shape of an object in the 3D space. The neural network model generates two components that are used to produce a 3D motion field representing the dynamic (non-rigid) part of the scene. The two components are information identifying dynamic and static portions of each image and the camera orientation. The dynamic portions of each image contain motion in the 3D space that is independent of the camera orientation. In other words, the motion in the 3D space (estimated 3D scene flow data) is separated from the motion of the camera.","['G06T7/254', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06N5/046', 'G06T3/0093', 'G06T3/18', 'G06T7/11', 'G06T7/194', 'G06T7/215', 'G06T7/285', 'G06T7/50', 'G06T7/60', 'G06T7/70', 'G06T7/90', 'G06V10/454', 'G06V10/772', 'G06V10/774', 'G06V10/82', 'G06V10/955', 'G06V20/64', 'G06T2207/10028', 'G06T2207/20084', 'G06V20/10']"
CN109815924B,"Expression recognition method, device and system","The invention provides an expression recognition method, device and system, relating to the technical field of image recognition, wherein the method comprises the following steps: acquiring a face image to be recognized; extracting the global features of the face image to obtain a semantic feature map of the face image; extracting features of the face image based on an attention mechanism to obtain an attention feature map of the face image; fusing the semantic feature map and the attention feature map to obtain a fused feature map; and carrying out expression recognition on the fusion characteristic graph to obtain an expression recognition result. The method and the device can effectively improve the accuracy of expression recognition.",[]
US20240223742A1,Depth-varying reprojection passthrough in video see-through (vst) extended reality (xr),"A method includes obtaining images of a scene captured using a stereo pair of imaging sensors of an XR device and depth data associated with the images, where the scene includes multiple objects. The method also includes obtaining volume-based 3D models of the objects. The method further includes, for one or more first objects, performing depth-based reprojection of the one or more 3D models of the one or more first objects to left and right virtual views based on one or more depths of the one or more first objects. The method also includes, for one or more second objects, performing constant-depth reprojection of the one or more 3D models of the one or more second objects to the left and right virtual views based on a specified depth. In addition, the method includes rendering the left and right virtual views for presentation by the XR device.","['H04N13/128', 'G06T17/00', 'G06T19/006', 'H04N13/239', 'H04N13/344']"
EP1066577B1,System and method for analyzing video content using detected text in video frames,"There is disclosed, for use in video text analysis system, a video processing device for searching video streams for one or more user-selected image text attributes. The video processing device comprises an image processor capable of detecting and extracting image text from video frames, determining attributes of the extracted image text, comparing the extracted image text attributes and the user-selected image text attributes, and, if a match occurs, modifying, transferring, and/or labeling at least a portion of the video stream in accordance with user commands. The invention uses the user-selected image text attributes to search through an archive of video clips to 1) locate particular types of events, such as news programs or sports events; 2) locate programs featuring particular persons or groups; 3) locate programs by name; 4) save or remove all or some commercials, and to otherwise sort, edit, and save all of, or portions of, video clips according to image text that appears in the frames of the video clips.","['G11B27/28', 'G06F16/7844', 'G06V20/635', 'G11B27/034', 'G11B27/105', 'G06V30/10']"
US10552962B2,Fast motion based and color assisted segmentation of video into region layers,"Techniques related to improved video frame segmentation based on motion, color, and texture are discussed. Such techniques may include segmenting a video frame of a video sequence based on differencing global motion or dominant motion from local motion in the video frame.","['G06T7/215', 'G06T7/11', 'G06T7/207', 'G06T7/223', 'G06T7/254', 'G06T2207/10024', 'G06T2207/20016', 'G06T2207/20021', 'G06T2207/20036', 'G06T2207/20076']"
US9865062B2,Systems and methods for determining a region in an image,A method for determining a region of an image is described. The method includes presenting an image of a scene including one or more objects. The method also includes receiving an input selecting a single point on the image corresponding to a target object. The method further includes obtaining a motion mask based on the image. The motion mask indicates a local motion section and a global motion section of the image. The method further includes determining a region in the image based on the selected point and the motion mask.,"['G06T7/0081', 'G06T7/11', 'G06T7/20', 'G06T7/215', 'G06T2207/10024', 'G06T2207/20101']"
CN107742102B,Gesture recognition method based on depth sensor,"The invention discloses a gesture recognition method based on a depth sensor, which comprises the following steps of sequentially acquiring depth flow information and human body skeleton node data; dividing a gesture area; performing orthogonalization and binarization on the 3D point cloud coordinates of the gesture area; extracting gesture features, normalizing and reducing dimensions; screening the gesture features to obtain an optimal gesture feature subset; training a gesture classifier by using a support vector machine to obtain a classification result; and filtering the gesture classification result. The invention makes up the defects of the prior art and improves the accuracy, stability and efficiency of gesture recognition.","['G06V40/28', 'G06F18/2411', 'G06V10/40']"
CN115861623A,"Blood vessel image segmentation method, device, electronic device and storage medium","The invention discloses a blood vessel image segmentation method, a blood vessel image segmentation device, electronic equipment and a storage medium. The method comprises the following steps: acquiring an initial blood vessel image, determining a coronary artery rough segmentation image of the initial blood vessel image, and determining a coronary artery skeleton line of the coronary artery rough segmentation image; performing edge identification on the coronary artery rough segmentation image to obtain at least one segmentation edge point in the coronary artery rough segmentation image, and determining at least one skeleton end point in each coronary artery skeleton line based on each segmentation edge point; determining a root endpoint and at least one tail branch endpoint in each skeleton endpoint, and determining a vein branch line segment in the coronary skeleton line based on a skeleton communication path between the root endpoint and each tail branch endpoint; determining a coronary segmentation image of the initial vessel image based on the coarse coronary segmentation image and the vein branch line segments. The technical effect of improving the coronary artery segmentation accuracy is achieved through the technical scheme disclosed by the invention.",[]
US11188783B2,Reverse neural network for object re-identification,"The invention relates to a method comprising receiving, by a neural network, a first image comprising at least one target object; receiving, by the neural network, a second image comprising at least one query object; and determining, by the neural network, whether the query object corresponds to the target object, wherein the neural network comprises a discriminator neural network of a generative adversarial network (GAN). The invention further relates to an apparatus and a computer program product that perform the method.","['G06K9/6215', 'G06V10/82', 'G06F18/22', 'G06F18/2413', 'G06F18/2415', 'G06F18/28', 'G06K9/00624', 'G06K9/00744', 'G06K9/4628', 'G06K9/6255', 'G06K9/627', 'G06K9/6277', 'G06N3/04', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/047', 'G06N3/0472', 'G06N3/0475', 'G06N3/08', 'G06N3/088', 'G06N3/09', 'G06N3/094', 'G06V10/454', 'G06V10/764', 'G06V10/772', 'G06V20/00', 'G06V20/46', 'G06F18/2148', 'G06K9/00281', 'G06K9/00288', 'G06K9/6257', 'G06V40/171', 'G06V40/172']"
US9478035B2,2D/3D localization and pose estimation of harness cables using a configurable structure representation for robot operations,"A robot is made to recognize and manipulate different types of cable harnesses in an assembly line. This is achieved by using a stereo camera system to define a 3D cloud of a given cable harness. Pose information of specific parts of the cable harness are determined from the 3D point cloud, and the cable harness is then re-presented as a collection of primitive geometric shapes of known dimensions, whose positions and orientations follow the spatial position of the represented cable harness. The robot can then manipulate the cable harness by using the simplified representation as a reference.","['H04N13/275', 'G06T17/10', 'G06T7/0046', 'G06K9/00214', 'G06K9/4671', 'G06K9/6211', 'G06T7/0075', 'G06T7/0081', 'G06T7/11', 'G06T7/187', 'G06T7/593', 'G06T7/75', 'G06V10/462', 'G06V10/757', 'G06V20/653', 'H04N13/0239', 'H04N13/025', 'H04N13/0275', 'H04N13/239', 'H04N13/25', 'G06K2209/19', 'G06T2207/10012', 'G06T2207/10028', 'G06T2207/20141', 'G06T2207/30108', 'G06V2201/06']"
CN112017189B,"Image segmentation method and device, computer equipment and storage medium","The application relates to an image segmentation method, an image segmentation device, computer equipment and a storage medium based on artificial intelligence. The method comprises the following steps: acquiring two-dimensional section images under different sections in the three-dimensional image; performing semantic segmentation on the target object in each tangent plane image; carrying out example segmentation on the target object in each section image; obtaining an initial segmentation result corresponding to the target object in each tangent plane image according to a semantic segmentation result and an example segmentation result corresponding to the target object in the same tangent plane image; and performing fusion processing on the initial segmentation results corresponding to each tangent plane image to obtain the segmentation result of the target object in the three-dimensional image. By adopting the method, the segmentation efficiency and the segmentation accuracy of the target object in the three-dimensional image data can be effectively improved.","['G06T7/10', 'G06F18/22', 'G06F18/241', 'G06F18/25', 'G06T2207/10004']"
US10062412B2,Hierarchical segmentation and quality measurement for video editing,"Methods for organizing media data by automatically segmenting media data into hierarchical layers of scenes are described. The media data may include metadata and content having still image, video or audio data. The metadata may be content-based (e.g., differences between neighboring frames, exposure data, key frame identification data, motion data, or face detection data) or non-content-based (e.g., exposure, focus, location, time) and used to prioritize and/or classify portions of video. The metadata may be generated at the time of image capture or during post-processing. Prioritization information, such as a score for various portions of the image data may be based on the metadata and/or image data. Classification information such as the type or quality of a scene may be determined based on the metadata and/or image data. The classification and prioritization information may be metadata and may be used to organize the media data.","['G11B27/031', 'G06K9/00718', 'G06K9/00751', 'G06K9/00765', 'G06V20/41', 'G06V20/47', 'G06V20/49', 'G11B27/28']"
WO2002048952A1,Structure-guided image processing and image feature enhancement,"A structure-guided image processing system uses geometric structure information to guide image feature extraction (102) and enhancement (106) of an input image (100) to produce a weight image output (118) and a mask image output (112). Geometric structure information may be apparent in the nature of the images, or it can in many cases be derived from CAD information (124). Idempotent processing and filtering operations minimize image distortion. Directional elongated structuring elements provide structure-guided selective processing and high speed filtering throughput.","['G06T7/12', 'G06T7/155', 'G06V10/34', 'G06V10/44']"
CN111445459B,A method and system for image defect detection based on deep twin network,"The application discloses an image defect detection method and system based on a depth twin network, wherein the method comprises the following steps: acquiring a normal image and a defect image; dividing the normal image and the defect image into image blocks with preset sizes; adopting a twin network to measure the similarity of image blocks at the corresponding positions of the normal image and the defect image; and taking the image blocks with the similarity larger than a preset threshold value as a background, wherein the rest image blocks are image blocks containing defects. The generalization capability of defect detection is improved by utilizing the twin network to measure the similarity of the images.","['G06T7/0004', 'G06F18/22', 'G06N3/045', 'G06N3/08', 'G06T7/194', 'G06T2207/20024', 'G06T2207/20081', 'G06T2207/20084', 'Y02P90/30']"
US8224092B2,Word detection method and system,"A method of characterizing a word image includes traversing the word image in steps with a window and at each of a plurality of the steps, identifying a window image. For each of the plurality of window images, a feature is extracted. The word image is characterized, based on the features extracted from the plurality of window images, wherein the features are considered as a loose collection with associated sequential information.",['G06V30/2264']
US12036979B2,Dynamic distance estimation output generation based on monocular video,"Aspects of the disclosure relate to a dynamic distance estimation output platform that utilizes improved computer vision and perspective transformation techniques to determine vehicle proximities from video footage. A computing platform may receive, from a visible light camera located in a first vehicle, a video output showing a second vehicle that is in front of the first vehicle. The computing platform may determine a longitudinal distance between the first vehicle and the second vehicle by determining an orthogonal distance between a center-of-projection corresponding to the visible light camera, and an intersection of a backside plane of the second vehicle and ground below the second vehicle. The computing platform may send, to an autonomous vehicle control system, a distance estimation output corresponding to the longitudinal distance, which may cause the autonomous vehicle control system to perform vehicle control actions.","['B60W30/09', 'B60W30/18', 'B60W40/09', 'B60W50/14', 'G05B13/027', 'G05D1/0088', 'G05D1/0221', 'G05D1/81', 'G06N3/08', 'G06V10/446', 'G06V10/454', 'G06V10/82', 'G06V20/20', 'G06V20/58', 'B60R2300/10', 'B60R2300/20', 'B60R2300/80', 'B60W2420/403', 'B60W2520/10', 'B60W2554/20', 'B60W2554/4041', 'G06V2201/08']"
US9619704B2,Fast articulated motion tracking,"The present technology relates to a computer-implemented method for tracking an object in a sequence of multi-view input video images comprising the steps of acquiring a model of the object, tracking the object in the multi-view input video image sequence, and using the model.","['G06K9/00536', 'G06T7/292', 'G06T17/00', 'G06T7/2046', 'G06T7/2093', 'G06T7/251', 'G06F2218/12', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20044', 'G06T2207/30196']"
US8649572B2,System and method for enabling the use of captured images through recognition,"An embodiment provides for enabling retrieval of a collection of captured images that form at least a portion of a library of images. For each image in the collection, a captured image may be analyzed to recognize information from image data contained in the captured image, and an index may be generated, where the index data is based on the recognized information. Using the index, functionality such as search and retrieval is enabled. Various recognition techniques, including those that use the face, clothing, apparel, and combinations of characteristics may be utilized. Recognition may be performed on, among other things, persons and text carried on objects.","['G06F16/53', 'G06F16/50', 'G06V40/107', 'G06F16/58', 'G06F16/5846', 'G06F16/587']"
CN107392988B,"Systems, methods and computer program products for rendering at variable sampling rates utilizing projected geometric distortion","Systems, methods, and computer program products are provided for rendering at variable sample rates. Vertex coordinates of the 3D primitive are received from the shader execution unit and arithmetic operations are performed on the vertex coordinates by the fixed operation circuitry to produce modified vertex coordinates in a homogeneous coordinate space. The modified vertex coordinates are transformed from the homogeneous coordinate space to a screen space to produce screen space vertex coordinates of a transformed 3D primitive, and the transformed 3D primitive is rasterized in the screen space using the screen space vertex coordinates to produce an image for display.","['G06T15/80', 'G06T15/10', 'G06T15/20', 'G06T15/30', 'G06T2215/06']"
US9436987B2,Geodesic distance based primitive segmentation and fitting for 3D modeling of non-rigid objects from 2D images,"A stereo camera system produces a stereo image pair of a cable harness, which is used to define a 3D point cloud of the cable harness at its current pose position. Pose information of specific parts of the cable harness are determined from the 3D point cloud, and the cable harness is then re-presented as a collection of primitive geometric shapes of known dimensions, whose positions and orientations follow the spatial position and orientation of the imaged cable harness. The length, position and number of geometric shapes are atomically determined from a 2D image segmentation of one of the images in the stereo image pair.","['G06T7/00', 'G06T17/10', 'G06T15/10', 'G06T7/0081', 'G06T7/11', 'G06T7/187', 'G06T7/593', 'G06T7/60', 'G06T7/602', 'G06T2200/08', 'G06T2207/10012', 'G06T2207/10028']"
CN111275730B,"Map area determination method, device, equipment and storage medium","The application relates to the field of image processing, and particularly discloses a method, a device, equipment and a storage medium for determining a map area, wherein the method comprises the following steps: acquiring an area image; performing image recognition on the area image to obtain a contour line for representing the area image, wherein the contour line comprises a plurality of pixel points; preprocessing a plurality of pixel points according to pixel point coordinates of the pixel points on the regional image to obtain a pixel point coordinate set; based on the mapping relation between the area image and the map, carrying out coordinate conversion on pixel point coordinates in the pixel point coordinate set to obtain a longitude and latitude coordinate set corresponding to the pixel point coordinate set; and drawing a target area corresponding to the contour line on the map according to the longitude and latitude coordinate set. The accuracy in acquiring longitude and latitude coordinates of an irregular area is improved.","['G06T7/187', 'G06F16/29', 'G06T7/11', 'G06T7/181']"
US9633282B2,Cross-trained convolutional neural networks using multimodal images,"Embodiments of a computer-implemented method for training a convolutional neural network (CNN) that is pre-trained using a set of color images are disclosed. The method comprises receiving a training dataset including multiple multidimensional images, each multidimensional image including a color image and a depth image; performing a fine-tuning of the pre-trained CNN using the depth image for each of the plurality of multidimensional images; obtaining a depth CNN based on the pre-trained CNN, wherein the depth CNN is associated with a first set of parameters; replicating the depth CNN to obtain a duplicate depth CNN being initialized with the first set of parameters; and obtaining a depth-enhanced color CNN based on the duplicate depth CNN being fine-tuned using the color image for each of the plurality of multidimensional images, wherein the depth-enhanced color CNN is associated with a second set of parameters.","['G06K9/6256', 'G06V10/56', 'G06F18/214', 'G06K9/4652', 'G06K9/48', 'G06K9/6232', 'G06V10/449', 'G06V20/647', 'G06V30/194']"
CN109740595B,Oblique vehicle detection and tracking system and method based on machine vision,"The invention provides a system and a method for detecting and tracking an oblique vehicle based on machine vision. The system comprises: CCD camera, USB data transmission port and computer terminal. The method comprises the following steps: in the vehicle detection stage, firstly, image preprocessing is carried out, region-of-interest extraction and improved lane line detection are combined, an oblique vehicle detection region is divided, a shadow region is extracted by adopting a self-adaptive threshold and maximum inter-class variance method, the shadow region is further extracted, the shadow line at the bottom of the vehicle is further extracted, the left and right boundaries of the vehicle are determined by combining Sobel vertical edge extraction, a suspected rectangular frame of a vehicle target is obtained, the features in the rectangular frame are extracted, the dimension of the features is reduced by adopting kernel principal component analysis, and detection and confirmation are carried out by utilizing an Adaboost cascade classifier; in the vehicle tracking stage, mean shift and Kalman filtering are combined, the vehicle detection result is used as an initial tracking target, and the tracking target is screened by using the coincidence degree of a rectangular frame. The system and the method can realize real-time vehicle detection and tracking and have higher accuracy.",[]
US11423075B2,Product auditing in point-of-sale images,"Examples methods, apparatus/systems and articles of manufacture for auditing point-of-sale images are disclosed herein. Example methods disclosed herein include comparing a region of interest of an image displayed via a user interface with a plurality of reference product images stored in a database to identify a plurality of candidate product images from the plurality of reference product images as potential matches to a first product depicted in the image. For example, the candidate product images are associated with respective confidence levels indicating respective likelihoods of matching the first product. Disclosed example methods also include displaying, via the user interface, the candidate product images simultaneously with the image in a manner based on the respective confidence levels, and automatically selecting a first one of the candidate product images as matching the first product based on the respective confidence levels.","['G06Q10/087', 'G06F16/51', 'G06F16/5854']"
US20240112035A1,3d object recognition using 3d convolutional neural network with depth based multi-scale filters,"Techniques related to training and implementing convolutional neural networks for object recognition are discussed. Such techniques may include applying, at a first convolutional layer of the convolutional neural network, 3D filters of different spatial sizes to an 3D input image segment to generate multi-scale feature maps such that each feature map has a pathway to fully connected layers of the convolutional neural network, which generate object recognition data corresponding to the 3D input image segment.","['G06N3/084', 'G06F18/213', 'G06N3/04', 'G06N3/045', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/56', 'G06V20/58', 'G06V20/64']"
CN112749576A,"Image recognition method and device, computing equipment and computer storage medium","The application describes an image recognition method, which can be applied to scenes such as but not limited to payment, car union, driving, social contact and the like. The method comprises the following steps: acquiring a plurality of sub-images corresponding to a target image, wherein each of the plurality of sub-images is a part of the target image, and the plurality of sub-images as a whole comprise all image data of the target image; determining feature information for each of the plurality of sub-images; based on repeated features appearing in the feature information of the plurality of sub-images, performing de-duplication processing on the feature information of each sub-image of the plurality of sub-images to obtain image feature information of the target image; identifying the target image based on image feature information of the target image.",['G06K7/146']
US8342687B2,Eye-tracking using a GPU,"Provided is a method of determining a gaze point of an eye watching a visual display controllable by a display signal. The method comprises generating a display signal using a graphics card in order for the visual display to produce a screen pattern; receiving a signal encoding an image of the eye including a corneo-scleral reflection of the screen pattern; and determining, based on in part the geometry of said reflection, a gaze point of the eye, wherein said determining a gaze point includes utilizing the graphics card as a parallel processor.","['G06T7/60', 'G02B27/0093', 'A61B3/113', 'G03B15/16', 'G06T1/00', 'G06V40/19']"
US7436981B2,Apparatus and method for processing video data,"An apparatus and methods for processing video data are described. The invention provides a representation of video data that can be used to assess agreement between the data and a fitting model for a particular parameterization of the data. This allows the comparison of different parameterization techniques and the selection of the optimum one for continued video processing of the particular data. The representation can be utilized in intermediate form as part of a larger process or as a feedback mechanism for processing video data. When utilized in its intermediate form, the invention can be used in processes for storage, enhancement, refinement, feature extraction, compression, coding, and transmission of video data. The invention serves to extract salient information in a robust and efficient manner while addressing the problems typically associated with video data sources.","['H04N19/543', 'H04N19/23', 'H04N19/527', 'H04N19/53', 'H04N19/54']"
CN108381549B,Binocular vision guide robot rapid grabbing method and device and storage medium,"The embodiment of the invention discloses a binocular vision guided robot rapid grabbing method, a device and a storage medium, which comprises the steps of correcting a left camera and a right camera, obtaining a coordinate position of a target point by adopting a matching algorithm based on an edge profile for a left view, solving a matching result on a right view by a self-adaptive weight matching algorithm aiming at the coordinate position of the target point, and obtaining a parallax value; converting the disparity value result into depth information; and calculating the space coordinate of the output target grabbing point according to the depth information, and guiding the robot to finish the rapid positioning grabbing action. The embodiment of the invention also discloses a device and a storage medium for the binocular vision guided robot to rapidly grab. By adopting the method, the interference of light rays can be reduced, the speed can be increased by 3-6 times compared with that of the traditional binocular vision three-dimensional positioning algorithm, the precision in the horizontal direction is as high as 0.1mm, the precision in the vertical direction is as high as within 1mm, and the method is particularly suitable for the grabbing precision requirement of a Scara robot.","['B25J9/16', 'B25J19/04', 'B25J9/10', 'B25J9/1679', 'G06T7/13']"
US20250093281A1,Damage detection from multi-view visual data,Images of an object may be captured at a computing device. Each of the images may be captured from a respective viewpoint based on image capture configuration information identifying one or more parameter values. A multiview image digital media representation of the object may be generated that includes some or all of the images of the object and that is navigable in one or more dimensions.,"['G01N21/9515', 'G06F3/011', 'G06F3/012', 'G06F3/04815', 'G06F3/04845', 'G06F9/453', 'G06Q10/20', 'G06Q40/08', 'G06T15/205', 'G06T19/003', 'G06T3/00', 'G06T3/06', 'G06T7/0004', 'G06T7/001', 'G06T7/30', 'G06T7/70', 'H04N13/117', 'H04N13/282', 'H04N23/633', 'H04N23/64', 'G01N2021/8887', 'G06N3/08', 'G06T2200/08', 'G06T2200/24', 'G06T2207/10028', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30108', 'G06T2207/30156', 'G06T2207/30244', 'G06T2207/30248', 'H04N23/698']"
US10049492B2,Method and apparatus for rendering facades of objects of interest from three-dimensional point clouds,"A method comprising: obtaining a plurality of three-dimensional (3D) point clouds about a plurality of objects of interest, each of said 3D point clouds being labelled to a category of objects of interest; rendering facades for the objects of interests categorized as buildings using an ambient occlusion method, where illumination of the point cloud is calculated based on light coming from a theoretical hemisphere or sphere around the object of interest; and rendering shapes of the objects of interests categorized as non-buildings by fitting predefined templates of street view objects to the point clouds labelled as non-buildings.","['G06T17/20', 'G06T17/05', 'G06T15/506', 'G06T7/521', 'G06T2200/04', 'G06T2210/12']"
CN108369484B,System and method for guiding hand-written graph input,"A system for guiding hand-drawn diagrams including textual and non-textual elements on a computing device is disclosed, the computing device including a processor and at least one non-transitory computer-readable medium for detecting and recognizing hand-drawn diagram element input under control of the processor, the at least one non-transitory computer-readable medium configured to cause display of a guide element associated with at least one diagram element of the displayed hand-drawn diagram input on an interactive display of the computing device, the guide element configured to depict the at least one diagram element in recognized form.","['G06F3/04883', 'G06F3/04817', 'G06F3/0482', 'G06F40/166', 'G06F40/171', 'G06V30/32', 'G06V30/387', 'G06V30/10']"
US11698529B2,Systems and methods for distributing a neural network across multiple computing devices,"Disclosed herein is a method for using a neural network across multiple devices. The method can include receiving, by a first device configured with a first one or more layers of a neural network, input data for processing via the neural network implemented across the first device and a second device. The method can include outputting, by the first one or more layers of the neural network implemented on the first device, a data set that is reduced in size relative to the input data while identifying one or more features of the input data for processing by a second one or more layers of the neural network. The method can include communicating, by the first device, the data set to the second device for processing via the second one or more layers of the neural network implemented on the second device.","['G02B27/017', 'G06N3/063', 'G02B27/0093', 'G02B27/0172', 'G06F3/011', 'G06F3/013', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'H04N13/106', 'G02B2027/0138', 'G02B2027/014', 'G02B7/06', 'G02B7/09']"
CN106446526B,Method and device for extracting entity relationship from electronic medical records,"The disclosure discloses a kind of electronic health record entity relation extraction method and device, belongs to medical data excavation applications.This method comprises: being indicated by convolutional neural networks model and term vectorization, the matrix after obtaining the mapping of electronic health record nature sentence；By the electronic health record nature input by sentence of test to the convolutional neural networks model trained, feature vector is obtained；Feature vector is input to the classifier trained, extracts the entity relationship of the electronic health record nature sentence of the test.In this way, the advantage of convolutional neural networks model is utilized, the relationship in electronic health record natural language between entity is excavated, provides technological approaches for automatic study electronic health record information.",[]
US20230043026A1,Learning-based active surface model for medical image segmentation,"A learning-based active surface model for medical image segmentation uses a method including: (a) data generation: obtaining medical images and associated ground truths, and splitting the sample images into a training set and a testing set; (b) raw segmentation: constructing a surface initialization network, parameters of the network trained by images and labels in the training set; (c) surface initialization: segmenting the images by the surface initialization network, and generating the point cloud data as the initial surface from the segmentation; (d) fine segmentation: constructing the surface evolution network, the parameters of the network trained by the initial surface obtained in step (c); (e) surface evolution: deforming the initial surface points along the offsets to obtain the predicted surface, the offsets presenting the prediction of the surface evolution network; (f) surface reconstruction: reconstructing the 3D volumes from the set of predicted surface points set to obtain the final segmentation results.","['G06T7/12', 'G06T17/20', 'G06T7/10', 'G06F18/214', 'G06K9/6256', 'G06N3/045', 'G06N3/08', 'G06T7/0012', 'G06T7/11', 'G06T7/13', 'G06T7/149', 'G06V10/22', 'G06V10/82', 'G06V20/64', 'G06T2200/04', 'G06T2207/10028', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30092', 'G06V2201/03']"
US7035456B2,Face detection in color images with complex background,"A method (100) of locating human faces, if present, in a cluttered scene captured on a digital image (105) is disclosed. The method (100) relies on a two step process, the first being the detection of segments with a high probability of being human skin in the color image (105), and to then determine a bounday box, or other boundary indication, to border each of those segments. The second step (140) is the analysis of features within each of those boundary boxes to determine which of the segments are likely to be a human face. As human skin is not highly textured, in order to detect segments with a high probability of being human skin, a binary texture map (121) is formed from the image (105), and segments having high texture are discarded.",['G06V40/161']
US7894689B2,Image stitching,"Disclosed are embodiments of systems and methods to stitch two or more images together into a composite image. By finding matching point pairs for a pair of images, a homography transform may be obtained for the pair of images. The homography transform may be used to generate a composite image of the image pair. In an embodiment, the process of identifying a homography transform may be iterated. In an embodiment, when forming the composite image, the transformed foreground regions may be selected such that there is no intersection of foreground pixel regions. In an embodiment, foreground pixel regions on the border of an image may be removed. The resulting composite image is a larger image generated from the selected regions from the input images. In embodiments, the process may be repeated for sets of images with more than two images.","['G06T7/33', 'G06T2200/32', 'G06T2207/10008', 'G06T2207/20164', 'G06T2207/30176']"
US9956054B2,Dynamic minimally invasive surgical-aware assistant,"A computer system that provides situational awareness and feedback during a surgical procedure is described. During operation, the computer system generates stereoscopic images at a first 3-dimensional (3D) location in an individual, where the stereoscopic images include image parallax and are based on data having a discrete spatial resolution and a predefined surgical plan for the individual. Then, the computer system provides the stereoscopic images to a display. When a surgical tool is advanced, location information of the surgical tool is updated in the stereoscopic images displayed are updated. Alternatively or additionally, When location information indicates a deviation from the predefined surgical plan during the surgical procedure, the computer system generates revised stereoscopic images that indicate: the deviation has occurred an update to the predefined surgical plan and/or how to return to the predefined surgical plan.","['H04N13/20', 'A61B90/37', 'A61B34/20', 'A61B34/25', 'G06F3/147', 'H04N13/004', 'H04N13/02', 'H04N13/04', 'H04N13/156', 'H04N13/30', 'A61B2034/2055', 'A61B2034/2065', 'A61B2034/252', 'G09G2340/0407', 'G09G2340/0464', 'H04N2201/0079']"
US9087232B2,3D object recognition,"A method, device, system, and computer program for object recognition of a 3D object of a certain object class using a statistical shape model for recovering 3D shapes from a 2D representation of the 3D object and comparing the recovered 3D shape with known 3D to 2D representations of at least one object of the object class.","['G06V20/653', 'G06T17/20', 'G06K9/00281', 'G06K9/00214', 'G06V20/64', 'G06V40/16', 'G06V40/171', 'G06V40/172']"
US20240156547A1,Generating augmented visualizations of surgical sites using semantic surgical representations,"A surgical action to be performed during a surgical procedure is predicted using machine learning based on images and surgical instrumentation data. An image/video capture device such as an endoscope, a wearable camera, a stationary camera, etc., can be used to capture the image(s). A surgeon can be provided an augmented visualization of the surgical procedure by displaying one or more graphical overlays based on the findings of the machine learning to enhance the surgeon's information.","['A61B34/25', 'A61B34/20', 'A61B90/36', 'A61B90/37', 'G06F18/24133', 'G06T11/00', 'G06T11/001', 'G06T7/0012', 'G06T7/11', 'G06T7/20', 'G06T7/50', 'G06T7/70', 'G06T7/74', 'G06V10/26', 'G06V10/774', 'G06V20/41', 'G06V20/50', 'G16H20/40', 'G16H30/40', 'G16H40/20', 'G16H40/67', 'G16H50/20', 'G16H50/70', 'G16H70/20', 'A61B2017/00026', 'A61B2017/00119', 'A61B2034/107', 'A61B2034/2065', 'A61B2034/252', 'A61B2090/364', 'A61B2090/365', 'A61B2090/371', 'A61B2090/372', 'G06T2207/10016', 'G06T2207/10068', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004', 'G06T2210/41', 'G06V2201/03', 'G06V2201/031', 'G06V2201/034']"
US8009921B2,Context dependent intelligent thumbnail images,"An apparatus and method are disclosed for context dependent cropping of a source image. The method includes identifying a context for the source image, identifying a visual class corresponding to the identified context from a set of visual classes, applying a class model to the source image to identify a candidate region of the image based on its relevance to the visual class, and identifying a subpart of the source image for cropping, based on the location of the candidate region.","['G06T11/60', 'G06V10/25', 'G06V20/10', 'H04N1/3873']"
US6937744B1,System and process for bootstrap initialization of nonparametric color models,"The present invention is embodied in a system and process for automatically learning a reliable color-based tracking system. The tracking system is learned by using information produced by an initial object model in combination with an initial tracking function to probabilistically determine the configuration of one or more target objects in a temporal sequence of images, and a data acquisition function for gathering observations relating to color in each image. The observations gathered by the data acquisition function include information that is relevant to parameters desired for a final color-based object model. A learning function then uses probabilistic methods to determine conditional probabilistic relationships between the observations and probabilistic target configuration information to learn a color-based object model automatically tailored to specific target objects. The learned object model is then used in combination with the final tracking function to probabilistically locate and track specific target objects in one or more sequential images.",['G06T7/20']
US12008797B2,Image segmentation method and image processing apparatus,"This application discloses an image segmentation method in the field of artificial intelligence. The method includes: obtaining an input image and a processing requirement; performing multi-layer feature extraction on the input image to obtain a plurality of feature maps; downsampling the plurality of feature maps to obtain a plurality of feature maps with a reference resolution, where the reference resolution is less than a resolution of the input image; fusing the plurality of feature maps with the reference resolution to obtain at least one feature map group; upsampling the feature map group by using a transformation matrix W, to obtain a target feature map group; and performing target processing on the target feature map group based on the processing requirement to obtain a target image.","['G06V10/457', 'G06F18/2135', 'G06F18/2137', 'G06F18/214', 'G06F18/253', 'G06N20/00', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06T3/40', 'G06V10/267', 'G06V10/454', 'G06V10/7715', 'G06V10/82']"
US9164589B2,Dynamic gesture based short-range human-machine interaction,"Systems, devices and methods are described including starting a gesture recognition engine in response to detecting an initiation gesture and using the gesture recognition engine to determine a hand posture and a hand trajectory in various depth images. The gesture recognition engine may then use the hand posture and the hand trajectory to recognize a dynamic hand gesture and provide corresponding user interface command.","['G06V40/28', 'G06F3/017', 'G06K9/00355']"
CN111754513B,"Product surface defect segmentation method, defect segmentation model learning method and device","The application discloses a product surface defect segmentation method based on twin contrast learning, which performs explicit ordering learning on coding features of a reference picture and a defect picture so as to automatically focus a defect part, and simultaneously performs implicit contrast learning on decoding features of the reference picture and the defect picture so as to finely predict a defect mask. In the application stage, obtaining a reference image and a to-be-detected image of a product; based on the reference graph and the graph to be measured, a twin coding network is used for obtaining the reference graph and the coding feature graph of the graph to be measured; based on the coding feature diagrams of the reference diagram and the diagram to be detected, a twin decoding network is applied to obtain decoding feature diagrams of the reference diagram and the diagram to be detected; performing feature fusion on the decoding feature graphs of the reference graph and the graph to be detected to generate a comparison feature graph; based on the contrast feature map, a segmentation mask of the map to be measured is obtained using convolution. The application also provides a defect segmentation model learning method and device, which can realize mask prediction of product surface defects on a pixel level and meet the high-order application requirements to a great extent.","['G06T7/0002', 'G06T3/4038', 'G06T7/12', 'G06T9/002', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084', 'Y02P90/30']"
US11868863B2,Systems and methods for joint learning of complex visual inspection tasks using computer vision,"A method for performing automatic visual inspection includes: capturing visual information of an object using a scanning system including a plurality of cameras; extracting, by a computing system including a processor and memory, one or more feature maps from the visual information using one or more feature extractors; classifying, by the computing system, the object by supplying the one or more feature maps to a complex classifier to compute a classification of the object, the complex classifier including: a plurality of simple classifiers, each simple classifier of the plurality of simple classifiers being configured to compute outputs representing a characteristic of the object; and one or more logical operators configured to combine the outputs of the simple classifiers to compute the classification of the object; and outputting, by the computing system, the classification of the object as a result of the automatic visual inspection.","['G06N20/10', 'G06F18/2411', 'G06F18/254', 'G06N3/04', 'G06N3/042', 'G06N3/044', 'G06N3/045', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06N3/096', 'G06N3/0985', 'G06N5/01', 'G06T7/0002', 'G06T7/50', 'G06T7/90', 'G06V10/765', 'G06V10/82', 'G06V20/80', 'G06V30/153', 'G06N3/048', 'G06N7/01']"
WO2015021473A1,"Apparatus, systems and methods for enrollment of irregular shaped objects","The present disclosure provides systems and methods for enrollment of irregular shaped objects. The system described herein includes an image capturing camera for capturing images of a package and a processing unit communicatively coupled to the camera. The processing unit may receive one or more images of the package from the camera. The processing unit may determine a first volume, a second volume, and the rectangle-score of the package. Responsive to determining the first volume, the second volume, and the rectangle-score of the package, a cuboid-score for the package is determined. Finally, the processing unit determines a shape of the package based on the cuboid-score.","['G01B21/02', 'B07C1/14', 'G01B11/00', 'G01B11/028', 'G01B5/0021', 'G06T7/62', 'G06V10/753', 'G06V20/64', 'G06T2207/30108']"
US20210342997A1,Computer Vision Systems and Methods for Vehicle Damage Detection with Reinforcement Learning,"Computer vision systems and methods for vehicle damage detection are provided. An embodiment of the system generates a dataset and trains a neural network with a plurality of images of the dataset to learn to detect an attribute of a vehicle present in an image of the dataset and to classify at least one feature of the detected attribute. The system can detect the attribute of the vehicle and classify the at least one feature of the detected attribute by the trained neural network. In addition, an embodiment of the system utilizes a neural network to reconstruct a vehicle from one or more digital images.","['G06T7/0004', 'G06F18/214', 'G06F18/24', 'G06K9/6256', 'G06K9/6267', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06T17/20', 'G06T7/0002', 'G06T7/11', 'G06T7/50', 'G06T7/70', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G07C5/006', 'G06F30/15', 'G06F30/17', 'G06F30/27', 'G06K2209/23', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30108', 'G06V2201/08', 'G06V2201/12']"
CN113160257B,"Image data labeling method, device, electronic equipment and storage medium","The application is applicable to the technical field of computers, and provides an image data labeling method, an image data labeling device, electronic equipment and a storage medium, wherein the image data labeling method comprises the following steps of: acquiring an initial annotation image, wherein the initial annotation image is an image obtained by inputting an original image into a first semantic segmentation model according to pixel types; determining the outline corresponding to the region where each type of pixel is located in the initial annotation image; performing polygon fitting on each outer contour to obtain polygons corresponding to the outer contours respectively; and marking the anchor points according to the vertexes of each polygon to obtain target marking data. The embodiment of the application can improve the image data labeling efficiency of semantic segmentation.","['G06T7/12', 'G06F18/214', 'G06F18/24', 'G06T11/203', 'G06T7/13', 'G06T7/187', 'G06T7/62', 'G06T2207/20081']"
US9508151B2,"Systems, methods, and devices for image matching and object recognition in images using image regions","A computer-implemented method for determining whether a first image contains at least a portion of a second image, includes: dividing set first image into multiple image regions; for a particular image region of the multiple image regions, determining a particular set of feature points associated with the particular image region; and attempting to match feature points in the particular set of feature points with second feature points associated with the second image to determine whether the particular image region of the first image contains at least a portion of the second image, wherein the first image is considered to contain at least a portion of a second image when the particular image region of the first image contains at least a portion of the second image.","['G06T7/73', 'G06T7/0042', 'G06K9/00', 'G06V10/462', 'G06V10/757', 'G06T2207/20021', 'G06V2201/09']"
US7783118B2,Method and apparatus for determining motion in images,A method of determining motion in a set of input images comprises registering the images in the set to identify stationary background content therein and generating difference images based on the registered images. The input images are segmented into regions and local motion within each of the regions is estimated. A determination as to whether the regions include moving content or static content is determined based on the difference images and the estimated local motion within the regions.,"['H04N5/145', 'G06T7/11', 'G06T7/155', 'G06T7/174', 'G06T7/194', 'G06T7/215', 'G06T2207/10016', 'G06T2207/20152']"
US20200085382A1,"Automated lesion detection, segmentation, and longitudinal identification","Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are commonly used to assess patients with known or suspected pathologies of the lungs and liver. In particular, identification and quantification of possibly malignant regions identified in these high-resolution images is essential for accurate and timely diagnosis. However, careful quantitative assessment of lung and liver lesions is tedious and time consuming. This disclosure describes an automated end-to-end pipeline for accurate lesion detection and segmentation.","['A61B5/055', 'A61B5/7264', 'A61B5/7267', 'A61B6/032', 'A61B6/5217', 'A61B6/563', 'G06F18/24143', 'G06N3/045', 'G06N3/0454', 'G06N3/084', 'G06T7/0016', 'G06T7/30', 'G06V10/764', 'G06V10/82', 'G16H50/30', 'G06N3/082', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30056', 'G06T2207/30064', 'G06T2207/30096', 'G06V2201/031']"
CN114494192B,Thoracolumbar fracture identification segmentation and detection positioning method based on deep learning,"The invention discloses a thoracolumbar fracture identification segmentation and detection positioning method based on deep learning, which comprises the following steps: s1: CT images are acquired, and a U-Net neural network is utilized to conduct thoracolumbar vertebra identification segmentation, so that a thoracolumbar vertebra target area with fracture is obtained; s2: in the target area of the thoracolumbar bones with fracture, the thoracolumbar fracture is detected and positioned by using a Faster-Rcnn deep learning network. The invention needs to locate the target area of thoracolumbar bones possibly with fracture from the CT image for subsequent deep analysis of the vertebral state. Therefore, the Faster-Rcnn deep learning network is used for detecting the fracture of the thoracolumbar bones, the relative position association relation of the concerned target in the image scene space is explored, the fracture position belongs to the thoracolumbar or the lumbar and the sections of the fracture position are positioned, and the detection segmentation and positioning method for the various fracture types of the thoracolumbar is formed, so that the positioning is accurate.","['G06T7/0012', 'G06F18/2415', 'G06F18/253', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T7/70', 'G16H30/20', 'G06T2207/10081', 'G06T2207/20132', 'G06T2207/30012']"
US9171204B2,Method of perspective correction for devanagari text,"An electronic device and method identify regions that are likely to be text in a natural image or video frame, followed by processing as follows: lines that are nearly vertical are automatically identified in a selected text region, oriented relative to the vertical axis within a predetermined range −max_theta to +max_theta, followed by determination of an angle θ of the identified lines, followed by use of the angle θ to perform perspective correction by warping the selected text region. After perspective correction in this manner, each text region is processed further, to recognize text therein, by performing OCR on each block among a sequence of blocks obtained by slicing the potential text region. Thereafter, the result of text recognition is used to display to the user, either the recognized text or any other information obtained by use of the recognized text.","['G06K9/00469', 'G06V30/1478', 'G06K9/3283', 'G06V30/416', 'G06K2009/363', 'G06V30/10', 'G06V30/1607']"
US11037051B2,3D plane detection and reconstruction using a monocular image,"Planar regions in three-dimensional scenes offer important geometric cues in a variety of three-dimensional perception tasks such as scene understanding, scene reconstruction, and robot navigation. Image analysis to detect planar regions can be performed by a deep learning architecture that includes a number of neural networks configured to estimate parameters for the planar regions. The neural networks process an image to detect an arbitrary number of plane objects in the image. Each plane object is associated with a number of estimated parameters including bounding box parameters, plane normal parameters, and a segmentation mask. Global parameters for the image, including a depth map, can also be estimated by one of the neural networks. Then, a segmentation refinement network jointly optimizes (i.e., refines) the segmentation masks for each instance of the plane objects and combines the refined segmentation masks to generate an aggregate segmentation mask for the image.","['G06N3/0454', 'G06N3/08', 'G06F18/21', 'G06F18/24', 'G06K9/6217', 'G06K9/6267', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/09', 'G06T7/11', 'G06T7/174', 'G06T7/50', 'G06T7/579', 'G06T2207/10016', 'G06T2207/30244', 'G06T2207/30261']"
US10839573B2,"Apparatus, systems, and methods for integrating digital media content into other digital media content","Some embodiments of the present disclosure provide a content integration system. The content integration system is configured to retrieve a source digital content, retrieve a target digital content, identify a region within the target digital content for placing or integrating the source digital content, and place or integrate the target digital content onto the identified region of the source digital content. The content integration system can be configured to place the source digital content into the target digital content in an aesthetically-pleasing, unobtrusive, engaging, and/or otherwise favorable manner. The content integration system can be particularly useful for advertisements, enhanced expression, entertainment, information, or communication.","['G06Q30/0276', 'G06K9/00765', 'G06K9/4642', 'G06T11/60', 'G06T7/11', 'G06T7/194', 'G06T7/246', 'G06T7/262', 'G06T7/44', 'G06T7/536', 'G06T7/90', 'G06V20/49', 'G06T2207/10004', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30204', 'G06T2207/30244']"
WO2021169128A1,"Method and apparatus for recognizing and quantifying fundus retina vessel, and device and storage medium","A method and apparatus for recognizing and quantifying a fundus retina vessel, and a device and a storage medium, applicable to the field of precision medicines. The method comprises: inputting an original fundus image into a pre-trained U-shaped convolutional neural network model for processing, to obtain a target feature map; performing optic disk segmentation based on the target feature map; segmenting the original fundus image to obtain the arteriovenous vessel recognition result; carrying out region-of-interest positioning based on the optic disk segmentation result; extracting a vessel centerline according to the arteriovenous vessel recognition result, detecting key points in the vessel centerline, removing the key points to obtain a plurality of mutually independent vessel segments, and correcting arteriovenous category information on each vessel segment; and obtaining the vessel diameter of each vessel segment after category information correction based on the extracted vessel centerline, and then quantifying arteriovenous vessels in a region of interest. The method facilitates improving the fundus retina artery and vein vessel identification precision, thereby improving the quantization precision.","['G06T7/0012', 'A61B3/12', 'G06N3/045', 'G06T7/10', 'G06T7/13', 'G06T7/62', 'G06T2207/20104', 'G06T2207/30041', 'G06T2207/30101']"
CN109117836B,A method and device for text detection and positioning in natural scenes based on focal loss function,"The invention discloses a method and a device for detecting and positioning characters in a natural scene based on a focus loss function. The method comprises the steps of preprocessing labeled data, constructing a text detection positioning network, referring to a focusing loss function as a part of a loss function in a training process, and detecting a natural scene picture to be detected. The method makes the label more suitable for the designed character detection network by adjusting the existing label; combining the multiple convolution layers based on the FCN network to make the multiple convolution layers more consistent with a character detection task; and the positive and negative samples are balanced in the training process by introducing a focusing loss function, so that the detection precision is improved. The invention can obtain the effect of high precision and high recall in character detection and positioning.",['G06V10/25']
US10776970B2,Method and apparatus for processing video image and computer readable medium,"Embodiments of the present application provide a method and an apparatus for processing a video image. The method includes: obtaining a video image to be processed and a business object to be displayed, wherein the video image comprises a background area and a foreground area comprising a target object non-overlapping with the background area; determining the background area of the video image; performing an action detection on the target object in the foreground area to obtain action detection data; determining a display position of the business object in the video image according to the action detection data; and drawing, according to the display position, the business object in the background area of the video image by means of computer graphics.","['G06T11/40', 'H04N21/23418', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T11/00', 'G06T7/11', 'G06T7/194', 'G06T7/73', 'G11B27/036', 'H04N21/23424', 'H04N21/44008', 'H04N21/44016', 'H04N21/4666', 'H04N21/812', 'G06N3/048', 'G06N3/0481', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084']"
US11714880B1,Hand pose estimation for machine learning based gesture recognition,"The technology disclosed performs hand pose estimation on a so-called “joint-by-joint” basis. So, when a plurality of estimates for the 28 hand joints are received from a plurality of expert networks (and from master experts in some high-confidence scenarios), the estimates are analyzed at a joint level and a final location for each joint is calculated based on the plurality of estimates for a particular joint. This is a novel solution discovered by the technology disclosed because nothing in the field of art determines hand pose estimates at such granularity and precision. Regarding granularity and precision, because hand pose estimates are computed on a joint-by-joint basis, this allows the technology disclosed to detect in real time even the minutest and most subtle hand movements, such a bend/yaw/tilt/roll of a segment of a finger or a tilt an occluded finger, as demonstrated supra in the Experimental Results section of this application.","['G06K9/6269', 'G06F18/2411', 'G06V40/11', 'G06K9/00355', 'G06K9/4604', 'G06T7/13', 'G06V10/25', 'G06V10/44', 'G06V10/774', 'G06V10/82', 'G06V20/64', 'G06V40/28', 'G06T2207/10028']"
CN107610087B,An automatic segmentation method of tongue coating based on deep learning,"The invention discloses a tongue coating automatic segmentation method based on deep learning, which comprises the following steps: s1, collecting and inputting an image containing the tongue coating; s2, detecting the tongue coating of the image containing the tongue coating by adopting a Faster R-CNN deep learning method, and automatically obtaining a preliminary tongue coating area image; s3, calibrating the preliminary tongue fur area image by adopting a VGG deep learning method to obtain a more accurate tongue fur area image; and S4, automatically segmenting the tongue fur image according to the calibrated tongue fur area image. The method realizes more accurate tongue fur segmentation based on the deep learning method of big data, and solves the problem of low tongue fur segmentation accuracy of the existing method.",[]
US20200410074A1,"Identity authentication method and apparatus, electronic device, and storage medium","An identity authentication method and apparatus, and a storage medium are provided. The identity authentication method includes: performing, by means of a first neural network, face detection on an image to be processed to obtain a face detection result, and performing, by means of a second neural network, document detection on the image to be processed to obtain a document detection result; determining whether the image to be processed is a valid identity authentication image according to the face detection result and the document detection result; and in response to determining that the image to be processed is a valid identity authentication image, performing identity authentication according to the face detection result and the document detection result to obtain an identity authentication result of the image to be processed.","['G06F21/32', 'G06F16/535', 'G06F18/22', 'G06F21/33', 'G06F21/45', 'G06K9/00248', 'G06K9/00268', 'G06K9/00288', 'G06K9/00899', 'G06K9/325', 'G06K9/6215', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06V10/82', 'G06V20/62', 'G06V30/19173', 'G06V30/413', 'G06V40/161', 'G06V40/165', 'G06V40/168', 'G06V40/169', 'G06V40/172', 'G06V40/40', 'G06T2207/30201']"
US8412710B2,Searching for handwritten annotations appearing a given distance from document content,A method of searching a database containing documents with handwritten annotations stored in association with document content is disclosed. The method receives as inputs a handwritten annotation as a handwritten search input and a content element as a content search input. A search is then performed of the database for handwritten annotations in a document matching the handwritten search input and less than a given physical distance from document content equal to the content search input in the document.,"['G06F16/50', 'G06F16/53', 'G06F16/93', 'G06V30/15', 'G06V30/268', 'G06V30/10', 'Y10S707/99933']"
US8005294B2,Cursive character handwriting recognition system and method,"A cursive character handwriting recognition system includes image processing means for processing an image of a handwritten word of one or more characters and classification means for determining an optimal string of one or more characters as composing the imaged word. The processing means segments the characters such that each character is made up of one or more segments and determines a sequence of the segments using an over-segmentation-relabeling algorithm. The system also includes feature extraction means for deriving a feature vector to represent feature information of one segment or a combination of several consecutive segments. The over-segmentation-relabeling algorithm places certain segments considered as diacritics or small segments so as to immediately precede or follow a segment of the associated main character body. Additionally, the system also includes classification means that processes each string of segments and outputs a number of optimal strings which could be matched against a given lexicon.",['G06V30/2268']
US12260679B1,Hand initialization for machine learning based gesture recognition,"The technology disclosed also initializes a new hand that enters the field of view of a gesture recognition system using a parallax detection module. The parallax detection module determines candidate regions of interest (ROI) for a given input hand image and computes depth, rotation and position information for the candidate ROI. Then, for each of the candidate ROI, an ImagePatch, which includes the hand, is extracted from the original input hand image to minimize processing of low-information pixels. Further, a hand classifier neural network is used to determine which ImagePatch most resembles a hand. For the qualified, most-hand like ImagePatch, a 3D virtual hand is initialized with depth, rotation and position matching that of the qualified ImagePatch.","['G06V40/28', 'G06V40/107', 'G06F18/217', 'G06F18/22', 'G06F3/011', 'G06F3/017', 'G06N3/045', 'G06N3/08', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V30/194', 'G06V40/113', 'G06V2201/033']"
US11804040B2,Keypoint-based sampling for pose estimation,"Systems and techniques are provided for determining one or more poses of one or more objects. For example, a process can include determining, using a machine learning system, a plurality of keypoints from an image. The plurality of keypoints are associated with at least one object in the image. The process can include determining a plurality of features from the machine learning system based on the plurality of keypoints. The process can include classifying the plurality of features into a plurality of joint types. The process can include determining pose parameters for the at least one object based on the plurality of joint types.","['G06V40/28', 'G06V20/20', 'G06F18/213', 'G06F18/2431', 'G06N3/04', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/084', 'G06N3/088', 'G06N3/09', 'G06T15/005', 'G06T19/006', 'G06T7/50', 'G06T7/73', 'G06T7/80', 'G06T9/002', 'G06V10/40', 'G06V10/454', 'G06V10/469', 'G06V10/764', 'G06V10/82', 'G06V40/11', 'G06T2200/08', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196', 'G06T2207/30244']"
CN111145170B,Medical image segmentation method based on deep learning,"The invention belongs to the technical field of medical image processing and computer vision, and particularly relates to a medical image segmentation method based on deep learning. The method disclosed by the invention integrates multiple technologies such as a multi-scale frame, a dense convolution network, an attention mechanism, a pyramid model, small sample enhancement and the like on the basis of U-Net Baseline, is beneficial to realizing feature reuse, recovering lost context information, inhibiting the response of an irrelevant area and improving the performance of a small ROI, solves the problems of pain points such as few ultrasonic image samples, low pixels, fuzzy boundaries, large differences and the like, and obtains an optimal segmentation effect.","['G06T7/0012', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T2207/20081', 'G06T2207/20084']"
CN118397588B,"Camera scene analysis method, system, equipment and medium for intelligent driving automobile","The invention provides a camera scene analysis method, a system, equipment and a medium for an intelligent driving automobile, wherein the method comprises the steps of acquiring image data, three-dimensional point cloud data and environmental parameters of the surrounding environment of the automobile; extracting dynamic objects around a vehicle from image data, and extracting features of the image data and three-dimensional point cloud data; carrying out space registration on the image data and the three-dimensional point cloud data, establishing an image point cloud model, and obtaining the structure relation and continuous frame data of the dynamic object in the image point cloud model; and (3) carrying out scene analysis by combining the extracted dynamic object and the image point cloud model, and adjusting the driving strategy of the intelligent driving automobile in real time based on the scene analysis result. The intelligent driving automobile driving method and the intelligent driving automobile driving system can improve the driving experience of the intelligent driving automobile, reduce the accident probability and improve the safety.","['G06V20/56', 'G06T7/10', 'G06T7/136', 'G06T7/20', 'G06T7/30', 'G06V10/40', 'G06V10/751', 'G06V10/764', 'G06V20/64', 'G06T2207/10028', 'G06T2207/30241', 'G06T2207/30248']"
US20250157033A1,Content based image retrieval for lesion analysis,"Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are commonly used to assess patients with known or suspected pathologies of the lungs and liver. In particular, identification and quantification of possibly malignant regions identified in these high-resolution images is essential for accurate and timely diagnosis. However, careful quantitative assessment of lung and liver lesions is tedious and time consuming. This disclosure describes an automated end-to-end pipeline for accurate lesion detection and segmentation.","['G16H50/20', 'G06N3/08', 'G06T7/0012', 'G06T7/11', 'G06T7/143', 'G06T7/194', 'G06V10/82', 'G16H10/60', 'G16H20/10', 'G16H20/40', 'G16H30/40', 'G16H50/70', 'G16H70/20', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/20036', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20152', 'G06T2207/30056', 'G06T2207/30064', 'G06T2207/30096']"
US8514264B2,Remote workspace sharing,"Existing remote workspace sharing systems are difficult to use. For example, changes made on a common work product by one user often appear abruptly on displays viewed by remote users. As a result the interaction is perceived as unnatural by the users and is often inefficient. Images of a display of a common work product are received from a camera at a first location. These images may also comprise information about objects between the display and the camera such as a user's hand editing a document on a tablet PC. These images are combined with images of the shared work product and displayed at remote locations. Advance information about remote user actions is then visible and facilitates collaborative mediation between users. Depth information may be used to influence the process of combining the images.","['G06Q10/10', 'H04N7/15']"
CN115170576B,Aluminum pipe surface defect detection method based on machine vision,"The invention relates to the field of image data processing, in particular to a machine vision-based aluminum pipe surface defect detection method, which comprises the following steps: acquiring a gray scale image before the aluminum pipe is denoised; performing sliding window on the gray level image before denoising, and dividing the center point of the sliding window into non-edge points or edge points; acquiring a denoised gray level image under each sliding window size: when the central point is a non-edge point, obtaining a non-edge point de-noised gray value by utilizing the gray values of the non-central point and the central point of the sliding window; when the central point is the edge point, obtaining the gray value of the edge point after denoising by using the gray value of the pixel point in each direction of the sliding window passing through the central point and the gray value of the central point; obtaining the signal-to-noise ratio of the de-noised gray scale image under each sliding window size, and further obtaining the optimal sliding window size; and performing threshold segmentation on the denoised gray scale image under the optimal sliding window size, and judging whether the surface of the aluminum pipe has defects or not by using the obtained target area and background area. The method is used for detecting the defects of the aluminum pipe, and can improve the detection accuracy.","['G06T7/0004', 'G06N3/08', 'G06T5/70', 'G06V10/267', 'G06V10/761', 'G06V10/764', 'G06V10/82']"
US10650257B2,Method and device for identifying the signaling state of at least one signaling device,"A method for identifying a signaling state of at least one signaling device including a traffic light includes obtaining at least one image which includes an image of the at least one signaling device, extracting a region of the at least one image which includes the image of the at least one signaling device, detecting the at least one signaling device within the extracted region of the at least one image, and detecting a signaling state of the signaling device after detecting the at least one signaling device within the extracted region.","['G06K9/00825', 'G06K9/3233', 'G06N20/10', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N5/046', 'G06T7/11', 'G06T7/90', 'G06V10/25', 'G06V20/584', 'G06K9/4652', 'G06T2207/20084', 'G06T2207/20164', 'G06T2207/30252', 'G06V10/56']"
US20210103763A1,"Method and apparatus for processing laser radar based sparse depth map, device and medium","Provided are a method and apparatus for processing a laser radar based sparse depth map, a device and a medium. The method for processing a laser radar based sparse depth map includes: inputting, into a neural network, the laser radar based sparse depth map; and acquiring, by the neural network, at least two feature maps, each of a respective different scale, for the laser radar based sparse depth map, performing valid point feature fusion for each of the at least two feature maps of the different scales, and processing the at least two feature maps having subjected to the valid point feature fusion, to obtain a processed depth map, wherein a number of valid points in the processed depth map is greater than a number of valid points in the laser radar based sparse depth map.","['G06K9/629', 'G01S17/89', 'G06T7/50', 'G01S17/894', 'G01S17/931', 'G06F18/213', 'G06F18/251', 'G06F18/253', 'G06K9/6232', 'G06N3/045', 'G06N3/0455', 'G06N3/08', 'G06T5/50', 'G06T5/60', 'G06T5/77', 'G06T7/521', 'G06V10/803', 'G06V10/82', 'G06V20/56', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/10044', 'G06T2207/10048', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06V20/58', 'G06V2201/12']"
CN111652828B,"Face image generation method, device, equipment and medium","The embodiment of the application discloses a face image generation method, a device, equipment and a medium, and relates to the computer vision technology. Wherein the method comprises the following steps: performing human image processing on the human face source image to obtain a candidate human face image with a target hairstyle; acquiring an image code of a candidate face image; the image coding is used for representing the face information from a preset feature dimension; and obtaining a preset number of target face images with target hairstyles by adjusting the image codes of the candidate face images based on the candidate face images. The embodiment of the application can realize the effect of generating a large number of face images which have a sense of reality and keep the same hairstyle, thereby improving the quality of the face images which can be presented to a user by the face-changing type image application program.","['G06T5/50', 'G06T15/005', 'G06T19/20', 'G06V10/267', 'G06V40/168', 'G06V40/172', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30201']"
US10825180B2,System and method for computer aided diagnosis,The present disclosure relates to a method for training a classifier. The method includes: acquiring an original image; determining a candidate target by segmenting the original image based on at least two segmentation models; determining a universal set of features by extracting features from the candidate target; determining a reference subset of features by selecting features from the universal set of features; and determining a classifier by performing classifier training based on the reference subset of features.,"['G06T7/11', 'G06F18/214', 'G06F18/2321', 'G06F18/2413', 'G06K9/6221', 'G06K9/6256', 'G06K9/627', 'G06T7/0012', 'G06V10/774', 'G06K2209/053', 'G06T2207/10072', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10104', 'G06T2207/10116', 'G06T2207/10132', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30064', 'G06V2201/032']"
US9171372B2,Depth estimation based on global motion,"This disclosure describes techniques for estimating a depth of image objects for a two-dimensional (2D) view of a video presentation. For example, a plurality of feature points may be determined for a 2D view. The plurality of feature points may be used to estimate global motion, e.g., motion of an observer (e.g., camera), of the 2D view. For example, the plurality of feature points may be used to generate a global motion frame difference. The global motion frame difference may be used to create a depth map for the 2D view, which may be used to generate an alternative view of the video presentation that may be used to display a three-dimensional (3D) video presentation.","['H04N13/128', 'G06T7/579', 'G06T7/0071', 'G06T7/00']"
CN112990191B,A Shot Boundary Detection and Key Frame Extraction Method Based on Subtitle Video,"The invention provides a shot boundary detection and key frame extraction method based on a subtitle video, which comprises the steps of extracting a video frame sequence from a video stream, carrying out coarse screening on the video frame sequence, and dividing each frame image subjected to the coarse screening into an upper area and a lower area; extracting image features from the upper region of each frame image, calculating cosine similarity of the image features between two adjacent frames, and determining key frames representing scene content transformation according to the similarity to obtain a first target key video frame sequence; extracting a character area from the lower area of each frame image, extracting a frame sequence with the character area, arranging the frame sequences according to the sequence, sequentially comparing the lengths of the character areas of adjacent frames, and if the length change differs by more than a preset number of pixels, judging the frame sequences as key frames to obtain a second target key video frame sequence; and integrating the two to obtain a final target key video frame sequence. The text information can be greatly utilized to achieve a more accurate effect while redundant information is eliminated.","['G06V20/635', 'G06N3/045', 'G06N3/08', 'G06V20/46', 'G06V20/49', 'G06V30/10']"
CN108985134B,Face living body detection and face brushing transaction method and system based on binocular camera,"The invention provides a face living body detection method and a face living body detection system based on a binocular camera, wherein the method comprises the following steps: step 1, acquiring video images of an object respectively corresponding to visible light and infrared light by using a binocular camera; step 2, respectively preprocessing the video images to obtain denoised images; step 3, respectively carrying out face detection on the denoised image to obtain a face region; step 4, extracting the key points of the human face corresponding to the two light rays in the human face area; step 5, aligning and correcting the face of the face area according to the key points of the face; and 6, extracting the depth features of the corrected face area under the infrared light by using a depth neural network, and detecting a real face and a forged face according to the depth features, wherein the depth neural network is FASDNet. The invention also provides a face brushing transaction method and system comprising the living body detection. According to the invention, active cooperation of users is not needed, so that the user experience is improved; meanwhile, the application range of the living body detection is expanded.","['G06V40/161', 'G06Q20/40145', 'G06V40/168', 'G06V40/45']"
US5796862A,Apparatus and method for identification of tissue regions in digital mammographic images,"A method of identifying tissue regions in a digital image representing a body part provides a digital image representing a body part having tissue regions. The digital image is scaled by subsampling or interpolation, and texture extraction is applied to the scaled image to produce a plurality of texture images each having a different measure of the image texture. The texture images are clustered, and the clustered image is labeled based on both pixel intensity values and geometric adjacency of pixels. A threshold is applied to the labeled image, which is then filtered using a largest component filter system to determine the largest geometrically connected region in the threshold image. The filtered image is subjected to edge detection to produce an edge image containing only edge pixels at high code value, and the edge image is operated on to produce a convex hull image which outlines the skin line of the tissue region of the body part provided in the digital image.","['G06T7/0012', 'G06T7/11', 'G06T7/12', 'G06T2207/10116', 'G06T2207/30068']"
CN111598770B,Object detection method and device based on three-dimensional data and two-dimensional image,"The application discloses an object detection method and device based on three-dimensional data and two-dimensional images, wherein the detection method comprises the following steps: acquiring environment two-dimensional images and three-dimensional point cloud data; identifying an object from the two-dimensional image, and acquiring the position and the boundary box of the object in the two-dimensional image; mapping data in the three-dimensional image into the two-dimensional image to obtain three-dimensional point cloud data in the range of the object boundary frame; filtering three-dimensional point cloud data within the bounding box; and calculating the physical center of the filtered three-dimensional point cloud data, and acquiring the three-dimensional coordinates of the central position of the identified object, so as to realize the accurate positioning of the three-dimensional coordinates of the object. The detection device comprises a plurality of cameras, a three-dimensional laser radar and a data processing center, wherein the relative positions of the cameras and the three-dimensional laser radar are fixed, and the visual range of the cameras in the horizontal direction is smaller than or equal to the scanning range of the three-dimensional laser radar. The detection device adopts a detection method to realize the identification of the object and the determination of the three-dimensional coordinates of the object.","['G06T3/08', 'G06T7/10', 'G06T7/70', 'G06T2207/10028', 'G06T2207/10044']"
AU2006225217B2,A method and apparatus for decoding handwritten characters using sub-stroke classification,"A handwritten character recognition system is disclosed. The system has an input device for receiving handwritten strokes, and a processor for classifying the handwritten strokes as an output character. The processor does so by calculating a degree of membership of the handwritten strokes to each of a plurality of character models. The character model that produces the highest degree of membership is selected and the handwritten strokes ate classified as the output character associated with the character model that produces the highest degree of membership.","['G06V10/75', 'G06V30/1423', 'G06V30/244']"
US11922654B2,"Mammographic image processing method and apparatus, system and medium","A computer device, obtains a mammographic image of a unilateral breast. The mammographic image includes a cranial-caudal (CC)-position mammographic image and a mediolateral-oblique (MLO)-position mammographic image. The computer device invokes a breast detection model to perform a prediction of a condition of the unilateral breast according to the CC-position mammographic image and the MLO-position mammographic image. The device obtains a prediction result of the unilateral breast, and generates and outputs a detection report that includes the prediction result.","['G06T7/73', 'A61B6/12', 'A61B6/502', 'G06F18/24', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06N3/096', 'G06T7/0012', 'G06T7/70', 'G16H15/00', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'A61B6/52', 'G06T2207/10116', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30068', 'G06T2207/30096', 'G06V2201/03']"
US11927738B2,Computational microscopy based-system and method for automated imaging and analysis of pathology specimens,"Described herein are systems and methods for assessing a biological sample. The methods include: characterizing a speckled pattern to be applied by a diffuser; positioning a biological sample relative to at least one coherent light source such that at least one coherent light source illuminates the biological sample; diffusing light produced by the at least one coherent light source; capturing a plurality of illuminated images with the embedded speckle pattern of the biological sample based on the diffused light; iteratively reconstructing the plurality of speckled illuminated images of the biological sample to recover an image stack of reconstructed images; stitching together each image in the image stack to create a whole slide image, wherein each image of the image stack at least partially overlaps with a neighboring image; and identifying one or more features of the biological sample. The methods may be performed by a near-field Fourier Ptychographic system.","['G02B21/367', 'G01N21/4788', 'G01N21/6458', 'G02B21/06', 'G02B21/086', 'G02B21/365', 'G02B27/58', 'G06T5/50', 'G06T5/60', 'G06T7/0012', 'G06T7/33', 'G06V20/693', 'G06V20/698', 'G01N2021/479', 'G06T2200/32', 'G06T2207/10056', 'G06T2207/10152', 'G06T2207/20056', 'G06T2207/20084', 'G06T2207/30024']"
US12272085B2,Method and system for scene image modification,System and method for rendering virtual objects onto an image.,"['G06T7/543', 'G06T7/11', 'G06T19/006', 'G06T19/20', 'G06T7/13', 'G06T7/194', 'G06T7/55', 'G06T7/60', 'G06T7/73', 'G06T7/90', 'G06T2200/04', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20016', 'G06T2207/20076', 'G06T2207/20084', 'G06T3/4038']"
US6154559A,System for classifying an individual's gaze direction,"A system is provided to classify the gaze direction of an individual obseng a number of surrounding objects. The system utilizes a qualitative approach in which frequently occurring head poses of the individual are automatically identified and labelled according to their association with the surrounding objects. In conjunction with processing of eye pose, this enables the classification of gaze direction. In one embodiment, each observed head pose of the individual is automatically associated with a bin in a ""pose-space histogram"". This histogram records the frequency of different head poses over an extended period of time. Given observations of a car driver, for example, the pose-space histogram develops peaks over time corresponding to the frequently viewed directions of toward the dashboard, toward the mirrors, toward the side window, and straight-ahead. Each peak is labelled using a qualitative description of the environment around the individual, such as the approximate relative directions of dashboard, mirrors, side window, and straight-ahead in the car example. The labelled histogram is then used to classify the head pose of the individual in all subsequent images. This head pose processing is augmented with eye pose processing, enabling the system to rapidly classify gaze direction without accurate a priori information about the calibration of the camera utilized to view the individual, without accurate a priori 3D measurements of the geometry of the environment around the individual, and without any need to compute accurate 3D metric measurements of the individual's location, head pose or eye direction at run-time.","['A61B3/113', 'G06V20/597', 'A61B5/1176', 'A61B5/7264', 'G06T7/74', 'G06V40/161', 'G06V40/19', 'G16H50/20']"
CN113624771B,PCBA rubber coating intelligent vision check out test set,"The invention relates to PCBA rubberizing intelligent vision detection equipment, which comprises a camera data acquisition module for acquiring PCBA images in real time, wherein the camera data acquisition module is connected with a data processing computer; the ultraviolet annular light source is used for illuminating the rubberized layer and emitting ultraviolet rays; the planar array light source is used for illuminating the surface of the PCBA and emitting common white light; the light source driver is respectively connected with the ultraviolet annular light source and the area array light source, the light source driver is connected with the data processing computer, and the data processing computer controls the light source driver to enable the ultraviolet annular light source and the area array light source to be lightened; the rotary platform is used for rotating the PCBA, the control driver is in control connection with the rotary platform, and the control driver is connected with the data processing computer; the mechanical arm is used for PCBA classification, is connected with the data processing computer, and the data processing computer analyzes and judges the PCBA gluing quality and classifies the PCBA through controlling the mechanical arm. And the automatic control is realized, and the detection precision and efficiency are high.","['G01N21/91', 'G01N21/8851', 'G01N2021/8887']"
US11288823B2,Logo recognition in images and videos,"Methods, apparatus, systems and articles of manufacture of logo recognition in images and videos are disclosed. An example method to detect a specific brand in images and video streams comprises accepting luminance images at a scale in an x direction Sx and a different scale in a y direction Sy in a neural network, and training the neural network with a set of training images for detected features associated with a specific brand.","['G06V10/82', 'G06F18/24', 'G06K9/4671', 'G06K9/6267', 'G06T7/11', 'G06T7/337', 'G06T7/60', 'G06V10/462', 'G06V10/764', 'G06K2009/4666', 'G06K2209/25', 'G06K9/4642', 'G06T2207/20052', 'G06V10/50', 'G06V2201/09']"
CN109872397B,Three-dimensional reconstruction method of airplane parts based on multi-view stereo vision,"The invention discloses a three-dimensional reconstruction method of an airplane part based on multi-eye stereo vision, which comprises the following steps: acquiring images of the tray and the multiple airplane parts based on multi-view stereoscopic vision, and performing integral modeling on the tray and the multiple airplane parts to obtain a three-dimensional model; meanwhile, performing image segmentation and data acquisition of a complex background on the acquired two-dimensional image; and (4) segmenting the three-dimensional model based on the two-dimensional segmentation image and the three-dimensional model, and finishing the acquisition of each three-dimensional part and the positioning of the space coordinate. The invention can effectively solve the part identification work of full-automatic airplane part spraying and provide all supports for matching, spraying track extraction and mechanical arm positioning in a subsequent part model library.",[]
US20220028126A1,Methods and Systems for Human Imperceptible Computerized Color Transfer,"The present disclosure includes systems and methods for color transfer. The method includes receiving a target image, and determining dominant source colors. The method further includes transforming the target image into a color model including a target luminance component and a target color information component. Additionally, the method includes segmenting the target image into a plurality of target segments based on the target color information component or the target luminance component and extracting dominant target colors from the target image by extracting information for at least one of the dominant target colors from each target segment of the plurality of target segments. Further, the method includes generating a color mapping relationship between the dominant target colors and the dominant source colors, and creating a recolored target image using the color mapping relationship.","['G06T11/001', 'G06T7/10', 'G06T7/11', 'G06T7/143', 'G06T7/90', 'H04N1/60', 'H04N1/644', 'G06T2207/10024', 'G06T2207/20076', 'G06T2207/30201']"
US11064902B2,"Systems, methods, and media for automatically diagnosing intraductal papillary mucinous neosplasms using multi-modal magnetic resonance imaging data","In accordance with some embodiments, systems, methods, and media for automatically diagnosing IPMNs using multi-modal MRI data are provided. In some embodiments, a system comprises: an MRI scanner; and a processor programmed to: prompt a user to select a slice of T1 and T2 MRI data including the subject's pancreas; generate minimum and maximum intensity projections based consecutive slices of the T1 and T2 MRI data; provide the projections to an image recognition CNN, and receive feature vectors for each from a fully connected layer; perform a canonical correlation analysis to determine correlations between the feature vectors; and provide a resultant vector to an SVM that determines whether the subject's pancreas includes IPMNs based on a vector.","['A61B5/055', 'A61B5/7267', 'G06N20/00', 'G06N20/10', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/096', 'G06T7/0012', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30096', 'G16H50/70']"
CN110648342B,A foam infrared image segmentation method based on NSST saliency detection and graph cut,"The invention relates to a foam infrared image segmentation method based on NSST significance detection and image segmentation, which comprises the following steps of firstly, carrying out NSST decomposition on a foam infrared image to obtain a low-frequency sub-band image and a multi-scale high-frequency sub-band; secondly, performing saliency detection on the low-frequency sub-band image by adopting a GBVS algorithm to obtain a saliency value and a visual saliency map; then, calculating a threshold value and a scale correlation coefficient for each high-frequency direction sub-band coefficient, and removing a noise coefficient, a nonlinear enhanced edge coefficient and a weak edge coefficient; and finally, performing graph segmentation on the NSST reconstructed image by combining visual saliency to obtain a segmentation result. The method has strong anti-interference capability and high segmentation precision.","['G06T7/12', 'G06T5/70', 'G06T2207/10048']"
US9042647B2,Adaptive character segmentation method and system for automated license plate recognition,"Methods, systems and processor-readable media for adaptive character segmentation in an automatic license plate recognition application. A region of interest can be identified in an image of a license plate acquired via an automatic license plate recognition engine. Characters in the image with respect to the region of interest can be segmented using a histogram projection associated with particular segmentation threshold parameters. The characters in the image can be iteratively validated if a minimum number of valid characters is determined based on the histogram projection and the particular segmentation threshold parameters to produce character images sufficient to identify the license plate.","['G06K9/00624', 'G06V20/62', 'G06V10/26', 'G06V20/625']"
US10572072B2,Depth-based touch detection,"Systems, methods, and computer readable media to improve the operation of detecting contact between a finger or other object and a surface are described. In general, techniques disclosed herein utilize a depth map to identify an object and a surface, and a classifier to determine when the object is touching the surface. Unlike the prior art, a measure of the object's “distance” is made relative to the surface and not the camera(s) thereby providing some measure of invariance with respect to camera pose. The object-surface distance measure can be used to construct an identifier or “feature vector” that, when applied to a classifier, generates an output indicative of whether the object is touching the surface. The classifier may be based on machine learning and can be trained off-line before run-time operations are commenced. In some embodiments, temporal filtering may be used to improve surface detection operations.","['G06F3/0425', 'G06F3/0416', 'G06K9/00087', 'G06V40/1365', 'G06V40/28', 'H04N13/271', 'H04N23/60', 'H04N23/90', 'G06F2203/04101']"
US10963041B2,Gesture recognition using multi-sensory data,"A system comprising: a camera configured to capture one or more images of a user's hand; and a computer configured to: receive the one or more captured images, apply a mapping function to the received one or more images, thereby yielding one or more coordinates associated with at least one feature of the user's hand, wherein the mapping function is derived from a set of labeled images that are produced by applying a machine learning algorithm to training data which comprises images of a trainer's hand, wherein the images are labeled with coordinates obtained from multiple magnetic sensors attached to the trainer's hand.","['G06V10/82', 'G06F18/24323', 'G06F3/00', 'G06F3/01', 'G06F3/017', 'G06F3/0304', 'G06K9/00355', 'G06K9/00389', 'G06K9/62', 'G06K9/6282', 'G06K9/78', 'G06V10/10', 'G06V10/764', 'G06V40/113', 'G06V40/28']"
CN111223093A,AOI defect detection method,"The invention belongs to the technical field of automatic optical detection of display panels, and discloses an AOI defect detection method, which comprises the steps of establishing a defect detection model, wherein the defect detection model comprises a generator network and a discriminator network and is used for acquiring defect related information; establishing a training set by using the positive sample, and training a generator network and a discriminator network by using the training set; updating the defect detection model by utilizing the generator network and the discriminator network obtained by training to obtain an updated defect detection model; collecting an image of a panel to be detected; and inputting the image of the panel to be detected into the updated defect detection model, detecting the defect and obtaining defect detection information. The invention solves the problems of obvious abnormal missing detection areas, insufficient utilization of a large number of positive samples and few negative samples in the panel defect detection method based on supervised learning in the prior art, can improve the defect detection capability of the whole AOI system and reduce the detection cost.","['G06T7/0004', 'G06T7/13', 'G06T2207/20081', 'G06T2207/20104', 'G06T2207/30121']"
US7720283B2,Background removal in a live video,"Exemplary systems and methods segment a foreground from a background image in a video sequence. In one implementation, a system refines a segmentation boundary between the foreground and the background image by attenuating background contrast while preserving contrast of the segmentation boundary itself, providing an accurate background cut of live video in real time. A substitute background may then be merged with the segmented foreground within the live video. The system can apply an adaptive background color mixture model to improve segmentation of foreground from background under various background changes, such as camera movement, illumination change, and movement of small objects in the background.","['H04N13/00', 'G06T7/11', 'G06T7/90', 'G06V10/28', 'H04N5/262', 'G06T2207/10016']"
WO2021004112A1,"Anomalous face detection method, anomaly identification method, device, apparatus, and medium","An anomalous face detection method, an anomaly identification method, a device, an apparatus, and a medium. The method comprises: acquiring video information of a user (S10); dividing the video information into N pieces of sub-video information (S20); separating the N pieces of sub-video information into images under detection, and obtaining N image sequences under detection (S30); applying a feature sub-face technique to the N image sequences under detection to perform face detection, and extracting face images (S40); performing feature value extraction on face images corresponding to each segmentation node, and obtaining a face feature of each of the face images (S50); performing similarity matching on each face feature corresponding to each segmentation node, and determining whether an image of a different face is present in the image sequences under detection (S60); and if so, performing a risk notification operation for the video information of the user (S70). The method can be used to improve both accuracy and efficiency of anomalous face detection.","['G06V40/161', 'G06V40/168', 'G06V40/172']"
JP2025023960A,"Eye rotation center determination, depth plane selection, and rendering camera positioning within a display system","To provide display systems, virtual reality, and augmented reality imaging and visualization systems.SOLUTION: A display system can include a head-mounted display configured to project light to an eye of a user to display virtual image content at different amounts of divergence and collimation. The display system can include an inward-facing imaging system that images the user's eye, and processing electronics that are in communication with the inward-facing imaging system and configured to obtain an estimate of a center of rotation of the user's eye. The display system may render image content with a virtual render camera positioned at the determined position of the center of rotation of the eye.SELECTED DRAWING: Figure 3","['G02B27/0179', 'A61B3/111', 'A61B3/113', 'G02B27/0081', 'G02B27/0093', 'G02B27/017', 'G02B27/0172', 'G02B27/0176', 'G02B30/00', 'G02B30/40', 'G06F1/163', 'G06F3/011', 'G06F3/012', 'G06F3/013', 'G06F3/017', 'G06F3/0346', 'G06F3/04815', 'G06F3/16', 'G06F3/167', 'G06T19/00', 'G06T3/40', 'G06T7/292', 'G06T7/70', 'G06V10/42', 'G06V10/467', 'G06V10/60', 'G06V40/193', 'H04N13/344', 'H04N13/383', 'G02B2027/0134', 'G02B2027/0138', 'G02B2027/0174', 'G02B2027/0185', 'G02B2027/0187']"
US10748035B2,Visually aided active learning for training object detector,An active learning system classifies multiple objects in an input image from the set of images with a classification metric indicative of uncertainty of each of the classified object to belong to one or different classes and determines a diversity metric of the input image indicative of diversity of the objects classified in the input image. The active learning system evaluates the diversity metric of the input image and to cause rendering of the input image on a display device based on a result of the evaluation and trains the classifier using the labelled objects of the input image.,"['G06K9/6262', 'G06V10/82', 'G06F18/217', 'G06F18/24', 'G06K9/00624', 'G06K9/6267', 'G06V10/776', 'G06V20/00']"
CN115131283B,"Defect detection and model training method, device, equipment and medium for target object","The application discloses a defect detection and model training method, device, equipment and medium of a target object, wherein the detection method detects and cuts a region of interest of a first image of the target object to obtain a defect subgraph, can quickly locate the position of a defect to be detected in the target object, detects the defect subgraph, is beneficial to reducing the overall data processing amount and improving the data processing efficiency; in the defect detection process, after the image feature data are extracted, semantic segmentation processing is carried out on the image feature data to determine outline information of the defects, further form parameters of the defects are determined, and defect classification processing is carried out on the image feature data to determine the types of the defects. According to the embodiment of the application, the defect detection is carried out by combining semantic segmentation with target object classification, so that the defect detection result of the target can be obtained more efficiently and accurately, and the robustness of defect positioning and identification is improved. The technical scheme of the application can be widely applied to the technical field of image processing.","['G06T7/0002', 'G06N3/084', 'G06T7/11', 'G06T7/187', 'G06T7/90', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'Y02P90/30']"
WO2019128508A1,"Method and apparatus for processing image, storage medium, and electronic device","A method and apparatus for processing an image, a storage medium, and an electronic device. The method comprises: matching, according to position information of a face key point in an original image, the corresponding target face image from a preset face database, and aligning the target face image with the face in the original image; revising the target face image on the basis of the trained convolutional neural network model and a face area in the original image; and fusing the revised target face image with the original image.","['G06T3/04', 'G06T3/00', 'G06T3/02', 'G06T5/50', 'G06T7/337', 'G06V40/168', 'G06T2207/20221', 'G06T2207/30201']"
US9179035B2,Method of editing static digital combined images comprising images of multiple objects,"A method of editing a combined image which includes a plurality of object images. This editing method includes displaying an outline rectangle which forms outlines of one or more object images within the combined image, and when a predetermined first operation is performed on an object image selected from among the object images located within the outline rectangle, segmenting the selected object image and another object image located within the outline rectangle, and alternatively, when a predetermined second operation is performed on an object image selected from among the object images located within the outline rectangle, combining the selected object image with another object image located within an outline rectangle other than the outline rectangle. After the segmenting or combining is performed, renewing displaying of the outline rectangle according the segmenting or combining result.","['H04N1/3873', 'H04N1/3878']"
US10986325B2,Scene flow estimation using shared features,"Scene flow represents the three-dimensional (3D) structure and movement of objects in a video sequence in three dimensions from frame-to-frame and is used to track objects and estimate speeds for autonomous driving applications. Scene flow is recovered by a neural network system from a video sequence captured from at least two viewpoints (e.g., cameras), such as a left-eye and right-eye of a viewer. An encoder portion of the system extracts features from frames of the video sequence. The features are input to a first decoder to predict optical flow and a second decoder to predict disparity. The optical flow represents pixel movement in (x,y) and the disparity represents pixel movement in z (depth). When combined, the optical flow and disparity represent the scene flow.","['H04N13/106', 'H04N13/122', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/0895', 'G06N3/09', 'H04N13/128', 'H04N2013/0081', 'H04N2013/0092']"
US5684887A,Background recovery in monocular vision,"A process for background information recovery in an image including an image of a moving object, comprises the steps of identifying regions of moving objects relative to the background; deriving a moving constellation containing moving objects in a minimum circumscribing polygon; deriving partial background images associated with respective positions of the moving constellation; and combining ones of the partial background images.",['G06T7/254']
CN109598715B,Material granularity online detection method based on machine vision,"The invention relates to the technical field of machine vision. The material granularity online detection method based on machine vision comprises the steps of collecting image information of material particles on a conveyor belt through a camera, collecting distance information of the material particles on the conveyor belt through a distance measurement module, and carrying out image analysis on the collected image information through a computer to obtain a granularity distribution map of the material. The invention adopts a particle segmentation algorithm based on convex hull analysis, judges whether the particles are under-segmented or not by detecting the convexity rate of the particles, and carries out secondary segmentation on the under-segmented particles to distinguish sticky particles in an image, thereby realizing the algorithm design of real-time online detection on the particle size distribution of the materials on the conveyor belt. The invention also adds a distance measurement module, and the measured distance value is substituted into a pixel calibration algorithm to eliminate the granularity measurement error caused by the thickness change of the material on the belt.","['G06T7/0004', 'G01N15/0227', 'G06T7/12', 'G06T7/181', 'G06T2207/10024', 'G06T2207/20024', 'G06T2207/30108']"
US11568590B2,"Cartoonlization processing method for image, electronic device, and storage medium","The disclosure discloses a cartoonlization processing method for an image, and relates to a field of computational vision, image processing, face recognition, deep learning technologies. The method includes: performing skin color recognition on a facial image to be processed to determine a target skin color of a face in the facial image; processing the facial image by utilizing any cartoonizing model in a cartoonizing model set to obtain a reference cartoonized image corresponding to the facial image in a case that the cartoonizing model set does not contain a cartoonizing model corresponding to the target skin color; determining a pixel adjustment parameter based on the target skin color and a reference skin color corresponding to the any cartoonizing model; and adjusting a pixel value of each pixel point in the reference cartoonized image based on the pixel adjustment parameter, to obtain a target cartoonized image corresponding to the facial image.","['G06T15/02', 'G06T3/04', 'G06T11/001', 'G06T5/50', 'G06T7/10', 'G06T7/11', 'G06T7/90', 'G06V10/774', 'G06V40/16', 'G06V40/171', 'G06T2207/20081', 'G06T2207/20221', 'G06T2207/30201']"
WO2021147563A1,"Object detection method and apparatus, electronic device, and computer readable storage medium","An object detection method and apparatus, an electronic device, and a computer readable storage medium. The object detection method comprises: acquiring an image to be inspected (S201); on the basis of said image, determining corner position information of each corner in said image and a centripetal shift tensor corresponding to each corner, the corner representing the position of a target object in said image (S202); and on the basis of the corner position information of each corner in said image and the centripetal shift tensor corresponding to each corner, determining the target object in said image (S203).","['G06V10/44', 'G06V20/20', 'G06T7/70', 'G06V10/25', 'G06V10/774', 'G06V10/82', 'G06T2207/20081', 'G06V2201/07']"
CN117576725A,RGB-D cross-modal pedestrian re-identification system and method based on attention feature fusion,"The invention discloses an RGB-D cross-mode pedestrian re-identification system and method based on attention feature fusion, whereinThe pedestrian re-recognition system comprises a local feature extraction branch, a global feature extraction branch, an attention feature fusion module and a recognition module; the local feature extraction branch performs feature extraction on the depth map and the RGB map to obtain local features F B The method comprises the steps of carrying out a first treatment on the surface of the The global feature extraction branch performs global feature extraction on the depth map and the RGB map, and enhances saliency stitching to obtain global features F R The method comprises the steps of carrying out a first treatment on the surface of the Attention feature fusion module pair local features F B And global feature F R Feature fusion is carried out to obtain fusion feature F C The method comprises the steps of carrying out a first treatment on the surface of the The recognition module calculates the similarity of the input depth image to be recognized and the RGB image, and obtains a recognition result according to the similarity. The system integrates the characteristics of inconsistent semantics and scales through an iterative multi-scale channel attention mechanism, improves the perception capability of the model on the body structure of the pedestrian, and further improves the accuracy of the model on the re-recognition of the cross-modal pedestrian.","['G06V40/10', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06V10/42', 'G06V10/44', 'G06V10/761', 'G06V10/806', 'G06V10/82', 'Y02T10/40']"
CN111179324B,Object pose estimation method based on fusion of color and depth information in six degrees of freedom,"The invention relates to a six-degree-of-freedom pose estimation method of an object based on color and depth information fusion, which comprises the following steps: acquiring a color image and a depth image of a target object, and performing instance segmentation on the color image; clipping a color image block containing a target object from the color image, and obtaining a target object point cloud from the depth image; extracting color features from the color image blocks, and combining the color features to the point cloud of the target object at the pixel level; performing point cloud processing on the point cloud of the target object to obtain a plurality of point cloud local area features and a global feature which are fused with color information and depth information, and combining the global feature into the point cloud local area features; and predicting the pose and the confidence of one target object by each local feature, and taking the pose corresponding to the highest confidence as a final estimation result. Compared with the prior art, the method combines color information and depth information, predicts the pose of the object by combining local features and global features, and has the advantages of strong robustness, high accuracy and the like.","['G06T7/33', 'G06F18/253', 'G06T7/11', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'Y02P90/30']"
WO2022007431A1,Positioning method for micro qr two-dimensional code,"A positioning method for a micro QR two-dimensional code, comprising: step 1, performing edge detection on a micro QR two-dimensional code image, acquiring a connected domain, and then extracting all the contours according to the connected domain; step 2, selecting a rectangular connected domain from the contours obtained in the step 1, and determining the centers of rectangular positioning blocks; step 3, calculating accurate coordinates of the centers of the positioning blocks according to the proportional relation of two-dimensional code positioning blocks; and step 4, using a central point of the positioning block as the starting point, respectively acquiring boundary points in the horizontal direction and boundary points in the vertical direction of a code area along the horizontal and vertical directions of a clock area, determining the code area boundary of a micro QR two-dimensional code according to the two boundary points and the central point of the positioning block. The method effectively solves the problem that the micro QR two-dimensional code cannot be positioned or cannot be accurately positioned.","['G06K7/1417', 'G06K7/1443', 'G06K7/1452', 'G06T5/70', 'G06T7/13', 'G06T2207/20032']"
US10755413B1,Method and system for medical imaging evaluation,This disclosure generally pertains to methods and systems for processing electronic data obtained from imaging or other diagnostic and evaluative medical procedures. Certain embodiments relate to methods for the development of deep learning algorithms that perform machine recognition of specific features and conditions in imaging and other medical data. Another embodiment provides systems configured to detect and localize medical abnormalities on medical imaging scans by a deep learning algorithm.,"['G06N3/08', 'G06F17/18', 'G06K9/3233', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/0472', 'G06N3/09', 'G06N5/02', 'G06T7/0012', 'G06T7/11', 'G06V10/25', 'G06V10/764', 'G06V10/82', 'G06N5/046', 'G06T2207/10116', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30061', 'G06V2201/03']"
US6735337B2,"Robust method for automatic reading of skewed, rotated or partially obscured characters","A character reading technique recognizes character strings in grayscale images where characters within such strings have poor contrast, are variable in position or rotation with respect to other characters in the string, or where portions of characters in the string are partially obscured. The method improves classification accuracy by improving the robustness of the underlying correlation operation. Characters are divided into regions before performing correlations. Based upon the relative individual region results, region results are combined into a whole character result. Using the characters that are read, a running checksum is computed and, based upon the checksum result, characters are replaced to produce a valid result.","['G06V20/62', 'G06V30/1902', 'G06V30/10']"
US10824916B2,Weakly supervised learning for classifying images,"Systems and methods for improving the accuracy of a computer system for object identification/classification through the use of weakly supervised learning are provided herein. In some embodiments, the method includes (a) receiving at least one set of curated data, wherein the curated data includes labeled images, (b) using the curated data to train a deep network model for identifying objects within images, wherein the trained deep network model has a first accuracy level for identifying objects, receiving a first target accuracy level for object identification of the deep network model, determining, automatically via the computer system, an amount of weakly labeled data needed to train the deep network model to achieve the first target accuracy level, and augmenting the deep network model using weakly supervised learning and the weakly labeled data to achieve the first target accuracy level for object identification by the deep network model.","['G06K9/6263', 'G06N3/084', 'G06F18/2113', 'G06F18/2178', 'G06F18/2413', 'G06K9/623', 'G06K9/627', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/0895', 'G06V10/7784', 'G06V10/82']"
US11222211B2,"Method and apparatus for segmenting video object, electronic device, and storage medium","A method and an apparatus for segmenting a video object, an electronic device, a storage medium, and a program include: performing, among at least some frames of a video, inter-frame transfer of an object segmentation result of a reference frame in sequence from the reference frame, to obtain an object segmentation result of at least one other frame among the at least some frames; determining other frames having lost objects with respect to the object segmentation result of the reference frame among the at least some frames; using the determined other frames as target frames to segment the lost objects, so as to update the object segmentation results of the target frames; and transferring the updated object segmentation results of the target frames to the at least one other frame in the video in sequence. The accuracy of video object segmentation results can therefore be improved.","['G06K9/00765', 'G06T7/11', 'G06F18/217', 'G06F18/22', 'G06K9/00744', 'G06K9/3233', 'G06K9/4609', 'G06K9/6201', 'G06K9/6262', 'G06T7/10', 'G06V10/25', 'G06V10/443', 'G06V10/75', 'G06V20/46', 'G06V20/49', 'G06T2207/10016']"
US7162073B1,Methods and apparatuses for detecting classifying and measuring spot defects in an image of an object,"A method is provided for detecting spot defects on an object when an allowable variation (called the “background”) in the appearance of the object can be modeled. Methods are also provided for measuring and classifying detected spot defects. An alignment model is used to align the image of the object, a background model is then used to estimate the (possibly different) background in each region, and each background is substantially removed from the image so as to form a foreground image on which blob analysis can be applied to detect spot defects, the blob analysis using a threshold image that accommodates different noise statistics for each region. The method facilitates robust spot defect inspection of fiber optic end faces, or of any object with different object regions. The method also allows use of blob analysis over a larger range of conditions, including conditions that make simple blob analysis infeasible.","['G01N21/8851', 'G06T7/0004', 'G06T7/174', 'G06T7/194', 'G06V10/28', 'G06T2207/10056', 'G06T2207/20081', 'G06V2201/06']"
US11423076B2,Image similarity-based group browsing,"Various approaches discussed herein enable browsing groups of visually similar items to an item of interest, wherein the item of interest may be identified in a query image, for example. One or more visual attributes associated with the item of interest are identified, and the visually similar items matching at least one of the visual attributes are grouped together, wherein the group is ranked according to the visually similar items' overall visual similarity to the item of interest, for example by using a visual similarity score and/or metric.","['G06F16/5838', 'G06F16/24578', 'G06F16/248', 'G06F16/287', 'G06F16/54', 'G06F16/5866']"
CN110084850B,A Dynamic Scene Visual Positioning Method Based on Image Semantic Segmentation,"The invention discloses a dynamic scene visual positioning method based on image semantic segmentation, belonging to the field of SLAM (Simultaneous Localization and Mapping, synchronous positioning and image construction). Firstly, segmenting a dynamic object in an original image by adopting a supervised learning mode in deep learning to obtain a semantic image; on the basis, ORB characteristic points are extracted from the original image, and the characteristic points of the dynamic object are removed according to the semantic image; and finally, positioning and tracking the camera motion by adopting a monocular SLAM method based on point characteristics based on the characteristic points after being removed. The positioning result shows that compared with the traditional method, the positioning accuracy of the method disclosed by the invention in a dynamic scene is improved by 13% to 30%.","['G06N3/045', 'G06T7/10', 'G06T7/73']"
US8031775B2,Analyzing camera captured video for key frames,"In order to analyze a digital video clip to determine candidate frames for subsequent key frame selection, a camera motion sensor is provided in the camera so that information is provided during image capture regarding camera motion including translation of the scene or camera, or scaling of the scene. The sensor includes an accelerometer or a lens motor sensor. A plurality of video segments is formed based on the global motion estimate and each segment is labeled in accordance with a predetermined series of camera motion classes. Thereafter, key frame candidates are extracted from the labeled segments and a confidence score is computed for each candidate by using rules corresponding to each camera motion class and a rule corresponding to object motion.","['H04N5/145', 'G11B27/28', 'G11B27/3027', 'H04N19/527', 'H04N19/87']"
US11074717B2,Detecting and estimating the pose of an object using a neural network model,"An object detection neural network receives an input image including an object and generates belief maps for vertices of a bounding volume that encloses the object. The belief maps are used, along with three-dimensional (3D) coordinates defining the bounding volume, to compute the pose of the object in 3D space during post-processing. When multiple objects are present in the image, the object detection neural network may also generate vector fields for the vertices. A vector field comprises vectors pointing from the vertex to a centroid of the object enclosed by the bounding volume defined by the vertex. The object detection neural network may be trained using images of computer-generated objects rendered in 3D scenes (e.g., photorealistic synthetic data). Automatically labelled training datasets may be easily constructed using the photorealistic synthetic data. The object detection neural network may be trained for object detection using only the photorealistic synthetic data.","['G06T7/74', 'G06N3/08', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/09', 'G06T7/73', 'G06N3/047', 'G06N3/0472', 'G06N3/048', 'G06N3/0481', 'G06T2207/20081', 'G06T2207/20084']"
US8379920B2,Real-time clothing recognition in surveillance videos,"Systems and methods are disclosed to recognize clothing from videos by detecting and tracking a human; performing face alignment and occlusal detection; and performing age and gender estimation, skin area extraction, and clothing segmentation to a linear support vector machine (SVM) to recognize clothing worn by the human.","['G06T7/11', 'G06T7/187', 'G06T7/194', 'G06V20/52', 'G06V40/103', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20156', 'G06T2207/30196', 'G06T2207/30201', 'G06V40/178']"
US20210256724A1,System and method for identifying items,"The method for item recognition can include: optionally calibrating a sampling system, determining visual data using the sampling system, determining a point cloud, determining region masks based on the point cloud, generating a surface reconstruction for each item, generating image segments for each item based on the surface reconstruction, and determining a class identifier for each item using the respective image segments.","['G06V20/64', 'G06F18/254', 'G06N3/04', 'G06T7/38', 'G06T7/557', 'G06T7/593', 'G06T7/62', 'G06T7/97', 'G06V10/145', 'G06V10/764', 'G06V10/809', 'G06V10/82', 'G06N20/10', 'G06N3/045', 'G06N5/01', 'G06V2201/121']"
US20210319340A1,Machine learning model confidence score validation,"A method comprising: receiving, as input, an image for classification by a trained machine learning model, generate a data set comprising a plurality of transformations of the image; applying, to each of the transformations in the data set, the trained machine learning model, to obtain a classification with respect to the transformation, wherein the classification has an associated confidence score; computing (i) a consensus classification based on all of the obtained classifications with respect to each of the transformations, and (ii) a consensus confidence score corresponding to the consensus classification, based on all of the associated confidence scores; and outputting the consensus classification and the corresponding consensus confidence score, as a classification result with respect to the image.","['G06N5/04', 'G06N3/08', 'G06N20/00', 'G06N3/044', 'G06N3/045']"
CN113767417B,Deep residual network for color filter array image denoising,"Embodiments of depth residual networks dedicated to color filter array mosaic patterns are described herein. A mosaic stride convolutional layer is introduced to match the mosaic pattern of the original image of the multispectral filter array (MSFA) or color filter array. An embodiment of data enhancement using MSFA shifts and dynamic noise is applied to make the model robust to different noise levels. By normalizing the L 1 loss function using the noise standard deviation, an implementation of the network optimization criterion can be created. Comprehensive experiments show that the implementation mode of the disclosed depth residual error network is superior to the denoising algorithm in the prior art in the MSFA field.","['G06T5/70', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G06T5/20', 'G06T5/50', 'G06T5/60', 'H04N25/135', 'G06N3/08', 'G06T2207/10024', 'G06T2207/10036', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20224']"
US20230377183A1,Depth-Aware Photo Editing,"The methods and systems described herein provide for depth-aware image editing and interactive features. In particular, a computer application may provide image-related features that utilize a combination of a (a) the depth map, and (b) segmentation data to process one or more images, and generate an edited version of the one or more images.","['G06T7/593', 'G06T5/77', 'G06T19/006', 'G06T19/20', 'G06T3/40', 'G06T7/174', 'H04N13/128', 'H04N13/271', 'G06T2200/24', 'G06T2207/10012', 'G06T2207/10028', 'G06T2207/20092', 'H04N2013/0081', 'H04N2013/0092']"
US10924676B2,Real-time visual effects for a live camera view,"Visual effects for element of interest can be displayed within a live camera view in real time or substantially using a processing pipeline that does not immediately display an acquired image until it has been updated with the effects. In various embodiments, software-based approaches, such as fast convolution algorithms, and/or hardware-based approaches, such as using a graphics processing unit (GPU), can be used reduce the time between acquiring an image and displaying the image with various visual effects. These visual effects can include automatically highlighting elements, augmenting the color, style, and/or size of elements, casting a shadow on elements, erasing elements, substituting elements, or shaking and jumbling elements, among other effects.","['H04N5/23293', 'G06T11/00', 'G06T19/006', 'H04N23/80', 'H04N5/23229', 'H04M2250/52']"
US10606933B2,Method and system for document image layout deconstruction and redisplay,"The invention converts a document originating in a page-image format into a form suitable for an arbitrarily sized display, by reformatting or “re-flowing” of the document to fit an arbitrarily sized display device. A two-stage system analyzes, or “deconstructs,” page image layout. The deconstruction includes both physical (geometric) and logical (functional) segmentation of page images. The segment that image elements may include blocks, lines, and/or words of text, and other segmented image elements. The segment that image elements are synthesized and converted into an intermediate structure. The intermediate data structure is then distilled or converted or redisplayed into any number of standard print formats.","['G06F17/2264', 'G06F40/151', 'G06F17/211', 'G06F17/2229', 'G06F17/24', 'G06F40/103', 'G06F40/131', 'G06F40/166', 'G06K9/00463', 'G06V30/414']"
US12315031B2,High fidelity interactive segmentation for video data with deep convolutional tessellations and context aware skip connections,Techniques related to automatically segmenting video frames into per pixel fidelity object of interest and background regions are discussed. Such techniques include applying tessellation to a video frame to generate feature frames corresponding to the video frame and applying a segmentation network implementing context aware skip connections to an input volume including the feature frames and a context feature volume corresponding to the video frame to generate a segmentation for the video frame.,"['G06T1/20', 'G06F18/241', 'G06F18/2413', 'G06N3/045', 'G06N3/08', 'G06T3/4046', 'G06T7/11', 'G06T7/174', 'G06T7/194', 'G06T9/002', 'G06V10/26', 'G06V10/764', 'G06V20/46', 'G06V20/49', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/20221']"
US8260048B2,Segmentation-based image processing system,"A digital image can be processed by an image processing method that calculates a gradient map for the digital image, calculates a density function for the gradient map, calculates a modified gradient map using the gradient map, the density function and the selected scale level, and segments the modified gradient map. Prior to segmenting the modified gradient map, a sub-image of the digital image can be segmented at the selected scale level to determine if the selected scale level will give the desired segmentation.","['G06T7/174', 'G06T7/11', 'G06T7/13', 'G06T7/136', 'G06T7/155', 'G06T2207/20152']"
CN108229296B,"Face skin attribute identification method and device, electronic equipment and storage medium","The embodiment of the invention discloses a method and a device for identifying human face skin attributes, electronic equipment and a computer storage medium, wherein the method comprises the following steps: extracting the features of the face image in the image to be recognized through each convolution layer in the neural network; fusing the extracted features of at least one shallower convolutional layer in the neural network with the extracted features of the last convolutional layer to obtain fused features; and predicting the skin attribute of the face image based on the fusion characteristics to obtain a prediction label of the skin attribute. In the embodiment of the invention, the shallow layer characteristic and the deep layer characteristic of the neural network are obtained by fusing the extracted characteristic of at least one shallow convolutional layer and the extracted characteristic of the last convolutional layer in the neural network, so that the comprehensive judgment of the skin attribute is realized; predicting the skin attribute of the face image based on the fusion characteristics; and the prediction of different skin attributes of the human face skin is realized.","['G06V40/168', 'G06F18/253', 'G06N3/045']"
CN110189314B,Image positioning method of automobile instrument panel based on machine vision,"The invention discloses a machine vision-based image positioning method for an automobile instrument panel, and provides a machine vision-based image positioning method for solving the problem of image positioning of the automobile instrument panel. And establishing a relation between space coordinates and pixel coordinates through nine-point calibration, coarsely positioning an automobile instrument panel by adopting a Patquick algorithm under the coordinate system, marginalizing an image according to a Sobel operator after positioning, finely positioning a scale line by adopting a Blob algorithm, and finely positioning a liquid crystal display by adopting a PatMax algorithm and the Blob algorithm. The method can accurately position the graphic information, reduce the display deviation of the automobile instrument panel, improve the automobile quality inspection rate and have important significance for the practical application of the image positioning of the automobile instrument panel.","['G06T5/70', 'G06T7/0004', 'G06T7/73', 'G06T7/80', 'G06T2207/30164']"
CN108805898B,Video image processing method and device,"The application provides a video image processing method and a video image processing device, wherein the method comprises the following steps: acquiring a current frame video image; performing image segmentation on the current frame video image to obtain a first mask image corresponding to the current frame video image; determining historical motion information of the current frame video image according to the current frame video image and the previous frame video image, and obtaining a third mask image corresponding to the current frame video image according to a second mask image and the historical motion information corresponding to the previous frame video image; and calculating according to the historical motion information to obtain fusion weight, and performing weighted fusion on the first mask image and the third mask image according to the fusion weight to obtain a fusion mask image of the current frame video image. The video image processing method and the video image processing device can avoid the problems of jitter and delay, improve the stability and the fluency of the video and improve the accuracy of motion tracking.","['G06T7/215', 'G06T7/10', 'G06T7/269', 'G06T2207/10016', 'G06T2207/20084', 'G06T2207/20221']"
US10720128B2,Real-time user adaptive foveated rendering,Foveated rendering based on user gaze tracking may be adjusted to account for the realities of human vision. Gaze tracking error and state parameters may be determined from gaze tracking data representing a user's gaze with respect to one or more images presented to a user. Adjusted foveation data representing an adjusted size and/or shape of one or more regions of interest in one or more images to be subsequently presented to a user may be generated based on the one or more gaze tracking error or state parameters. Foveated image data representing one or more foveated images may be generated with the adjusted foveation data. The foveated images are characterized by level of detail within the one or more regions of interest and lower level of detail outside the one or more regions of interest. The foveated images may then be presented to the user.,"['G09G5/391', 'G06F3/013', 'G02B27/0093', 'G02B27/0101', 'G06F1/3265', 'G02B2027/0138', 'G02B27/017', 'G09G2340/0407', 'Y02D10/00', 'Y02D10/153']"
US10192313B2,"Networked capture and 3D display of localized, segmented images","Systems, devices and methods are described including receiving a source image having a foreground portion and a background portion, where the background portion includes image content of a three-dimensional (3D) environment. A camera pose of the source image may be determined by comparing features of the source image to image features of target images of the 3D environment and using the camera pose to segment the foreground portion from the background portion may generate a segmented source image. The resulting segmented source image and the associated camera pose may be stored in a networked database. The camera pose and segmented source image may be used to provide a simulation of the foreground portion in a virtual 3D environment.","['G06T7/0081', 'H04N13/30', 'G06V40/161', 'G06K9/00228', 'G06K9/342', 'G06T19/20', 'G06T7/11', 'G06T7/174', 'G06T7/194', 'G06T7/74', 'G06V10/267', 'G06T2207/10012', 'G06T2207/10024', 'G06T2207/20221', 'G06T2207/20224', 'G06T2207/30196', 'G06T2207/30201', 'G06T2207/30244']"
CA3044063C,Systems and methods for performing fingerprint based user authentication using imagery captured using mobile devices,"Technologies are presented herein in support of a system and method for performing fingerprint recognition. Embodiments of the present invention concern a system and method for capturing a user's biometric features and generating an identifier characterizing the user's biometric features using a mobile device such as a smartphone. The biometric identifier is generated using imagery captured of a plurality of fingers of a user for the purposes of authenticating/identifying the user according to the captured biometrics and determining the user's liveness. The present disclosure also describes additional techniques for preventing erroneous authentication caused by spoofing. In some examples, the anti-spoofing techniques may include capturing one or more images of a user's fingers and analyzing the captured images for indications of liveness.","['G06V40/1353', 'G06F18/24143', 'G06F21/32', 'G06N3/0464', 'G06N3/09', 'G06V10/44', 'G06V10/7553', 'G06V10/82', 'G06V40/1318', 'G06V40/1359', 'G06V40/1371', 'G06V40/1376', 'G06V40/1388', 'G06V40/1394', 'G06N3/045', 'G06N7/01', 'G06T2219/2016', 'G06V40/117']"
CN111243093B,"Three-dimensional face grid generation method, device, equipment and storage medium","The application provides a method, a device, equipment and a storage medium for generating a three-dimensional face grid, and relates to the technical field of computer vision of artificial intelligence. The method comprises the following steps: collecting n image pairs of a target face, wherein the image pairs comprise color images and depth images; extracting a face point cloud in the image pair, wherein the face point cloud comprises face key points of the target face; determining the camera pose of the image pair according to the positions of the face key points in the target face in the image pair; fusing the face point clouds in the image pair into the same three-dimensional space according to the camera gesture; and generating a three-dimensional face grid of the target face according to the fused face point cloud in the three-dimensional space. Aiming at the reconstruction of the three-dimensional face grid of the mobile terminal, the application provides a complete pipeline (pipeline) reconstruction flow, and the quick reconstruction of the three-dimensional face grid of the mobile terminal can be realized.",['G06T17/20']
US6184926B1,System and method for detecting a human face in uncontrolled environments,"The present invention provides for the detection of human heads, faces and eyes in real-time and in uncontrolled environments. The present invention may be implemented with commercially available components, such as a standard video camera and a frame grabber, on a personal computer (PC) platform. The approach used by the present invention is based on a probabilistic framework that uses a deformable template model to describe the human face. The present invention works both with simple head-and-shoulder video sequences, as well as with complex video scenes with multiple people and random motion. The present invention is able to locate the eyes from different head poses (rotations in image plane as well as in depth). The information provided by the location of the eyes may be used to extract faces from a frontal pose in a video sequence. The extracted frontal frames can be passed to recognition and classification systems (or the like) for further processing.","['G06V40/162', 'G06V40/19']"
US11132794B2,System and method for detection of suspicious tissue regions in an endoscopic procedure,"An image processing system connected to an endoscope and processing in real-time endoscopic images to identify suspicious tissues such as polyps or cancer. The system applies preprocessing tools to clean the received images and then applies in parallel a plurality of detectors both conventional detectors and models of supervised machine learning-based detectors. A post processing is also applied in order select the regions which are most probable to be suspicious among the detected regions. Frames identified as showing suspicious tissues can be marked on an output video display. Optionally, the size, type and boundaries of the suspected tissue can also be identified and marked.","['G06T7/0012', 'A61B1/00009', 'A61B1/000094', 'A61B1/000095', 'A61B1/000096', 'A61B1/31', 'G06T7/246', 'G06T2207/10016', 'G06T2207/10028', 'G06T2207/10068', 'G06T2207/20084', 'G06T2207/30028', 'G06T2207/30032', 'G06T2207/30096']"
US10656724B2,"Operating environment comprising multiple client devices, multiple displays, multiple users, and gestural control","Embodiments described herein includes a system comprising a processor coupled to display devices, sensors, remote client devices, and computer applications. The computer applications orchestrate content of the remote client devices simultaneously across at least one of the display devices and the remote client devices, and allow simultaneous control of the display devices. The simultaneous control includes automatically detecting a gesture of at least one object from gesture data received via the sensors. The gesture data is absolute three-space location data of an instantaneous state of the at least one object at a point in time and space. The detecting comprises aggregating the gesture data, and identifying the gesture using only the gesture data. The computer applications translate the gesture to a gesture signal, and control at least one of the display devices and the remote client devices in response to the gesture signal.",['G06F3/017']
US9147255B1,Rapid object detection by combining structural information from image segmentation with bio-inspired attentional mechanisms,"Described is a system for rapid object detection combining structural information with bio-inspired attentional mechanisms. The system oversegments an input image into a set of superpixels, where each superpixel comprises a plurality of pixels. For each superpixel, a bounding box defining a region of the input image representing a detection hypothesis is determined. An average residual saliency (ARS) is calculated for the plurality of pixels belonging to each superpixel. Each detection hypothesis that is out of a range of a predetermined threshold value for object size is eliminated. Next, each remaining detection hypothesis having an ARS below a predetermined threshold value is eliminated. Then, color contrast is calculated for the region defined by the bounding box for each remaining detection hypothesis. Each detection hypothesis having a color contrast below a predetermined threshold is eliminated. Finally, the remaining detection hypotheses are output to a classifier for object recognition.","['G06T7/11', 'G06T7/0079', 'G06T7/143', 'G06T7/162', 'G06V10/462', 'G06T2207/10024', 'G06T2207/20072', 'G06T2207/20076']"
CN110458883B,"Medical image processing system, method, device and equipment","The application belongs to the technical field of medical image processing, mainly relates to a computer vision technology in artificial intelligence, and discloses a system, a method, a device and equipment for processing a medical image, wherein the method for processing the medical image comprises the steps of acquiring an original medical image to be identified, and obtaining a lesion region block in the original medical image through a segmentation model; and performing morphological operation on the obtained lesion area block to obtain the operated lesion area block and a lesion outline in the lesion area block. In this way, the precise boundary of the lesion outline can be determined, and the lesion position can be precisely positioned.","['G06N3/045', 'G06T5/30', 'G06T5/40', 'G06T5/70', 'G06T7/0012', 'G06T7/11', 'G06T7/62', 'G06T7/70']"
US7702131B2,Segmenting images and simulating motion blur using an image sequence,"A sequence of images depicts a foreground object in motion. A base image is selected, and the other images in the sequence are co-registered with the base image in order to align the images to a common coordinate system. A background image and a binary foreground mask are generated from the sequence of aligned images. By applying the foreground mask to a chosen one of the aligned images, a representation of the moving object is extracted. After blurring the background image, the extracted representation may be superimposed onto the blurred background image to produce a new image.","['G06T5/70', 'G06T11/00', 'G06T7/11', 'G06T7/194', 'G06T7/215', 'H04N5/2625', 'G06T2207/10016', 'G06T2207/20036']"
CN110020592B,"Object detection model training method, device, computer equipment and storage medium","The invention discloses an object detection model training method, an object detection model training device, computer equipment and a storage medium, and relates to the field of artificial intelligence. The object detection model training method comprises the following steps: obtaining a training sample; inputting a training sample into an object detection model for model training, wherein the object detection model comprises a detection module, a classification module and a discrimination module; obtaining detection loss generated by the detection module, classification loss generated by the classification module and discrimination loss generated by the discrimination module in the model training process; and updating the object detection model according to the detection loss, the classification loss and the discrimination loss to obtain a target object detection model. The object detection model obtained by training by the object detection model training method can effectively improve the object detection accuracy.","['G06N3/084', 'G06V20/10', 'G06V2201/07']"
US10628700B2,"Fast and robust face detection, region extraction, and tracking for improved video coding","Techniques related to improved video coding based on face detection, region extraction, and tracking are discussed. Such techniques may include performing a facial search of a video frame to determine candidate face regions in the video frame, testing the candidate face regions based on skin tone information to determine valid and invalid face regions, rejecting invalid face regions, and encoding the video frame based on valid face regions to generate a coded bitstream.","['G06K9/4652', 'G06F18/2148', 'G06F18/2178', 'G06K9/00281', 'G06K9/4614', 'G06K9/6257', 'G06K9/6263', 'G06V10/446', 'G06V10/56', 'G06V10/7747', 'G06V10/7784', 'G06V40/171', 'H04N19/119', 'H04N19/167', 'H04N19/543', 'H04N19/85', 'H04N19/136']"
CN111191737B,Fine-grained image classification method based on multi-scale repeated attention mechanism,"The invention discloses a fine granularity image classification method based on a multi-scale repeated attention mechanism, which comprises the following steps: randomly cutting the training data set, and horizontally overturning to strengthen the data set so as to prevent overfitting; introducing a repeated attention mechanism in a multi-scale mode on the basis of a ResNet50 model; inputting a training sample, uniformly inputting the training sample into 448 x 448, using a multi-scale repeated attention model as a feature extractor, splicing feature matrixes obtained by different scales, and sending the feature matrixes into a full-connection layer and softmax for classification training; and storing a final training model, and testing on a testing set to obtain the final accuracy. The invention improves the classification accuracy of the fine-grained images.","['G06F18/24', 'G06F18/214', 'G06F18/253']"
US20240303880A1,"Method of generating image sample, method of recognizing text, device and medium","A method of generating an image sample, which relates to a field of an artificial intelligence technology, in particular to fields of a deep learning technology and a computer vision technology. The method includes: generating a handwritten text image according to at least one handwritten sample image; and generating a target sample image with an annotation box according to the handwritten text image and a background image, where the annotation box is used to represent a region in which the handwritten text image is located in the background image. The present disclosure further provides a method of recognizing a text, an electronic device and a storage medium.","['G06N3/044', 'G06T11/203', 'G06T11/60', 'G06N3/08', 'G06T3/02', 'G06T3/60', 'G06T7/10', 'G06V10/82', 'G06V30/10', 'G06V30/148']"
US11282185B2,"Information processing device, information processing method, and storage medium","There is provided with an information processing device. A defect detecting unit detects a defect of an object in an input image. An extracting unit extracts a feature amount pertaining to a partial image of the defect from the input image, on the basis of a result of detecting the defect. An attribute determining unit determines an attribute of the defect using the feature amount pertaining to the partial image of the defect.","['G06T7/0004', 'G06F18/2431', 'G06F3/14', 'G06K9/4642', 'G06K9/628', 'G06T7/11', 'G06T7/62', 'G06T7/73', 'G06V10/764', 'G06V10/82', 'G06V20/52', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30132', 'G06T2207/30184']"
US5845288A,Automated system for indexing graphical documents having associated text labels,"A system for image indexing for the domain of technical manual documents depicting line drawing images of physical equipment. Such line drawings are often associated with text labels that indicate references to component parts in an associated table. The indexing system locates regions containing a machine part in relevant pages of the manual in response to a query describing the part. The query supplied by the user is a textual description of the machine part from which an associated geometric description of the object is retrieved. The indexing mechanism includes two stages, i.e., selection and recognition. The selection phase involves isolating relevant images from the document database using a combination to textual and visual processing and indicating the appropriate regions within those images that are likely to contain the queried machine part. Model-based object recognition then confirms the presence of the part at that location by finding a match of features between the part and the model using a 3D model description associated with the textual query.","['G06F16/58', 'G06F16/5846', 'G06F16/5854', 'Y10S707/99933', 'Y10S707/99943', 'Y10S707/99945']"
US20220148333A1,Method and system for estimating eye-related geometric parameters of a user,"Method for estimating eye-related geometric parameters of a user, comprising the steps of: a. retrieving one input image observation corresponding to an image of the eye; b. using a learning machine for computing a plurality of image segmentation maps, so as to classify each pixel into one eye region; c. generating through a set of geometric parameters an image geometric model of the user's eye; d. comparing the image geometric model with an image segmentation map; e. computing a model correspondence value indicating if said input image observation corresponds to the geometric model; f. repeating steps c. to e. if the value computed under step e. is below an optimal value wherein one parameter is changed for each iteration until said model correspondence value reaches the optimal value, and g. retrieving the eye-related geometric parameters from the latest model of the user's eye.","['G06V40/197', 'G06V40/18', 'G06T1/60', 'G06T11/00', 'G06T7/11', 'G06T7/32', 'G06T7/60', 'G06V10/22', 'G06V10/759', 'G06V10/82', 'G06T2207/20021', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30201']"
CN116071772A,"Content extraction method, device and equipment for professional document and storage medium","The invention discloses a content extraction method, device and equipment of a professional document and a storage medium. The method comprises the following steps: inputting a document image of a professional document into a document target detection network model, and intercepting to obtain a local document image of each data format in the document image, wherein the data formats comprise at least one of characters, tables and pictures; respectively carrying out word recognition processing on the local document images of all the data formats, and extracting processing results to obtain editable files with uniform data formats; and cleaning the data of the editable file through a data model to obtain a target editable file. The embodiment of the invention can improve the extraction efficiency of the professional document content.","['G06V30/413', 'G06V10/82', 'G06V30/153', 'Y02D10/00']"
CN112686812B,"Bank card tilt correction detection method, device, readable storage medium and terminal","The application provides a bank card inclination correction detection method, a device, a readable storage medium and a terminal, which fully combine the advantages of the bank card inclination correction technology (Bankcard Tilt Correction, BTC) with the deep learning technology and the traditional image processing method, can obtain the certificate segmentation and correction results with high accuracy and high robustness aiming at various and complex user input images, provides a basis for subsequent certificate detection, classification and information extraction, improves the application range of certificate identification, and can be widely applied in the fields of security, finance and the like.","['G06V10/82', 'G06N3/0464', 'G06N3/09', 'G06V10/243', 'G06V30/162', 'G06V30/18067', 'G06V30/414', 'G06N20/00', 'Y02D10/00']"
CN111476292B,Small sample element learning training method for medical image classification processing artificial intelligence,"The small sample element learning training method for medical image classification processing artificial intelligence is characterized by constructing three element learners including a multi-scale CNN feature extractor, a measurement learner and a classification discriminator and designing measurement standards on the element learners at the same time; on each task of the training set, a target set is learned through support set distance measurement, a measurement standard is finally obtained through learning, and then for a new task of the test set, the target set can be rapidly and correctly classified only by means of a small number of samples of the support set; the practicability and reliability of artificial intelligent automatic detection in the field of difficult and complicated diseases are improved by adopting meta-learning, the defect that the single disease data amount is small and the disease is dispersed is overcome, the classification and sample-less learning training of a medical image processing intelligent system is completed, the accuracy of filtration is obviously improved, the accuracy of medical image classification is effectively enhanced, the production efficiency is greatly improved, and the upgrading of an intelligent diagnosis deep learning technology in the medical industry is facilitated.","['G06V10/751', 'G06F18/24133', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06T3/4038', 'G06T7/11', 'G06T2207/20081', 'G06T2207/20084']"
CN113344857B,"Defect detection network training method, defect detection method and storage medium","A training method, a defect detection method and a storage medium of a defect detection network are provided, the defect detection network comprises a primary feature extraction network, a feature fusion selection layer and a defect semantic analysis network, and the training method comprises the following steps: inputting the template image, the positive sample image and the negative sample image into a primary feature extraction network to obtain respective primary features, and constructing a first loss function according to the primary features; inputting the three primary characteristics into a characteristic fusion selection layer to obtain fusion characteristics; inputting the fusion features into a defect semantic analysis network to obtain a defect confidence image and a defect classification predicted image; and constructing a second loss function according to the defect confidence coefficient image, the defect segmentation marking image, the defect classification prediction image and the defect classification marking image, and training the defect detection network according to the first loss function and the second loss function. The defect detection network obtained after training can automatically extract and select features, and can realize accurate defect position positioning and detection of different types of defects.","['G06T7/0004', 'G06F18/2415', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06T5/50', 'G06T7/11', 'G06T7/136', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221']"
WO2020125221A1,"Image processing method and apparatus, electronic device, and computer readable storage medium","Disclosed by embodiments of the present disclosure are an image processing method and apparatus, an electronic device, and a computer readable storage medium, wherein the method comprises: acquiring an image to be registered and a reference image used for registration; inputting the image to be registered and the reference image into a preset neural network model, the preset neural network model being trained on the basis of mutual information loss between the image to be registered and the preset reference image; on the basis of the preset neural network model, registering the image to be registered to the reference image, and acquiring a registration result, which may increase the accuracy and real-time performance of image registration.","['G06T7/337', 'G06N3/084', 'G06N3/04', 'G06N3/08', 'G06N3/088', 'G06T5/50', 'G06T7/0014', 'G06T7/33', 'G06T7/38', 'G16H50/20', 'G06N3/045', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004']"
US10867430B2,Method and system of 3D reconstruction with volume-based filtering for image processing,"A system, article, and method of 3D reconstruction with volume-based filtering for image processing.","['G06T7/593', 'G06K9/00664', 'G06K9/6202', 'G06T15/06', 'G06T15/08', 'G06T15/205', 'G06T17/00', 'G06T19/20', 'G06T7/194', 'G06V20/10', 'G06T2207/10012', 'G06T2207/10028', 'G06T2207/20024', 'G06T2207/20084', 'G06T2207/20224', 'G06T2210/56']"
US8411149B2,"Method and device for identifying and extracting images of multiple users, and for recognizing user gestures","The invention relates to a method for identifying and extracting images of one or more users in an interactive environment comprising the steps of: —obtaining a depth map (7) of a scene in the form of an array of depth values, and an image (8) of said scene in the form of a corresponding array of pixel values, said depth map (7) and said image (8) being registered; applying a coordinate transformation to said depth map (7) and said image (8) for obtaining a corresponding array (15) containing the 3D positions in a real-world coordinates system and pixel values points; —grouping said points according to their relative positions, by using a clustering process (18) so that each group contains points that are in the same region of space and correspond to a user location (19); —defining individual volumes of interest (20) each corresponding to one of said user locations (19); —selecting, from said array (15) containing the 3D positions and pixel values, the points located in said volumes of interest for obtaining segmentation masks (35) for each user; —applying said segmentation masks (35) to said image (8) for extracting images of said users. The invention also relates to a method for recognizing gestures of said users.","['G06V40/103', 'A63F13/213', 'A63F13/27', 'A63F13/42', 'A63F13/52', 'A63F13/56', 'G06F3/0304', 'G06T7/11', 'G06T7/174', 'G06T7/215', 'G06T7/254', 'G06T7/50', 'G06V10/426', 'A63F2300/1093', 'A63F2300/6045', 'A63F2300/6607', 'A63F2300/6623', 'A63F2300/8023', 'G06T2207/10028', 'G06T2207/30196']"
US20220358634A1,Methods and systems of utilizing image processing systems to measure objects,"Methods, systems, and techniques for utilizing image processing systems to measure damage to vehicles include utilizing an image processing system to generate a heat map of an image of a damaged vehicle, where the heat map is indicative of a damaged area of the vehicle, and determining at least one measurement of the damaged area based on the heat map and a depth of field indicator corresponding to the image. In some embodiments, the image processing system also determines one or more types of damage of the damaged area, and/or also generates a segmentation map of the depicted vehicle and utilizes the segmentation map in conjunction with the heat map to measure damaged areas and locations thereof on the vehicle depicted within the image. In some embodiments, the techniques include determining the depth of field indicator of the image or portions thereof.","['G06V20/70', 'G06T7/11', 'G06F3/0481', 'G06F3/04845', 'G06T3/0093', 'G06T3/18', 'G06T7/0002', 'G06T7/0004', 'G06T7/10', 'G06T7/50', 'G06T7/70', 'G06V10/26', 'G06V10/764', 'G06V10/766', 'G06V10/774', 'G06V10/82', 'G06V10/945', 'G06F3/0482', 'G06Q10/20', 'G06Q30/0283', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30252', 'G06V2201/08']"
US20250182290A1,Human pose analysis system and method,"System and method for extracting human pose information from an image, comprising a feature extractor connected to a database, a convolutional neural network (CNN) with a plurality of CNN layers. Said system/method further comprising at least one of the following modules: a 2D body skeleton detector for determining 2D body skeleton information from the human-related image features; a body silhouette detector for determining body silhouette information from the human-related image features; a hand silhouette detector for determining hand silhouette detector from the human-related image features; a hand skeleton detector for determining hand skeleton from the human-related image features; a 3D body skeleton detector for determining 3D body skeleton from the human-related image features; and a facial keypoints detector for determining facial keypoints from the human-related image features.","['A61B5/1116', 'A61B5/1128', 'G06F18/214', 'G06F18/24133', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T7/73', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V40/10', 'G06V40/103', 'G06V40/171', 'A61B2576/00', 'G06N3/084', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196']"
CN112949572B,Mask wearing detection method based on Slim-YOLOv3,"The invention belongs to the technical field of deep learning target detection and computer vision, and particularly relates to a mask wearing condition detection method based on Slim-YOLOv3, which comprises the following steps: acquiring face video data in real time, and preprocessing the face video data; inputting the preprocessed face image into a trained Slim-YOLOv3 model, and judging whether the user wears the mask correctly; according to the method, the Slim-YOLOv 3-based mask wearing condition video detection method is adopted, and an improved unsupervised self-classification method is adopted to perform subclass classification on data which do not normally wear the mask, so that a mask wearing video detection task can be more accurately and rapidly realized. And the proposed network is more concise, so that the application cost is further reduced.","['G06V40/165', 'G06N3/045', 'G06N3/048', 'G06N3/08', 'G06V40/171', 'G06V40/172']"
CN109409371B,Systems and methods for semantic segmentation of images,The invention provides a method and a system for detecting instances of objects in an input image. The method for detecting the instance of the object in the image comprises the following steps: extracting a plurality of core instance features from an input image; calculating a plurality of feature maps under multi-scale resolution according to the core example features; calculating a detection frame according to the characteristics of the core example; calculating a segmentation mask under the multi-scale resolution of the feature map for each detection frame of the detection frames; merging the plurality of segmentation masks at the multi-scale resolution to generate an instance mask for each object detected in the image; refining the confidence score of the instance mask by computing a pixel-level metric by an auxiliary network; and outputting the instance mask as the detected instance.,"['G06V10/267', 'G06V10/7715', 'G06F18/24143', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06T11/60', 'G06T5/20', 'G06T7/11', 'G06T7/70', 'G06V10/25', 'G06V10/776', 'G06V10/82', 'G06V20/10', 'G06V20/70', 'G06V30/19173', 'G06V30/274', 'G06N3/044', 'G06T2207/20016', 'G06V2201/07']"
CN111507390B,Storage box body identification and positioning method based on contour features,"The invention provides a storage box body identification and positioning method based on contour features, which comprises the following steps: (1) acquiring color images and depth images of box cargo; (2) Performing foreground segmentation on the color image and the depth image, mapping the depth blocks onto the color image, marking color area blocks falling into the same depth block, and merging the color area blocks to be used as a final box cargo foreground image; (3) Performing linear edge detection on a box cargo foreground image, and connecting edges by taking a rectangle as a contour target to obtain a candidate target closed contour; (4) And selecting the box body recognition result from the candidate target closed contour, wherein the box body rectangular feature meets the preset box body rectangular feature. The method is used for identifying and positioning the box goods based on the depth image and the color image, improves the calculation efficiency, has the advantages of low cost and strong real-time performance, and is suitable for identifying and positioning the box goods in complex environments such as factories and warehouses.","['G06F18/22', 'G06V10/267', 'G06V10/30', 'G06V10/44']"
US12229217B1,Machine learning based gesture recognition,"The technology disclosed introduces two types of neural networks: “master” or “generalists” networks and “expert” or “specialists” networks. Both, master networks and expert networks, are fully connected neural networks that take a feature vector of an input hand image and produce a prediction of the hand pose. Master networks and expert networks differ from each other based on the data on which they are trained. In particular, master networks are trained on the entire data set. In contrast, expert networks are trained only on a subset of the entire dataset. In regards to the hand poses, master networks are trained on the input image data representing all available hand poses comprising the training data (including both real and simulated hand images).","['G06F18/214', 'G06T7/75', 'G06F18/24', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06T7/248', 'G06T7/285', 'G06T7/74', 'G06V10/70', 'G06V10/82', 'G06V20/64', 'G06V40/11', 'G06V40/28', 'G06F3/011', 'G06F3/017', 'G06N3/0442', 'G06N3/088', 'G06T2207/10012', 'G06T2207/10021', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196']"
CN111209952B,Underwater target detection method based on improved SSD and migration learning,"The invention relates to an underwater target detection method based on improved SSD and transfer learning, which solves the problems of under fitting, low recognition rate and poor robustness of underwater target detection in the prior art, and effectively improves the underwater target detection recognition rate and visual effect. The invention comprises the following steps: step 1, preparation: reading underwater images shot under the condition of deep water by using a computer, and carrying out fusion processing on the underwater background images and the underwater target scene images to obtain more underwater images; step 2, preprocessing an image and removing noise based on a filter; step 3, a feature extraction stage based on a backbone network; step 4, a network model training stage based on transfer learning: training an Underwater-SSD network by using a transfer learning method; step 5, soft-NMS target detection stage based on softening non-maximum suppression algorithm: and adding a softening non-maximum suppression algorithm after the model is subjected to the transfer learning training.","['G06F18/253', 'G06F18/217', 'G06N3/045', 'G06V10/30']"
CN117911814A,Zero sample image processing system and processing method for cross-modal semantic alignment,"The invention relates to the technical field of artificial intelligence and target recognition, and particularly discloses a zero sample image processing system and a processing method for cross-modal semantic alignment, wherein the zero sample image processing system comprises an image data acquisition module, a multi-channel image fusion module and a multi-channel image fusion module, wherein the image data acquisition module is used for receiving image information acquired by different sensors under an embedded platform; the visual feature extraction module is used for extracting visual features by using the depth convolution neural network to obtain image visual features; a semantic information acquisition module; a semantic feature extraction module; and the cross-modal semantic alignment module is used for carrying out alignment constraint on the visual features and the semantic features by using a cross-modal semantic alignment algorithm, establishing a cross-modal mapping relation, carrying out image processing on a new input image, and outputting image target category and position information. Aiming at high-altitude environment dynamic image data, the invention adopts a multipath sensor to acquire images, designs an embedded real-time system to perform rapid zero sample identification and detection on the images, and can efficiently and reliably realize real-time perception decision of scarce high-altitude image data.","['G06V10/806', 'G06V10/20', 'G06V10/40', 'G06V10/82']"
US6584221B1,Method for image retrieval with multiple regions of interest,A method for representing an image in an image retrieval database first separates and filters images to extract color and texture features. The color and texture features of each image are partitioned into a plurality of blocks. A joint distribution of the color features and a joint distribution of the texture features are estimated for each block. The estimated joint distributions are stored in the database with each image to enable retrieval of the images by comparing the estimated joint distributions.,"['G06F16/5846', 'G06F16/5838', 'G06F18/40', 'G06V10/56', 'Y10S707/99936']"
CN113221889B,Chip character anti-interference recognition method and device,"The invention relates to a chip character recognition method, in particular to an anti-interference chip character recognition method, which comprises the following steps: (1) providing a camera and a light source; (2) Calling a template, and extracting a chip image only comprising chips after a frame of image is acquired by a camera and matched with the template; (3) extracting a chip character image containing only character areas: (4) Sequentially carrying out noise reduction treatment and threshold segmentation on the chip character image to obtain a binarized image, and carrying out character segmentation on the binarized image to obtain a single character; (5) Character defect detection, namely obtaining defect character optimization and character recognition of a defect character (6): (6-1) defective character optimization; (6-2) the artificial neural network character recognition to obtain a final character string. The method can effectively reduce partial interference of the environment when the image is acquired, prevent errors of the chip placement angle, and cooperate with other detection equipment, so that labor cost is obviously reduced, and production efficiency is improved.","['G06V10/25', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T7/136', 'G06T7/62', 'G06V10/30', 'G06V30/153', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20104', 'G06T2207/30148']"
US10671656B2,"Method for recommending text content based on concern, and computer device","A method for recommending a text content based on a concern, a computer device, and a non-transitory computer readable storage medium are provided. The method includes: acquiring a query input by a user, and acquiring a reference text content selected by the user from search results corresponding to the query; generating a term vector of the query according to a term relative to the query in the reference text content; determining the concern of the user from a plurality of reference concerns according to similarities between the term vector of the query and term vectors of the plurality of reference concerns; and recommending the text content matched with the concern to the user.","['G06F16/338', 'G06F16/3344', 'G06F16/3347', 'G06F18/22', 'G06F40/30']"
CN117115443B,A segmentation method for identifying small infrared targets,"The invention provides a segmentation method for identifying infrared small targets, which comprises the following steps: capturing local patch characteristic detail information and global position information in a small target image by adopting a local patch and global attention network model which are asymmetrically fused in multiple scales; introducing a multiscale asymmetric fusion module into a local patch and global attention network model, and reserving pixels at the up-sampling key positions; and introducing a micro-recognition module into the global attention network, fusing low-level features and high-level features, and realizing extraction of small-target weak edge pixels. The global and local patch attention modules based on gating axial attention fully integrate global position and local characteristic information of the small target, and enhance semantic connection of context; and the problem of key pixel loss in the up-sampling process is effectively solved; the jump connection can fully understand the deep semantic features and the shallow semantic features, so that the key pixels are reserved; and the low-level features and the high-level features are fused, so that the small-target weak edge pixel extraction is realized.","['G06V10/26', 'G06N3/0442', 'G06N3/0464', 'G06V10/806', 'G06V10/82', 'G06V2201/07', 'Y02T10/40']"
US20200257979A1,"Normalization method and apparatus for deep neural network, and storage media","Embodiments of the present disclosure disclose normalization methods and apparatuses for a deep neural network, devices, and storage media. The method includes: inputting an input data set into a deep neural network, the input data set including at least one piece of input data; normalizing a feature map set output by means of a network layer in the deep neural network from at least one dimension to obtain at least one dimension variance and at least one dimension mean; and determining a normalized target feature map set based on the at least one dimension variance and the at least one dimension mean. Based on the embodiments of the present disclosure, normalization is performed along at least one dimension so that statistics information of each dimension of a normalization operation is covered, thereby ensuring good robustness of statistics in each dimension without excessively depending on the batch size.","['G06N3/084', 'G06F16/2264', 'G06N3/045', 'G06N3/0464', 'G06N3/048', 'G06N3/063', 'G06N3/08', 'G06N3/044']"
CN114600165B,Systems and methods for surface modeling using polarization cues,"A computer-implemented method for surface modeling includes receiving one or more polarized raw frames of a surface of a physical object, the polarized raw frames captured at different linear polarization angles by way of a polarization filter, extracting one or more first tensors in one or more polarized representation spaces from the polarized raw frames, and detecting a surface characteristic of the surface of the physical object based on the one or more first tensors in the one or more polarized representation spaces.","['G01B11/24', 'G02B21/0016', 'G02B21/0092', 'G06F18/2413', 'G06T7/0004', 'G06T7/55', 'G06V10/145', 'G06V10/60', 'G06V10/764', 'G06V10/82', 'G06V20/80', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30156', 'G06V2201/06']"
US8150192B2,System and method for feature score mapping and visualization of medical images,"A system to collect and analyze medical image data is applicable for multi-modality medical imaging systems, such as x-ray, MRI, and the like. Medical image data is collected and analyzed to determine one or more regions of interest. A selected region of interest is further analyzed to determine morphological characteristics. A feature library, which may be in the form of a data base, is used to analyze the image on a pixel-by-pixel basis to determine the degree to which the region of interest matches selected morphological characteristics, such as shape or margin. The resultant data is used to generate a map indicating a continuum over which the region of interest matches the morphological characteristics. A display receives the map data and applies it to the video image to thereby provide the user with a visualization of the degree to which the region of interest matches the morphological characteristics.","['G06T7/0012', 'G06T2200/24', 'G06T2207/10088', 'G06T2207/10116', 'G06T2207/30068']"
CN112330589A,Method and device for estimating pose and computer readable storage medium,"The disclosure provides a method and a device for estimating pose and a computer readable storage medium, and relates to the technical field of computers. The method for estimating the pose information comprises the following steps: determining first pose information of a target in a current image by using a visual odometer, wherein the current image is a key frame in an image sequence of the target; predicting a neural network by using pre-trained key points, and determining second attitude information of the target in the current image; and performing data fusion on the first pose information and the second pose information to obtain fusion pose information of the target in the current image. According to the method and the device, the deep learning technology and the visual odometer technology are fused, so that not only can the accumulated estimation error brought by the visual odometer be reduced, but also the estimation pose jitter brought by the deep learning technology can be relieved, and the pose information of the target in the image can be estimated more accurately and stably.","['G06T7/97', 'G06N3/045', 'G06N3/08', 'G06T7/73', 'G06T2207/20081', 'G06T2207/20084']"
CN111369581B,"Image processing method, device, equipment and storage medium","The embodiment of the application discloses an image processing method, an image processing device, image processing equipment and a storage medium, and belongs to the field of image processing. The method comprises the following steps: acquiring an original image, wherein the original image comprises at least one target object; inputting an original image into a first prediction model to obtain a first transparent channel image output by the first prediction model, wherein the first transparent channel image comprises predicted transparency values corresponding to all pixel points in the original image; inputting the first transparent channel image and the original image into a second prediction model to obtain a second transparent channel image output by the second prediction model, wherein the fineness of the second transparent channel image is higher than that of the first transparent channel image; and dividing the original image according to the second transparent channel image to obtain an image corresponding to the target object. Compared with the image segmentation method in the related art, the method has the advantages that a three-dimensional image is not required to be introduced, the transparent channel image can be directly generated from the original image, and the accuracy of image segmentation is further improved.","['G06T7/11', 'G06N3/045', 'G06N3/084', 'G06T7/194', 'G06T2207/20016', 'G06T2207/20081', 'Y02T10/40']"
US12118721B2,Systems and methods for image preprocessing,"A method and apparatus of a device that classifies an image is described. In an exemplary embodiment, the device segments the image into a region of interest that includes information useful for classification and a background region by applying a first convolutional neural network. In addition, the device tiles the region of interest into a set of tiles. For each tile, the device extracts a feature vector of that tile by applying a second convolutional neural network, where the features of the feature vectors represent local descriptors of the tile. Furthermore, the device processes the extracted feature vectors of the set of tiles to classify the image.","['G06T7/0012', 'G06F18/214', 'G06F18/2163', 'G06F18/217', 'G06F18/23', 'G06F18/2413', 'G06N3/04', 'G06N3/0455', 'G06N3/0464', 'G06N3/0895', 'G06N3/09', 'G06N3/096', 'G06T7/11', 'G06T7/194', 'G06V10/32', 'G06V10/50', 'G06V10/764', 'G06V10/82', 'G06V20/695', 'G06V20/698', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024']"
US10102450B2,Superpixel generation with improved spatial coherency,A method for generating superpixels for an image comprising pixels and an apparatus configured to perform the method. A clustering stage clusters the pixels into initial clusters. A determination stage then selects spatially coherent parts of the clusters and determines unconnected fragments of the clusters. Pixels of these unconnected fragments are iteratively assigned to adjacent clusters by an assignment stage using a contour evolution process.,"['G06K9/6218', 'G06T7/11', 'G06F18/23', 'G06F18/24137', 'G06F18/285', 'G06K9/6227', 'G06T7/12', 'G06T7/187', 'G06V10/762', 'G06V10/764', 'G06T2207/20016', 'G06T2207/20116']"
US20190147296A1,Creating an image utilizing a map representing different classes of pixels,"A method, computer readable medium, and system are disclosed for creating an image utilizing a map representing different classes of specific pixels within a scene. One or more computing systems use the map to create a preliminary image. This preliminary image is then compared to an original image that was used to create the map. A determination is made whether the preliminary image matches the original image, and results of the determination are used to adjust the computing systems that created the preliminary image, which improves a performance of such computing systems. The adjusted computing systems are then used to create images based on different input maps representing various object classes of specific pixels within a scene.","['G06K9/6257', 'G06V10/454', 'G06F18/2148', 'G06F18/24133', 'G06K9/6857', 'G06K9/726', 'G06T1/20', 'G06T11/001', 'G06V10/82', 'G06V30/19173', 'G06V30/2504', 'G06V30/274']"
CN111709483B,Multi-feature-based super-pixel clustering method and equipment,"The invention provides a super-pixel clustering method and equipment based on multiple features. Through a clustering phase and a merging phase. In the clustering stage, judging the attribution of pixels by measuring the similarity among the pixels, and clustering to obtain pre-segmentation super pixels; and (3) starting from the color characteristics and the spatial position characteristics of the pixels, adding boundary characteristic factors to harmonize the similarity between the pixels in the adjacent regions of the boundary in the similarity measurement of the clustering stage. The resulting superpixel edges are closely fitted, but the segmentation is too fine and needs further correction. In the merging stage, the similarity degree between the super pixels is measured according to the uniqueness of the content of the super pixels, and scattered super pixels are aggregated to obtain the final super pixel. Compared with the prior art, the method has higher performance in keeping the super-pixel boundary and the object boundary in the image fitted.","['G06F18/23', 'G06F18/253', 'G06V10/267']"
US20190205758A1,Gland segmentation with deeply-supervised multi-level deconvolution networks,"Pathological analysis needs instance-level labeling on a histologic image with high accurate boundaries required. To this end, embodiments of the present invention provide a deep model that employs the DeepLab basis and the multi-layer deconvolution network basis in a unified model. The model is a deeply supervised network that allows to represent multi-scale and multi-level features. It achieved segmentation on the benchmark dataset at a level of accuracy which is significantly beyond all top ranking methods in the 2015 MICCAI Gland Segmentation Challenge. Moreover, the overall performance of the model surpasses the state-of-the-art Deep Multi-channel Neural Networks published most recently, and the model is structurally much simpler, more computational efficient and weight-lighted to learn.","['G06V10/82', 'A61B5/72', 'G06F18/214', 'G06F18/2415', 'G06F18/2431', 'G06K9/6232', 'G06K9/6256', 'G06K9/6277', 'G06K9/628', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'G06T7/0012', 'G06T7/11', 'G06T7/13', 'G06V10/454', 'G06V20/695', 'G06V20/698', 'G16H30/40', 'G06K2209/05', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024', 'G06V2201/03']"
US20210243362A1,Techniques for enhanced image capture using a computer-vision network,"Disclosed are techniques for enhancing two-dimensional (2D) image capture of subjects (e.g., a physical structure, such as a residential building) to maximize the feature correspondences available for three-dimensional (3D) model reconstruction. More specifically, disclosed is a computer-vision network configured to provide viewfinder interfaces and analyses to guide the improved capture of an intended subject for specified purposes. Additionally, the computer-vision network can be configured to generate a metric representing a quality of feature correspondences between images of a complete set of images used for reconstructing a 3D model of a physical structure. The computer-vision network can also be configured to generate feedback at or before image capture time to guide improvements to the quality of feature correspondences between a pair of images.","['H04N5/23222', 'H04N23/61', 'H04N23/64', 'G06T17/00', 'G06T7/11', 'G06T7/12', 'G06T7/174', 'G06T7/277', 'G06T7/74', 'G06V10/26', 'G06V10/44', 'H04N23/633', 'H04N23/635', 'H04N5/232945', 'G06F3/167', 'G06T15/00', 'G06T2207/20072', 'G06T2207/20084', 'G06T2210/00', 'G06V30/19013', 'G06V30/19107', 'G06V30/414']"
EP1085456B1,Character segmentation method for vehicle license plate recognition,"A method for segmenting and recognizing license plates, in accordance with the present invention includes capturing an image of a license plate (100) and preprocessing the image to prepare the image for segmentation and recognition (110). Forward splitting regions of the license plate image into suspected characters regions, the suspected character regions are recognized in the license plate image and a confidence score is provided based on a probability of a correct match (114). If the suspected characters have a confidence score below a threshold value, backward merging is employed which merges adjacent suspected character regions (116). The backward merged suspected character regions in the license plate are recognized. If the backward merged suspected character regions have a confidence score below the threshold value, the backward merging and recognizing steps are repeated to improve the confidence score. <IMAGE>","['G06V30/153', 'G06V20/625', 'G06V30/10']"
CN114882198B,"A target determination method, device, equipment and medium","The embodiment of the invention discloses a target determining method, a target determining device, target determining equipment and a target determining medium. The method comprises the steps of carrying out target recognition on initial point cloud data based on a target recognition model, determining a first target point cloud set, carrying out clustering processing on residual point cloud data by adopting a clustering algorithm, and determining a second target point cloud set, wherein the residual point cloud data are point cloud data except the first target point cloud set in the initial point cloud data, and determining targets according to the first target point cloud set and the second target point cloud set. According to the technical scheme, the aim of identifying the target from the point cloud data is fulfilled, the point cloud data is subjected to deep learning model and clustering algorithm processing successively, the targets corresponding to the dense point cloud and the sparse point cloud are respectively obtained, the final target is determined from the two identification results, the effects of high target identification accuracy and good stability are achieved, and the target recall rate is improved.","['G06T17/20', 'G06N20/00', 'G06V10/762']"
CN110070570B,An obstacle detection system and method based on depth information,"The invention discloses an obstacle detection system and method based on depth information, and mainly relates to the field of computer vision. The method comprises the following steps: firstly, acquiring image information of an environment through a depth camera, and acquiring point cloud information of the environment and objects in the environment through the equipment; filtering the acquired depth point cloud information, removing noise points and useless points in the image, and performing plane fitting on the acquired depth map; determining a preliminary point cloud set of the obstacle according to the depth map and the plane fitting result; extracting characteristic points of the point cloud information, and calculating descriptors of the characteristic points; and performing feature matching on the descriptors of the scene and the target object to obtain a detection result of the obstacle in the scene. The method is not affected by environmental changes, and color elements are added for detection according to the characteristics of the sensor, so that the accuracy of obstacle detection is improved.","['G06T7/0002', 'G06T7/50', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20164', 'G06T2207/20192']"
US6654506B1,Method for automatically creating cropped and zoomed versions of photographic images,"A method and computer program/system for cropping a digital image includes inputting a belief map of a photographic image, (a belief value at a location in the belief map indicates an importance of a photographic subject at the same location in the photographic image), selecting a zoom factor and a crop window, clustering regions of the belief map to identify background portions, secondary portions and main portions, positioning the crop window such that the crop window is centered around a main portion having a highest belief value, moving the crop window such that the crop window is included completely within the image, moving the crop window such that a sum of belief values is at a maximum, and cropping the image according to the crop window.","['G06T11/00', 'G06F18/23', 'G06V10/25', 'G06V10/762']"
CN113012185B,"Image processing method, device, computer equipment and storage medium","The application relates to an image processing method, an image processing device, a computer device and a storage medium. The method comprises the following steps: obtaining a target image to be subjected to style conversion; acquiring a brightness channel image corresponding to the target image; acquiring a target edge image corresponding to the target image; carrying out quantization processing on pixel values corresponding to all pixel points in the brightness channel image to obtain a quantized image; performing image fusion on the target edge image and the quantized image to obtain a fusion image; and acquiring a color channel image corresponding to the target image, and acquiring a style-converted image corresponding to the target image based on the fusion image and the color channel image. The image processing effect can be improved by adopting the method.","['G06T7/181', 'G06T5/70', 'G06T5/94', 'G06T7/13', 'G06T7/90', 'G06T2207/10024', 'G06T2207/20221', 'Y02D10/00']"
US20190251571A1,Transaction verification system,"An analytics server for use in a transaction system with a terminal for performing authenticated user-initiated transactions and generating transaction data including a user identity associated with each transaction and a camera for capturing image data of a user performing an authenticated transaction at the terminal is configured to extract user characteristic features from the image data associated with authenticated transactions and iteratively update a user database of the user characteristic features over multiple authenticated transactions. The analytics server is further configured to compute a match score, based on preset rules, of the user characteristic features for a current transaction with the user characteristic features associated with a current user stored in the user database, and raise an alarm when the match score fails to meet a threshold value.","['G06V40/168', 'G06Q20/40145', 'G06F21/32']"
CN111696164B,"Self-adaptive window width and window level adjusting method, device, computer system and storage medium","The invention relates to the technical field of artificial intelligence, and discloses a self-adaptive window width and window level adjusting method, a device, a computer system and a storage medium, wherein the self-adaptive window width and window level adjusting method comprises the following steps: extracting gray values of pixels in an image to be regulated and summarizing to obtain an input feature vector; calculating a truncated adjustment coefficient of each gray value in the input feature vector through a guidable truncated model, summarizing to form a truncated adjustment vector, and adjusting the input feature vector according to the truncated adjustment vector to generate an output feature vector; and sending the output characteristic vector to a preset neural network, updating the weight of the conductive truncated model by the neural network according to the output characteristic vector, so that the conductive truncated model generates the output characteristic vector which accords with a neural network loss function, and generating a window width and level image according to the output characteristic vector. The window width and window level image obtained by the method meets the requirements of a user on window width and window level adjustment and meets the requirements of a neural network on window width and window level processing or classification.","['G06T11/003', 'G06N3/045', 'G06N3/048', 'G06N3/084', 'Y02T10/40']"
CN109389608B,There is the fuzzy clustering image partition method of noise immunity using plane as cluster centre,"The invention discloses a kind of using plane as the fuzzy clustering image partition method of the noise immunity of cluster centre, method includes the following steps: firstly, each term coefficient and threshold value etc. in objective function and initialized target function, random initializtion subordinated-degree matrix；The minimization of object function is set to calculate each term coefficient and fuzzy membership matrix for updating cluster plane；Value based on updated fuzzy membership matrix calculating target function, terminate when the absolute value of the difference of the target function value of iteration twice in succession is less than iteration when termination condition or method are limited beyond maximum number of iterations, otherwise, continue iteration and executes update, according to the maximum each pixel of criterion classification marker of degree of membership, preliminary classification is completed；Image border is extracted on classification results, and local window is chosen as center pixel using marginal point and carries out degree of membership division again；According to the fuzzy membership matrix of cluster output, obtains data point and belong to certain a kind of degree of membership, according to maximum probability principle to the just classification marker of each data point, complete image segmentation.Method of the invention replaces cluster centre to carry out image segmentation using cluster plane, the gray value information and location information of pixel can be considered simultaneously, ideal image segmentation is achieved, and eliminates the influence of noise well, improves the quality of image segmentation and the stability of segmentation effect.","['G06T7/11', 'G06F18/23213', 'G06T2207/10004']"
CN111798475B,Indoor environment 3D semantic map construction method based on point cloud deep learning,"The invention relates to an indoor environment 3D semantic map construction method based on point cloud deep learning, which comprises the following four parts: (1) Acquiring a color map and a depth map of an indoor environment by using a depth camera; (2) Constructing a point cloud deep learning network to acquire 3D semantic information of an object in the environment; (3) detecting dynamic objects and eliminating dynamic feature points; (4) Solving the motion of a camera to realize a visual odometer, and constructing and optimizing a local map; (5) Constructing a target semantic library according to the obtained 3D point cloud semantic information; (6) And carrying out semantic fusion on the local map according to the target semantic library, and constructing an octree semantic map. Compared with the prior art, the method has the advantages that the characteristic points in the dynamic object mask are removed by combining the semantic category information, the influence of the dynamic object on positioning and mapping is effectively reduced, the 3D semantic acquisition mode is more direct and efficient, and better positioning mapping capability and semantic perception effect are achieved.","['G06T7/13', 'G06F16/29', 'G06F18/24', 'G06N3/045', 'G06T7/11', 'G06T2207/10028', 'G06T2207/20016']"
CN111814654B,Markov random field-based remote tower video target tagging method,"The invention discloses a Markov random field-based remote tower video target tagging method, which comprises the following steps: establishing a model: solving sparse representation of the continuous video frame sequence by using a greedy algorithm to obtain initial estimation of a background; solving the image segmentation problem by using a recurrent neural network to obtain a foreground target tracking result and background estimation; and establishing a corresponding relation between the position of the target coordinate point in the world coordinate system and the broadcast type automatic correlation monitoring data by adopting a nearest neighbor method, thereby associating the label information in the broadcast type automatic correlation monitoring to the video and realizing automatic label hanging. According to the method, a sparse sampling mode is utilized, the data set of calculation operation is reduced, and the complexity of background calculation is reduced; and taking the background as input, and automatically forming the optimized estimation of the foreground target by utilizing the self-optimization characteristic of the Hopfield network.","['G06V20/41', 'G06F18/295', 'G06N3/08', 'G06V20/42', 'G06V20/49']"
CN112819772B,High-precision rapid pattern detection and recognition method,"The invention discloses a high-precision rapid pattern detection and recognition method which is characterized by adopting machine vision to perform seven steps of image shooting acquisition, image enhancement pretreatment, edge detection, key point selection, curve fitting, attribute parameter operation and pattern reconstruction on a workpiece pattern. The method is suitable for workpiece pattern detection, recognition and reconstruction in the intelligent manufacturing field.","['G06T7/0004', 'G06T5/20', 'G06T5/90', 'G06T7/13', 'G06T7/136', 'G06T7/181', 'G06T7/64', 'G06T2207/20012', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20192', 'G06T2207/30164']"
US10373312B2,Automated skin lesion segmentation using deep side layers,"A method for computer-aided diagnosis of skin lesions includes obtaining a dermoscopic image, convolving the dermoscopic image in a plurality of convolutional layers, obtaining deconvolved outputs of at least two convolutional layers of the plurality of convolutional layers, obtaining side-output feature maps by applying loss functions to the deconvolved outputs of the at least two convolutional layers, obtaining a first concatenated feature map by concatenating the side-output feature maps with different first weights, obtaining a second concatenated feature map by concatenating the side-output feature maps with different second weights, and producing a final score map by convolving the first and second concatenated feature maps in a final convolutional layer followed by a loss layer. Also disclosed: a computer-readable medium embodying instructions for the method, and an apparatus configured to implement the method.","['G06T7/0012', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30088', 'G06T2207/30096']"
US11922712B2,Technologies for content analysis,Various computing technologies for content analysis.,"['G06V30/422', 'G06F18/40', 'G06F3/0482', 'G06F3/0484', 'G06F9/451', 'G06Q50/184', 'G06T3/60', 'G06V30/226', 'G06V30/413', 'G06V30/416', 'G06F2203/04803', 'G06V30/10']"
US11657510B2,Automatic sizing and placement of text within a digital image,"This disclosure involves the automatic sizing and placement of text within an image background. For example, a computing system obtains reference font size information for a font type to be applied to message text for display on a digital image. The computing system detects, within an image background of the digital image, a target region having proportions that enclose the message text based on the reference font size information. The computing system determines a target font size for the message text. The target font size allows the message text, when rendered in the font type at the target font size, to fit within the target region of the image background. The computing system generates a combined digital image by rendering the message text in the font type at the target font size within the target region of the image background.","['G06T11/60', 'G06T7/11', 'G06T7/194']"
US9158995B2,Data driven localization using task-dependent representations,"A computer implemented method for localization of an object, such as a license plate, in an input image includes generating a task-dependent representation of the input image based on relevance scores for the object to be localized. The relevance scores are output by a classifier for a plurality of locations in the input image, such as patches. The classifier is trained on patches extracted from training images and their respective relevance labels. One or more similar images are identified from a set of images, based on a comparison of the task-dependent representation of the input image and task-dependent representations of images in the set of images. A location of the object in the input image is identified based on object location annotations for the similar images.","['G06K9/6217', 'G06V20/62', 'G06F18/21', 'G06F18/24', 'G06K9/3233', 'G06K9/325', 'G06K9/6267', 'G06T7/0081', 'G06V10/25', 'G06K2209/15', 'G06T7/2033', 'G06V20/625']"
US10936051B2,"Power management for gesture recognition in virtual, augmented, and mixed reality (xR) applications","Power management systems and methods for gesture recognition in virtual, augmented, and mixed reality (xR) applications is described. In an illustrative, non-limiting embodiment, an Information Handling System (IHS) may include a processor and a memory coupled to the processor, the memory having program instructions stored thereon that, upon execution by the processor, cause the IHS to: receive frames captured by a camera of a Head-Mounted Device (HMD) worn by a user in an xR application, wherein the HMD is coupled to a plurality of infrared (IR) emitters; identify a state of a gesture sequence being performed by the user based upon the frames; and select a level of illumination provided by the plurality of IR emitters in response to the state.","['G06F3/011', 'G02B27/017', 'G06F3/017', 'G02B2027/0138', 'G02B2027/014', 'G02B2027/0187']"
US10061392B2,Control system for navigating a principal dimension of a data space,"Systems and methods are described for navigating through a data space. The navigating comprises detecting a gesture of a body from gesture data received via a detector. The gesture data is absolute three-space location data of an instantaneous state of the body at a point in time and physical space. The detecting comprises identifying the gesture using the gesture data. The navigating comprises translating the gesture to a gesture signal, and navigating through the data space in response to the gesture signal. The data space is a data-representational space comprising a dataset represented in the physical space.","['G06F3/017', 'G06F3/0325', 'G06K9/00342', 'G06K9/00375', 'G06T7/00', 'G06V40/107', 'G06V40/23', 'G09G5/00', 'G09G5/08', 'G06F3/1423', 'G06F3/1454', 'G06K2009/3225', 'G06V10/245', 'G09G2354/00']"
CN110458805B,"Plane detection method, computing device and circuit system","The application discloses a plane detection method, a computing device and a circuit system, wherein the method comprises the following steps: the computing equipment acquires image data to be processed, and then the image data to be processed is segmented to obtain N sub-image data, wherein N is an integer larger than 1. And then, determining point cloud information corresponding to at least one sub-image data in the N sub-image data, clustering point clouds corresponding to the N sub-image data according to the point cloud information corresponding to at least one sub-image data in the N sub-image data to obtain K crude extraction planes, and optimizing the K crude extraction planes to obtain L optimized planes, wherein K is a positive integer not greater than N, and L is a positive integer not greater than K. This may enable the computing device to detect more than one plane in the image data.","['G06F18/2321', 'G06T17/005', 'G06T7/0002', 'G06T7/136', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20221']"
US10728527B2,Tilts as a measure of user engagement for multiview interactive digital media representations,"Various embodiments of the present invention relate generally to systems and methods for analyzing and manipulating images and video. According to particular embodiments, the spatial relationship between multiple images and video is analyzed together with location information data, for purposes of creating a representation referred to herein as a multi-view interactive digital media representation for presentation on a device. Once a multi-view interactive digital media representation is generated, a user can provide navigational inputs, such via tilting of the device, which alter the presentation state of the multi-view interactive digital media representation. The navigational inputs can be analyzed to determine metrics which indicate a user's interest in the multi-view interactive digital media representation.","['H04N13/279', 'G06F3/04815', 'G06F1/1694', 'G06F3/0485', 'H04N13/117', 'H04N13/243']"
WO2020134769A1,"Image processing method and apparatus, electronic device, and computer readable storage medium","Disclosed in embodiments of the present application are an image processing method and apparatus, an electronic device, and a computer readable storage medium. The method comprises: obtaining an image to be registered and a reference image used for registration; inputting the image to be registered and the reference image into a preset neural network model, wherein a target function for measuring similarity in training of the preset neural network model comprises correlation coefficient loss of a preset image to be registered and a preset reference image; and registering the image to be registered with the reference image on the basis of the preset neural network model to obtain a registration result. The accuracy and real-time performance of image registration can be improved.","['G06T7/33', 'G06N3/04', 'G06N3/045', 'G06T3/14', 'G06T3/40', 'G06T7/32', 'G06T2207/10016', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30061']"
CN110555481B,"Portrait style recognition method, device and computer readable storage medium","The embodiment of the application discloses a portrait style identification method, a portrait style identification device and a computer readable storage medium, wherein the embodiment of the application can acquire an image to be identified; dividing an area where the portrait is from the image to be identified to obtain a target identification image; based on different convolution kernels of the recognition model after the multi-task training, extracting features of different dimensions of the target recognition image to obtain image feature information of different dimensions; based on a first full-connection layer of the recognition model after the multi-task training, fusing different dimensionalities corresponding to the image characteristic information to obtain style type probability information of the portrait; and determining the style corresponding to the portrait based on the style probability information. Therefore, the style type of the person in the image to be identified can be accurately identified.","['G06F18/253', 'G06N3/045', 'G06N3/08']"
CN112614060B,"Face image hair rendering method and device, electronic equipment and medium","The application discloses a method, a device, electronic equipment and a medium for rendering human face image hair. The method comprises the following steps: acquiring a hair region in an image to be processed, wherein a first color value of each pixel point is in an RGB format; according to a preset linear transformation relation, carrying out correction processing on each channel component in the first color value to obtain a corrected first color value; the difference degree between the first color value of the corrected target pixel point and the brightness value of the target pixel point is larger than the difference degree between the first color value of the target pixel point and the brightness value of the target pixel point before correction, and the target pixel point is any pixel point in the hair area; converting the corrected first color value into an HSV format to obtain a second color value of each pixel point; acquiring a reference color value of an HSV format; obtaining target color values of each pixel point RGB format according to the reference color values and the second color values; and rendering the hair area by using the target color value of each pixel point to obtain a rendered image.","['G06T5/77', 'G06T11/001', 'G06T2207/30201']"
US9480439B2,Segmentation and fracture detection in CT images,"Provided is a new hierarchical methodology having a series of computational steps such as adaptive window creation, 2-D SWT application, masking, and boundary tracing is proposed. The techniques and systems are able to detect and quantify fracture as well as to generate recommendations for decision-making and treatment planning in traumatic pelvic injuries.","['A61B6/032', 'A61B6/505', 'A61B6/5211', 'G06K9/6204', 'G06T7/0014', 'G06V10/752', 'G16H50/20', 'G06T2200/04', 'G06T2207/10081', 'G06T2207/30008']"
CN112598687B,"Image segmentation method and device, storage medium and electronic equipment","The disclosure relates to an image segmentation method and device, a storage medium and electronic equipment, and relates to the technical field of image processing, wherein the method comprises the following steps: traversing current mask image data corresponding to an image to be processed to obtain outline edge data of outline edges of a foreground image included in the current mask image data, and marking the outline edge data of the foreground image to obtain edge segmentation data; expanding the image area from the contour edge to an image area far away from the contour edge according to the edge segmentation data to determine transition data comprising data belonging to a foreground image and data belonging to a background image; and generating a target mask image according to the edge segmentation data and the transition data, and performing image processing on the image to be processed according to the target mask image. The present disclosure makes the segmentation edges between background and foreground images more natural.","['G06T7/12', 'G06T15/205', 'G06T2207/20104', 'Y02T10/40']"
US9846932B2,Defect detection method for display panel based on histogram of oriented gradient,"In order to detect defects in display panels by an automatic way accurately and quickly, the present invention proposes a method combining image feature extraction and classifier model. It calculates the histograms of oriented gradient (HOG) of images of the display panel collected by an industrial camera of a detection apparatus as feature vectors. Then use them as input samples to train the classifier model to recognize the defects of the display panel.","['G06V10/462', 'G06F18/2411', 'G06K9/4671', 'G06T7/0004', 'G06T7/001', 'G06T2207/20081', 'G06T2207/30121', 'G06T2207/30141']"
US8682648B2,Methods and systems for assessing the quality of automatically generated text,"A set of ordered characters is received in association with information specifying the locations of the characters within the image of the document. Language-conditional character probabilities for each character are determined based on a set of language models and the ordering of the characters. Neighbor characters associated with a target character are identified based on the locations of the characters. Language-conditional character probabilities associated with the neighbor characters and language-conditional character probabilities associated with the target character are combined to generate a local language-conditional likelihood associated with the target character, the local language-conditional likelihood representing a concordance of the target character to a language model.","['G06V30/153', 'G06F40/253', 'G06F40/51', 'G06V30/246', 'G06V30/268']"
US20220309653A1,System and method for attention-based classification of high-resolution microscopy images,"This invention provides a system and method for analyzing and classifying imaged from whole slides of tissue. A source of image data transmits images of the tissue on the whole slides to a GPU. The GPU performs a feature extraction process that identifies and segments regions of interests in each of the images, and an attention network that, based upon training from an expert, identifies trained characteristics can comprise cancerous and/or pre-cancerous conditions/e.g. those associated with a gastrointestinal tract, such ad Barret's Esophagus. The feature extraction process can include a convolutional neural network (CNN). The attention network can be adapted performs attention/based weighting of features relative to the trained characteristics, and/or the attention network can include 3D convolutional filters. The image data is acquired using an image sensor having approximately 100 Megapixel resolution.","['G06T7/0012', 'G06T7/11', 'G06V10/82', 'G06T2207/10056', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004', 'G06T2207/30024']"
CN110569731B,Face recognition method and device and electronic equipment,"The application relates to a face recognition method, a face recognition device and electronic equipment, wherein the method comprises the following steps: when the face in the face image of the user is partially occluded, extracting the global features of the face from the face image based on an attention extraction mode, dividing the face image into a plurality of sub-images, and extracting the local features of the face corresponding to the sub-images from each sub-image in the plurality of sub-images; combining the global features with local features of the human face extracted from at least part of sub-images in the plurality of sub-images to obtain combined features of the human face; and carrying out face recognition on the user based on the combined features of the face and the combined features of the face in the reference face image. When the face of the user is identified under the condition that the face is partially shielded, the similarity of the global features of the face and the similarity of the local features of the face are considered at the same time, the similarity of the face is judged comprehensively, and the accuracy of face identification under the condition that the face is partially shielded is improved.","['G06V40/161', 'G06V40/169', 'G06V40/171']"
US8340357B2,Moving object detection method and moving object detection apparatus,"A moving object detection method is provided which can accurately perform segmentation on an image including an object such as a person that moves changing shape. The method includes: accepting pictures included in the video (S101); calculating movement trajectories by detecting motions between two temporally adjoining pictures in units of blocks constituting each of the pictures and each including one or more pixels, and concatenating detected motions for all the pictures (S102 and S103); calculating distances each indicating similarity between the calculated movement trajectories (S104); and performing segmentation by performing geodetic distance transformation by combining distances smaller than a predetermined threshold from among the calculated distances, detecting a discontinuity point in a distribution of the calculated geodetic distances, and classifying, into one cluster, movement trajectories separate from each other at a geodetic distance smaller than a length indicating the detected discontinuity point (S105 and 106).","['G06T7/215', 'G06T2207/10016', 'G06T2207/30196', 'G06T2207/30241']"
CN113658132B,Computer vision-based structural part weld joint detection method,"The invention relates to the technical field of computer vision, in particular to a structural part weld joint detection method based on computer vision. The method comprises the following steps: collecting the assembled structural part image, and identifying a target image with a welding seam in the structural part image by using a neural network; acquiring a class activation diagram of the target image with a weld as a sensitive area according to a neural network corresponding to the target image; performing threshold segmentation on the gray level image of the class activation image to obtain a minimum external rectangle of the welding line; amplifying the minimum circumscribed rectangle to obtain a weld joint segmentation area, and cutting out the weld joint segmentation area in the target image as a weld joint reference image and partitioning the weld joint reference image; and obtaining the comprehensive difference between every two pixel points by calculating the color distance, the space distance and the similar activation difference between every two pixel points in the welding seam reference image, and performing super-pixel segmentation on the welding seam reference image according to the comprehensive difference and the number of subareas to obtain a welding seam area. The embodiment of the invention can improve the accuracy of super-pixel segmentation.","['G06T7/0004', 'G01N21/8851', 'G06F18/2321', 'G06T7/11', 'G06T7/136', 'G06T7/187', 'G01N2021/8887', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30152', 'Y02P90/30']"
CN112418236B,Automobile drivable area planning method based on multitask neural network,"The invention discloses a method for planning a drivable area of an automobile based on a multitask neural network, and relates to the fields of deep learning, computer vision, auxiliary driving, image processing and the like. Firstly, constructing a lightweight multi-task neural network with semantic segmentation and target detection functions based on a hard parameter sharing mechanism; secondly, according to a network output format, a training set is made and a corresponding loss function mathematical model is constructed; training the network again, and performing back propagation to optimize network parameters by using the loss function mathematical model provided by the invention; and finally, applying the multitask neural network to the planning of the automobile drivable area. The multi-task neural network not only has a lane segmentation function, but also has a vehicle and pedestrian detection function, and can play a role in anti-collision early warning of the vehicle through monocular distance measurement, so that the collision probability of the vehicle is reduced.","['G06V10/267', 'G06F18/214', 'G06F18/23213', 'G06N3/045', 'G06T11/001', 'G06V20/588', 'G06V2201/07']"
US10872227B2,"Automatic object recognition method and system thereof, shopping device and storage medium","An automatic object recognition method and an automatic object recognition system, a shopping device and a non-transitory computer readable storage medium are disclosed. The automatic object recognition method includes: acquiring a first image and a second image, both the first image and the second image including a same object, and the second image being a depth image; extracting a feature point of the object based on the first image; obtaining a joint feature based on the feature point from the first image and depth information of the second image; and recognizing the object based on the joint feature.","['G06K9/00201', 'G06V20/64', 'G06V20/20', 'G06K9/34', 'G06K9/46', 'G06T7/33', 'G06T7/38', 'G06T7/50', 'G06V10/751', 'G06F18/2411', 'G06F18/24143', 'G06F18/24147', 'G06T2207/10024', 'G06T2207/10028']"
US11042994B2,Systems and methods for gaze tracking from arbitrary viewpoints,"A system for determining the gaze direction of a subject includes a camera, a computing device and a machine-readable instruction set. The camera is positioned in an environment to capture image data of head of a subject. The computing device is communicatively coupled to the camera and the computing device includes a processor and a non-transitory computer-readable memory. The machine-readable instruction set is stored in the non-transitory computer-readable memory and causes the computing device to: receive image data from the camera, analyze the image data using a convolutional neural network trained on an image dataset comprising images of a head of a subject captured from viewpoints distributed around up to 360-degrees of head yaw, and predict a gaze direction vector of the subject based upon a combination of head appearance and eye appearance image data from the image dataset.","['G06T7/292', 'G06T7/73', 'G06F3/012', 'G06F3/013', 'G06F3/0304', 'G06N3/045', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06N5/046', 'G06T7/246', 'H04N23/63', 'H04N23/90', 'H04N5/23229', 'H04N5/23293', 'H04N5/247', 'G06N3/048', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196']"
US10354129B2,Hand gesture recognition for virtual reality and augmented reality devices,"A system for hand gesture recognition includes a display, camera, memory, and processor. When the processor is to execute instructions, the processor is to estimate one or more motion vectors of an object using a pair of consecutive frames and estimate an average motion vector of the object. The processor may also estimate an average motion vector of the object, determine a first histogram of optical flow (HOOF) based on the one or more motion vectors and the average motion vector, determine depth values based on motion vectors from the first HOOF, and determine a second histogram of optical flow (HOOF) based on the depth values. The processor is also to obtain a descriptor based on histogram values from a histogram of optical flow (HOOF) of the one or more motion vectors, a shape descriptor, and the average motion vector and classify the descriptor as a gesture.","['G06V40/28', 'G06K9/00355', 'G06F3/017', 'G06T7/269', 'G06T2207/10024', 'G06T2207/10048', 'G06T2207/20081', 'G06T2207/30196']"
US9881234B2,Systems and methods for end-to-end object detection,"Presented are systems and methods that provide a unified end-to-end detection pipeline for object detection that achieves impressive performance in detecting very small and highly overlapped objects in face and car images. Various embodiments of the present disclosure provide for an accurate and efficient one-stage FCN-based object detector that may be optimized end-to-end during training. Certain embodiments train the object detector on a single scale using jitter-augmentation integrated landmark localization information through joint multi-task learning to improve the performance and accuracy of end-to-end object detection. Various embodiments apply hard negative mining techniques during training to bootstrap detection performance. The presented are systems and methods are highly suitable for situations where region proposal generation methods may fail, and they outperform many existing sliding window fashion FCN detection frameworks when detecting objects at small scales and under heavy occlusion conditions.","['G06V10/82', 'G06V40/165', 'G06K9/6232', 'G06F18/2148', 'G06F18/2163', 'G06F18/2411', 'G06K9/6257', 'G06K9/6261', 'G06K9/6269', 'G06V10/454', 'G06V2201/08']"
CN119693312A,"Welding defect detection method, device, equipment, storage medium and program product","The application discloses a welding defect detection method, a welding defect detection device, welding defect detection equipment, a storage medium and a computer program product, which relate to the technical field of defect detection, wherein a defect general detection model is obtained through training based on a pre-training data set obtained through data enhancement, a defect sample image of a target product is based on the defect general detection model, a target detection model for detecting the defect of the target product is obtained through fine adjustment, and finally the defect in an actual defect image of the target product is obtained through detection of the target detection model. Therefore, the welding defects in the welding images and the quantization results of the welding defects to be quantized can be determined through the general model of the welding defect detection model, and the corresponding defect detection model is not required to be trained for single products, so that the detection requirements of multiple varieties and short periods of the production line are met.","['G06T7/0004', 'G06N3/0455', 'G06N3/0464', 'G06V10/764', 'G06V10/82', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30152', 'Y02P90/30']"
US8488888B2,Classification of posture states,"Systems and methods for estimating a posture of a body part of a user are disclosed. In one disclosed embodiment, an image is received from a sensor, where the image includes at least a portion of an image of the user including the body part. The skeleton information of the user is estimated from the image, a region of the image corresponding to the body part is identified at least partially based on the skeleton information, and a shape descriptor is extracted for the region and the shape descriptor is classified based on training data to estimate the posture of the body part.","['G06F3/011', 'G06F3/017', 'G06V40/103', 'G06V40/107', 'G06V40/23']"
CN110827242B,Intracranial aneurysm detection method and system based on convolutional neural network,"The invention belongs to the field of medical image processing, and discloses a convolutional neural network-based intracranial aneurysm detection method and system, wherein the detection method is based on a three-dimensional Time-of-Flight magnetic resonance (3D) vessel imaging image (3D TOF MRA), blood vessels are extracted firstly, then a series of cubic pixel blocks are extracted along the central line of the blood vessels to serve as regions of Interest (ROI), and maximum density Projection in multiple directions is carried out on each ROI to obtain a maximum density Projection image (maximum Intensity Projection, MIP); and classifying the MIP map by using the trained convolutional neural network by taking the MIP map as input, wherein the obtained classification result reflects whether the ROI contains the aneurysm, and further, whether the object to be detected has the intracranial aneurysm is judged. The invention improves the whole process of the method and the setting mode of each functional module component in the corresponding system device, so that the detection method and the system have higher classification accuracy and sensitivity.","['G06T7/0012', 'G06T17/00', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/30096', 'G06T2207/30101']"
US8587613B2,System and method for comparing and reviewing documents,"A document processing system for accurately and efficiently analyzing documents and methods for making and using same. Each incoming document includes at least one section of textual content and is provided in an electronic form or as a paper-based document that is converted into an electronic form. Since many categories of documents, such as legal and accounting documents, often include one or more common text sections with similar textual content, the document processing system compares the documents to identify and classify the common text sections. The document comparison can be further enhanced by dividing the document into document segments and comparing the document segments; whereas, the conversion of paper-based documents likewise can be improved by comparing the resultant electronic document with a library of standard phrases, sentences, and paragraphs. The document processing system thereby enables an image of the document to be manipulated, as desired, to facilitate its review.",['G06V30/40']
WO2020078888A1,System for co-registration of medical images using a classifier,"Disclosed is a system for analysis of microscopic image data representing a plurality of images acquired from cells. The system comprises a data processing system which is configured to read and/or generate (120) segmentation data for each of the images. For each of the images, the segmentation data are indicative of a segmentation of at least a portion of the respective image into one or more image regions so that each of the image regions is a member of one or more predefined classes of image content. The data processing system further generates co- registration data using at least portions of the segmentation data for co-registering at least portions of different ones of the images. The data processing system further generates mapping data using at least portions of the segmentation data for mapping between image regions of different images.","['G06T7/30', 'G06T7/0012', 'G06T7/11', 'G06V10/82', 'G06V20/695', 'G06V20/698', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024']"
US11367271B2,Similarity propagation for one-shot and few-shot image segmentation,"Embodiments of the present invention provide systems, methods, and computer storage media for one-shot and few-shot image segmentation on classes of objects that were not represented during training. In some embodiments, a dual prediction scheme may be applied in which query and support masks are jointly predicted using a shared decoder, which aids in similarity propagation between the query and support features. Additionally or alternatively, foreground and background attentive fusion may be applied to utilize cues from foreground and background feature similarities between the query and support images. Finally, to prevent overfitting on class-conditional similarities across training classes, input channel averaging may be applied for the query image during training. Accordingly, the techniques described herein may be used to achieve state-of-the-art performance for both one-shot and few-shot segmentation tasks.","['G06V10/462', 'G06V10/82', 'G06F18/2193', 'G06F18/253', 'G06K9/6265', 'G06K9/629', 'G06V10/25', 'G06V10/26', 'G06V10/454', 'G06V10/761']"
CN111368742B,Reconstruction and identification method and system of double yellow traffic marking lines based on video analysis,"The invention discloses a reconstruction and identification method and a system of double yellow traffic markings based on video analysis. The method of the invention comprises the steps of: s1, removing interference of a moving target based on a Gaussian mixture model method to obtain a static background image of an input video; s2, extracting a double-yellow-line target Sift characteristic by adopting a word bag BOW model, and performing supervised learning by using a multi-core support vector machine classifier to obtain an interested region only comprising the double-yellow-line target and the surrounding pavement; s3, segmenting a double-yellow-line target in the image of the region of interest based on a color texture feature clustering method, and eliminating the influence of abrasion type double-yellow lines through morphological processing; s4, performing positioning reconstruction on the truncated part of the double yellow lines by adopting a curve fitting method based on a least square method. The system of the invention corresponds to the method. The method can effectively reduce the interference of the surrounding environment, is more sensitive to the residual target information under the pollution conditions of abrasion, cutoff and the like, has high detection precision and strong robustness.","['G06V20/54', 'G06F18/23213', 'G06F18/2411', 'G06T7/11', 'G06T7/136', 'G06T7/194', 'G06T7/90', 'G06V10/44', 'G06V10/56', 'G06V20/40', 'G06T2207/10016', 'G06T2207/30256', 'Y02T10/40']"
US20190286229A1,"Eye tracking method, electronic device, and non-transitory computer readable storage medium","An eye tracking method includes: constructing, by a processing circuit, an eye model; analyzing, by the processing circuit, a first head center position, according to a plurality of first pupil shape information and the eye model, wherein the plurality of first pupil shape information correspond to a plurality of first gazing vectors; capturing, by a camera circuit, a first image of the eye; analyzing, by the processing circuit, a determined gazing vector, according to the eye model and the first image; and adjusting, by the processing circuit, the first head center position according to an actual pupil shape information group and a plurality of simulated pupil shape information groups.","['G02B27/0093', 'G02B27/0172', 'G06F3/013', 'G06K9/00604', 'G06V40/19', 'G06V40/193', 'G02B2027/0138', 'G02B2027/014']"
US11842514B1,Determining a pose of an object from rgb-d images,"A system and method for detecting a pose of an object is described. An augmented reality display device accesses first sensor data from an image sensor and a depth sensor of the augmented reality display device. The first sensor data includes a first plurality of images of an object and corresponding depth data relative to the augmented reality display device and the object. The augmented reality display device detects first features corresponding to the object by applying a convolutional neural network to the first sensor data, forms a plurality of training clusters based on the first features, and stores the plurality of training clusters in a training database.","['G06T7/73', 'G06T7/74', 'G06T19/006', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30244']"
US5566246A,System and method for ranking and extracting salient contours for target recognition,"A system and method for obtaining salient contours from two-dimensional images acquired by a sensor is disclosed which process the two-dimensional images with an edge detector to produce edgels from each of the images, link the edgels into lists known as contours, compute a saliency value for each of the contours, rank the contours in decreasing order of saliency, and select certain ones of the ranked contours based on the requirements of a particular vision application.","['G06T7/12', 'G06T7/181', 'G06V10/255', 'G06V10/44', 'G06V10/443', 'G06V10/462', 'G06T2207/10016', 'G06T2207/10032', 'G06T2207/10048', 'G06T2207/20016', 'G06T2207/30212']"
CN110992356B,Target object detection method and device and computer equipment,"The application relates to a target object detection method, a target object detection device and computer equipment, wherein the method comprises the following steps: performing plane fitting on the surface three-dimensional point cloud of the box-shaped target object under the overlooking angle to obtain a fitting plane; projecting each point in the three-dimensional point cloud into a corresponding fitting plane; performing rectangle detection in the fitting plane to obtain a first rectangle; the first rectangle is determined by the point to which the three-dimensional point cloud is projected; acquiring a two-dimensional image of the target object under the overlooking angle; correcting the first rectangle in the fitting plane according to the outline of the target object in the two-dimensional image; and determining the position of the target object in the three-dimensional space according to the corrected second rectangle. The method improves the positioning precision of the target object.","['G06T7/0002', 'G06T5/80', 'G06T7/11', 'G06T7/13', 'G06T7/181', 'G06T7/66', 'G06T2207/10028']"
CN110570393B,A method for detecting defects in the window area of mobile phone glass cover based on machine vision,"The invention discloses a mobile phone glass cover plate window area defect detection method based on machine vision, which comprises the following steps: step one, acquiring a mobile phone screen image; step two, performing rough detection on a mobile phone screen area; step three, extracting defects of the mobile phone screen image through a threshold segmentation algorithm; step four, connecting scattered points of the dense point cluster area by using a clustering algorithm; step five, classifying defects by using a neural network classifier; step six, extracting the area, the length and the radius of the defect area, and comparing according to the detection standard; step seven, reclassifying the defects by using a deep learning classifier; and step eight, counting various defect information and quantity. The detection algorithm of the invention follows the principle of coarse detection and then fine detection, can rapidly and accurately extract the defects of pits, scratches, dirt, broken filaments and the like in the screen area of the glass cover plate of the mobile phone with different types, and can adjust the detection precision according to the detection standards of different product window areas.","['G01N21/8851', 'G01N21/94', 'G06F18/241', 'G06T7/0004', 'G06T7/136', 'G06T7/187', 'G06V10/751', 'G01N2021/8854', 'G01N2021/8887', 'G06T2207/20032', 'G06T2207/20081', 'G06T2207/20084', 'Y02P90/30']"
CN118212403A,Small target detection method based on spatial context feature pyramid,"The invention provides a small target detection method based on a spatial context feature pyramid, which comprises the following steps: adding an aggregation route on the space pyramid to construct a space context pyramid block module; constructing a dynamic perception up-sampling module; merging the spatial context pyramid block modules after the backbone network of the base network Yolov s and after the feature map of the 3 outputs of the neck network; adding a dynamic perception up-sampling module after the first two convolution layers of a neck network of a base network Yolov s to generate three-dimension feature graphs, and inputting the feature graphs into a detection head network to obtain a small target detection model; training the small target detection model by adopting EIoU loss function to obtain a trained small target detection model; and inputting the image to be detected into a trained small target detection model to obtain a small target detection result. According to the invention, the context characteristic information of the small target object is fully extracted, the space information and the semantic information are fully aggregated in a larger receptive field, the small target object and the background information are more comprehensively distinguished, and the detection precision of small target detection is improved.","['G06V10/25', 'G06N3/0464', 'G06N3/047', 'G06N3/08', 'G06V10/40', 'G06V10/454', 'G06V10/806', 'G06V2201/07']"
US9846946B2,Objection recognition in a 3D scene,A method comprising: obtaining a three-dimensional (3D) point cloud about at least one object of interest; detecting ground and/or building objects from 3D point cloud data using an unsupervised segmentation method; removing the ground and/or building objects from the 3D point cloud data; and detecting one or more vertical objects from the remaining 3D point cloud data using a supervised segmentation method.,"['G06T7/0091', 'G06T19/00', 'G06K9/00201', 'G06T17/00', 'G06T7/10', 'G06T7/11', 'G06T7/136', 'G06T7/155', 'G06T7/50', 'G06T7/521', 'G06T7/75', 'G06V20/00', 'G06V20/56', 'G06V20/64', 'G06T2207/10028', 'G06T2207/20021', 'G06T2207/20036', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20112']"
WO2020248371A1,"Road damage detection method and apparatus, computer device, and storage medium","The present application discloses a road damage detection method and apparatus, a computer device and a storage medium. The present invention acquires an image to be detected, and determines whether the resolution of said image is greater than a resolution threshold; if the resolution of said image is greater than the resolution threshold, segments said image to obtain N image blocks to be detected; then inputs each of said image blocks into a pre-set road damage detection model for detection, so as to obtain detection information concerning each of said image blocks; and finally combines said detection information concerning each of said image blocks to obtain road damage information concerning said image, not only solving the problem that a road damage detection result is inaccurate, but also further improving the efficiency of road damage detection.","['G06T7/0008', 'G06T7/11', 'G06T7/136', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30132']"
CN113033604B,"A vehicle detection method, system and storage medium based on SF-YOLOv4 network model","The invention discloses a vehicle detection method, a vehicle detection system and a storage medium based on an SF-YOLOv4 network model, wherein the method comprises the steps of training the constructed SF-YOLOv4 network model by using a pre-constructed sample set, and further acquiring the trained SF-YOLOv4 network model; the constructed SF-YOLOv4 network model comprises a feature extraction network Darknet-17 and F-FPN structure, a pre-constructed sample set comprises a group of road video images marked by vehicle targets, the target road video images are input into the trained SF-YOLOv4 network model to obtain the vehicle targets in the images, the system comprises a module for executing the steps of each method, and a storage medium stores a computer program for executing the method; the method is used for quickly and accurately detecting the vehicle target in the video image.","['G06F18/2148', 'G06F18/2431', 'G06V10/40', 'G06V20/584']"
US9330307B2,Learning based estimation of hand and finger pose,"A method for processing data includes receiving a depth map of a scene containing a human hand, the depth map consisting of a matrix of pixels having respective pixel depth values. The method continues by extracting from the depth map information based on the depth values in a plurality of positions distributed over the human hand and processing the information in order to estimate respective candidate positions of the finger joints. The pose of the human hand is estimated by choosing a combination of the positions of the finger joints, responsively to anatomical constraints of the hand, that gives a hand configuration that is most anatomically probable among the candidate positions.","['G06K9/00382', 'G06V20/64', 'G06K9/00201', 'G06K9/4671', 'G06V10/462', 'G06V40/11']"
US10818092B2,Robust optical disambiguation and tracking of two or more hand-held controllers with passive optical and inertial tracking,"Methods for disambiguation and tracking of two or more wireless hand-held controllers with passive optical and inertial tracking within a system having a head mounted virtual or augmented reality display device having a forward facing optical sensor having a field of view, and wherein the display device interfaces with wireless hand-held inertial controllers for providing user input to the display device, with each controller two passive optically reflective markers, one marker being position at or adjacent each end of the controller and being separated by a known distance, and each controller also including an onboard inertial measurement unit for providing inertial data corresponding to its orientation.","['G06T19/006', 'G06F3/011', 'G02B27/017', 'G02B27/0172', 'G06F3/012', 'G06F3/013', 'G06F3/0325', 'G06F3/0346', 'G02B2027/014', 'G02B2027/0178']"
US20240169566A1,Systems and methods for real-time multiple modality image alignment,"A method for aligning multiple depth cameras in an environment based on image data can include accessing, by one or more processors, a plurality of first point cloud data points corresponding to a first pose relative to a subject and a plurality of second point cloud data points corresponding to a second pose relative to the subject. The method can include determining, by the one or more processors, a frame of reference for image data based on at least one of the first pose or the second pose. The method can include transforming, by the one or more processors, at least one of the plurality of first point cloud data points or the plurality of second point cloud data points to align with the frame of reference.","['G16H30/40', 'G06T7/33', 'G06T1/20', 'G06T3/14', 'G06T3/40', 'G06T7/11', 'G06T7/251', 'G06T7/344', 'G06T7/521', 'G06T7/70', 'G06T2207/10028', 'G06T2207/10072', 'G06T2207/10088', 'G06T2207/20072', 'G06T2207/20221']"
US7336827B2,"System, process and software arrangement for recognizing handwritten characters","A process and system are provided for determining a most likely combination of characters. In particular, character data which includes information indicative of at least one handwritten character is obtained. The character data includes at least one set of segmentation points for the handwritten character. Then, a score can be provided for each particular character of a set of previously stored characters based on a comparison between the character data and the previously stored particular character. In addition, it is possible to compare visual aspects of the handwritten character to visual aspects of each of the previously stored characters for determining likely characters. Also, a Fisher Matching procedure can be used on the character data to ascertain the likely characters. A plurality of handwritten characters can include a first character that is connected to a second character of the handwritten characters via a ligature. A further score of the ligature can be determined based on a starting point of the ligature and an ending point of the ligature. Furthermore, the first character can be connected to the second character via a transition. A particular score of the transition can be ascertained based on a difference in length of the first character and the second character. This particular score can be combined with a score corresponding to the particular handwritten characters.","['G06V30/2272', 'G06V30/1423']"
CN117934489A,Fundus hard exudate segmentation method based on residual error and pyramid segmentation attention,"The invention discloses a fundus hard exudate segmentation method based on residual error and pyramid segmentation attention, which adopts ResNet for pre-training as a trunk feature extraction part of a UNet network to enhance the extraction capability of fundus image features; the PSA module is added in the bottleneck layer of the UNet network, so that a multiscale receptive field can be obtained, and multiscale space information with finer granularity can be effectively extracted; an improved residual error attention mechanism module is added between jump connections of the UNet network, so that interference of shallow redundant information can be reduced, and degradation phenomenon possibly occurring in the network is avoided; in addition, in order to solve the problem of imbalance of positive and negative samples, a joint Loss function of weighted cross entropy Loss and Dice Loss is designed. The method has important significance for improving the accuracy and the robustness of fundus image analysis, provides a more reliable and efficient auxiliary tool for early diagnosis and treatment of fundus diseases, and is expected to have important influence on diagnosis and prevention of diabetic retinopathy.","['G06T7/10', 'G06N3/0455', 'G06N3/0464', 'G06N3/084', 'G06T7/0012', 'G06V10/40', 'G06V10/774', 'G06V10/806', 'G06V10/82', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30041', 'G06T2207/30096', 'G06V2201/03']"
US11373305B2,"Image processing method and device, computer apparatus, and storage medium","An image processing method is provided, including: obtaining a target image; invoking an image recognition model including: a backbone network, a pooling module and a dilated convolution module that are connected to the backbone network and that are parallel to each other, and a fusion module connected to the pooling module and the dilated convolution module; performing feature extraction on the target image by extracting, using the backbone network, a feature map of the target image, separately processing, using the pooling module and the dilated convolution module, the feature map, to obtain a first result outputted by the pooling module and a second result outputted by the dilated convolution module, and fusing the first result and the second result by using the fusion module into a model recognition result of the target image; and determining a semantic segmentation labeled image of the target image based on the model recognition result.","['G06F18/241', 'G06T7/0012', 'G06F18/214', 'G06F18/254', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T1/20', 'G06T7/174', 'G06V10/764', 'G06V10/806', 'G06V10/809', 'G06V10/82', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024', 'G06T2207/30092', 'G06T2207/30096', 'G06V2201/03']"
US9761054B2,Augmented reality computing with inertial sensors,"Example embodiments of the present disclosure provide techniques for receiving measurements from one or more inertial sensors (i.e. accelerometer and angular rate gyros) attached to a device with a camera or other environment capture capability. In one embodiment, the inertial measurements may be combined with pose estimates obtained from computer vision algorithms executing with real time camera images. Using such inertial measurements, a system may more quickly and efficiently obtain higher accuracy orientation estimates of the device with respect to an object known to be stationary in the environment.","['G06T19/006', 'G06F3/011', 'G06F3/012', 'G06T3/60']"
US10540579B2,Two-dimensional document processing,"Disclosed herein are system, method, and computer program product embodiments for processing a document. In an embodiment, a document processing system may receive a document. The document processing system may perform optical character recognition to obtain character information and positioning information for the characters. The document processing system may generate a down-sampled two-dimensional character grid for the document. The document processing system may apply a convolutional neural network to the character grid to obtain semantic meaning for the document. The convolutional neural network may produce a segmentation mask and bounding boxes to correspond to the document.","['G06K9/726', 'G06V10/82', 'G06F18/24133', 'G06K9/00456', 'G06K9/00463', 'G06N3/04', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06V30/18057', 'G06V30/19173', 'G06V30/274', 'G06V30/412', 'G06V30/413', 'G06V30/414', 'G06K2209/01', 'G06N3/045', 'G06N3/0454', 'G06V30/10']"
US11227405B2,Determining positions and orientations of objects,Methods and apparatus for determining poses of objects acquire plural images of the objects from different points of view. The images may be obtained by plural cameras arranged in a planar array. Each image may be processed to identify features such as contours of objects. The images may be projected onto different depth planes to yield depth plane images. The depth plane images for each depth plane may be compared to identify features lying in the depth plane. A pattern matching algorithm may be performed on the features lying in the depth plane to determine the poses of one or more of the objects. The described apparatus and methods may be applied in bin-picking and other applications.,"['H04N17/002', 'G06F18/24147', 'G06K9/6212', 'G06K9/6276', 'G06T7/13', 'G06T7/33', 'G06T7/44', 'G06T7/55', 'G06T7/73', 'H04N13/128', 'H04N13/243', 'H04N13/254', 'H04N13/271', 'H04N23/90', 'H04N25/615', 'H04N5/2226', 'G03B15/02', 'G03B35/08', 'G06T2207/30164', 'H04N2013/0081']"
US8199985B2,Automatic interpretation of 3-D medicine images of the brain and methods for producing intermediate results,"Methods for fully automatic quantification and interpretation of three dimensional images of the brain or other organs. A system for Computer Aided Diagnosis (CAD) of diseases affecting cerebral cortex from SPECT images of the brain, where said images may represent cerebral blood flow (CBF). The methods include image processing, statistical shape models, a virtual brain atlas, reference databases and machine learning.","['G06T7/0012', 'G06T7/12', 'G06T7/149', 'G06V10/7553', 'G06T2207/10108', 'G06T2207/20124', 'G06T2207/20128', 'G06T2207/30016', 'G06V2201/03']"
US12380544B2,Lane detection method integratedly using image enhancement and deep convolutional neural network,"A lane detection method integratedly using image enhancement and a deep convolutional neural network. On the assumption that lanes have similar widths in a local region of an image and a lane can be segmented into several image blocks, each of which contains lane marking in the center, a method based on a deep convolutional neural network is provided to detect lane marking blocks in the image. Input to the model includes road images captured by a camera as well as a set of enhanced images generated by the contrast limited adaptive histogram equalization (CLAHE) algorithm. The method according to the present disclosure can effectively overcome difficulties of lane detection under complex imaging conditions, such as poor image quality, and small lane marking targets, so as to achieve better robustness.","['G06V20/588', 'G06T7/0002', 'G06N3/045', 'G06N3/08', 'G06T5/40', 'G06T5/90', 'G06T7/90', 'G06V10/20', 'G06V10/454', 'G06V10/753', 'G06V10/82', 'G06T2207/10024', 'G06T2207/20061', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30256']"
US7835568B2,Method and apparatus for image-based photorealistic 3D face modeling,"An apparatus and method for image-based 3D photorealistic head modeling are provided. The method for creating a 3D photorealistic head model includes: detecting frontal and profile features in input frontal and profile images; generating a 3D head model by fitting a 3D genetic model using the detected facial features; generating a realistic texture from the input frontal and profile images; and mapping the texture onto the 3D head model. In the apparatus and method, data obtained using a relatively cheap device, such as a digital camera, can be processed in an automated manner, and satisfactory results can be obtained even from imperfect input data. In other words, facial features can be extracted in an automated manner, and a robust “human-quality” face analysis algorithm is used.","['G06T17/10', 'G06T7/00']"
US5577135A,Handwriting signal processing front-end for handwriting recognizers,"A handwriting signal processing front-end method and apparatus for a handwriting training and recognition system which includes non-uniform segmentation and feature extraction in combination with multiple vector quantization. In a training phase, digitized handwriting samples are partitioned into segments of unequal length. Features are extracted from the segments and are grouped to form feature vectors for each segment. Groups of adjacent from feature vectors are then combined to form input frames. Feature-specific vectors are formed by grouping features of the same type from each of the feature vectors within a frame. Multiple vector quantization is then performed on each feature-specific vector to statistically model the distributions of the vectors for each feature by identifying clusters of the vectors and determining the mean locations of the vectors in the clusters. Each mean location is represented by a codebook symbol and this information is stored in a codebook for each feature. These codebooks are then used to train a recognition system. In the testing phase, where the recognition system is to identify handwriting, digitized test handwriting is first processed as in the training phase to generate feature-specific vectors from input frames. Multiple vector quantization is then performed on each feature-specific vector to represent the feature-specific vector using the codebook symbols that were generated for that feature during training. The resulting series of codebook symbols effects a reduced representation of the sampled handwriting data and is used for subsequent handwriting recognition.","['G06V30/36', 'G06F18/23']"
CN108304873B,Target detection method and system based on high-resolution optical satellite remote sensing image,"The invention relates to a target detection method and a system thereof based on a high-resolution optical satellite remote sensing image, wherein the method comprises the steps of obtaining a marked target positive sample and a marked background negative sample to form a training sample; extracting a plurality of different weak characteristic channels aiming at a training sample, and acquiring a candidate region according to the plurality of different weak characteristic channels; acquiring a context scene of the candidate region, extracting features of the candidate region and the context scene of the candidate region, and fusing the extracted features to form candidate region features; training the training samples to obtain a classifier; classifying the candidate region characteristics by using a classifier to obtain a target region containing a target; and carrying out duplicate removal processing on the target area to obtain a detection target. The invention realizes the target detection on the remote sensing image with the enlarged width, and optimizes the target detection effects of the target with the close distance and the target with the unusual length-width ratio.","['G06F18/214', 'G06F18/2411', 'G06T5/50', 'G06V10/40', 'G06T2207/10044', 'G06T2207/20081']"
US11551357B2,Systems and methods for processing images of slides for digital pathology,"Systems and methods are disclosed for receiving a target electronic image corresponding to a target specimen, the target specimen comprising a tissue sample of a patient, applying a machine learning system to the target electronic image to determine at least one characteristic of the target specimen and/or at least one characteristic of the target electronic image, the machine learning system having been generated by processing a plurality of training images to predict at least one characteristic, the training images comprising images of human tissue and/or images that are algorithmically generated, and outputting the target electronic image identifying an area of interest based on the at least one characteristic of the target specimen and/or the at least one characteristic of the target electronic image.","['G06T7/0012', 'G06N20/00', 'G06T11/001', 'G06T3/40', 'G06V10/25', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024']"
WO2020199593A1,"Image segmentation model training method and apparatus, image segmentation method and apparatus, and device and medium","Disclosed are an image segmentation model training method and apparatus, an image segmentation method and apparatus, and a device and a medium. The image segmentation model training method comprises: performing down-sampling on an eye fundus image, and inputting a down-sampling result into a region proposal network, so as to obtain regions of interest of different scales and classifications thereof; performing multi-scale fusion segmentation on the regions of interest and regions of interest after up-sampling; and adjusting parameters of down-sampling, the region proposal network and up-sampling according to the value of a loss function until the value of the loss function is within a preset error range.","['G06F18/24', 'G06T7/11', 'G06V10/25', 'G06T2207/10101', 'G06T2207/20081', 'G06T2207/30041', 'Y02T10/40']"
CN115699114B,Method and apparatus for image augmentation for analysis,"Systems and techniques for facial image augmentation are provided. An example method may include obtaining a first image of a captured face. Using the first image, the method may determine a UV facial position map comprising a two-dimensional (2D) representation of a three-dimensional (3D) structure of the face using a predictive model. The method may generate a 3D model of the face based on the UV face location map. The method may generate an extended 3D model of the face by extending the 3D model to include region(s) beyond the boundaries of the 3D model. The region(s) may include a forehead region, a region surrounding at least a portion of the face, and/or other regions. The method may generate a second image based on the extended 3D model, the second image depicting the face in a rotated position relative to a position of the face in the first image.","['G06T15/20', 'G06F18/214', 'G06T11/001', 'G06T17/20', 'G06T19/20', 'G06T7/50', 'G06T7/70', 'G06V10/454', 'G06V10/764', 'G06V20/64', 'G06V40/161', 'G06V40/171', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30201', 'G06T2219/2016']"
CN110458830B,"Image processing method, image processing apparatus, server, and storage medium","The invention discloses an image processing method, an image processing device, a server and a storage medium, and belongs to the technical field of computer vision. The method comprises the following steps: generating a foreground point distance image and a background point distance image based on a mammary gland MRI image to be labeled, a foreground point and a background point, wherein the image distance is determined according to the coordinate distance and the gray value distance between a pixel point and the foreground point or the background point; and inputting the breast MRI image to be labeled, the foreground point distance image and the background point distance image into the second segmentation model, and outputting a tumor region. According to the foreground point and the background point, the foreground point distance image and the background point distance image are obtained, and the distance image can be directly determined according to the coordinate distance and the gray value distance between the pixel point and the foreground point or the background point without traversing and exploring the path distances of all possible paths, so that the calculated amount in the image processing process is reduced, the resource consumption is reduced, and the processing time is shortened.","['G06T7/0014', 'G06F18/214', 'G06F18/22', 'G06T7/0012', 'G06T7/11', 'G06T7/194', 'G06V10/26', 'G06V10/761', 'G06V10/774', 'A61B5/055', 'A61B5/4312', 'G06T2200/04', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20101', 'G06T2207/20104', 'G06T2207/30068', 'G06T2207/30096', 'G06V2201/03']"
US8537112B2,Control system for navigating a principal dimension of a data space,"Systems and methods are described for navigating through a data space. The navigating comprises detecting a gesture of a body from gesture data received via a detector. The gesture data is absolute three-space location data of an instantaneous state of the body at a point in time and physical space. The detecting comprises identifying the gesture using the gesture data. The navigating comprises translating the gesture to a gesture signal, and navigating through the data space in response to the gesture signal. The data space is a data-representational space comprising a dataset represented in the physical space.","['G06F3/017', 'G06F3/0325', 'G06V40/107', 'G06V10/245']"
CN111414826B,"Landmark arrow identification method, device and storage medium","The embodiment of the application discloses a method, equipment and a storage medium for identifying a landmark arrow, which are used for realizing the high identification of the type of the landmark arrow based on a preset detection model of machine learning, assisting in determining a lane number by using the detection results of a lane line and a vehicle, improving the detection and identification accuracy of a blocked landmark arrow and providing effective support for a driver to adapt to the landmark arrow changed due to the road condition change. The identification method comprises the following steps: acquiring a road image corresponding to a target vehicle; inputting the road image into a first preset detection model to obtain a vehicle detection result and a landmark arrow detection recognition result, and inputting the road image into a second preset detection model to obtain a lane line detection result; determining the spatial distribution of the relative scene of the landmark arrow based on the detection and identification result of the landmark arrow; and determining the lane number according to the spatial distribution of the relative scene of the landmark arrow, the vehicle detection result and the lane line detection result, so that the target vehicle runs according to the landmark arrow corresponding to the lane number.","['G06V20/588', 'G06V20/582']"
US8798362B2,Clothing search in images,"Methods, systems, and computer readable media with executable instructions, and/or logic are provided for clothing search in images. An example method of clothing search in images can include characterizing clothing within a plurality of reference images using a processor, and characterizing clothing within a query image using a processor. A number of the plurality of reference images having clothing with similar color features as clothing of the query image is identified using a processor. A subset of the identified number of the plurality of reference images having clothing with predefined non-color attributes as clothing of the query image are selected using a processor.","['G06F16/5838', 'G06F18/24323', 'G06V10/764', 'G06V40/103']"
US10198623B2,Three-dimensional facial recognition method and system,"The present disclosure provides a three-dimensional facial recognition method and system. The method includes: performing pose estimation on an input binocular vision image pair by using a three-dimensional facial reference model, to obtain a pose parameter and a virtual image pair of the three-dimensional facial reference model with respect to the binocular vision image pair; reconstructing a facial depth image of the binocular vision image pair by using the virtual image pair as prior information; detecting, according to the pose parameter, a local grid scale-invariant feature descriptor corresponding to an interest point in the facial depth image; and generating a recognition result of the binocular vision image pair according to the detected local grid scale-invariant feature descriptor and training data having attached category annotations. The present disclosure can reduce computational costs and required storage space.","['G06V40/165', 'G06K9/00248', 'G06K9/00281', 'G06K9/00288', 'G06K9/4671', 'G06V10/462', 'G06V40/171', 'G06V40/172', 'G06K9/4676', 'G06V10/464']"
CN111189837B,Cigarette appearance online detection method and device,"The invention relates to cigarette appearance detection, in particular to an online cigarette appearance detection method and a device thereof, which comprises the following steps: s1: respectively collecting three-dimensional information images of the cigarette holder end surface cavity, cigarette holder end surface appearance images and cigarette circumference surface appearance images at corresponding positions of a cigarette making machine; s2: transmitting the acquired image to an image processor for processing as follows: s3: when the image in the step 2) is processed by the image processor, good products or bad products are judged, when the image processor judges the good products or the bad products, the image processor sends a signal to the rejection controller, and the rejection controller acts to reject the bad product cigarettes; the deep learning model used by the invention can accurately classify the defects, and can provide information for manufacturers to remove mechanical faults and improve production schemes.","['G01N21/8851', 'G01N2021/8854', 'G01N2021/8887']"
CN111563909B,Semantic segmentation method for complex street view image,"A semantic segmentation method for complex street view images. The invention provides a new method for performing semantic segmentation on a complex street view image by adopting a global convolutional neural network based on successful application of a deep learning method in the field of computer vision comprising image semantic segmentation, and aims to effectively solve the problems of under segmentation and over segmentation in the process of segmentation of the complex street view image and remarkably improve the precision and speed of image segmentation. The method mainly comprises four-stage method flows of information input, an encoder, a decoder and information output. The encoding module mainly comprises a DCNN part and an R-ASPP part, and the decoding module mainly comprises an AT-Decoder part. The DCNN can effectively extract low-level features containing position information, the R-ASPP can furthest extract high-level semantic features containing geometric and texture information, and the AT-Decoder can effectively fuse the low-level detail features and the high-level semantic features.","['G06T7/11', 'G06F18/2415', 'G06F18/253', 'G06T2207/20016', 'Y02T10/40']"
US11182904B2,Systems and methods for image segmentation,"The present disclosure may provide a method for segmenting an image. The method may include obtaining an image and related information. The image may include a tumor region. The method may also include determining a region of interest in the image. The region of interest may include the tumor region. The method may also include performing a first segmentation of the region of interest to obtain a first segmentation result. The first segmentation may include: determining tumor morphology relating to the tumor region; performing a second segmentation of the region of interest to obtain a second segmentation result; and optimizing, based on the tumor morphology, the second segmentation result to obtain the first segmentation result.","['G06T7/11', 'G06K9/4609', 'G06T7/0012', 'G06T7/13', 'G06T7/136', 'G06T7/149', 'G06T7/155', 'G06T7/162', 'G06T7/168', 'G06T7/174', 'G06T7/187', 'G06V10/25', 'G06V10/44', 'G06V10/758', 'G06T2207/10072', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10104', 'G06T2207/10124', 'G06T2207/20016', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20104', 'G06T2207/20156', 'G06T2207/20161', 'G06T2207/30096', 'G06V2201/03']"
US8775424B2,System for creative image navigation and exploration,"A system and method for assisting a user in navigation of an image dataset are disclosed. The method includes receiving a user's text query, retrieving images responsive to the query from an image dataset, providing for receiving the user's selection of a first feature selected from a set of available first features via a graphical user interface, providing for receiving the user's selection of a second feature selected from a set of available second features different from the first features via the graphical user interface, and displaying at least some of the retrieved images on the graphical user interface. The displayed images are arranged, e.g., grouped, according to levels and/or combinations of levels of the user-selected first and second features.","['G06F16/54', 'G06F16/58', 'G06N20/00', 'G06V20/10', 'G06V20/70']"
US11967083B1,Method and apparatus for performing segmentation of an image,"A method and system for segmenting a plurality of images. The method comprises the steps of segmenting the image through a novel clustering technique that is, generating a composite depth map including temporally stable segments of the image as well as segments in subsequent images that have changed. These changes may be determined by determining one or more differences between the temporally stable depth map and segments included in one or more subsequent frames. Thereafter, the portions of the one or more subsequent frames that include segments including changes from their corresponding segments in the temporally stable depth map are processed and are combined with the segments from the temporally stable depth map to compute their associated disparities in one or more subsequent frames. The images may include a pair of stereo images acquired through a stereo camera system at a substantially similar time.","['G06T7/11', 'G06T7/10', 'G06T7/174', 'G06T7/187', 'G06T7/194', 'G06T7/50', 'G06T7/55', 'G06T7/593', 'H04N13/128', 'G06T2207/10012', 'G06T2207/10021', 'G06T2207/20112', 'G06T2207/20228', 'H04N2013/0081', 'H04N2013/0092']"
CA2874993C,Image processing for video matting,"Systems, apparatuses, methods, and computer program products perform image processing in an environment in which depth information and color data of a scene including a player and a background are received from a capture device, and in which an image of the player combined with video data is output.","['A63F13/213', 'A63F13/52', 'A63F13/65', 'G06T7/11', 'G06T7/174', 'G06T7/194', 'H04N13/122', 'H04N5/272', 'A63F2300/1093', 'A63F2300/6692', 'A63F2300/69', 'A63F2300/695', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/30196']"
CN110188792B,Image feature acquisition method of MRI three-dimensional image of prostate,"The application relates to an image characteristic acquisition method of a prostate MRI three-dimensional image. According to the application, the prostate T2WI image is subjected to automatic organ segmentation to obtain a corresponding prostate organ region, the prostate organ region is mapped onto the registered ADC and DWI images based on the segmentation result, a multi-parameter prostate organ region is obtained as the input of a discrimination model, and the image characteristics are obtained by combining a multi-parameter MRI image and a deep learning algorithm. The application establishes an automatic prostate organ segmentation model based on a large amount of prostate image data, reduces the interference of irrelevant background information on a discrimination model, fuses the characteristics of multi-parameter MRI images by using a deep learning method, realizes high-precision feature extraction of prostate cancer, and provides a basis for improving the diagnosis efficiency and accuracy of the prostate cancer.","['G06F18/214', 'G06N3/045', 'G06T7/11', 'G16H30/40', 'Y02T10/40']"
US20200380365A1,"Learning apparatus, method, and program","There is provided a learning apparatus, a method, and a program that can prevent overlearning and improve generalization performance while suppressing deterioration of convergence performance in learning. A learning apparatus includes a learning unit that performs learning of a neural network composed of a plurality of layers and including a plurality of skip connections in which an output from a first layer to a second layer which is a layer next to the first layer is branched to skip the second layer and is connected to an input of a third layer located downstream of the second layer, a connection invalidating unit that invalidates at least one of the skip connections in a case where the learning is performed, and a learning control unit that changes the skip connection to be invalidated by the connection invalidating unit and causes the learning unit to perform the learning.","['G06N3/08', 'G06N20/20', 'G06F17/18', 'G06N3/045', 'G06N3/047', 'G06N3/0472', 'G06N3/084', 'G06T7/00', 'G06N3/048']"
CN104871180B,Text image quality based feedback for OCR,"An electronic device and method capture a plurality of images of a scene of the real world at a number of zoom levels, the scene of the real world containing text of one or more sizes. The electronic device and method then extract one or more text regions from each of the plurality of images, then analyze attributes related to OCR in one or more versions of a first text region as extracted from one or more of the plurality of images. Providing a version of the first text region as input to an OCR when an attribute has a value that meets limitations of optical character recognition, OCR, in the version of the first text region.","['G06V30/1456', 'G06V20/63', 'G06V30/10']"
US10482196B2,Modeling point cloud data using hierarchies of Gaussian mixture models,"A method, computer readable medium, and system are disclosed for generating a Gaussian mixture model hierarchy. The method includes the steps of receiving point cloud data defining a plurality of points; defining a Gaussian Mixture Model (GMM) hierarchy that includes a number of mixels, each mixel encoding parameters for a probabilistic occupancy map; and adjusting the parameters for one or more probabilistic occupancy maps based on the point cloud data utilizing a number of iterations of an Expectation-Maximum (EM) algorithm.","['G06F17/18', 'G06F17/5009', 'G06F18/231', 'G06F18/2415', 'G06F18/24323', 'G06F30/20', 'G06K9/00986', 'G06K9/6219', 'G06K9/6277', 'G06K9/6282', 'G06N5/01', 'G06N7/01', 'G06V10/7625', 'G06V10/764', 'G06V10/955', 'G06F2111/10', 'G06F2217/16']"
US9189687B2,Assisted video surveillance of persons-of-interest,"Methods, systems and media are described for computer-assisted video surveillance. Methods may support detection of moving persons in video frames, extraction of features of the detected moving persons and identification of which detected moving persons are likely matches to a person of interest. Identification of the likely matches may be determined using an attribute-based search, and/or using a specific person-based search. The method may include using likely matches confirmed as images of the person of interest to reconstruct a path of the person of interest.","['G06K9/00771', 'G06V20/58', 'G06K9/00805', 'G06K9/3241', 'G06V10/255', 'G06V20/52', 'H04N7/18']"
US8433133B2,Method and apparatus for detecting objects in an image,"A method of detecting generally rectangular objects in an image comprises determining candidate rectangles in the image based on detected corners in the image, ranking the candidate rectangles on the basis of a set of differentiating factors and detecting objects in the images based on the ranked candidate rectangles.","['G06T7/143', 'G06T7/12', 'G06T7/181', 'G06T2207/10008', 'G06T2207/30176']"
US7149330B2,Rapid computer modeling of faces for animation,"Described herein is a technique for creating a 3D face model using images obtained from an inexpensive camera associated with a general-purpose computer. Two still images of the user are captured, and two video sequences. The user is asked to identify five facial features, which are used to calculate a mask and to perform fitting operations. Based on a comparison of the still images, deformation vectors are applied to a neutral face model to create the 3D model. The video sequences are used to create a texture map. The process of creating the texture map references the previously obtained 3D model to determine poses of the sequential video images.","['G06T13/40', 'G06T15/205', 'G06T17/00', 'G06T17/10', 'G06T17/20', 'G06T7/251', 'G06T7/55', 'G06T7/579', 'G06T7/74', 'G06V20/64', 'G06V40/165', 'G06V40/168', 'G06V40/171', 'H04N19/162', 'H04N19/503', 'G06T2200/08', 'G06T2207/10012', 'G06T2207/10016', 'G06T2207/10021', 'G06T2207/30201', 'G06T9/001']"
WO2022228349A1,Colorectal cancer digital pathological image differentiation method and system based on weakly supervised learning,"Disclosed in the present application are a colorectal cancer digital pathological image differentiation method and system based on weakly supervised learning. The colorectal cancer digital pathological image differentiation system based on weakly supervised learning comprises: a collection module, which is used for collecting a colorectal cancer digital pathological image data set; a pre-processing module, which is used for pre-processing data in the collected data set, so as to obtain pre-processed data; a first classification module, which is used for constructing a sampling block differentiation model on the basis of a weakly supervised learning algorithm, and for inputting the pre-processed data into the constructed sampling block differentiation model for processing, so as to obtain a classification result of all pathological image blocks in a full slice sampling package; and a second classification module, which is used for constructing a decision fusion model, and for inputting the obtained classification result of the pathological image blocks into the decision fusion model for fusion processing, so as to obtain a classification result of a full-digital pathological image.","['G06F18/24', 'G06F18/25', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G16H30/20', 'G16H50/20']"
US7379195B2,Device for the detection of an object on a vehicle seat,"A device for the detection of an object on a vehicle seat, including a camera and a processing unit for the generation of a two-dimensional representation of the object is provided. The device also includes at least two illumination sources, which are positioned in such a manner that the object can be illuminated from different directions, and the processing unit is suitable to generate a three-dimensional representation of the object from two dimensional representations, each generated under different illumination conditions.","['B60R21/01538', 'B60R21/01', 'B60R21/015', 'B60R21/01556', 'G06T7/00', 'G06T7/20', 'G06T7/586', 'G06V40/10', 'G06T2207/30268']"
US11302315B2,Digital video fingerprinting using motion segmentation,"Methods of processing video are presented to generate signatures for motion segmented regions over two or more frames. Two frames are differenced using an adaptive threshold to generate a two-frame difference image. The adaptive threshold is based on a motion histogram analysis which may vary according to motion history data. Also, a count of pixels is determined in image regions of the motion adapted two-frame difference image which identifies when the count is not within a threshold range to modify the motion adaptive threshold. A motion history image is created from the two-frame difference image. The motion history image is segmented to generate one or more motion segmented regions and a descriptor and a signature are generated for a selected motion segmented region.","['G06V30/18086', 'G06F16/00', 'G06F16/45', 'G06F16/48', 'G06F16/783', 'G06F18/22', 'G06K9/00718', 'G06K9/4642', 'G06K9/6215', 'G06T7/215', 'G06T7/248', 'G06T7/254', 'G06V10/50', 'G06V20/41', 'G06V20/46', 'G06V20/49', 'G06V20/635', 'G10L15/02', 'G10L15/063', 'G10L15/10', 'G10L15/142', 'G10L15/20', 'G10L21/0232', 'G10L25/81', 'G06F16/44', 'G06F16/906', 'G06T2207/10016', 'G06T2207/20004', 'G06T2207/20224', 'G06V30/10', 'G10L2015/025', 'G10L2021/02166']"
US11715207B2,Learning-based spine vertebra localization and segmentation in 3D CT,"Described herein is a novel method and system for segmentation of the spine using 3D volumetric data. In embodiments, a method includes an extracting step, localization step, and segmentation step. The extracting step comprises detecting the spine centerline and the spine canal centerline. The localization step comprises localizing the vertebra and intervertebral disc centers. Background and foreground constraints are created for each vertebra digit. Segmentation is performed for each vertebra digit and based on the hard constraints.","['G06T7/11', 'G06T7/143', 'G06T7/162', 'G06T2207/10081', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/30012', 'G06T2207/30172']"
US10165248B2,Optimization method of image depth information and image processing apparatus,"An optimization method of image depth information and an image processing apparatus are provided. A to-be-repaired depth map generated based on a left image and a right image is obtained. A superpixel segmenting process is performed on the left image or the right image to obtain multiple superpixels. A plurality of image segments are obtained by aggregating the superpixels according to pixel information in the superpixels. A hole filling process is performed on holes of the to-be-repaired depth map to obtain a hole-filled depth map. A statistical analysis is performed on first valid depth values of the to-be-repaired depth map and second valid depth values of the hole-filled depth map to obtain a plurality of optimized depth values by using the ranges of the image segments, the ranges of the superpixels, the to-be-repaired depth map, and the hole-filled depth map.","['H04N13/128', 'H04N13/106', 'H04N13/156', 'H04N13/239', 'H04N2013/0081']"
US8659548B2,Enhanced camera-based input,"Enhanced camera-based input, in which a detection region surrounding a user is defined in an image of the user within a scene, and a position of an object (such as a hand) within the detection region is detected. Additionally, a control (such as a key of a virtual keyboard) in a user interface is interacted with based on the detected position of the object.","['G06F3/0482', 'G06F3/011', 'G06F3/017', 'G06F3/0236', 'G06F3/03', 'G06F3/0304', 'G06F3/04842']"
US10775886B2,Reducing rendering computation and power consumption by detecting saccades and blinks,"Gaze tracking data may be analyzed to determine the onset and duration of a vision interrupting event, such as a blink or saccade. Presentation of images to a viewer may then be suspended during the vision interrupting event and resumed in sufficient time to ensure that the viewer sees the image at the time the vision interrupting event has concluded.","['G06F3/013', 'G06T15/00', 'G06T1/20']"
CN111709416B,"License plate positioning method, device, system and storage medium","The invention is applicable to the technical field of machine vision, and provides a license plate positioning method, a license plate positioning device, a license plate positioning system and a storage medium, wherein the license plate positioning method comprises the following steps: the center and the size of a rough prediction frame of a license plate contained in a vehicle in an image to be detected are obtained through a trained license plate positioning network model, the center of the rough prediction frame of the license plate is taken as the center, the region contained in the rough prediction frame of the license plate is expanded to obtain a local region surrounding the license plate, the local region is scaled to a fixed size based on a sharing feature map, four vertexes of the license plate are regressed in the scaled local region, and the outer frame of the license plate is obtained according to the four vertexes, so that the training complexity and the running time of the vehicle positioning model are reduced in a mode of sharing the feature map, the calculated amount in the vehicle detection process is reduced, the detection efficiency is improved, and the accuracy of small-size and multi-directional license plate detection is improved through detecting the license plate in the local range.","['G06V10/25', 'G06F18/214', 'G06F18/2193', 'G06N3/045', 'G06V20/625']"
US9244607B2,System and method for image processing using multi-touch gestures,"Various embodiments of a system and methods for processing digital images using multi-touch gestures are described. A multi-touch gestural input set which comprises a plurality of touch gestures may be applied to a display of an image. The gestural input set may include different gesture types, such as mobile and stationary gestures. Each gesture type may indicate a different image processing constraint that may be applied to modify the digital image. Stationary gestures may indicate constrained regions of the image that are not subject to modification. Mobile gestures may indicate regions of the image which may be subject to modification. Characteristics of the mobile gestures, such as velocity and/or pressure, may also indicate an amount by which an image may be modified over the region indicated by the mobile gesture. Image masks, which separate foreground and background regions of an image, may also be specified by the gestural input set.","['G06F3/04883', 'G06F3/04845', 'G06T11/60', 'G06F2203/04808']"
US11676278B2,Deep learning for dense semantic segmentation in video with automated interactivity and improved temporal coherence,"Techniques related to automatically segmenting video frames into per pixel dense object of interest and background regions are discussed. Such techniques include applying a segmentation convolutional neural network (CNN) to a CNN input including a current video frame, a previous video frame, an object of interest indicator frame, a motion frame, and multiple feature frames each including features compressed from feature layers of an object classification convolutional neural network as applied to the current video frame to generate candidate segmentations and selecting one of the candidate segmentations as a final segmentation of the current video frame.","['G06T7/11', 'G06F18/217', 'G06F18/24', 'G06N3/045', 'G06N3/08', 'G06T7/12', 'G06T7/136', 'G06T7/174', 'G06T7/194', 'G06T7/20', 'G06T7/215', 'G06T7/70', 'G06T7/97', 'G06V10/25', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/41', 'G06V20/42', 'G06V20/46', 'G06V20/49', 'G06T2200/24', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084']"
US10621779B1,Artificial intelligence based generation and analysis of 3D models,Artificial intelligence based techniques are used for analysis of 3D objects in conjunction with each other. A 3D model of two or more 3D objects is generated. Features of 3D objects are matched to develop a correspondence between the 3D objects. Two 3D objects are geometrically mapped and an object is overlayed on another 3D object to obtain a superimposed object. Match analysis of 3D objects is performed based on machine learning based models to determine how well the objects are spatially matched. The analysis of the objects is used in augmented reality applications.,"['G06T17/00', 'G06F18/214', 'G06F18/22', 'G06F18/253', 'G06K9/6256', 'G06K9/629', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N5/00', 'G06T7/50', 'G06T7/62', 'G06V10/761', 'G06V10/7715', 'G06V10/82', 'G06V20/20', 'G06V20/653', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196']"
US10776606B2,Methods for delineating cellular regions and classifying regions of histopathology and microanatomy,"Embodiments disclosed herein provide methods and systems for delineating cell nuclei and classifying regions of histopathology or microanatomy while remaining invariant to batch effects. These systems and methods can include providing a plurality of reference images of histology sections. A first set of basis functions can then be determined from the reference images. Then, the histopathology or microanatomy of the histology sections can be classified by reference to the first set of basis functions, or reference to human engineered features. A second set of basis functions can then be calculated for delineating cell nuclei from the reference images and delineating the nuclear regions of the histology sections based on the second set of basis functions.","['G06K9/00147', 'G06V10/7715', 'G06F18/2136', 'G06F18/28', 'G06K9/6249', 'G06K9/6255', 'G06V10/772', 'G06V20/698']"
CN109753913B,Computationally Efficient Multimodal Video Semantic Segmentation Method,"The invention discloses a method for calculating efficient multi-mode video semantic segmentation. The method has three different processing modes for each video frame input. The realization modules are respectively as follows: semantic segmentation module, optical flow module and mixing module. And automatically determining each input video frame to carry out different processing modes through the mode judging module. The method utilizes the position information in the video frames and the optical flow information between frames to combine semantic segmentation with optical flow in space and time. The fine result of the semantic segmentation module is reserved, and the operation speed is greatly improved due to the combination of the optical flow. Compared with the widely applied deeplab, the method has the running speed of 2fps, and the method realizes the rapid semantic segmentation with the running speed of 12fps on the cityscapes data set. Compared with the existing method, the method better obtains the compromise between precision and processing speed.",[]
US6049619A,Method and apparatus for detecting moving objects in two- and three-dimensional scenes,A method and apparatus for detecting moving objects in both two-dimensional and three-dimensional scenes. The method repetitively applies a two-dimensional transformation to a plurality of images representing a scene to identify misaligned regions within the images. Any residual motion represented by the misaligned regions that may be classified as a moving object within the scene is further processed by a three-dimensional technique that removes parallax motion from the residual motion. The result is motion contained in an epipolar flow field which is only due to a moving object within the scene.,"['G06T7/20', 'G06T7/285', 'G06V10/147', 'G06V10/24', 'G06T2207/10021', 'H04N13/221', 'H04N2013/0085']"
US9098773B2,System and method of detecting objects in scene point cloud,"A system and method of detecting one or more objects in a three-dimensional point cloud scene are provided. The method includes receiving a three-dimensional point cloud scene, the three-dimensional point cloud scene comprising a plurality of points; classifying at least a portion of the plurality of points in the three-dimensional point cloud into two or more categories by applying a classifying-oriented three-dimensional local descriptor and learning-based classifier; extracting from the three-dimensional point cloud scene one or more clusters of points utilizing the two or more categories by applying at least one of segmenting and clustering; and matching the extracted clusters with objects within a library by applying a matching-oriented three-dimensional local descriptor.","['G06T7/11', 'G06K9/6212', 'G06F18/214', 'G06F18/2411', 'G06F18/2431', 'G06K9/6256', 'G06K9/628', 'G06T7/0097', 'G06T7/155', 'G06V10/50', 'G06V10/758', 'G06V10/764', 'G06V20/653', 'G06T2207/10028', 'G06T2207/20081']"
US10223788B2,Skin lesion segmentation using deep convolution networks guided by local unsupervised learning,"A dermoscopic lesion area is identified by: Obtaining a dermoscopic image and running a convolutional neural network image classifier on the dermoscopic image to obtain pixelwise lesion prediction scores. Segmenting the dermoscopic image into super-pixels, and computing for each super-pixel an average of the pixelwise prediction scores for pixels within that super-pixel. Computing a mean prediction score across the plurality of super-pixels. Assigning a confidence indicator of “1” to each super-pixel with a prediction score equal or greater than the mean prediction score, and a confidence indicator of “0” to each super-pixel with a prediction score less than the mean prediction score. Constructing a super-pixel graph G=(V,E,W) wherein","['G06T7/0012', 'G06F18/2411', 'G06K9/4628', 'G06K9/6269', 'G06N3/0464', 'G06N3/09', 'G06T7/11', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06N3/045', 'G06N3/0454', 'G06T2207/20084', 'G06T2207/30088', 'G06T2207/30096']"
US20210374466A1,Water level monitoring method based on cluster partition and scale recognition,"Disclosed is a water level monitoring method based on cluster partition and scale recognition. The water level monitoring method comprises the following steps: 1) obtaining an original image at time t from a real-time monitoring video; 2) intercepting a water gauge area in the original image, and marking an end of the water gauge as a position of the water level; 3) binarizing an image of the water gauge area, and dividing the image of water gauge area processed by a cluster method into several subsections according to three sides of symbol “E”; 4) recognizing a content of each subsections, and obtaining a numerical value in a last subsection containing numbers prior to an area where the water level is located; and 5) calculating and displaying the water level according to the height of the subsections and the numerical value obtained in step 4).","['G06T7/11', 'G06K9/6223', 'G06F17/11', 'G06F18/214', 'G06F18/23213', 'G06F18/24147', 'G06K9/6256', 'G06K9/6276', 'G06T7/136', 'G06T7/194', 'G06V10/26', 'G06V10/28', 'G06V10/763', 'G06V10/764', 'G06V10/774', 'G06T2207/30181']"
US20230290120A1,"Image classification method and apparatus, computer device, and storage medium","Disclosed are an image classification method performed by a computer device. The method includes: acquiring an image feature of a pathological image; extracting, for each scale in multiple scales, a local feature corresponding to the scale from the image feature; splicing the local features respectively corresponding to the scales to obtain a spliced image feature; and classifying the spliced image feature to obtain a category to which the pathological image belongs. According to the method provided in the embodiments of this application, the local features corresponding to different scales contain different information, so that the finally obtained spliced image feature contains feature information corresponding to different scales, and the feature information of the spliced image feature is enriched. The category to which the pathological image belongs is determined based on the spliced image feature, so that the accuracy of the category is ensured.","['G06V10/451', 'G06V10/764', 'G06N3/04', 'G06N3/08', 'G06V10/26', 'G06V10/44', 'G06V10/82', 'G06V2201/03']"
CN110852285B,"Object detection method and device, computer equipment and storage medium","The application relates to an object detection method, an object detection device, a computer device and a storage medium, which relate to an artificial intelligence image identification technology, and the method comprises the following steps: acquiring a target image set, wherein the target image set comprises one or more target images; acquiring a trained first object detection model; inputting each target image in the target image set into the first object detection model, and outputting a model detection result of each candidate image region corresponding to the target image, wherein the candidate image region is obtained by segmenting the target image, and the candidate image region has a standard detection result; screening the candidate image area according to the difference between the model detection result of the candidate image area and the standard detection result to serve as a target image area; and performing model training on the second object detection model according to the target image area to obtain a trained second object detection model. The method can improve the object detection accuracy.","['G06V40/168', 'G06F18/241', 'G06T3/4038', 'G06V10/44', 'G06V40/172', 'G06T2200/32']"
CN111460921B,Lane line detection method based on multitasking semantic segmentation,"The invention discloses a lane line detection method based on multi-task semantic segmentation, and relates to image processing and computer vision technologies. The lane line detection method comprises the following steps: acquiring a road image; constructing a full convolution decoding network based on feature extraction coding network and multi-task branch information fusion of a convolution neural network; outputting a lane line binary segmentation mask image through the network, and then carrying out point set division on lane line pixels in a lane line binary image through a morphological processing method; and finally, performing polynomial fitting on each pixel point set to obtain a final lane line distinguishing result. By the lane line detection method, the problem that the geometric form of the segmentation result is incomplete due to serious loss of lane line information is solved, and lane lines in various shapes can be identified in a complex scene.","['G06V20/588', 'G06F18/253', 'G06N3/045', 'G06V10/267', 'G06V10/454', 'Y02T10/40']"
US20200302241A1,Techniques for training machine learning,"A system and method are provided for training a machine learning system. In an embodiment, the system generates a three-dimensional model of an environment using a video sequence that includes individual frames taken from a variety of perspectives and environmental conditions. An object in the environment is identified and labeled, in some examples, by an operator, and a three-dimensional model of the object is created. Training data for the machine learning system is created by applying the label to the individual video frames of the video sequence, or by applying a rendering of the three-dimensional model to additional images or video sequences.","['G06N20/20', 'G06K9/6263', 'G06F18/2178', 'G06F18/41', 'G06K9/00718', 'G06K9/6254', 'G06N20/00', 'G06N5/01', 'G06N5/04', 'G06T17/00', 'G06T7/70', 'G06V10/772', 'G06V10/774', 'G06V20/41', 'G06V20/64', 'G06N3/08', 'G06T2200/24', 'G06T2207/10016', 'G06T2207/10048', 'G06T2207/20081']"
CN108717569B,Expansion full-convolution neural network device and construction method thereof,"The invention discloses an expansion full convolution neural network and a construction method thereof. The neural network comprises a convolutional neural network, a feature extraction module and a feature fusion module which are connected in sequence. The construction method comprises the following steps: selecting a convolutional neural network: removing full-connection layers and classification layers for classification in the convolutional neural network, only leaving a convolutional layer and a pooling layer in the middle, and extracting a feature map from the convolutional layer and the pooling layer; a structural feature extraction module: the feature extraction module comprises a plurality of expansion up-sampling modules which are connected in series, and each expansion up-sampling module respectively comprises a feature map merging layer, an expansion convolution layer and a deconvolution layer; constructing a feature fusion module: the feature fusion module comprises a dense expansion fusion volume block and an deconvolution layer. The method effectively solves the problems of feature extraction and fusion in the convolutional neural network, and can be applied to the task of labeling the pixel level of the image.","['G06N3/045', 'G06N3/06']"
CN111046732B,Pedestrian re-recognition method based on multi-granularity semantic analysis and storage medium,"The invention discloses a pedestrian re-identification method and a storage medium based on multi-granularity semantic analysis, comprising a training step and a testing step, wherein the training step comprises the following steps: analyzing the pedestrian image in the training sample into pedestrian images of semantic areas with a plurality of granularities by utilizing a human semantic analysis algorithm, wherein at least one granularity contains transition information among different semantic areas; inputting the pedestrian image of each semantic region into a corresponding convolutional neural network, and performing classification training on each semantic region to obtain a classifier of each semantic region; extracting features of the pedestrian images in the corresponding semantic areas by using the classifier, and carrying out feature fusion on the extracted features to obtain pedestrian feature descriptors; the testing step comprises the following steps: and carrying out pedestrian re-recognition on the pedestrian image of the test sample by using the classifier and the pedestrian feature descriptors obtained in the training step. The invention realizes the high alignment of the semantics by the multi-granularity human body semantic analysis mode, fully utilizes the transitional information among the human body semantic areas, and has high recognition accuracy.","['G06V40/10', 'G06F18/253', 'G06N3/045', 'G06N3/08', 'Y02T10/40']"
US8446392B2,"Method for determining the location of a pointer in a pointer input region, and interactive input system executing the method","An interactive input system comprises a touch surface, and imaging system associated with the touch surface and processing structure. The imaging system comprises an optical sensor array that spans at least a portion of the area of the touch surface, and the processing structure communicates with the imaging system and analyzes images received by the imaging system to determine whether at least one pointer is near to the touch surface and the location of the at least one pointer in respect of the touch surface based at least in part on the edge sharpness of the at least one pointer.","['G06F3/0416', 'G06F3/0412', 'G06F3/042', 'G06F2203/04101']"
EP3579196A1,"Human clothing transfer method, system and device","There is provided a method and system comprising determining in a first image a person and a first 3D human pose and body shape fitting model, wherein the person has a first pose and a first clothing, determining in a second image a person and a second 3D human pose and body shape fitting model, wherein the person has a second pose and a second clothing, and generating, by use of the first 3D human pose and body shape fitting model and by use of the second 3D human pose and body shape fitting model, an image comprising the person of the first image in the first pose and with the second clothing and/or generating, by use of the first 3D human pose and body shape fitting model and by use of the second 3D human pose and body shape fitting model, an image comprising the person of the second image in the second pose and with the first clothing.","['G06T19/00', 'G06F18/214', 'G06T13/40', 'G06T17/20', 'G06T19/20', 'G06T7/11', 'G06V40/10', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196', 'G06T2210/16', 'G06T2219/2012']"
CN110807836B,"Three-dimensional face model generation method, device, equipment and medium","The application discloses a method, a device, equipment and a medium for generating a three-dimensional face model, and belongs to the technical field of artificial intelligence computer vision. The method comprises the following steps: acquiring an input three-dimensional face grid of a target object and a standard face model corresponding to a standard object; dividing the three-dimensional face grid and the standard face model into at least two face subregions according to a corresponding relation; respectively fitting each face sub-region in the standard face model to a corresponding face sub-region in the three-dimensional face grid; and after the at least two face sub-regions are fitted, carrying out fusion processing on the adjacent face sub-regions to obtain a three-dimensional face model corresponding to the target object. By fitting each divided face sub-region, the quality of the output three-dimensional face model is improved, the phenomenon that the three-dimensional face model generates distortion is avoided, and the three-dimensional expression base derived based on the three-dimensional face model is enabled to better accord with the expression generated by the target object.",['G06T19/20']
US8917910B2,Image segmentation based on approximation of segmentation similarity,"A system and a method for image segmentation use segmentation maps of one or more similar images as a basis for the segmentation. The method includes generating an image signature for an input image to be segmented and identifying at least one similar image from a set of images, based on the image signature of the input image and image signatures of images in the set of images. The similarity may be computed after first projecting the image signatures into a feature space where similarity is more likely to agree with segmentation map similarity. The input image is segmented, based on the segmentation map of one or more of the at least one identified similar images.","['G06T7/11', 'G06T7/143', 'G06T7/162', 'G06V10/7715', 'G06V20/63', 'G06V30/148', 'G06V30/19127', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20104', 'G06V20/625', 'G06V30/10']"
CN110097568B,A Video Object Detection and Segmentation Method Based on Spatiotemporal Dual Branch Network,"The invention discloses a video object detection and segmentation method based on a space-time dual-branch network, which comprises the following steps: taking video as data input, dividing the video into a plurality of continuous RGB images through video sequence preprocessing, inputting the RGB images into a space branch network, and fine-tuning relatively few pixel marking foreground masks to generate object segmentation image information; then, a target detector trained by the space-time combined network is entered to carry out boundary overlapping degree scoring, all candidate boundary boxes with overlapping degrees larger than a threshold value are input into a target classifier to detect the category of the target, and the scoring of the target category is output; and the boundary of the object is finely corrected by the target filter to be segmented, and finally, the coordinate information of the object in the image and the corresponding target class are output, so that the detection and segmentation of the video object in the complex scene are realized. The method can be applied to actual scenes with a plurality of interference targets and extremely complex interference targets, and the accuracy of target object detection and segmentation under the complex scenes is improved.","['G06F18/2411', 'G06N3/045', 'G06N3/084', 'G06T7/181', 'G06T2207/10016', 'Y02T10/40']"
US8284207B2,Method of generating digital images of objects in 3D scenes while eliminating object overdrawing within the multiple graphics processing pipeline (GPPLS) of a parallel graphics processing system generating partial color-based complementary-type images along the viewing direction using black pixel rendering and subsequent recompositing operations,"A multi-pass method of generating an image frame of a 3D scene while eliminating the overdrawing of objects within the multiple graphics processing pipelines (GPPLs) supported on a parallel graphics processing system The GPPLs include a primary GPPL, and each GPPL, includes a color frame buffer and Z depth buffer. The GPPLs support an object-division based parallel graphics rendering process, in which the 3D scene is decomposed into objects that are assigned to particular GPPLs for processing. The multi-pass method involves, during a first pass, locally a Global Depth Map (GDM) which is provided to the Z depth buffer of each GPPL. This step involves the transmission of graphics commands and data for all objects in the image frame, to all GPPLs to be rendered. Then, during subsequent passes, a complementary-type partial image consisting of visible pixels only is generated within the color buffer of each GPPL using the GDM and a Z test filter supported by the Z depth buffer. After subsequent passes are performed, a complete color image is recomposited within the primary GPPL, using the complementary-type partial images stored in the color buffers of the GPPLs, without comparing or recompositing depth values in the Z depth buffers.","['G06T1/20', 'G06F9/5044', 'G06T15/005', 'G06F2209/501', 'G06T2210/52', 'G09G5/14', 'G09G5/363', 'G09G5/397']"
US8792732B1,Automatic large scale video object recognition,"An object recognition system performs a number of rounds of dimensionality reduction and consistency learning on visual content items such as videos and still images, resulting in a set of feature vectors that accurately predict the presence of a visual object represented by a given object name within an visual content item. The feature vectors are stored in association with the object name which they represent and with an indication of the number of rounds of dimensionality reduction and consistency learning that produced them. The feature vectors and the indication can be used for various purposes, such as quickly determining a visual content item containing a visual representation of a given object name.","['G06V10/761', 'G06F18/22']"
CN111858843B,Text classification method and device,"The application provides a text classification method and a text classification device, wherein the method comprises the following steps: determining the probability value of the text to be classified belonging to the category corresponding to each classification label; if the determined probability values are smaller than the preset values, selecting a preset number of classification labels as candidate classification labels according to the probability values corresponding to each classification label; for each candidate classification label, determining the similarity between the text to be classified and the candidate classification label based on the sentence vector corresponding to each sentence in the text to be classified and the sentence vector corresponding to the candidate classification label; and selecting a candidate classification label with the maximum similarity with the text to be classified as a target classification label of the text to be classified. According to the technical scheme, the text is classified by using the sentence vectors in the text, rather than classifying the text by using the word vectors of the words in the text, the semantics of the text can be completely determined, and the accuracy of text classification is effectively improved.","['G06F16/3344', 'G06F16/35', 'Y02D10/00']"
CN108960036B,"Three-dimensional human body posture prediction method, device, medium and equipment","The embodiment of the application discloses a three-dimensional human body posture prediction method, a neural network training method, a device, electronic equipment, a computer readable storage medium and a computer program, wherein the three-dimensional human body posture prediction method mainly comprises the following steps: acquiring human body key point information in an image to be processed and the depth front-back relation of the human body key points; providing the human body key point information and the depth context of the human body key points to a first neural network, and executing three-dimensional human body posture prediction processing through the first neural network to obtain a three-dimensional human body posture prediction result; and the depth front-back relation of the human body key points is used for representing the depth position relative relation between the human body key points. The technical scheme provided by the application is favorable for improving the accuracy of the three-dimensional human body posture prediction, so that adverse effects on interactive entertainment, behavior analysis and the like due to the fact that the three-dimensional human body posture prediction is wrong are avoided.","['G06V40/103', 'G06N3/045', 'G06N3/08']"
US11791044B2,System for generating medical reports for imaging studies,"A software system for assisting a physician's diagnosis and reporting based on medical imaging includes software tools for pre-processing medical images, collecting findings, and automatically generating medical reports. A pre-processing software component generates an anatomical segmentation and/or computer-aided diagnosis based on an analysis of a medical image. A finding collecting software component displays the image, and facilitates rapid and efficient entry of associated findings by displaying a filtered list of templates associated with a selected region of the image and/or a computer-aided diagnosis. When the physician selects a template from the filtered list, the template may be displayed with entry options pre-filled based, e.g., on any computer-aided diagnosis. After the physician edits and/or confirms the entries, a report generation component uses the entries to generate a medical report.","['G16H50/20', 'G06F40/169', 'G06F40/58', 'G06T7/0012', 'G06T7/11', 'G16H10/60', 'G16H15/00', 'G16H30/00', 'G16H30/20', 'G16H30/40', 'G16H40/67', 'G06F40/174', 'G06F40/186', 'G06T2207/30004']"
CN110866924B,Line structured light center line extraction method and storage medium,"The invention relates to a method for extracting a line structured light center line, which belongs to the technical field of machine vision and comprises the following steps: the method comprises the following steps of carrying out a series of processing on an image collected by a CCD industrial camera, such as clipping, image graying, image enhancement, image denoising, image binarization, morphological opening and closing operation, image light strip area segmentation and the like; thinning processing is carried out by adopting a thinning algorithm to obtain an image containing the central line of the single-pixel light bar; the Steger algorithm is improved. Firstly, determining a region of interest, and performing median filtering on the region; secondly, moving on an image line according to the determined constraint threshold and a 1 multiplied by 5 movable template to find out a rough central point; then solving a Hessian matrix through separability and symmetry of the Gaussian function; and finally, performing Taylor secondary expansion to obtain a sub-pixel level central coordinate. The algorithm has good connectivity, no burr, simple operation, high operation efficiency, high extraction speed and high precision. The invention can meet the real-time requirement of the visual detection system.","['G06T7/11', 'G06N3/045', 'G06T5/70', 'G06T7/136', 'G06T2207/20104']"
US6480615B1,Motion estimation within a sequence of data frames using optical flow with adaptive gradients,"The optical flow of an array of pixels in an image field is determined using adaptive spatial and temporal gradients. Artifacts are avoided for image objects which are moving smoothly relative to the image field background. Data from three image frames are used to determine optical flow. A parameter is defined and determined frame by frame which is used to determine whether to consider the data looking forward from frame k to k+1 or the data looking backward from frame k-1 to frame k in initializing spatial and or temporal gradients for frame k. The parameter identifies signifies the areas of occlusion, so that the gradients looking backward from frame k-1 to frame k can be used for the occluded pixel regions. The gradients looking forward are used in the other areas.","['G06T7/269', 'G06T7/215', 'G06T7/248', 'G06T2207/10016']"
US8781221B2,Hand gesture recognition system,"A cost-effective and computationally efficient hand gesture recognition system for detecting and/or tracking a face region and/or a hand region in a series of images. A skin segmentation model is updated with skin pixel information from the face and iteratively applied to the pixels in the hand region, to more accurately identify the pixels in the hand region given current lighting conditions around the image. Shape features are then extracted from the image, and based on the shape features, a hand gesture is identified in the image. The identified hand gesture may be used to generate a command signal to control the operation of an application or system.","['G06V10/56', 'G06F18/24137', 'G06F3/017', 'G06V10/764', 'G06V40/161', 'G06V40/28']"
US8879796B2,Region refocusing for data-driven object localization,"A system and method are provided for segmenting an image. The method includes computing an image signature for an input image. One or more similar images are identified from a first set of images, based on the image signature of the input image and image signatures of images in the first set of images. The similar image or images are used to define a cropped region of the input image and a second image signature is computed, this time for the cropped region. One or more similar images are identified from a second set of images, based on the cropped image signature and the image signatures of images in the second set of images. The input image is segmented based on a segmentation map of at least one of the similar images identified in the second set of images.","['G06V20/62', 'G06F18/2113', 'G06V10/771', 'G06V10/7715', 'G06V20/625']"
CN110798636B,Subtitle generating method and device and electronic equipment,"The disclosure provides a subtitle generating method and device and electronic equipment; relates to the technical field of artificial intelligence. The subtitle generating method comprises the following steps: extracting keywords from a subtitle text and determining weight information of each keyword; determining acoustic characteristic parameters of the keywords according to audio data corresponding to the subtitle text; determining the highlight effect of the keywords according to the weight information and the acoustic characteristic parameters of the keywords; and generating a target subtitle according to the subtitle text and the highlighting effect of the key words in the subtitle text, and adding the target subtitle to a target image. The information content richness and the information transmission efficiency of the subtitles can be improved.","['H04N5/278', 'G06F16/313', 'G10L15/26', 'H04N21/4312', 'H04N21/4394', 'H04N21/44', 'H04N21/4884', 'H04N5/265', 'H04N5/445']"
US8611728B2,Video matting based on foreground-background constraint propagation,"A method for propagating user-provided foreground-background constraint information for a first video frame to subsequent frames allows extraction of moving foreground objects with minimal user interaction. Video matting is performed wherein constraints derived from user input with respect to a first frame are propagated to subsequent frames using the estimated alpha matte of each frame. The matte of a frame is processed in order to arrive at a rough foreground-background segmentation which is then used for estimating the matte of the next frame. At each frame, the propagated constraints are used by an image matting method for estimating the corresponding matte which is in turn used for propagating the constraints to the next frame, and so on.","['G06T11/006', 'G06T7/254', 'G06T11/00', 'G06T11/60', 'G06T11/80', 'G06T7/174', 'G06T2207/10016', 'G06T2207/20092', 'G06T2207/20096']"
WO2022116677A1,"Target object grasping method and apparatus, storage medium, and electronic device","A target object grasping method, comprising: determining target coordinates of each input point in a target object according to a 3D point cloud corresponding to the target object; generating a seed point according to the target coordinates of each input point by means of a downsampling method; inputting original coordinates of each seed point into a grasping axis endpoint prediction model to obtain grasping axis endpoint coordinates of the target object; and determining a grasping pose of a robot according to the grasping axis endpoint coordinates, and controlling the robot to adjust the pose according to the grasping pose and grasp the target object according to the grasping axis endpoint coordinates. The method improves the accuracy of selecting a target object grasping position. Also provided are a target object grasping apparatus, a storage medium, and an electronic device.","['B25J9/1664', 'B25J9/1602', 'B25J9/1697']"
CN107330376B,Lane line identification method and system,"The invention provides a lane line identification method and a lane line identification system, wherein the lane line identification method comprises the following steps: defining an interested area for the acquired gray level image containing the lane line; performing multi-scale matched filtering by adopting a matched filter; performing top hat transformation on the image interesting region subjected to multi-scale matching filtering; screening lane line characteristic points by setting a gray adaptive threshold and comparing the gray adaptive threshold with the pixel points to be detected; carrying out inverse projection transformation on the screened lane line characteristic points to obtain a two-dimensional grid map; and performing neighborhood clustering processing on the two-dimensional grid map, performing curve fitting on the lane line pixel points, and determining the lane line of the lane where the vehicle is located by combining the current position of the vehicle. The method and the device realize the characteristic enhancement of the lane line area in various environments, greatly weaken the influence of the change of external conditions on the lane line identification based on machine vision, and improve the accuracy of the subsequent lane line identification.","['G06V20/588', 'G06V10/25']"
US10867447B2,Overlaying 3D augmented reality content on real-world objects using image segmentation,"Various embodiments are generally directed to techniques of overlaying a virtual object on a physical object in augmented reality (AR). A computing device may receive one or more images of the physical object, perform analysis on the images (such as image segmentation) to generate a digital outline, and determine a position and a scale of the physical object based at least in part on the digital outline. The computing device may configure (e.g., rotate, scale) a 3D model of the physical object to match the determined position and scale of the physical object. The computing device may place or overlay a 3D virtual object on the physical object in AR based on a predefined location relation between the 3D virtual object and the 3D model of the physical object, and further, generate a composite view of the placement or overlay.","['G06T19/006', 'G06K9/00664', 'G06T17/00', 'G06T7/12', 'G06T7/13', 'G06T7/50', 'G06T7/60', 'G06T7/75', 'G06V10/44', 'G06V10/764', 'G06V10/82', 'G06V20/10', 'G06V20/20', 'G06V20/647', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20092', 'G06T2207/30252']"
US5539841A,Method for comparing image sections to determine similarity therebetween,"A method for comparing two image sections consisting of a plurality of image signals, or pixels, where each image section represents a token (e.g., character, symbol, glyph, string of components, or similar units of semantic understanding), in order to identify when similar tokens are present within the image sections. The invention further operates without the need for individually detecting and/or identifying the components making up the tokens. In one embodiment, the method relies upon the detection of connected components within words to first isolate individual word tokens and then applies a two stage process where dilated images of the tokens are compared with model representations of the tokens to determine the relative similarity therebetween.","['G06V30/19067', 'G06V30/262', 'G06V30/10']"
US9471148B2,Control system for navigating a principal dimension of a data space,"Systems and methods are described for navigating through a data space. The navigating comprises detecting a gesture of a body from gesture data received via a detector. The gesture data is absolute three-space location data of an instantaneous state of the body at a point in time and physical space. The detecting comprises identifying the gesture using the gesture data. The navigating comprises translating the gesture to a gesture signal, and navigating through the data space in response to the gesture signal. The data space is a data-representational space comprising a dataset represented in the physical space.","['G06F3/017', 'G06F3/0325', 'G06K9/00355', 'G06V40/28', 'G06K2009/3225', 'G06V10/245']"
US10956967B2,Generating and providing augmented reality representations of recommended products based on style similarity in relation to real-world surroundings,"The present disclosure relates to systems, methods, and non-transitory computer readable media for generating augmented reality representations of recommended products based on style similarity with real-world surroundings. For example, the disclosed systems can identify a real-world object within a camera feed and can utilize a 2D-3D alignment algorithm to identify a three-dimensional model that matches the real-world object. In addition, the disclosed systems can utilize a style similarity algorithm to generate style similarity scores for products in relation to the identified three-dimensional model. The disclosed systems can also utilize a color compatibility algorithm to generate color compatibility scores for products, and the systems can determine overall scores for products based on a combination of style similarity scores and color compatibility scores. The disclosed systems can further generate AR representations of recommended products based on the overall scores.","['G06Q30/0631', 'G06Q30/0643', 'G06T19/006']"
US7920745B2,Method and apparatus for performing constrained spectral clustering of digital image data,"A method and an apparatus process digital images. The method according to one embodiment accesses element data representing a plurality of elements belonging to a plurality of digital images; performs a similarity analysis between the elements from the plurality of elements to obtain inter-relational data results relating to the elements; and performs clustering of the plurality of elements, the step of performing clustering including incorporating in the inter-relational data results at least one hard constraint relating to elements from the plurality of elements, to obtain constrained inter-relational data results, performing a spectral analysis to obtain eigenvector results from the constrained inter-relational data results, and performing discretization of the eigenvector results using constrained clustering with a criterion to enforce the at least one hard constraint to obtain clusters.","['G06F16/583', 'G06F18/2323', 'G06V10/7635', 'G06V40/10', 'G06V40/172']"
US11144889B2,Automatic assessment of damage and repair costs in vehicles,"A system and method are provided for automatically estimating a repair cost for a vehicle. A method includes: receiving, at a server computing device over an electronic network, one or more images of a damaged vehicle from a client computing device; performing image processing operations on each of the one or more images to detect external damage to a first set of parts of the vehicle; inferring internal damage to a second set of parts of the vehicle based on the detected external damage; and, calculating an estimated repair cost for the vehicle based on the detected external damage and inferred internal damage based on accessing a parts database that includes repair and labor costs for each part in the first and second sets of parts.","['G06Q10/20', 'G06N20/20', 'G06N3/045', 'G06N3/0454', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06N3/096', 'G06N5/003', 'G06N5/01', 'G06N7/005', 'G06N7/01', 'G06Q30/0283', 'G06Q40/08', 'G06T19/20', 'G06T7/0004', 'G06T7/001', 'G06T7/33', 'G06T2207/10024', 'G06T2207/20084', 'G06T2207/30136', 'G06T2207/30252', 'G06T2219/2008']"
US20210078597A1,"Method and apparatus for determining an orientation of a target object, method and apparatus for controlling intelligent driving control, and device","Provided are a method and apparatus for determining an orientation of a target object, a method and apparatus for controlling intelligent driving, an electronic device, a computer-readable storage medium and a computer program. The method for determining an orientation of a target object includes that: a visible surface of a target object in an image is acquired; position information of multiple points in the visible surface in a horizontal plane of a Three-Dimensional (3D) space is acquired; and an orientation of the target object is determined based on the position information.","['B60W60/0016', 'G06T7/11', 'B60W30/14', 'G06N3/04', 'G06N3/045', 'G06T7/50', 'G06T7/73', 'B60W2420/403', 'B60W2420/42', 'G06T2207/10016', 'G06T2207/10028', 'G06T2207/20084', 'G06T2207/30248', 'G06T2207/30252', 'G06T2210/12']"
US7944454B2,System and method for user monitoring interface of 3-D video streams from multiple cameras,"Embodiments of the present invention introduce a user navigation interface that allows a user to monitor/navigate video streams captured from multiple cameras. It integrates video streams from multiple cameras with the semantic layout into a 3-D immersive environment and renders the video streams in multiple displays on a user navigation interface. It conveys the spatial distribution of the cameras as well as their fields of view and allows a user to navigate freely or switch among preset views. This description is not intended to be a complete description of, or limit the scope of, the invention. Other features, aspects, and objects of the invention can be obtained from a review of the specification, the figures, and the claims.",['H04N7/181']
US7689016B2,Automatic detection of critical dermoscopy features for malignant melanoma diagnosis,Improved methods for computer-aided analysis of identifying features of skin lesions from digital images of the lesions are provided. Improved preprocessing of the image that 1) eliminates artifacts that occlude or distort skin lesion features and 2) identifies groups of pixels within the skin lesion that represent features and/or facilitate the quantification of features are provided including improved digital hair removal algorithms. Improved methods for analyzing lesion features are also provided.,"['G06T7/0012', 'G06T5/77', 'G06T7/12', 'G06T7/155', 'G16H30/40', 'G16H50/20', 'G06T2207/10024', 'G06T2207/20152', 'G06T2207/30088', 'G06T2207/30096', 'Y02A90/10']"
US10147185B2,Interactive segmentation,"A method for three-dimensional interactive segmentation, including: receiving a three-dimensional medical image of an interior volume of a patient's body; automatically performing three dimensional segmentation on the three dimensional medical image to detect and define a region of interest, wherein the performing of the three dimensional segmentation comprises automatically determining a boundary defining the region of interest; receiving from a user spatial information indicating one or more regions of disagreement in the three-dimensional medical image with respect to the determined boundary; and updating the three dimensional segmentation of the three dimensional medical image based on the spatial information received from the user, wherein the updating comprises updating the determined boundary based on the spatial information to redefine the area of interest.","['G16H40/63', 'G06T7/0012', 'G06F19/00', 'G06F19/321', 'G06T7/12', 'G16H30/20', 'G16H30/40', 'G16H50/50', 'G16Z99/00', 'A61B2576/00', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10132', 'G06T2207/20101', 'G06T2207/20104']"
US11315253B2,Computer vision system and method,"An image processing method for segmenting an image, the method comprising:","['G06N3/082', 'G06F18/251', 'G06F18/254', 'G06K9/6289', 'G06N20/00', 'G06N3/04', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/0495', 'G06N3/09', 'G06N3/096', 'G06T1/20', 'G06T3/4046', 'G06T5/30', 'G06T7/10', 'G06T7/11', 'G06V10/454', 'G06V10/809', 'G06V10/82', 'G06V20/58', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30252']"
CA2027253C,Editing text in an image,"Character level text editing is performed on an image without recognizing characters, by operating on a character-size array obtained from a two-dimensional array defining an image region. A processor, in response to a request for a text editing operation, accesses an edit data structure that includes the image region array and performs the operation. The character-size array is obtained by dividing the image region array when necessary.
An image region array that includes more than one line is divided along interline spaces. An image region array that includes one line is divided along intercharacter spaces. Character-size arrays are divided out of larger arrays by finding connected component bounding boxes, and then determining from the bounding boxes whether the connected components are likely to form a character. If so, the connected components are used to obtain the character-size array and spatial data about position, size, and shape of the character. Smaller arrays and spatial data can replace a larger array in the edit data structure. Smaller arrays are obtained only as necessary to perform a requested text editing operation, and if the edit data structure is not otherwise modified, obtaining a smaller array does not necessitate redrawing of the display. In addition to character level editing, a text editing operation can be performed on a sequence of arrays, such as a word, line, or a sequence that begins on one line and ends on another. The spatial data can be used to position arrays after insertion or deletion, to advance a cursor through the text, and to justify a line of arrays. A
character-size array can be assigned to a keyboard key, and the key may then be used to insert that array into the text or to request a search for otherarrays matching that array.",['G06T11/60']
US9292928B2,Depth constrained superpixel-based depth map refinement,"A method of forming a refined depth map DR of an image I using a binary depth map DI of the image, said method comprising segmenting (315) the image into a superpixel image SREP, defining (330) a foreground and a background in the superpixel image SREP, to form a superpixel depth map DS, intersecting (450) the respective foreground and the background of the superpixel depth map DS with the binary depth map DI determined independently of the superpixel image SREP, to define a trimap T consisting of a foreground region, a background region and an unknown region, and forming the refined binary depth map DR of the image from the trimap T by reclassifying (355, 365) the pixels in the unknown region as either foreground or background based on a comparison (510) of the pixel values in the unknown region with pixel values in at least one of the other trimap regions.","['G06T7/194', 'G06T7/0081', 'G06T7/11', 'G06T2207/10028', 'G06T2207/20144']"
WO2024077812A1,Single building three-dimensional reconstruction method based on point cloud semantic segmentation and structure fitting,"The present invention provides a single building three-dimensional reconstruction method based on point cloud semantic segmentation and structure fitting. The method mainly comprises: preprocessing building target point cloud data; training a building point cloud semantic segmentation network, and classifying building structure data; performing geometric classification and three-dimensional parametric fitting of a building structure; and constructing a building structure prior information topological relationship and combining single building structures. According to the present invention, when coping with the problem in the field of semantic reconstruction of a single building, the three-dimensional reconstruction, from point cloud data to semantic information, of a building structure primitive model can be implemented, and the segmentation effect, the reconstruction precision, the processing efficiency, etc. are accordingly improved.","['G06F30/13', 'G06N3/08', 'G06T17/10', 'G06T5/00', 'G06V10/762']"
US9129191B2,Semantic object selection,"Techniques are disclosed herein that enable digital images to be segmented based on a user's semantic input. In other words, given an input image of a person walking a dog adjacent to a tree, a user can simply provide the semantic input “dog” and the system will segment the dog from the other elements in the image. If the user provides other semantic input, such as “person” or “tree”, the system will segment the person or the tree, respectively, from the same image. Using semantic input advantageously eliminates any need for a user to directly interact with the input image through a tedious process of painting brush strokes, tracing boundaries, clicking target points, and/or drawing bounding boxes. Thus semantic input represents an easier and more intuitive way for users to interact with an image segmentation interface, thereby enabling novice users to take advantage of advanced image segmentation techniques.","['G06K9/726', 'G06F16/7328', 'G06F16/532', 'G06F16/583', 'G06F16/7837', 'G06F17/30247', 'G06F17/30277', 'G06F17/3079', 'G06F17/30825', 'G06F18/214', 'G06F18/22', 'G06F18/295', 'G06T11/60', 'G06T7/0081', 'G06T7/0087', 'G06T7/11', 'G06T7/143', 'G06T7/194', 'G06V10/761', 'G06V10/774', 'G06V10/85', 'G06V20/35', 'G06T2200/24', 'G06T2207/20076', 'G06T2207/20092']"
US8488863B2,"Combinational pixel-by-pixel and object-level classifying, segmenting, and agglomerating in performing quantitative image analysis that distinguishes between healthy non-cancerous and cancerous cell nuclei and delineates nuclear, cytoplasm, and stromal material objects from stained biological tissue materials","Quantitative object and spatial arrangement-level analysis of tissue are detailed using expert (pathologist) input to guide the classification process. A two-step method is disclosed for imaging tissue, by classifying one or more biological materials, e.g. nuclei, cytoplasm, and stroma, in the tissue into one or more identified classes on a pixel-by-pixel basis, and segmenting the identified classes to agglomerate one or more sets of identified pixels into segmented regions. Typically, the one or more biological materials comprises nuclear material, cytoplasm material, and stromal material. The method further allows a user to markup the image subsequent to the classification to re-classify said materials. The markup is performed via a graphic user interface to edit designated regions in the image.","['G06T7/0012', 'G06F18/2115', 'G06F18/29', 'G06T7/11', 'G06T7/187', 'G06V10/426', 'G06V10/771', 'G06V10/84', 'G06V20/695', 'G06T2207/10056', 'G06T2207/30024', 'G06T2207/30068']"
US9142026B2,"Confidence map, method for generating the same and method for refining a disparity map","A method for generating a confidence map comprising a plurality of confidence values, each being assigned to a respective disparity value in a disparity map assigned to at least two stereo images each having a plurality of pixels, wherein a single confidence value is determined for each disparity value, and wherein for determination of the confidence value at least a first confidence value based on a match quality between a pixel or a group of pixels in the first stereo image and a corresponding pixel or a corresponding group of pixels in the second stereo image and a second confidence value based on a consistency of the corresponding disparity estimates is taken into account.","['G06T7/593', 'G06T7/0075', 'G06T2207/10021', 'G06T2207/20028']"
CN110728200B,Real-time pedestrian detection method and system based on deep learning,"The invention discloses a real-time pedestrian detection method and a real-time pedestrian detection system based on deep learning. The passthrough layer structure in the network performs feature fusion with shallow features by performing up-sampling operation on deep features, and then outputs a deep feature map with smaller resolution and a feature map with higher resolution, wherein the feature map is fused with coarse-granularity features and fine-granularity features. And finally, carrying out regression and prediction on the two feature graphs with different scales, and outputting a bounding box and confidence of each pedestrian detection result. According to the method, in an actual monitoring scene, a real-time pedestrian detection method based on high-definition videos, which meets the requirements of the actual scene, is realized, and the detection efficiency is improved under the condition that the accuracy is ensured.","['G06V40/10', 'G06F18/214', 'G06F18/241', 'G06N3/045', 'G06V10/40', 'Y02T10/40']"
US9207773B1,Two-dimensional method and system enabling three-dimensional user interaction with a device,"User interaction with a display is detected substantially simultaneously using at least two cameras whose intersecting FOVs define a three-dimensional hover zone within which user interactions can be imaged. Separately and collectively image data is analyzed to identify a relatively few user landmarks. A substantially unambiguous correspondence is established between the same landmark on each acquired image, and a three-dimensional reconstruction is made in a common coordinate system. Preferably cameras are modeled to have characteristics of pinhole cameras, enabling rectified epipolar geometric analysis to facilitate more rapid disambiguation among potential landmark points. Consequently processing overhead is substantially reduced, as are latency times. Landmark identification and position information is convertible into a command causing the display to respond appropriately to a user gesture. Advantageously size of the hover zone can far exceed size of the display, making the invention usable with smart phones as well as large size entertainment TVs.","['G06F3/0304', 'G06F3/011', 'G06F3/017', 'G06F3/042', 'G06K9/00214', 'G06V10/462', 'G06V20/653', 'G06V40/113', 'G06F2203/04104', 'G06F2203/04108']"
US8457414B2,Detection of textural defects using a one class support vector machine,"Method for detecting textural defects in an image. The image, which may have an irregular visual texture, may be received. The image may be decomposed into a plurality of subbands. The image may be portioned into a plurality of partitions. A plurality of grey-level co-occurrence matrices (GLCMs) may be determined for each partition. A plurality of second-order statistical attributes may be extracted for each GLCM. A feature vector may be constructed for each partition, where the feature vector includes the second order statistical attributes for each GLCM for the partition. Each partition may be classified based on the feature vector for the respective partition. Classification of the partitions may utilize a one-class support vector machine, and may determine if a defect is present in the image.","['G06T7/0004', 'G06T7/45', 'G06V10/42', 'G06T2207/20016', 'G06T2207/20064']"
US11334763B2,"Image processing methods, training methods, apparatuses, devices, media, and programs","An image processing method includes: inputting a to-be-processed image into a neural network; and forming discrete feature data of the to-be-processed image via the neural network, where the neural network is trained based on guidance information, and during the training process, the neural network is taken as a student neural network; the guidance information includes: a difference between discrete feature data formed by a teacher neural network for an image sample and discrete feature data formed by the student neural network for the image sample.","['G06F18/214', 'G06N3/08', 'G06K9/6257', 'G06F18/2148', 'G06F18/2178', 'G06F18/24', 'G06K9/6263', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/0495', 'G06N3/09', 'G06N3/096', 'G06V10/40', 'G06V10/454', 'G06V10/764', 'G06V10/82']"
US8600141B2,Method and device for the segmentation of a lesion,"In a method and a device for the segmentation of a lesion, a quantity of candidate regions for the lesion is determined proceeding from a starting region by means of a candidate unit. The candidate regions are arranged substantially concentrically to the starting region. A candidate region closest to the lesion is selected automatically by means of a selection unit or manually by means of a correction unit from the quantity of candidate regions. The selected candidate region is a reliable and robust basis for a classification of the lesion.","['G06T7/0012', 'G06T7/11', 'G06T2207/10116', 'G06T2207/30068', 'G06T2207/30196']"
US12026938B2,Neural architecture search method and image processing method and apparatus,"Example neural architecture search methods and image processing methods and apparatuses in the field of computer vision in the field of artificial intelligence are provided. The example neural architecture search method includes determining search space and a plurality of construction units, superimposing the plurality of construction units to obtain a search network, adjusting, in the search space, network architectures of the construction units in the search network, to obtain optimized construction units, and establishing a target neural network based on the optimized construction units. In each construction unit, some channels of an output feature map of each node are processed by using a to-be-selected operation to obtain a processed feature map, and the processed feature map and a remaining feature map are stitched and then input to a next node.","['G06N3/045', 'G06N3/044', 'G06N3/0464', 'G06N3/082', 'G06N3/084', 'G06N3/09', 'G06N3/0985', 'G06N3/10', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06N3/063']"
US7424157B2,Apparatus and method for processing image data,"A computer system and methods for processing image data, such as video data, are described. The invention provides a representation of video data that can be used to assess agreement between the data and a fitting model for a particular parameterization of the data. This allows the comparison of different parameterization techniques and the selection of the optimum one for continued video processing of the particular data. The representation can be utilized in intermediate form as part of a larger process or as a feedback mechanism for processing video data. When utilized in its intermediate form, the invention can be used in processes for storage, enhancement, refinement, feature extraction, compression, coding, and transmission of video data. The invention serves to extract salient information in a robust and efficient manner while addressing the problems typically associated with video data sources.","['H04N19/00', 'G06T7/40', 'G06T7/215', 'G06T7/246', 'G06V10/40', 'G06V20/40', 'H04N19/132', 'H04N19/154', 'H04N19/18', 'H04N19/20', 'H04N19/33', 'H04N19/51', 'H04N19/54', 'H04N19/63', 'H04N19/647', 'G06T2207/30241']"
US6973212B2,Graph cuts for binary segmentation of n-dimensional images from object and background seeds,"Disclosed is a method of segmenting one or more objects from one or more backgrounds in an image, the method comprising defining a plurality of image nodes, each said image node corresponding to one or more pixels of said image, connecting pairs of adjacent nodes with n-links, each said n-link weighted with an n-link cost, defining a source node, defining a sink node, defining one or more object seeds, said object seeds corresponding to image nodes within said objects, defining one or more background seeds, said background seeds corresponding to image nodes within said backgrounds, connecting said source node with each said object seed with a plurality of t-links, connecting said sink node with each said background seed with a plurality of t-links, wherein each said t-links is weighted with a t-link cost, and calculating a segmentation cut having the smallest total cost of all cuts separating said source from said sink, wherein said total cost of each said cut is defined as the sum of the costs of all said n-links and t-links that each said cut severs.","['G06T7/11', 'G06F18/2323', 'G06T7/162', 'G06V10/267', 'G06T2207/10072', 'G06T2207/20101']"
CN113378580B,"Document layout analysis method, model training method, device and equipment","The invention provides a document layout analysis method, a model training method, a device and equipment, relates to the technical field of artificial intelligence, in particular to the technical field of computer vision and deep learning, and can be applied to smart cities and smart financial scenes. The document layout analysis method comprises the following steps: acquiring text semantic features, text image features and text position features of text contents included in a document image to be processed; performing feature fusion on the obtained fusion features to obtain fusion features; and determining text position information and/or text type information corresponding to text content included in the document image to be processed based on the fusion characteristics. By the method, the text semantic features, the text image features and the text position features of the document image to be processed can be utilized to determine the text position information and/or the text type information aiming at the document image to be processed, so that the effect of layout analysis can be improved in a complex layout and a complex background.","['G06F40/30', 'G06F18/2415', 'G06F18/25']"
US9875303B2,System and process for building a catalog using visual objects,"A method including: clustering a plurality of records, each record comprises at least one object image and at least one textual field associated with the object, to yield a plurality of clusters such that the object images in each cluster exhibit between them a visual similarity above a specified value; associating each cluster with a label by applying a dictionary function to the textual fields of each cluster, wherein the label reflects a common semantic factor of the textual fields of each cluster, wherein the common semantic factor has a value above a specified threshold. Accordingly, the visual similarity provides a measure of resemblances between two visual objects that can be based on at least one of: the fit between their color distribution such as correlation between their HSV color histograms, the fit between their texture, the fit between their shapes, the correlation between this edge histograms and face similarity.","['G06F16/355', 'G06F17/3071', 'G06F16/51', 'G06F16/583', 'G06F17/30247', 'G06F17/3028', 'G06Q30/0603']"
CN108351746B,System and method for guiding handwriting input,"A system, method and computer program product for guiding handwriting input to a computing device are provided. The computing device includes a processor and at least one system application for recognizing handwritten input under control of the processor. The at least one system application is configured to cause display of a guide element adjacent to at least a portion of a display of digital ink corresponding to the handwritten input on an interactive display of the computer device. The guide element is configured to guide further handwriting input.","['G06F3/04883', 'G06F3/0236', 'G06F3/0237', 'G06F3/0484', 'G06V30/1423', 'G06V30/32', 'G06V30/387']"
US11210521B2,"Information processing apparatus, image display apparatus, control method for information processing apparatus and image display apparatus, and computer program",An information processing apparatus includes a storing section configured to store scenario information and device information associated with the scenario information. The scenario information includes information configured such that another information processing apparatus executes presentation or reception of predetermined information when the other information processing apparatus determines that a predetermined condition is satisfied. The device information includes information representing specifications of hardware of the other information processing apparatus required when the other information processing apparatus executes at least one of the determination that the predetermined condition is satisfied and the presentation or the reception of the predetermined information.,"['G06V20/20', 'G02B27/017', 'G06K9/00671', 'G06F3/147', 'G06F3/167', 'G06T19/006', 'H04N23/633', 'H04N5/232939', 'G02B2027/0138', 'G02B2027/014', 'G02B2027/0141', 'G09G2340/12', 'H04N23/63']"
US11488294B2,"Method for detecting display screen quality, apparatus, electronic device and storage medium","Provided are a method for detecting display screen quality, an apparatus, an electronic device and a storage medium. The method includes: receiving a quality detection request sent by a console deployed on a display screen production line, the quality detection request including a display screen image collected by an image collecting device on the display screen production line; inputting the display screen image into a defect detection model to obtain a defect detection result, the defect detection model being obtained by training historical defective display screen images using a structure of deep convolutional neural networks and an object detection algorithm; and determining, according to the defect detection result, a defect on a display screen corresponding to the display screen image, a defect category corresponding to the defect, and a position corresponding to the defect.","['G06T7/0002', 'G06T7/0008', 'G01N21/95', 'G06T7/0004', 'G01N21/8851', 'G02F1/1309', 'G06N3/02', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'H10K71/70', 'G01N2021/9513', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30121']"
US8797417B2,"Image restoration method in computer vision system, including method and apparatus for identifying raindrops on a windshield","A vehicle is equipped with a camera (which may be a stereoscopic camera) and a computer for processing the image data acquired by the camera. The image acquired by the camera is processed by the computer, and features are extracted therefrom. The features are further processed by various techniques such as object detection/segmentation and object tracking/classification. The acquired images are sometimes contaminated by optical occlusions such as raindrops, stone-chippings and dirt on the windshield. In such a case, the occluded parts of the image are reconstructed by optical flow estimation or stereo disparity estimation. The fully reconstructed image is then used for intended applications.","['G06T5/77', 'G06T5/005', 'G06T5/50', 'G06T2207/10021', 'G06T2207/30252']"
CN111967262B,Method and device for determining entity tags,"The application discloses a method and a device for determining an entity tag, which relate to the technical field of natural language processing, the technical field of big data processing and the technical field of deep learning, and specifically comprise the following steps: acquiring an entity tag library corresponding to a document type of a target document, wherein the entity tag library comprises a plurality of entity tags corresponding to the document type; matching the target document with the entity tag library to obtain a plurality of candidate entity tags successfully matched; acquiring attribute characteristics of a target document, and acquiring label characteristics corresponding to each candidate entity label according to the target document; inputting the attribute features and the tag features into a pre-trained tag identification model, and acquiring a first confidence coefficient corresponding to each candidate entity tag; and determining the target entity label of the target document from the plurality of candidate entity labels according to the first confidence. Therefore, the determination of the entity tag is realized in a semi-automatic mode, the accuracy and recall rate of the determination of the entity tag are improved, and the labor cost is reduced.","['G06F40/295', 'G06F16/2465', 'G06F16/319', 'G06F16/367', 'G06F40/30', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'Y02D10/00']"
US20230206700A1,Biometric facial recognition and liveness detector using ai computer vision,"A method, system, and computer readable medium are described to capture, detect, and recognize faces using machine learning and a single-stage face detector. A method to determine live faces from spoof 2D and spoof 3D images using a liveness score as well as a method to classify faces using machine learning deep convolutional neural networks is also described.","['G06V40/45', 'G06V10/774', 'G06V10/82', 'G06V40/165', 'G06V40/171', 'G06V40/172', 'G06V40/193', 'G06V40/20']"
US6665423B1,Method and system for object-oriented motion-based video description,"An object-oriented method for describing the content of a video sequence comprises the steps of (a) establishing a temporal object-based segment for an object of interest; (b) describing the temporal object-based segment by describing one or more semantic motions of the object within its temporal object-based segment; and (c) describing the temporal object-based segment by describing one or more semantic interactions of the object with one or more other objects within its temporal object-based segment. The semantic motions of the object may be further described in terms of the properties of elementary coherent motions within the semantic motion. Additionally, the semantic interactions of the object may be further described in terms of the properties of the elementary spatio-temporal relationships among the interacting objects.","['H04N19/25', 'H04N19/543']"
AU2020319589B2,Region proposal networks for automated bounding box detection and text segmentation,"Arbitrary image data may be transformed into data suitable for optical character recognition (OCR) processing. A processor may generate a plurality of intermediate feature layers of an image using convolutional neural network (CNN) processing. For each intermediate feature layer, the processor may generate at least one text proposal using a region proposal network (RPN). The at least one text proposal may comprise a portion of the intermediate feature layer that is predicted to contain text. The processor may merge the text proposals with one another to form a patch of the image that is predicted to contain text. The processor may determine outer coordinates of the patch. The outer coordinates may comprise at least leftmost, rightmost, topmost, and bottommost coordinates. The processor may generate a quadrilateral of the image that is a smallest quadrilateral including the leftmost, rightmost, topmost, and bottommost coordinates.","['G06V10/82', 'G06F18/214', 'G06N3/0442', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06V20/62', 'G06V30/153', 'G06V30/19173', 'G06V30/412', 'G06V30/414', 'G06V30/10']"
US9910497B2,Gestural control of autonomous and semi-autonomous systems,"Systems and methods are described for controlling a remote system. The controlling involves detecting a gesture of a body from gesture data received via a detector. The gesture comprises an instantaneous state of the body. The gesture data is absolute three-space location data of an instantaneous state of the body at a point in time and physical space. The detecting aggregates only the gesture data of the gesture at an instant in time and identifies the gesture using the gesture data. The controlling then translates the gesture to a gesture signal, and controls a component of the remote system in response to the gesture signal.","['G06F3/017', 'G06F3/0325', 'G06K9/00375', 'G06V40/107', 'G06K2009/3225', 'G06V10/245']"
US8836777B2,Automatic detection of vertical gaze using an embedded imaging device,"A method of detecting and applying a vertical gaze direction of a face within a digital image includes analyzing one or both eyes of a face within an acquired image, including determining a degree of coverage of an eye ball by an eye lid within the digital image. Based on the determined degree of coverage of the eye ball by the eye lid, an approximate direction of vertical eye gaze is determined. A further action is selected based on the determined approximate direction of vertical eye gaze.","['G06F3/013', 'G06K9/00335', 'G06K9/00597', 'G06V40/176', 'G06V40/18', 'G06V40/20', 'G09C5/00']"
US9033238B2,Methods and arrangements for sensing identification information from objects,"In one arrangement, retail product packaging is digitally watermarked over most of its extent to allow machine identification by one or more inexpensive cameras at retail checkouts. Such a system also considers image fingerprints, product configuration, barcodes and other available information in identifying products. Imagery captured by conventional or plenoptic cameras is processed to derive several perspective-transformed views, which are provided to the identification system—increasing throughput by minimizing the need to manually reposition items for identification. Crinkles and other deformations in product packaging are optically sensed, allowing the surface to be virtually flattened to aid identification. A marked conveyor belt at the checkout station increases speed and accuracy, and provides other benefits to both shoppers and sellers. A great variety of other features are also detailed.","['G07G1/0036', 'G06K7/10861', 'G06K7/1456', 'G06K7/1443', 'G06K7/1452', 'G07G1/0045']"
US8533204B2,Text-based searching of image data,"A method and system are disclosed for conducting text-based searches of images using a visual signature associated with each image. A measure of string similarity between a query and an annotation associated with each entry in a first database is computed, and based upon the computed string similarity measures, a set of entries from the first database is selected. Each entry of the first database also includes an associated visual signature. At least one entry is then retrieved from a second database based upon a measure of visual similarity between a visual signature of each of the entries in the second database and the visual signatures of the entries in the selected set. Information corresponding to the retrieved entries from the second database is then generated.","['G06F16/532', 'G06F16/58', 'G06F16/5838', 'G06F16/587', 'G06F18/2415', 'G06V20/63', 'G06V30/18152', 'G06V30/19173', 'G06V20/625', 'G06V30/10']"
US7457472B2,Apparatus and method for processing video data,"An apparatus and methods for processing video data are described. The invention provides a representation of video data that can be used to assess agreement between the data and a fitting model for a particular parameterization of the data. This allows the comparison of different parameterization techniques and the selection of the optimum one for continued video processing of the particular data. The representation can be utilized in intermediate form as part of a larger process or as a feedback mechanism for processing video data. When utilized in its intermediate form, the invention can be used in processes for storage, enhancement, refinement, feature extraction, compression, coding, and transmission of video data. The invention serves to extract salient information in a robust and efficient manner while addressing the problems typically associated with video data sources.","['G06T7/251', 'G06V10/28', 'G06V10/446', 'G06V10/755', 'G06T2207/10016', 'G06T2207/30201']"
US10339362B2,Systems and methods for performing fingerprint based user authentication using imagery captured using mobile devices,"Technologies are presented herein in support of a system and method for performing fingerprint recognition. Embodiments of the present invention concern a system and method for capturing a user's biometric features and generating an identifier characterizing the user's biometric features using a mobile device such as a smartphone. The biometric identifier is generated using imagery captured of a plurality of fingers of a user for the purposes of authenticating/identifying the user according to the captured biometrics and determining the user's liveness. The present disclosure also describes additional techniques for preventing erroneous authentication caused by spoofing. In some examples, the anti-spoofing techniques may include capturing one or more images of a user's fingers and analyzing the captured images for indications of liveness.","['G06K9/00093', 'G06F21/32', 'G06K9/00013', 'G06K9/00073', 'G06K9/0008', 'G06K9/00114', 'G06K9/00382', 'G06K9/00892', 'G06K9/42', 'G06K9/66', 'G06V10/32', 'G06V40/11', 'G06V40/13', 'G06V40/1353', 'G06V40/1359', 'G06V40/1371', 'G06V40/1388', 'G06V40/70']"
US10893797B2,User interface for efficiently displaying relevant OCT imaging data,"The present invention is an OCT imaging system user interface for efficiently providing relevant image displays to the user. These displays are used during image acquisition to align patients and verify acquisition image quality. During image analysis, these displays indicate positional relationships between displayed data images, automatically display suspicious analysis, automatically display diagnostic data, simultaneously display similar data from multiple visits, improve access to archived data, and provide other improvements for efficient data presentation of relevant information.","['A61B3/102', 'A61B3/0025', 'A61B3/0058', 'A61B5/0066', 'A61B5/7445', 'A61B5/748', 'G01B9/02091', 'G06F19/321', 'G06T15/08', 'G06T19/00', 'G06T19/003', 'G16H30/20', 'G16H40/63', 'G06T2210/41', 'G06T2219/028']"
US6870945B2,Video object tracking by estimating and subtracting background,"An object is tracked among a plurality of image frames. In an initial frame an operator selects an object. The object is distinguished from the remaining background portion of the image to yield a background and a foreground. A model of the background is used and updated in subsequent frames. A model of the foreground is used and updated in the subsequent frames. Pixels in subsequent frames are classified as belonging to the background or the foreground. In subsequent frames, decisions are made, including: which pixels do not belong to the background; which pixels in the foreground are to be updated; which pixels in the background were observed incorrectly in the current frame; and which background pixels are being observed for the first time. In addition, mask filtering is performed to correct errors, eliminate small islands and maintain spatial and temporal coherency of a foreground mask.","['G06T7/254', 'G06T7/12', 'G06T7/155', 'G06T2207/10016']"
US8170271B2,System and method for test tube and cap identification,"A system for identifying test tube types and properties in a sample handling machine using visual information automatically obtained by an optical imager and then processed using vision processing methods. The system includes an optical imager positioned to capture images containing one or more test tubes in a rack and a microcontroller programmed to extract predetermined regions of interest and interpret the optical information in the image to decipher the dimension of the test tubes, determine the presence or absence of caps on the test tubes, decode any encoded data, and interpret custom symbologies. The system may then determine the nature of the test tubes or other containers presented before the image and provide that information to the sample handling machine to assist with processing of samples.","['G01N35/00732', 'G06T7/0008', 'G06V10/25', 'B01L3/5453', 'G01N2035/00752', 'G06T2207/30108']"
CN112966691B,"Multi-scale text detection method, device and electronic device based on semantic segmentation","The invention relates to the field of deep learning and computer vision, in particular to a multi-scale text detection method and device based on semantic segmentation and electronic equipment; the method comprises the steps of collecting character images and preprocessing the character images; the character images comprise training character images and character images to be detected; inputting the preprocessed character image into a semantically segmented character detection network, and outputting a text boundary area label and a text center area label of the character image; performing binarization fusion on a text boundary region corresponding to the character image and a text center region to obtain a character image after segmentation fusion; carrying out post-processing on the character image after segmentation and fusion to determine a character area, namely the coordinate position of the character; the invention performs supervised learning through the double labels, fully utilizes high-level semantic features and reduces potential semantic feature learning.","['G06V30/153', 'G06F18/253', 'G06N3/045', 'G06N3/08', 'G06V30/10']"
US10470734B2,Characterizing lung nodule risk with quantitative nodule and perinodular radiomics,"Embodiments associated with classifying a region of tissue using features extracted from nodules and surrounding structures. One example apparatus includes a feature extraction circuit configured to automatically extract a first set of quantitative features from a nodule represented in at least one CT image, and automatically extract a second set of quantitative features from the lung parenchyma region immediately surrounding the nodule represented in the at least one CT image; a feature selection circuit configured to select an optimally predictive feature set from the first set of quantitative features and the second set of quantitative features; and a training circuit configured to train a classifier using the optimally predictive feature set to assign malignancy risk to a lung nodule represented in a CT image of a region of tissue demonstrating lung nodules. A prognosis or treatment plan may be provided based on the malignancy risk.","['A61B6/5217', 'A61B10/0041', 'A61B5/055', 'A61B5/08', 'A61B5/4312', 'A61B5/7267', 'A61B6/032', 'G01R33/5601', 'G06F18/24', 'G06K9/00147', 'G06K9/46', 'G06K9/4604', 'G06K9/6267', 'G06T11/003', 'G06T11/008', 'G06T7/0012', 'G06T7/11', 'G06T7/136', 'G06T7/40', 'G06T7/62', 'G06V10/44', 'G06V20/698', 'G16H50/30', 'A61B2034/105', 'A61B6/12', 'G01R33/5608', 'G06F18/2411', 'G06K9/00', 'G06K9/6269', 'G06T2207/10081', 'G06T2207/10096', 'G06T2207/20081', 'G06T2207/30064', 'G06T2207/30068', 'G06T2207/30101', 'G06T2211/404']"
US8884984B2,Fusing virtual content into real content,"A system that includes a head mounted display device and a processing unit connected to the head mounted display device is used to fuse virtual content into real content. In one embodiment, the processing unit is in communication with a hub computing device. The system creates a volumetric model of a space, segments the model into objects, identifies one or more of the objects including a first object, and displays a virtual image over the first object on a display (of the head mounted display) that allows actual direct viewing of at least a portion of the space through the display.","['G02B27/017', 'G06F3/005', 'G06F3/012', 'G06K9/00664', 'G06T19/006', 'G06V20/10', 'G02B2027/0127', 'G02B2027/014']"
CN112257774B,"Target detection method, device, equipment and storage medium based on federal learning","The invention relates to an artificial intelligence technology, and discloses a target detection method based on federal learning, which comprises the following steps: pruning is carried out on the initial target detection model by adopting random weights, so that a lightweight target detection model is obtained; training the lightweight target detection model based on a local data set to obtain a plurality of corresponding model gradient parameters, and sending the model gradient parameters to a server; receiving global gradient parameters obtained by fusing the model gradient parameters by the server according to a federal average algorithm; updating the initial target detection model by using the global gradient parameters, and returning to the training step until a preset termination condition is met to obtain a target detection model; and carrying out target detection on the image to be detected by using the target detection model. In addition, the present invention relates to blockchain techniques in which local data sets may be stored in blockchain nodes. The invention can improve the detection speed and the detection accuracy of target detection.","['G06F18/214', 'G06N3/045', 'G06N3/082', 'G06N3/084', 'G06V2201/07', 'Y02D10/00']"
US8417037B2,Methods and systems for representation and matching of video content,"The described methods and systems provide for the representation and matching of video content, including spatio-temporal matching of different video sequences. A particular method of determining temporal correspondence between different sets of video data inputs the sets of video data and represents the video data as ordered sequences of visual nucleotides. Temporally corresponding subsets of video data are determined by aligning the sequences of visual nucleotides.","['H04N21/4828', 'G06F16/7847', 'G06V20/48', 'H04N21/44008']"
CN111986291A,Automatic composition of content-aware sampling regions for content-aware filling,"Embodiments of the present disclosure relate to automatic composition of content-aware sampling regions for content-aware filling. Embodiments of the present invention provide systems, methods, and computer storage media for automatically synthesizing content-aware sampling regions for hole-filling algorithms, such as content-aware filling. Given a source image and a hole (or other target area to be filled), a sampling region may be synthesized by identifying bands of pixels surrounding the hole, clustering the pixels based on one or more characteristics (e.g., color, x/y coordinates, depth, focus, etc.), passing each resulting cluster as a foreground pixel to a segmentation algorithm, and then merging the resulting pixels to form the sampling region. The sampled regions may be stored in a constraint mask and passed to a hole filling algorithm, such as content aware filling, to synthesize a fill for a hole (or other target region) from patches sampled from the synthesized sampled regions.","['G06T5/77', 'G06T11/40', 'G06F18/23', 'G06T5/70', 'G06T7/194', 'G06T7/70', 'G06T7/90', 'G06T2207/10024', 'G06T2207/20104', 'G06T2207/20156', 'G06T5/20', 'G06T7/11']"
US6608942B1,Method for smoothing jagged edges in digital images,"A method of smoothing jagged edges in graphical data. The method detects one or more edge of a selected pixel in the graphical data, dependent upon intensities of the selected pixel and another pixel surrounding a respective site of one or more edges and predetermied gradients of at least the selected pixel and the respective site surrounding the pixel. An adapted convolution mask is applied to the selected pixel and a predetermined neighborhood of pixels containing the selected pixel. The coefficient values of the convolution mask are dependent upon one or more detected edges.","['G06T5/70', 'G06T5/30', 'G06T7/13', 'G06K2215/0074', 'G06T2200/12', 'G06T2207/20192']"
US12198416B2,Systems and methods for identifying and segmenting objects from images,"Systems and methods for identifying and segmenting objects from images include a preprocessing module configured to adjust a size of a source image; a region-proposal module configured to propose one or more regions of interest in the size-adjusted source image; and a prediction module configured to predict a classification, bounding box coordinates, and mask. Such systems and methods may utilize end-to-end training of the modules using adversarial loss, facilitating the use of a small training set, and can be configured to process historical documents, such as large images comprising text. The preprocessing module within the systems and methods can utilize a conventional image scaler in tandem with a custom image scaler to provide a resized image suitable for GPU processing, and the region-proposal module can utilize a region-proposal network from a single-stage detection model in tandem with a two-stage detection model paradigm to capture substantially all particles in an image.","['G06N3/045', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06N3/094', 'G06T3/4046', 'G06T7/11', 'G06V10/32', 'G06V10/82', 'G06V30/19173', 'G06V30/414', 'G06N3/08', 'G06T2207/20021', 'G06T2207/20024']"
US11783486B2,Depicting humans in text-defined outfits,"Generating images and videos depicting a human subject wearing textually defined attire is described. An image generation system receives a two-dimensional reference image depicting a person and a textual description describing target clothing in which the person is to be depicted as wearing. To maintain a personal identity of the person, the image generation system implements a generative model, trained using both discriminator loss and perceptual quality loss, which is configured to generate images from text. In some implementations, the image generation system is configured to train the generative model to output visually realistic images depicting the human subject in the target clothing. The image generation system is further configured to apply the trained generative model to process individual frames of a reference video depicting a person and output frames depicting the person wearing textually described target clothing.","['G06T7/11', 'G06F18/214', 'G06F18/217', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/08', 'G06N3/09', 'G06N3/094', 'G06T11/00', 'G06T11/60', 'G06T7/20', 'G06T7/70', 'G06V10/774', 'G06V10/776', 'G06V10/82', 'G06V40/10', 'G06V40/103', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30168', 'G06T2207/30196']"
WO2020107716A1,"Target image segmentation method and apparatus, and device","Disclosed are a target image segmentation method and apparatus, and a device. The method comprises: positioning, according to an initial target region setting condition, an initial region of a target in an image to be processed and using a color histogram and a visual salient histogram of the initial target region as target features; selecting target pixel points of a plurality of support vectors from said image by using a pre-built SVM model, sample features of each target pixel point consisting of its own color histogram and visual salient histogram; determining a salient region in said image on the basis of the calculated feature distance between each target pixel point and the initial target region; and calculating the weight of each pixel point in the salient region according to the target features and each sample feature and deleting pixel points having weights that do not meet a preset condition in the salient region to obtain a final region as a target region for image extraction and segmentation. The present application improves the stability and accuracy of target image segmentation.","['G06T7/11', 'G06F18/2411']"
CN107833270B,Real-time object three-dimensional reconstruction method based on depth camera,"A real-time object three-dimensional reconstruction method based on a depth camera comprises the steps of sequentially obtaining a frame of depth image from the depth camera as a current frame and preprocessing the frame of depth image; estimating the relative pose of the current frame and the previous frame by a centroid ICP (inductively coupled plasma) algorithm, and calculating the accurate pose of the current frame camera by using the accurate pose of the previous frame and the relative pose of the current frame and the previous frame; fusing current frame data into a local TSDF (time delay and distortion distribution) by using the accurate pose of the camera; and (3) acquiring the point cloud fused on the local TSDF in the step (3) from the local TSDF, taking the point cloud as the point cloud of the previous frame, or performing matching fusion on the local TSDF and the global TSDF, and initializing the local TSDF. The method has the advantages of avoiding the failure of the ICP matching algorithm, reducing the accumulated error and improving the model precision, and is suitable for reconstructing the specified object or person.","['G06T17/00', 'G06T5/50', 'G06T7/136', 'G06T7/30', 'G06T2207/10028', 'G06T2207/20221']"
TWI773189B,"Method of detecting object based on artificial intelligence, device, equipment and computer-readable storage medium","The present disclosure discloses a method of detecting an object based on artificial intelligence, a device, an equipment and a computer-readable storage medium, which belong to the technical field of image processing. The method includes: using the object detection model to extract feature maps of different scales from the tableau image to determine the image location information of the object and the first confidence that the object belongs to each category; obtaining a destination area where the object is located from the tableau image; obtaining a second confidence that the object belongs to each category by using an object retrieval model to compare the destination area with the sample images of each category; selecting the category with the largest sum of the first confidence and the second confidence in each category as the target category to which the object belongs. According to the present disclosure, the object detection model and the object retrieval model are combined to perform two category predictions. The object retrieval model corrects the prediction results of the object detection model to improve the accuracy of the prediction results.","['G06V10/7715', 'G06N3/08', 'G06F18/2431', 'G06F18/253', 'G06N3/045', 'G06N3/0464', 'G06N3/0495', 'G06N3/09', 'G06T3/40', 'G06T7/73', 'G06V10/464', 'G06V10/761', 'G06V10/82', 'G06N3/063', 'G06T2207/20016', 'G06T2207/20072', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06V2201/07']"
US12360588B2,Virtual models for communications between user devices and external observers,"Systems and methods for interactions between an autonomous vehicle and one or more external observers include virtual models of drivers the autonomous vehicle. The virtual models may be generated by the autonomous vehicle and displayed to one or more external observers, and in some cases using devices worn by the external observers. The virtual models may facilitate interactions between the external observers and the autonomous vehicle using gestures or other visual cues. The virtual models may be encrypted with characteristics of an external observer, such as the external observer's face image, iris, or other representative features. Multiple virtual models for multiple external observers may be simultaneously used for multiple communications while preventing interference due to possible overlap of the multiple virtual models.","['G06F3/011', 'B60Q1/503', 'B60Q1/5037', 'B60Q1/507', 'B60Q1/543', 'G06F18/2148', 'G06F18/24', 'G06F18/2413', 'G06F3/013', 'G06F3/017', 'G06F3/0304', 'G06N3/045', 'G06N3/08', 'G06T19/00', 'G06V10/40', 'G06V10/764', 'G06V10/82', 'G06V20/20', 'G06V20/56', 'G06V40/18', 'G06V40/20', 'G09C1/00']"
CN107909600B,Unmanned aerial vehicle real-time moving target classification and detection method based on vision,"The invention discloses a vision-based real-time moving target classification and detection method for an unmanned aerial vehicle, wherein target identification is carried out by means of a leading edge technology of deep learning, the unmanned aerial vehicle accurately identifies similar target objects in a video through an advanced YOLOv2 algorithm, and statistics and marking are carried out on the similar target objects so as to be convenient for users to use; when a user selects a certain specific target object in the identified similar target objects at the ground station, the unmanned aerial vehicle system extracts the characteristics of the specific target object by using an ORB algorithm, continuously matches the characteristics of the similar target object extracted from each frame of video, and comprehensively obtains the finally matched specific target and the position of the specific target object by combining the motion track trend of the selected target. Specific target identification provides guarantee for subsequent automatic functions of automatic target tracking, accurate landing and the like. By the method, the real-time airborne target identification and detection of the unmanned aerial vehicle in a complex dynamic background are achieved, the identification of similar targets and specific targets is included, and the requirements of detection speed, precision and the like are met.","['G06T7/246', 'G01C11/00', 'G01C11/36', 'G06T5/80', 'G06T7/33', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/30241']"
US6562077B2,Sorting image segments into clusters based on a distance measurement,"A programming interface of document search system enables a user to dynamically specifying features of documents recorded in a corpus of documents. The programming interface provides category and format flexibility for defining different genre of documents. The document search system initially segments document images into one or more layout objects. Each layout object identifies a structural element in a document such as text blocks, graphics, or halftones. Subsequently, the document search system computes a set of attributes for each of the identified layout objects. The set of attributes are used to describe the layout structure of a page image of a document in terms of the spatial relations that layout objects have to frames of reference that are defined by other layout objects. Using the set of attributes a user defines features of a document with the programming interface. After receiving a feature or attribute and a set of document images selected by a user, the system forms a set of image segments by identifying those layout objects in the set of document images that make up the selected feature or attribute. The system then sorts the set of image segments into meaningful groupings of objects which have similarities and/or recurring patterns. Subsequently, document images in the set of document images are ordered and displayed to a user in accordance with the meaningful groupings.","['G06V30/418', 'G06F16/93', 'G06V30/414']"
AU2020321911B2,Region proposal networks for automated bounding box detection and text segmentation,"A processor may generate a plurality of intermediate feature layers of an image using convolutional neural network (CNN) processing. For each intermediate feature layer, the processor may generate a plurality of text proposals using a region proposal network (RPN). Each text proposal may comprise a portion of the intermediate feature layer that is predicted to contain text. The processor may perform OCR processing on image data within a plurality of regions of the image to generate a text result for each region. Each region may comprise at least one of the text proposals. The processor may assemble the text results into a text string comprising the text results ordered according to a spatial order in which the plurality of regions appear within the image.","['G06V10/82', 'G06F18/214', 'G06F40/295', 'G06F40/30', 'G06N3/0442', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06V20/62', 'G06V30/153', 'G06V30/19173', 'G06V30/412', 'G06F2207/4824', 'G06V30/10']"
US10855936B2,Skeleton-based effects and background replacement,"Various embodiments of the present invention relate generally to systems and methods for analyzing and manipulating images and video. In particular, a multi-view interactive digital media representation (MVIDMR) of a person can be generated from live images of a person captured from a hand-held camera. Using the image data from the live images, a skeleton of the person and a boundary between the person and a background can be determined from different viewing angles and across multiple images. Using the skeleton and the boundary data, effects can be added to the person, such as wings. The effects can change from image to image to account for the different viewing angles of the person captured in each image.","['H04N5/272', 'G06K9/00369', 'G06K9/00671', 'G06T13/40', 'G06T19/006', 'G06T7/11', 'G06T7/194', 'G06T7/75', 'G06V10/34', 'G06V10/426', 'G06V20/20', 'G06V40/103', 'H04N13/111', 'H04N13/282', 'H04N21/21805', 'H04N21/41407', 'H04N23/62', 'H04N23/90', 'H04N5/23216', 'H04N5/2628', 'H04N5/265', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20084', 'G06T2207/30196', 'H04N13/279', 'H04N2005/2726']"
US10074006B2,Methods and systems for disease classification,"This invention describes methods and systems for use of computer vision systems for classification of biological cells as an aid in disease diagnostics. More particularly the present invention describes a process comprising employing a robust and discriminative color space which will help provide segmentation of the cells; employing a segmentation algorithm, such as a feature-based level set, that will be able to segment the cells using a different k-phase-segmentation process, which detect for example, if a while blood cell occurs for segmenting the internal components of the cell robustly; employing a combination of different type of features including shape, texture, and invariant information, and employing a classification step to associate abnormal cell characteristics with disease states.","['G06V20/695', 'G06K9/00147', 'G06K9/0014', 'G06K9/4604', 'G06K9/4652', 'G06T7/0012', 'G06T7/187', 'G06T7/90', 'G06V20/698', 'H04N23/63', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/30024', 'H04N5/23293']"
US7755619B2,Automatic 3D face-modeling from video,"Systems and methods perform automatic 3D face modeling. In one implementation, a brief video clip of a user's head turning from front to side provides enough input for automatically achieving a model that includes 2D feature matches, 3D head pose, 3D face shape, and facial textures. The video clip of the user may be of poor quality. In a two layer iterative method, the video clip is divided into segments. Flow-based feature estimation and model-based feature refinement are applied recursively to each segment. Then the feature estimation and refinement are iteratively applied across all the segments. The entire modeling method is automatic and the two layer iterative method provides speed and efficiency, especially when sparse bundle adjustment is applied to boost efficiency.","['G06T17/20', 'G06T7/55', 'G06T2200/08']"
US20070286499A1,Method for Classifying Digital Image Data,"The present invention relates to a method for classifying digital image data (ID) which solves the problem that images (I) having image areas with artificial text overlay have to be detected and classified in video signals in a reliable manner. Therefore, a robust detection in the presence of additive noise is proposed which can be invariant with respect to interlaced or progressive modes of video sequences.",['G06V20/62']
US20160086333A1,Tracking Objects In Bowl-Shaped Imaging Systems,Technologies for determining a distance of an object from a vehicle include a computing device to identify an object captured in a fisheye image generated by a fisheye camera of the vehicle. The computing device projects a contour of the identified object on a selected virtual plane that is located outside the vehicle and selected from a predefined set of virtual planes based on a location of the identified object relative to the vehicle. The computing device identifies a bottom of the projected contour on the selected virtual plane and determines an intersection point of an imaginary line with a ground plane coincident with a plane on which the vehicle is positioned. The imaginary line passes through each of the identified bottom of the projected contour and the fisheye camera. The computing device determines a location of the identified object relative to the vehicle based on the determined intersection point and the identified bottom of the projected contour.,"['G06T7/73', 'G06T7/0042', 'B60R1/00', 'B60R1/27', 'G06T3/0018', 'G06T3/047', 'G06T3/12', 'G06T7/536', 'G06V20/58', 'H04N23/698', 'H04N5/23238', 'B60R2300/10', 'B60R2300/30', 'B60R2300/303', 'B60R2300/80', 'G06T2207/30252']"
CN102682305B,Automatic screening system and automatic screening method using thin-prep cytology test,"The invention discloses an automatic screening system and an automatic screening method using a thin-prep cytology test. The system comprises an image acquisition module, an image segmentation module, a target recognition module and a comprehensive analysis module. The method provided by the invention respectively adopts a three-point evolutionary uniform sampling method to guarantee the reliability of automatic focusing, the coarse-to-fine segmentation algorithm to improve the segmentation accuracy of pathological cell nucleuses, a series of filters to rapidly filter various impurities, and a cascade classifier training method to greatly reduce the false positive rate, and uses relative features to conduct comprehensive analysis. The method disclosed by the invention has the advantages of high sensitivity over pathological cells, high specificity over normal cells, high recognition speed and high automation degree, so that the diagnostic accuracy can be improved and the workload of a cytopathologist is reduced at the same time.",[]
CN112801900B,Video blurring removal method for generating countermeasure network based on bidirectional circular convolution,"The invention discloses a video blur removal method for generating an countermeasure network based on bidirectional circular convolution, which comprises the following steps: step one: generating a high-quality blur-removed restored video by using a clear video generation network; step two: classifying and judging the restored video and the reference clear video by utilizing a judging network; step three: constructing a loss function to train two networks, namely a clear video generation network and a discrimination network; and (3) outputting: and processing the blurred video by using the trained clear video generation network. The method takes the generated countermeasure network as a basic framework, and utilizes the time sequence relationship contained in the two paths of circulating neural network sequences for transmitting information along different directions; a fusion reconstruction module is introduced to reconstruct the current frame, and the global residual error connection is utilized to improve the network expression capacity and the convergence rate; the network is trained with content loss and countermeasures loss. The invention can be combined with various image and video application systems, helps to improve the quality of the shot video, and has wide market prospect and application value.","['G06T5/73', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'Y02T10/40']"
WO2021051604A1,"Method for identifying text region of osd, and device and storage medium","The present application relates to the technical field of image identification, and provides a method for identifying a text region of OSD. The method comprises: preprocessing an OSD file to obtain a frame-by-frame image, performing binarization threshold filtration on the frame-by-frame image by using a canny algorithm, and performing frame contour acquisition on the filtered image and finding discrete points of the contour of a text region; encapsulating the discrete points into at least two polygonal bounding boxes, and screening the bounding boxes according to a preset condition; amplifying the bounding boxes obtained by screening in proportion by means of an expansion algorithm; taking, according to a preset condition, a union of the bounding boxes remained after connection; and regressively calculating, according to the width of fonts in the text region, rectangular ranges of the bounding boxes after union, selecting the bounding box having the smallest rectangular range, and taking the region corresponding to the bounding box as a text region to be identified. According to the present application, a segmentation method based on edge detection is used for segmenting OSD texts and obtain segmentation regions, so that the effect of shielding the interference of the dynamic change of illumination on detection can be implemented.","['G06V20/635', 'G06V10/267', 'G06V10/28', 'G06V30/153', 'G06V30/10']"
US9384423B2,System and method for OCR output verification,"A system and method for computing confidence in an output of a text recognition system includes performing character recognition on an input text image with a text recognition system to generate a candidate string of characters. A first representation is generated, based on the candidate string of characters, and a second representation is generated based on the input text image. A confidence in the candidate string of characters is computed based on a computed similarity between the first and second representations in a common embedding space.","['G06K9/72', 'G06V30/2272', 'G06K9/00872', 'G06K9/3258', 'G06K9/6232', 'G06V10/7715', 'G06V20/63', 'G06V30/19127', 'G06K2209/15', 'G06V20/625']"
US10044944B2,Automatic composition of video with dynamic background and composite frames selected based on foreground object criteria,"A processing device generates composite images from a sequence of images. The composite images may be used as frames of video. A foreground/background segmentation is performed at selected frames to extract a plurality of foreground object images depicting a foreground object at different locations as it moves across a scene. The foreground object images are stored to a foreground object list. The foreground object images in the foreground object list are overlaid onto subsequent video frames that follow the respective frames from which they were extracted, thereby generating a composite video.","['H04N5/272', 'G06T11/60', 'G06T15/503', 'G06T5/002', 'G06T5/70', 'G06T7/11', 'G06T7/143', 'G06T7/174', 'G06T7/194', 'G06T7/20', 'G06T7/254', 'H04N13/0022', 'H04N13/128', 'H04N23/743', 'H04N5/2356', 'G06T2207/10016', 'G06T2207/10021', 'G06T2207/20036', 'H04N2013/0085', 'H04N2013/0092']"
US7756325B2,Estimating 3D shape and texture of a 3D object based on a 2D image of the 3D object,"Disclosed is an improved algorithm for estimating the 3D shape of a 3-dimensional object, such as a human face, based on information retrieved from a single photograph by recovering parameters of a 3-dimensional model and methods and systems using the same. Beside the pixel intensity, the invention uses various image features in a multi-features fitting algorithm (MFF) that has a wider radius of convergence and a higher level of precision and provides thereby better results.","['G06T17/00', 'G06T7/70', 'G06V20/647', 'G06V40/169', 'B33Y50/00', 'G06T2200/04']"
CN114708461B,"Classification method, device, equipment and storage medium based on multimodal learning model","The invention relates to an artificial intelligence technology and discloses a classification method based on a multi-mode learning model, which comprises the steps of obtaining an image to be processed, extracting image characteristics of the image to be processed, extracting text characteristics of the image to be processed to obtain a text characteristic set, mapping the image characteristics and the text characteristic set into a dimension space of a pre-built characteristic fusion model to obtain a characteristic fusion space, respectively calculating fusion characteristic vectors of image characteristics and each text characteristic in the characteristic fusion space, inputting the fusion characteristic vectors into a pre-trained classification model, and executing classification operation to classify target categories of the image to be processed. In addition, the invention also relates to a blockchain technology, and the image to be processed can be stored in nodes of the blockchain. The invention also provides a classifying device, equipment and storage medium based on the multi-modal learning model. The invention can improve the accuracy in image classification.","['G06F18/24', 'G06F18/253', 'G06N3/045', 'G06N3/08']"
US6009196A,Method for classifying non-running text in an image,"The present invention is a method for analyzing image data, and more particularly for analyzing of image data representing images containing text to partition the image into running and non-running text regions and to further classify the non-running text regions therein. The present invention utilizes characteristics of running text regions to identify such regions and to subsequently group all non-running text regions into related groups prior to the classification of the non-running text regions. Classification of the non-running text regions is accomplished by analyzing whether the non-running text regions exhibit pronounced horizontal and/or vertical alignment of the text blocks therein. Once the analysis is complete, alignment information is used to determine the number of ""rows"" and ""columns"" so as to classify the non-running text region as text, a horizontal sequence, a vertical sequence, or a table.",['G06V30/413']
WO2022151755A1,"Target detection method and apparatus, and electronic device, storage medium, computer program product and computer program","A target detection method and apparatus, and an electronic device, a storage medium, a computer program product and a computer program. The method comprises: performing feature extraction on a first image to be detected, so as to obtain a first feature map for a plurality of scales of the first image (S11); and processing the first feature map for the plurality of scales of the first image by means of a trained target detection network, so as to obtain the location of a first object of a target category in the first image (S12).","['G06T7/0012', 'G06F18/24', 'G06T7/12', 'G06T7/62', 'G06T7/73', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30064', 'G06V2201/07']"
US10475186B2,Segmentation of objects in videos using color and depth information,"Techniques are provided for segmentation of objects in video frames. A methodology implementing the techniques according to an embodiment includes receiving image frames, including an initial reference frame, and receiving a mask to outline a region in the reference frame that contains the object to be segmented. The method also includes calculating Gaussian mixture models associated with both the masked region and a background region external to the masked region. The method further includes segmenting the object from a current frame based on a modelling of the pixels within an active area of the current frame as a Markov Random Field of nodes for cost minimization. The costs are based in part on the Gaussian mixture models. The active area is based on the segmentation of a previous frame and on an estimation of optical flow between the previous frame and the current frame.","['G06T7/11', 'G06T7/136', 'G06T7/143', 'G06T7/174', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20112']"
US10733482B1,Object height estimation from monocular images,"Systems and methods for estimating a height of an object from a monocular image are described herein. Objects are detected in the image, each object being indicated by a region of interest. The image is then cropped for each region of interest and the cropped image scaled to a predetermined size. The cropped and scaled image is then input into a convolutional neural network (CNN), the output of which is an estimated height for the object. The height may be represented by a mean of a probability distribution of possible sizes, a standard deviation, as well as a level of confidence. A location of the object may be determined based on the estimated height and region of interest. A ground truth dataset may be generated for training the CNN by simultaneously capturing a LIDAR sequence with a monocular image sequence.","['G06K9/6277', 'G06T7/62', 'G05D1/0088', 'G05D1/0246', 'G06F18/214', 'G06F18/2415', 'G06F18/253', 'G06K9/4604', 'G06K9/6256', 'G06K9/66', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T3/40', 'G06T7/50', 'G06T7/74', 'G06V10/25', 'G06V10/806', 'G06V10/82', 'G06V20/56', 'G06V30/19173', 'G06T2207/10004', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30261', 'G06T2210/12']"
US10599924B2,Semantic page segmentation of vector graphics documents,"Disclosed systems and methods categorize text regions of an electronic document into document object types based on a combination of semantic information and appearance information from the electronic document. A page segmentation application executing on a computing device accesses textual feature representations that represent text portions in a vector space, where a set of pixels from the page is mapped to a textual feature representation. The page segmentation application generates a visual feature representation, which corresponds to an appearance of a document portion including the set of pixels, by applying a neural network to the page of the electronic document. The page segmentation application generates an output page segmentation of the electronic document by applying the neural network to the textual feature representation and the visual feature representation.","['G06K9/00456', 'G06N3/08', 'G06F18/214', 'G06F18/2413', 'G06K9/00463', 'G06K9/3233', 'G06K9/34', 'G06K9/6256', 'G06K9/627', 'G06N3/045', 'G06N3/0454', 'G06N3/0455', 'G06N3/0464', 'G06N3/0895', 'G06N3/09', 'G06V10/764', 'G06V10/82', 'G06V30/147', 'G06V30/148', 'G06V30/19147', 'G06V30/413', 'G06V30/414', 'G06K2209/01', 'G06N3/048', 'G06N3/0481', 'G06V30/10']"
US11321769B2,System and method for automatically generating three-dimensional virtual garment model using product description,"A method and system for generating a three-dimensional (3D) model of a garment. The method includes: providing an image of the garment; generating a 3D garment representation based on the image; registering the 3D garment representation to a 3D model to obtain a preliminary 3D garment model; projecting the preliminary 3D garment model dressed on the 3D model to an 2D projected image; and comparing the 2D projected image with the image of the garment, so as to refine the preliminary 3D garment model to obtain the final 3D model of the garment.","['G06Q30/0643', 'G06F18/2431', 'G06K9/628', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T15/04', 'G06T15/205', 'G06T7/162', 'G06T7/33', 'G06T7/75', 'G06V10/267', 'G06V10/454', 'G06V10/82', 'G06V20/64']"
CN109543502B,A Semantic Segmentation Method Based on Deep Multiscale Neural Network,"The invention relates to a semantic segmentation method based on a deep multi-scale neural network, which comprises the following steps: collecting images containing various objects of different categories, labeling all the objects of interest in each image, and labeling the object category of each pixel point of the content as image label information; dividing an image set; dividing the collected images into a training set, a verification set and a test set, wherein the training set is used for training the convolutional neural network, the verification set is used for selecting an optimal training model, and the test set is used for subsequent test model effects or actual application; designing a depth-based multi-scale neural network structure to effectively realize object detection, inputting data, forward calculating a prediction result and loss cost, calculating the gradient of parameters through a back propagation algorithm and updating the parameters; and updating the parameters iteratively, and finishing model training when the cost function curve converges.","['G06V20/588', 'G06N3/045', 'G06N3/084', 'G06T7/11', 'G06V20/584', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30252', 'Y02T10/40']"
US9968257B1,Volumetric quantification of cardiovascular structures from medical imaging,"A computing system accesses complementary image data of a biological tissue structure (BTS), which can include a human cardiovascular structure. The complementary image data is comprised of two-dimensional images which represent different views of the BTS respectively aligned with a plurality of distinct image planes. A plurality of separate convolutional neural networks (CNNs) are used to respectively process each of the plurality of two-dimensional images. Each CNN determines a probability map which is then adaptively fused into a single segmented output. A contouring operation is automatically performed to calculate at least one clinical measurement and/or create at least one 3D volume.","['A61B5/0044', 'A61B5/0035', 'A61B5/02007', 'A61B5/055', 'A61B5/7267', 'G06T7/11', 'A61B2576/023', 'A61B6/03', 'A61B6/503', 'A61B6/5247', 'G06T2207/20084', 'G06T2207/30048', 'G16H30/40', 'G16H50/70']"
WO2022126377A1,"Traffic lane line detection method and apparatus, and terminal device and readable storage medium","A traffic lane line detection method and apparatus, and a terminal device and a readable storage medium, applicable to the technical field of computer vision and image processing. The method comprises: obtaining a road image of the current scene (S201); and inputting the road image into a trained neural network model for processing, and outputting a detection result of a traffic lane line in the road image of the current scene, wherein the trained neural network model is obtained by training according to sample images in a training set and a semantic segmentation model, and the sample images in the training set comprise collected road images of multiple scenes and marked images corresponding to the road images of multiple scenes (S202). The method can solve the problems that most of deep learning models used for lane recognition at present are relatively large in calculation amount, and the models are relatively complex and unfavorable for meeting the requirement on real-time performance in an actual application scene of an automatic driving task.",['G06F18/00']
US8340437B2,Methods and systems for determining optimal features for classifying patterns or objects in images,Provided are methods for determining optimal features for classifying patterns or objects. Also provided are methods for image analysis. Further provided are methods for image searching.,"['G06V10/771', 'G06F18/2115', 'G06F18/2415', 'G06T7/0014', 'G06V2201/03']"
US9082038B2,Dram c adjustment of automatic license plate recognition processing based on vehicle class information,"Methods and systems for improving automated license plate recognition performance. One or more images of a vehicle can be captured via an automated license plate recognition engine. Vehicle class information associated with the vehicle can be obtained using the automated license place recognition engine. Such vehicle class information can be analyzed with respect to the vehicle. Finally, data can be dynamically adjusted with respect to the vehicle based on a per image basis to enhance recognition of the vehicle via the automated license plate recognition engine.","['G06V20/63', 'G06K9/3258', 'G06V30/248', 'G06V20/625']"
US11544503B2,Domain alignment for object detection domain adaptation tasks,"A domain alignment technique for cross-domain object detection tasks is introduced. During a preliminary pretraining phase, an object detection model is pretrained to detect objects in images associated with a source domain using a source dataset of images associated with the source domain. After completing the pretraining phase, a domain adaptation phase is performed using the source dataset and a target dataset to adapt the pretrained object detection model to detect objects in images associated with the target domain. The domain adaptation phase may involve the use of various domain alignment modules that, for example, perform multi-scale pixel/path alignment based on input feature maps or perform instance-level alignment based on input region proposals.","['G06K9/6257', 'G06V30/40', 'G06F18/2148', 'G06F18/217', 'G06F18/2413', 'G06F18/2431', 'G06K9/6232', 'G06K9/6262', 'G06K9/628', 'G06N3/045', 'G06N3/0464', 'G06N3/084', 'G06N3/0895', 'G06N3/09', 'G06N3/094', 'G06N3/096', 'G06N5/046', 'G06V10/764', 'G06V10/82', 'G06V30/19147', 'G06V30/1916']"
US10991093B2,"Systems, methods and media for automatically generating a bone age assessment from a radiograph","In accordance with some embodiments, systems, methods and media for generating a bone age assessment. In some embodiments, a method comprises: receiving an x-ray image of a subject's left hand and wrist; converting the image to a predetermined size; identifying, without user intervention, a first portion of the image corresponding to the hand and wrist; processing the first portion of the image to increase contrast between bones and non-bones to generate a processed image; causing a trained convolution neural network to determine a bone age based on the processed image; receiving an indication of the bone age; causing the bone age to be presented to a user as the result of a bone age assessment; and causing the bone age and the image to be stored in an electronic medical record associated with the subject.","['G06T7/0012', 'A61B6/505', 'A61B6/5217', 'G06F18/214', 'G06F18/2413', 'G06K9/6256', 'G06K9/627', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/096', 'G06N3/0985', 'G06V10/764', 'G06V10/82', 'G16H10/60', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'A61B5/4509', 'G06K2209/055', 'G06T2207/10116', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30008', 'G06V2201/033']"
US12374126B2,"Obstacle detection method and apparatus, computer device, and storage medium","An obstacle detection method can improve the accuracy of determining a relative positional relationship between two or more obstacles that are obstructed or obscured during automated driving. A road scene image of a road where a target vehicle is located is acquired. Obstacle recognition is performed to obtain region information and depth-of-field information corresponding to each obstacle in the road scene image. Target obstacles in an occlusion relationship and a relative depth-of-field relationship between the target obstacles are determined. A ranging result of each obstacle is acquired using a ranging apparatus corresponding to the target vehicle. An obstacle detection result of the road is determined based on the relative depth of field relationship between the target obstacles and the ranging result of each obstacle, thereby improving the accuracy of determining a positional relationship of obstructed or obscured obstacles during automated driving.","['G06V10/82', 'G01C11/04', 'G05D1/0246', 'G06F18/23', 'G06F18/25', 'G06N3/045', 'G06T7/11', 'G06T7/149', 'G06T7/50', 'G06V10/25', 'G06V10/26', 'G06V10/44', 'G06V10/52', 'G06V10/764', 'G06V20/56', 'G06V20/58', 'G06N3/0464', 'G06N3/09', 'G06T2207/10028', 'G06T2207/30261']"
US12266113B2,Automatically segmenting and adjusting images,"A device automatically segments an image into different regions and automatically adjusts perceived exposure-levels or other characteristics associated with each of the different regions, to produce pictures that exceed expectations for the type of optics and camera equipment being used and in some cases, the pictures even resemble other high-quality photography created using professional equipment and photo editing software. A machine-learned model is trained to automatically segment an image into distinct regions. The model outputs one or more masks that define the distinct regions. The mask(s) are refined using a guided filter or other technique to ensure that edges of the mask(s) conform to edges of objects depicted in the image. By applying the mask(s) to the image, the device can individually adjust respective characteristics of each of the different regions to produce a higher-quality picture of a scene.","['G06T7/11', 'G06T5/60', 'G06T5/70', 'G06T5/94', 'G06V10/764', 'G06T2207/20081']"
CN113785305B,"Method, device and equipment for detecting inclined characters","The application discloses a method for detecting inclined characters, relates to the field of artificial intelligence, and in particular relates to the field of computer vision. The method comprises the following steps: the character angle detection model acquires the inclination angle of characters in the original image; and carrying out angle correction on the original image according to the obtained inclination angle to obtain an angle correction image, wherein characters in the angle correction image are generally horizontal. The text box detection model then validates at least one text box from the angle corrected image. The text box detection model acquires the position information of each text box, and intercepts at least one text box sub-image from the angle correction image according to the position information of at least one text box, wherein each text box sub-image comprises a series of characters; the text angle detection model and the text box detection model adopt different neural network models. The method can improve the accuracy of detecting the area where the inclined text is located from the image.",['G06V10/24']
CN110428428B,"An image semantic segmentation method, electronic device and readable storage medium","The invention discloses an image semantic segmentation method, electronic equipment and a readable storage medium, wherein based on an FCN model with depth feature fusion, the traditional convolution operation is replaced by cavity convolution, original images with different resolutions are constructed to form an image pyramid, the FCN model is input in a layering mode, the output features of the upper layer are fused with the next layer, the layers are fused to the bottom layer from top to bottom, the output features of the bottom layer are subjected to transposition convolution, the output resolution is consistent with the input images of the bottom layer, the sensitivity of target positioning is improved, and then optimization processing is carried out through a fully-connected conditional random field, so that the segmentation precision is ensured, and a better segmentation effect is obtained.","['G06T7/11', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084']"
US8649592B2,System for background subtraction with 3D camera,"A system for background image subtraction includes a computing device coupled with a 3D video camera, a processor of the device programmed to receive a video feed from the camera containing images of one or more subject that include depth information. The processor, for an image: segments pixels and corresponding depth information into three different regions including foreground (FG), background (BG), and unclear (UC); categorizes UC pixels as FG or BG using a function that considers the color and background history (BGH) information associated with the UC pixels and the color and BGH information associated with pixels near the UC pixels; examines the pixels marked as FG and applies temporal and spatial filters to smooth boundaries of the FG regions; constructs a new image by overlaying the FG regions on top of a new background; displays a video feed of the new image in a display device; and continually maintains the BGH.","['G06T5/50', 'G06T7/11', 'G06T7/12', 'G06T7/174', 'G06T7/194', 'G06T7/90', 'G06V10/28', 'G06V20/40', 'G06V20/64', 'H04N5/2226', 'H04N5/272', 'G06T2207/10021', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20224']"
US10922793B2,Guided hallucination for missing image content using a neural network,"Missing image content is generated using a neural network. In an embodiment, a high resolution image and associated high resolution semantic label map are generated from a low resolution image and associated low resolution semantic label map. The input image/map pair (low resolution image and associated low resolution semantic label map) lacks detail and is therefore missing content. Rather than simply enhancing the input image/map pair, data missing in the input image/map pair is improvised or hallucinated by a neural network, creating plausible content while maintaining spatio-temporal consistency. Missing content is hallucinated to generate a detailed zoomed in portion of an image. Missing content is hallucinated to generate different variations of an image, such as different seasons or weather conditions for a driving video.","['G06T5/77', 'G06T5/005', 'G06F18/214', 'G06K9/6256', 'G06K9/726', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/088', 'G06N3/0895', 'G06N3/09', 'G06N3/094', 'G06T3/4046', 'G06T5/50', 'G06T5/60', 'G06V30/274', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084']"
CN106960195B,Crowd counting method and device based on deep learning,"The embodiment of the invention discloses a crowd counting method and device based on deep learning. The method comprises the following steps: dividing a crowd picture to be detected into a plurality of picture blocks; determining a head candidate frame region and the confidence coefficient of the head candidate frame region in the plurality of picture blocks based on an RPN candidate frame generation model obtained by pre-training; screening the determined human head candidate frame region according to the confidence coefficient to obtain a region to be detected; and performing classification prediction on the region to be detected based on a Fast-RCNN correction model obtained through pre-training, and determining the number of people in the picture of the people to be detected according to the result of the classification prediction. The embodiment of the invention provides a crowd counting method under high resolution, which improves the accuracy and robustness of crowd counting.","['G06V20/53', 'G06F18/214']"
US5579360A,Mass detection by computer using digital mammograms of the same breast taken from different viewing directions,"Two digital radiologic images taken from different viewing directions of a same region, notably a mammogram study of the same breast, are automatically processed by a computer to identify suspect masses by producing processed mammogram images in which suspect spots, the skinline and the nipple are marked or enhanced. Candidate suspect spots are initially identified by individually processing each mammogram image separately. Then characteristics of the candidate spots identified in the mammogram images taken from different viewing directions are compared to eliminate false positives. The characteristics compared include position of the spots relative to the explicitly detected nipple, and their size, shape, brightness and brightness variance. The candidate spots are separately identified by thresholding the breast region of the mammogram at 20 or more threshold levels determined from a histogram of the image to discriminate spots, and classifying the spots by size, shape and variance in intensity of the pixels comprising the spot. Overlaps are resolved after the comparison between views.","['A61B6/502', 'A61B6/4216', 'A61B6/4233', 'G06T5/50']"
EP4181015A1,Method and device for recognizing handheld object,"A handheld object recognition method related to the field of artificial intelligence is provided, including: obtaining location information of each of one or more detected objects in a to-be-recognized image, and obtaining a first label of each detected object, where the location information of each detected object is location information of the detected object in the to-be-recognized image, the first label of the detected object indicates a type of the detected object, and the type of the detected object is used to represent a handheld relationship of the detected object; obtaining a handheld object from the one or more detected objects based on the first label of each detected object, and obtaining location information of the handheld object from the location information of the one or more detected objects; and recognizing the handheld object in the to-be-recognized image based on the location information of the handheld object to obtain a recognition result of the handheld object. According to embodiments of this application, when an object or a hand is obviously blocked, the handheld object can be accurately determined. In this way, the handheld object is recognized.","['G06V10/764', 'G06V10/46', 'G06V40/107', 'G06F16/583', 'G06F18/23213', 'G06F18/241', 'G06F18/2431', 'G06N3/04', 'G06N3/045', 'G06N3/08', 'G06V20/10', 'G06V20/70', 'G06F18/214', 'G06V10/25', 'G06V10/40']"
US8644578B1,Method and apparatus of identifying objects of interest using imaging scans,"In general, embodiments of the invention comprise systems and methods for delineating objects from imaging scans. According to certain aspects, methods of the invention include aggregating homologous objects from multiple scans into categories, developing profiles or characteristics for these categories, matching newly identified objects to these pre-existing categories for object identification, and red-flagging, or visualization of identified objects.","['G06T17/00', 'G06F18/28', 'G06T7/13', 'G06T7/181', 'G06V10/772', 'G06T2200/08', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/30008']"
US9398349B2,"Comment information generation device, and comment display device","A comment information generation device includes: a video input section, to which a video is input; an information input section, to which positional information is input to display a comment to track an object in the video; an initial trajectory acquisition section that acquires an initial trajectory that is a trajectory of the object corresponding to the positional information; a trajectory extending section that acquires an extended trajectory by acquiring a following trajectory that is a trajectory having a starting point after an ending point of the initial trajectory, collecting a first comment assigned in a vicinity of the initial trajectory and a second comment assigned in a vicinity of the following trajectory, and connecting the following trajectory to the initial trajectory on a basis of the first comment and the second comment; and an output section that outputs the extended trajectory as comment information.","['H04N21/8133', 'H04N21/23418', 'H04N21/4788', 'H04N21/8583']"
US10657325B2,Method for parsing query based on artificial intelligence and computer device,"A method for parsing a query based on artificial intelligence, a computer device and a computer program product are provided. The method may include: acquiring annotated data including an annotated search instance, an annotated template instance, a field of a search intent and a category in the field, an attribute of a term in the annotated search instance and a concrete term included in the attribute; performing a training on the annotated data so as to generate an instance dictionary, a template dictionary and an inverted index dictionary; acquiring a target query to be identified; and parsing the target query based on the instance dictionary, the template dictionary and the inverted index dictionary, so as to acquire a target parsing result corresponding to the target query.","['G06F16/951', 'G06F16/374', 'G06F16/90335', 'G06F40/205', 'G06F40/247', 'G06N20/20', 'G06N5/003', 'G06N5/01', 'G06N5/02', 'G06N5/022', 'G06F40/242']"
US10002286B1,System and method for face recognition robust to multiple degradations,"A novel, unified technique for analyzing, synthesizing, and recognizing faces captured by various modalities, and under a variety of often seen real-world circumstances, using thin-plate splines for densification of points of a face from an initial set of fiducial points.","['G06K9/00288', 'G06V40/168', 'G06F16/51', 'G06F16/583', 'G06F17/30247', 'G06F17/3028', 'G06K9/00255', 'G06T7/40', 'G06V10/70', 'G06V40/172', 'G06T2207/30204']"
CN111340754B,A Method Based on Detection and Classification of Aircraft Skin Surface Defects,"The invention discloses a method for detecting and classifying surface defects of an aircraft skin, which comprises the following steps: carrying out multi-view super-resolution reconstruction preprocessing on the image, and fusing image information shot by different cameras in space to obtain an image with higher resolution; feature extraction, namely acquiring physical measurement features of defects based on multi-view measurement, and adopting GPU acceleration to train and accelerate a Convolutional Neural Network (CNN) to mine depth features capable of representing aircraft skin defects; feature fusion and classification recognition, and data fusion and classification are performed based on the SVM model of the multi-mode weighted combination. The invention develops the research of the aircraft skin detection technology, develops image monitoring and identification aiming at the surface defects of the aircraft skin so as to realize non-contact and nondestructive accurate measurement, fuses, extracts and classifies multi-mode measurement data, automatically identifies defect types, and provides a new way for realizing the accurate maintenance and management of the aircraft.","['G06T7/0002', 'G06F18/214', 'G06F18/2411', 'G06N3/045', 'G06T5/73', 'G06T7/11', 'G06T7/62']"
US7376246B2,Subspace projection based non-rigid object tracking with particle filters,"A method tracks non-rigid objects in a video acquired of a cluttered scene by a camera. The method uses a particle filter. The tracking includes the following steps: motion transition estimation, contour deformation detection, and contour regulation. The method uses a dynamic affine transform model and employs the particle filter to estimate the parameters of the model. The method generates a probabilistic map of deformation for tracking the contour of the object followed by a projection step to constrain or regulate the contour in a contour subspace.","['G06T7/277', 'G06V10/245', 'G06V10/755', 'G06T2207/30241']"
AU2021240222B2,Eye pose identification using eye features,"Systems and methods for eye pose identification using features of an eye are described. Embodiments of the systems and methods can include segmenting an iris of an eye in the eye image to obtain pupillary and limbic boundaries of the eye, determining two angular coordinates (e.g., pitch and yaw) of an eye pose using the pupillary and limbic boundaries of the eye, identifying an eye feature of the eye (e.g., an iris feature or a scleral feature), determining a third angular coordinate (e.g., roll) of the eye pose using the identified eye feature, and utilizing the eye pose measurement for display of an image or a biometric application. In some implementations, iris segmentation may not be performed, and the two angular coordinates are determined from eye features.","['G02B27/0093', 'G02B27/017', 'G02B27/0172', 'G06F3/013', 'G06T7/60', 'G06V40/193', 'G06V40/197', 'G02B2027/0138', 'G02B2027/014', 'G02B2027/0187', 'G06T2207/30041', 'G06T2207/30196']"
CN111639692B,Shadow detection method based on attention mechanism,"The invention discloses a shadow detection method based on an attention mechanism, which comprises the following steps: acquiring a common shadow data set to be processed for training a system; acquiring a shadow image to be detected from camera equipment or a local hard disk; preprocessing a public shadow data set and obtaining a corresponding training set and a corresponding testing set; preprocessing a shadow image to be detected; building and fusing all modules of a system core neural network to form a convolutional neural network based on an attention mechanism; calculating network prediction and label loss, and adjusting network parameters according to the loss; completely training the deep convolutional neural network and inputting a preprocessed shadow image to be detected to the deep convolutional neural network; and outputting a shadow detection result, and performing shadow attribute classification on each pixel to finish a shadow detection process of the custom data. The invention increases the extraction capability of the shadow features, reduces the relevance between semantics, and improves the generalization and the high efficiency of a detection system, so that the shadow detection result is more accurate.","['G06F18/214', 'G06F18/253', 'G06N3/045', 'G06N3/08']"
CN107871106B,Face detection method and device,"The embodiment of the invention provides a face detection method and a face detection device, wherein the method comprises the following steps: extracting a plurality of face features of different hierarchical networks from a face image to be detected by adopting a convolutional neural network model trained in advance to obtain a plurality of face feature vectors corresponding to the different hierarchical networks; fusing a plurality of face feature vectors into one face feature vector; performing dimensionality reduction on the face feature vectors subjected to the fusion processing to obtain two face feature vectors with the same dimensionality; performing face detection processing on one face feature vector in the two face feature vectors to obtain a face detection result; and simultaneously carrying out attitude estimation processing on the other face feature vector in the two face feature vectors to obtain an attitude estimation result. The method can be used for more richly describing the image by using the characteristics of face detection and posture estimation, has higher accuracy and reduces the error rate of subsequent face detection; and a plurality of related tasks can be executed simultaneously, and the performance of a single task is improved. And the task processing efficiency is improved.","['G06V40/161', 'G06N3/08', 'G06V40/172']"
US11176589B2,Dynamically generated machine learning models and visualization thereof,"Systems and methods provide real-time machine learning modeling, evaluation, and visualization. A computing system can receive input values for attributes of a new item listing. Concurrently, the system can analyze previous related item listings. The system can process the attributes and associated values of the new listing and previous listings to extract features and associated values. The system can apply the features of previous listings to machine learning algorithms to generate a machine learning model directed towards a target objective, such as maximizing a selling price or a selling probability for the new listing. The system can determine a likely outcome of the new listing by applying user-input values to the model and alternative outcomes by substituting one or more user-input values. If the alternative outcomes represent better results, then the system can present suggestions to revise the new listing to replace one or more user-input values with substitute values.","['G06Q30/0631', 'G06F16/901', 'G06N20/00', 'G06Q30/0202']"
US10204283B2,"Image recognizing apparatus, image recognizing method, and storage medium","The accuracy of estimating the category of an object in an image and its region is improved. The present invention detects as to whether each of a plural types of objects is included in an object image, forms a plurality of local regions in a region including the object detected, and calculates a feature quantity of the plurality of local regions formed. Furthermore, the present invention selects a discriminant criterion adapted to the type of the object detected, from a plurality of discriminant criteria for discriminating the plural types of objects, and determines, based on the discriminant criterion selected and the feature quantity calculated, a region of the object detected from the plurality of local regions.","['G06K9/34', 'G06V10/26', 'G06F18/254', 'G06F18/285', 'G06K9/6227', 'G06K9/6292', 'G06V10/809', 'G06V10/87', 'G06K9/00664', 'G06V20/10']"
CN109191369B,"Method, storage medium and device for converting 2D picture collection to 3D model","The invention provides a method, a storage medium and a device for converting a 2D picture set into a 3D model, wherein the method comprises the following steps: step 11: identifying the category of each object and the contour information of each object contained in each picture in the 2D picture set through a deep learning algorithm; step 12: extracting detail characteristic information of each object in each picture through a computer vision algorithm, wherein the detail characteristic information at least comprises texture information, color characteristic information and shape characteristic information of each object; step 13: matching the 3D model based on the category, the shape characteristic information and the contour information of each object, wherein the successfully matched 3D model is the 3D model of each object; step 14: texture information and color feature information for each object are mapped onto the 3D model for each object. Based on the method provided by the invention, the realistic 3D model is constructed, so that the defects of generating the 3D model and the 3D video based on parallax are overcome, the user experience of the 3D video or the 3D model is improved, and the entertainment and the interestingness are enhanced.","['G06N20/00', 'G06T3/08', 'G06F18/23', 'G06F18/24', 'G06N3/045', 'G06N3/0464', 'G06N3/0495', 'G06N3/09', 'G06T15/04', 'G06T17/00', 'G06T19/006', 'G06T19/20', 'G06T7/70', 'G06V20/46', 'G06N3/044', 'G06N3/08', 'G06T2200/04', 'G06T2200/08', 'G06T2207/20084', 'G06T2207/30244', 'G06T2219/2012']"
US5987094A,Computer-assisted method and apparatus for the detection of lung nodules,"A computer-assisted diagnostic (CAD) method and apparatus are described for the enhancement, detection, and classification of suspicious regions in digital x-ray images, with particular emphasis on lung nodule detection using chest x-ray images. An objective is to improve the sensitivity and specificity of detection of suspicious areas such as nodules, while maintaining a low false positive detection rate. A modular CAD technique has been developed to be potentially automatic and to be used as a second-opinion method for lung nodule detection. The method consists of using a plurality of CAD modules to preprocess and enhance image features, including image preprocessing, selective enhancement, segmentation, and feature extraction using a multiresolution/multiorientation wavelet transform and a computationally efficient filter, a 1.5 D circular pattern filter.","['G01N23/046', 'G06T7/0012', 'G01N2223/419', 'G01N2223/612']"
US8111923B2,System and method for object class localization and semantic class based image segmentation,"An automated image processing system and method are provided for class-based segmentation of a digital image. The method includes extracting a plurality of patches of an input image. For each patch, at least one feature is extracted. The feature may be a high level feature which is derived from the application of a generative model to a representation of low level feature(s) of the patch. For each patch, and for at least one object class from a set of object classes, a relevance score for the patch, based on the at least one feature, is computed. For at least some or all of the pixels of the image, a relevance score for the at least one object class based on the patch scores is computed. An object class is assigned to each of the pixels based on the computed relevance score for the at least one object class, allowing the image to be segmented and the segments labeled, based on object class.","['G06T7/11', 'G06V10/267', 'G06V10/464', 'G06V20/00', 'G06T2207/10024', 'G06T2207/20021', 'G06T2207/30236', 'G06T2207/30252']"
CN113034652B,"Virtual image driving method, device, equipment and storage medium","The embodiment of the invention discloses an avatar driving method, device, equipment and storage medium. The method comprises the steps of obtaining a target video frame, inputting the target video frame into an avatar driving model, wherein the target video frame comprises a target object corresponding to a target avatar, extracting image features and human body key point features corresponding to the target video frame through the avatar driving model, carrying out feature fusion on the image features and the human body key point features, obtaining a driving signal predicted by the avatar driving model according to the fusion features, and driving the target avatar to perform the same action as the target object in the target video frame according to the driving signal. According to the technical scheme provided by the embodiment of the invention, the image features and the human body key point features are fused to drive the virtual image, so that the real-time performance is met, and the driving accuracy of the virtual image is improved.","['G06T13/40', 'G06F18/214', 'G06F18/253', 'G06V40/10', 'G06V40/20']"
CN111178245B,"Lane line detection method, lane line detection device, computer equipment and storage medium","The application relates to a lane line detection method, a lane line detection device, computer equipment and a storage medium. The method comprises the following steps: receiving a lane line image to be detected and a frame serial number thereof; invoking a pre-trained segmentation network, inputting the lane line image to be detected into a backbone network of the segmentation network, and extracting sharing characteristics of the lane line image to be detected; when the lane line image to be detected is determined not to be a key frame image according to the frame sequence number, acquiring a lane line instance cluster image corresponding to the key frame image according to the frame sequence number; inputting the shared features into a semantic segmentation branch network of a segmentation network, and carrying out semantic segmentation on the lane line image to be detected through the semantic segmentation branch network to obtain a binary lane line image corresponding to the lane line image to be detected; and carrying out instance classification on the binary lane line images according to the lane line instance clustering images of the key frame images to obtain instance lane line images. By adopting the method, the precision can be improved and the time consumption can be reduced.","['G06V20/588', 'G06F18/23', 'G06F18/241', 'G06N3/045', 'G06N3/08', 'Y02T10/40']"
CN111951384B,Three-dimensional face reconstruction method and system based on single face picture,"The invention discloses a three-dimensional face reconstruction method and system based on a single face picture, and belongs to the technical field of computer vision and curved surface reconstruction. Aiming at the problems that a model reconstructed by point cloud is rough, a convolutional neural network model is sunk into a local suboptimal solution due to a loss function established by pixel layer information in the prior art, and a three-dimensional face reconstruction effect is not robust enough when a picture with a large face angle is subjected to three-dimensional face reconstruction. The invention fully utilizes the corresponding relation between the input face picture and the rendered face picture in the deep feature space, trains the end-to-end three-dimensional face reconstruction regression network, improves the quality of three-dimensional face reconstruction, and ensures that the calculation result is more accurate.","['G06T17/00', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06V40/161', 'G06V40/168', 'G06V40/172', 'G06T2200/04', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30201']"
JP5325899B2,Intrusion alarm video processor,"Binarization is performed using a threshold image obtained by multiplying a variation in each pixel value of an input image with a coefficient. Although the variation is time-averaged based on an update coefficient for each pixel, the update coefficient is switched depending on whether or not a relevant pixel belongs to the object. Subsequently, from the binary image, an initial detection zone is formed and a spatial filtering process is performed thereto. The spatial filtering process includes at least one of skeleton analysis processing, object mask processing, morphology processing, and section analysis processing. For a tracking zone, the temporal positional change thereof is tracked, and the noise is reduced. Some of the tracking zones are removed, and the remaining zones are integrated into a cluster, and furthermore the cluster selection is performed based on the dimensions in real space.","['G08B13/1961', 'G06T7/194', 'G06T7/254', 'G06T2207/20044']"
US10083521B1,Content recommendation based on color match,"Approaches attempt to determine information that can help to produce more useful recommendations to be displayed in a situation where no, or little, information is available that indicates a relationship between content provided through an electronic marketplace or other content provider. For example, data available that relates to an item in a product catalog, for example color data, can be analyzed and aggregated in order to attempt to locate other items that are related and relevant to the item, at least as it relates to color and categorization of the content. Such approaches can include, for example, analyzing images, articles, and other sources of electronic content to attempt to locate items that might be relevant to the item of interest. In a clothing example, this can include accessory items that might be worn with an apparel item of interest, match the apparel item of interest, be frequently utilized or exhibited with the apparel item of interest, include a matching and/or complementary color to the apparel item of interest, etc.","['G06T7/408', 'G06F17/3025', 'G06F17/30253', 'G06F17/30256', 'G06F17/30274', 'G06F17/30277', 'G06K9/00362', 'G06K9/6255', 'G06K9/6256', 'G06Q30/0603', 'G06Q30/0631', 'G06T1/0007', 'G06V10/56', 'G06V10/82', 'G09G5/02', 'G06T2207/10024', 'G06T2207/20132', 'G06T2210/16']"
CN110555901B,"Method, device, equipment and storage medium for positioning and mapping dynamic and static scenes","The invention discloses a method, a device, equipment and a storage medium for positioning and mapping a dynamic scene and a static scene, wherein the method for positioning and mapping the dynamic scene and the static scene comprises the following steps: sequentially acquiring image frames shot by a camera; screening out key frames in the image frames, and determining camera poses corresponding to the key frames; clustering the key frame, dividing the key frame into a plurality of blocks, determining dynamic blocks in the key frame, and filtering map points corresponding to all feature points in the dynamic blocks in a local map maintained by the system; and local map building is carried out according to the key frames, and the camera pose corresponding to the key frames and the local map maintained by the system are optimized. The method, the device, the equipment and the storage medium for positioning and mapping the dynamic and static scenes reduce the influence of dynamic points on positioning and mapping, and do not need to consume a large amount of computing resources.","['G06T5/77', 'G06T15/005', 'G06T5/94', 'G06T2207/10012', 'G06T2207/30181']"
US11763485B1,"Deep learning based robot target recognition and motion detection method, storage medium and apparatus","The present invention discloses deep learning based robot target recognition and motion detection methods, storage media and devices, the method consists of the following steps: Step S1. adding masks to regions where potentially dynamic objects are located through instance segmentation networks incorporating attention mechanisms and positional coding; Step S2, estimation of the camera pose using static feature points outside the instance segmentation mask in the scene; Step S3, estimation of the object pose transformation matrix from the camera pose; Step S4, determining the state of motion of the object's characteristic points from the relationship between motion parallax and differential entropy, and thus the state of motion of the object as a whole; Step S5, rejects the dynamic objects therein and repairs the static background of the rejected area for positional estimation and map construction. The invention improves the accuracy of segmented boundaries of occluded dynamic objects, and the rejection of dynamic region feature points reduces the impact of dynamic objects on the system.","['G06T7/73', 'G06F18/253', 'G06N3/045', 'G06N3/08', 'G06T5/50', 'G06T7/246', 'G06T7/579', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30241', 'G06T2207/30244', 'Y02T10/40']"
RU2656708C1,Method for separating texts and illustrations in images of documents using a descriptor of document spectrum and two-level clustering,FIELD: image processing means.,"['G06F7/00', 'G06V30/413', 'G06F18/00', 'G06F18/23', 'G06F18/285', 'G06F40/00', 'G06T1/00', 'G06T3/40', 'G06T7/11', 'G06T7/187', 'G06V10/40', 'G06V10/762']"
US5799100A,Computer-assisted method and apparatus for analysis of x-ray images using wavelet transforms,"A computer-assisted diagnostic (CAD) method and apparatus are described for the enhancement and detection of suspicious regions in digital X-ray images, with particular emphasis on early cancer detection using digital mammography. An objective is to improve the sensitivity of detection of suspicious areas such as masses, while maintaining a low false positive detection rate, and to classify masses as benign or malignant. A modular CAD technique has been developed as a potentially automatic and/or second-opinion method for mass detection and classification in digital mammography that may in turn be readily modified for application with different digital X-ray detectors with varying gray-scale and resolution characteristics. The method consists of using a plurality of CAD modules to preprocess and enhance image features in the gray-level, the directional texture, and the morphological domains.","['G06T7/0012', 'Y10S128/922']"
CN112444242B,Pose optimization method and device,"Disclosed are a pose optimization method, a pose optimization device, a computer readable storage medium and an electronic device, wherein the pose optimization method comprises the following steps: acquiring a first pose corresponding to the image acquisition equipment when acquiring the current frame image; determining a visual space region corresponding to the first pose; determining the space coordinates of map points falling into the visible space area in the vector map; determining characteristic information of the current frame image; determining a projection error of the map point in the current frame image according to the first pose, the space coordinates of the map point and the characteristic information; and optimizing the first pose according to the projection error to obtain an optimized first pose. According to the technical scheme, the spatial information of the object in the image is not required to be recovered, so that the storage space of the electronic equipment is saved, and the pose optimization efficiency is improved.","['G01C21/005', 'G01C21/165', 'Y02T10/40']"
CN108446585B,"Target tracking method and device, computer equipment and storage medium","The present application relates to a target tracking method, system, computer device and storage medium. The method comprises the following steps: acquiring a target video through a video acquisition device, acquiring a current image in the target video, and acquiring a current target in the current image according to a target three-dimensional model; calculating the similarity between the current target and the historical target in the previous frame of image according to a training similarity equation; when a current target with the similarity larger than or equal to a preset value with a historical target in a previous frame image exists in the current image, establishing a tracking relation between the current target with the similarity larger than or equal to the preset value and the historical target; and obtaining the next frame of image as the current image, and continuing to obtain the current target in the current image according to the target three-dimensional model. By adopting the method, each tracking target can be more accurately tracked, the influence of noise on target tracking is effectively eliminated, and the accuracy of the target tracking technology is improved.","['G06V20/42', 'G06F18/22', 'G06T17/00', 'G06T7/11', 'G06T7/215', 'G06T7/246', 'G06V40/10', 'G06T2207/10016', 'G06T2207/10028', 'G06T2207/20021', 'G06T2207/20081']"
US8320666B2,Real-time image and video matting,"A system and method implemented as a software tool for generating alpha matte sequences in real-time for the purposes of background or foreground substitution in digital images and video. The system and method is based on a set of modified Poisson equations that are derived for handling multichannel color vectors. Greater robustness is achieved by computing an initial alpha matte in color space. Real-time processing speed is achieved through optimizing the algorithm for parallel processing on the GPUs. For online video matting, a modified background cut algorithm is implemented to separate foreground and background, which guides the automatic trimap generation. Quantitative evaluation on still images shows that the alpha mattes extracted using the present invention has improved accuracy over existing state-of-the-art offline image matting techniques.","['G06T7/11', 'G06T2207/10024', 'G06T2207/10028']"
CN104937635B,More hypothesis target tracking devices based on model,"The disclosure describes a kind of target tracking device, assesses the frame of the data of one or more targets (such as body part, body and/or object) as acquired in depth camera.Previous frame in (multiple) target the position in joint and the data from present frame be used for determining (multiple) target in present frame joint position.In order to execute the task, tracker propose several hypothesis and then assessment data with verify mutually it will be assumed that.The hypothesis for being most preferably adapted to the data generated by depth camera is selected, and correspondingly maps the joint of (multiple) target.","['G06F3/017', 'G06F18/28', 'G06F3/011', 'G06F3/0304', 'G06T7/251', 'G06V10/772', 'G06V40/20', 'G06T2207/10028', 'G06T2207/30196']"
US10776936B2,Point cloud matching method,"A method comprising: providing a first 3D point cloud and a second 3D point cloud about an object obtained using different sensing techniques; removing a scale difference between the 3D point clouds based on a mean distance of points in corresponding subsets of the first and second 3D point clouds; arranging the 3D point clouds in a two-level structure, wherein a first level is a macro structure describing boundaries of the object and a second level is a micro structure consisting of supervoxels of the 3D point cloud; constructing a first graph from the first 3D point cloud and a second graph from the second 3D point cloud such that the supervoxels represent nodes of the graphs and adjacencies of the supervoxels represents edges of the graphs; matching the first and second graph for obtaining a transformation matrix; and registering the 3D point clouds together by applying the transformation matrix.","['G06T7/33', 'G06T7/37', 'G06T7/30', 'G06T7/38', 'G06T17/00', 'G06T2200/04', 'G06T2207/10028', 'G06T2207/20072']"
JP5243529B2,Camera pose estimation apparatus and method for extended reality image,"An apparatus for providing an estimate for a 3D camera pose relative to a scene from 2D image data of a 2D image frame provided by the camera is provided, the apparatus using four types of observations: (a) detected 2D-3D point correspondences; (b) tracked 2D-3D point correspondences; (c) motion model observations; and (d) edge observations.","['H04N13/204', 'G06T7/75', 'G06T7/251', 'G06T2207/10016', 'G06T2207/30244']"
WO2022077917A1,"Instance segmentation model sample screening method and apparatus, computer device and medium","An instance segmentation model sample screening method, which relates to artificial intelligence, and can be used for a medical image analysis assistance scenario. The method comprises: reading an original data set; picking out, on the basis of an active learning manner and from an unlabeled set, first samples to be labeled, information amounts of which are greater than those of remaining samples, so as to obtain a first labeled set in a manner of manually labeling the plurality of first samples to be labeled; selecting, on the basis of a semi-supervised learning manner and from all the remaining samples, second samples to be labeled, confidence coefficients of which are higher than a set value, so as to obtain a second labeled set in a manner of pseudo-labeling said second samples; and taking the first labeled set, the second labeled set and a labeled set together as a training set. By means of the method, a manual sample labeling amount is reduced, and a large number of samples used for training an image instance segmentation model can be obtained, such that a more ideal accuracy for the instance segmentation model can be realized. In addition, the method further relates to blockchain technology, and both an original data set and a training set can be stored in a blockchain.","['G06F18/24', 'G06F18/214', 'G06F18/217', 'G06T7/11', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30016', 'G06T2207/30041', 'Y02P90/30']"
US20220366194A1,Computer Vision Systems and Methods for Blind Localization of Image Forgery,Computer vision systems and methods for localizing image forgery are provided. The system generates a constrained convolution via a plurality of learned rich filters. The system trains a convolutional neural network with the constrained convolution and a plurality of images of a dataset to learn a low level representation of each image among the plurality of images. The low level representation is indicative of a statistical signature of at least one source camera model of each image. The system can determine a splicing manipulation localization by the trained convolutional neural network.,"['G06N3/08', 'G06K9/6265', 'G06F18/2148', 'G06F18/2193', 'G06K9/6257', 'G06N3/04', 'G06N3/045', 'G06N7/01', 'G06V10/30', 'G06V10/50', 'G06V10/764', 'G06V10/7796', 'G06V10/82', 'G06V20/40', 'G06V20/90']"
CN111066063B,Systems and Methods for Depth Estimation Using Affinities Learned by Convolutional Spatial Propagation Networks,"Systems and methods are presented for improving the speed and quality of real-time pixel-by-pixel depth estimation of scene layouts from a single image through the use of an end-to-end Convolutional Spatial Propagation Network (CSPN). An efficient linear propagation model uses recursive convolution operations to perform propagation. The affinity between adjacent pixels can be learned by a deep Convolutional Neural Network (CNN). Given a single image, CSPN can be applied to two depth estimation tasks: (1) Refine the depth output of the existing method, and (2) convert sparse depth samples into dense depth maps, e.g., by embedding the depth samples within the propagation process. This conversion ensures that sparse input depth values remain in the final depth map and that the conversion runs in real time and is therefore well suited for robotics and autopilot applications where sparse but accurate depth measurements, e.g., from LiDAR, can be fused with image data.","['G06T7/50', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084']"
US12100146B2,Assessing risk of breast cancer recurrence,"The subject disclosure presents systems and computer-implemented methods for assessing a risk of cancer recurrence in a patient based on a holistic integration of large amounts of prognostic information for said patient into a single comparative prognostic dataset. A risk classification system may be trained using the large amounts of information from a cohort of training slides from several patients, along with survival data for said patients. For example, a machine-learning-based binary classifier in the risk classification system may be trained using a set of granular image features computed from a plurality of slides corresponding to several cancer patients whose survival information is known and input into the system. The trained classifier may be used to classify image features from one or more test patients into a low-risk or high-risk group.","['G06T7/0012', 'C12Q1/6886', 'G06F18/211', 'G06F18/253', 'G06V10/771', 'G06V10/806', 'G06V20/698', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/10064', 'G06T2207/20081', 'G06T2207/30024', 'G06T2207/30068', 'G06T2207/30072', 'G06T2207/30096']"
CN113985445B,3D target detection algorithm based on camera and laser radar data fusion,"According to the 3D target detection algorithm based on the fusion of the camera and the laser radar data, a YOLOv model is selected as a 2D target detector, and target detection and tracking are carried out on a 2D image layer, so that a 2D boundary frame for target detection is obtained; constructing a mapping relation between image data collected by a camera and point cloud data collected by a laser radar according to a calibration principle of the camera and the laser radar, and synchronizing the image data collected by the camera and the point cloud data collected by the laser radar; taking point cloud data acquired by a laser radar as input, and adopting a ground segmentation algorithm and a point cloud clipping technology to segment background point cloud and foreground point cloud in the point cloud data; and projecting the segmented foreground point cloud to a camera imaging plane according to the mapping relation between the image data and the point cloud data, taking the foreground point cloud in the 2D boundary box of the target as a point cloud candidate area of the target, and extracting the 3D boundary box of the target through European clustering to realize 3D detection of the target. 3D target detection in a dynamic scene can be realized on the premise of not depending on the position of a guiding person.","['G01S17/931', 'G01S17/86', 'G06F18/251', 'G06T7/194', 'G06T2207/10028']"
US11210787B1,Systems and methods for processing electronic images,"An image processing method including receiving a target image of a slide corresponding to a target specimen comprising a tissue sample of a patient; generating a machine learning system by processing a plurality of training images, each training image comprising an image of human tissue and a label characterizing at least one of a slide morphology, a diagnostic value, a pathologist review outcome, and an analytic difficulty; automatically identifying, using the machine learning system, an area of interest of the target image by analyzing microscopic features extracted from multiple image regions in the target image; determining, using the machine learning system, a probability of a target feature being present in the area of interest of the target image based on an average probability; and determining, using the machine learning system, a prioritization value, of a plurality of prioritization values.","['G16H30/40', 'G06F18/214', 'G06N20/00', 'G06T7/0012', 'G06V10/70', 'G16B40/20', 'G16H10/40', 'G16H40/20', 'G16H50/20', 'G16H70/20', 'G16H70/60', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024', 'G06T2207/30096', 'G06T2207/30204', 'G06V2201/03', 'G06V2201/04']"
CN108805209B,Lung nodule screening method based on deep learning,"The invention discloses a pulmonary nodule screening method based on deep learning, which comprises the following steps: s1, preprocessing the lung CT image to extract a lung parenchyma part; s2, detecting the preprocessed lung CT image by using a lung nodule detection network, and detecting the position of a lung nodule to obtain the position of a lung nodule candidate region; s3, further classifying the position areas of the nodule candidate areas, and screening out real nodule areas by utilizing a deep learning classification network; and S4, grading the malignancy degree of the real nodule area by using a deep learning classification network. According to the lung nodule screening method, the lung nodule detection is carried out aiming at the CT image, the malignancy degree of the target nodule is judged, the workload of a doctor is greatly reduced, and the diagnosis efficiency of the doctor is improved.","['G06F18/24', 'G06N3/045', 'G06V2201/032']"
CN116740070B,Plastic pipeline appearance defect detection method based on machine vision,"The invention relates to the technical field of image data processing, in particular to a plastic pipeline appearance defect detection method based on machine vision, which comprises the following steps: acquiring an image, performing tile segmentation, acquiring a pixel point target degree according to the gradient size and the aggregation degree of the pixel points in a window, calculating a correlation coefficient of the target degree, acquiring a weighted target degree of the target pixel point according to the target degree and the correlation coefficient, determining a segmentation double threshold of the window image according to the weighted target degree, and determining a defect region. According to the method, the image is subjected to window segmentation, the target pixel points are obtained according to the aggregation degree and the correlation factor of the high gradient pixel points in the window, and the window double-threshold value is determined according to the gray value range of the target pixel points to perform local self-adaptive threshold segmentation. The threshold segmentation is not affected by the gray level change of the surface of the plastic pipe, and the defect in the image can be segmented and displayed better according to the characteristics of the defect.","['G06T7/0004', 'G06T7/11', 'G06T7/136', 'G06T7/194', 'G06T2207/10004', 'Y02P90/30']"
CN106462746B,Analyzing digital holographic microscopy data for hematology applications,"A method for analyzing Digital Holographic Microscopy (DHM) data for hematology applications includes receiving a plurality of DHM images acquired using a digital holographic microscopy system. One or more connected components are identified in each of the plurality of DHM images, and one or more training white blood cell images are generated from the one or more connected components. The classifier is trained using the one or more training white blood cell images to identify a plurality of white blood cell types. A classifier may be applied to the new white blood cell image to determine a plurality of probability values, each respective probability value corresponding to one of the plurality of white blood cell types. The new white blood cell image and the plurality of probability values may then be presented in a graphical user interface.","['G01N15/1433', 'G01N33/49', 'G03H1/0443', 'G06F18/241', 'G06F18/2411', 'G06T7/0012', 'G06T7/136', 'G06V10/764', 'G06V20/69', 'G06V20/698', 'G01N2015/016', 'G01N2015/1006', 'G03H2001/005', 'G06T2207/30104']"
CN111126325B,Intelligent personnel security identification statistical method based on video,"The invention belongs to computer vision, deep learning and target detection technologies, and particularly relates to an intelligent personnel security identification statistical method based on videos. The invention combines the traditional image processing method, the deep learning neural network and the traditional machine learning classification method to realize accurate detection and identification of the position information and the number of small target personnel in the working environment and the conditions of wearing safety helmets and working clothes, and simultaneously combines the good real-time performance of the single-step target processing process of the original PyramidBox detection algorithm and the advantages of combining the context environment information to realize accurate detection of fuzzy and small targets and the like. Due to the complexity of the actual working environment, the requirement can not be met only by identifying the safety helmet, the invention realizes the simultaneous detection of the head area and the body area of the small target personnel in the visual field, thereby not only achieving the detection and tracking of the position of the personnel, but also meeting the safety standard requirement in the engineering.","['G06V20/52', 'G06F18/214', 'G06F18/2411', 'G06F18/253', 'G06T7/73', 'G06V10/50', 'G06V40/10', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/30196', 'G06T2207/30232']"
CN115131566B,Automatic image segmentation method based on superpixel and improved fuzzy C-means clustering,"The embodiment of the invention provides an automatic image segmentation method based on super pixels and improved fuzzy C-means clustering. The method comprises the following steps: selecting any neighborhood from the target image, calculating local gray level change information of pixel points in the neighborhood and spatial distance information between the pixel points in the neighborhood and the central pixel point, and counting local gray level change information influence weights and spatial distance information influence weights to obtain an optimized weighted image; pre-segmenting the optimized weighted image by using an SLIC algorithm to obtain super pixels; carrying out density peak clustering on the super pixels to obtain cluster numbers; and adopting an optimized membership improvement fuzzy C-means clustering algorithm to perform secondary segmentation on the super pixels. In this way, the number of clusters can be automatically determined without manual participation, the influence of noise points on image segmentation is effectively reduced, and the image segmentation efficiency and the image segmentation precision are improved.","['G06V10/26', 'G06V10/762']"
US6922485B2,Method of image segmentation for object-based image retrieval,"This paper provides a new image segmentation algorithm for object-based image retrieval. The system partitions multi-dimensional images into disjoint regions of coherent color and texture. In order to distinguish the object contour lines from texture edge, the description length of the line is used as discriminating criteria. Visual attribute values are assigned to each region.","['G06T7/11', 'G06F16/5838', 'G06F16/5854', 'G06T7/12', 'G06T7/187', 'G06V10/267', 'G06V10/56', 'G06T2207/10024', 'G06T2207/20064']"
US8875016B2,Method and apparatus to convert digital ink images for use in a structured text/graphics editor,"In accordance with one aspect of the present invention, disclosed is an image analysis and conversion method and system, where digital ink images are converted to structured object representations of the digital ink images, capable of being edited by a structured text/graphics editor.","['G06V30/333', 'G06K9/00409', 'G06K9/00463', 'G06V30/414']"
US10037610B1,"Method for tracking and segmenting a target object in an image using Markov Chain, and device using the same","A method for tracking a target object in frames of video data using Absorbing Markov Chain (AMC), including steps of: (a) acquiring a bounding box containing the target object in a current frame and a segmentation result for the target object in a previous frame; (b) obtaining obtain a region of interest (ROI) in the current frame by enlarging the bounding box to contain a portion of background information surrounding the target object; (c) acquiring information on local regions within the ROI in the current frame; (d) constructing an AMC graph using at least part of the local regions within the region of interest (ROI) in the current frame and local regions within a region of interest (ROI) in the previous frame; and (e) acquiring a segmentation result for the target object within the current frame by thresholding individual nodes in the AMC graph using absorption times thereof.","['G06T7/246', 'G06T7/11', 'G06T7/12', 'G06T7/13', 'G06T7/136', 'G06T7/143', 'G06T7/162', 'G06T7/248', 'G06T7/277', 'G06V20/40', 'G06T2207/10016', 'G06T2207/20072', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/30221', 'G06T2210/12']"
US6903751B2,System and method for editing electronic images,"A graphical input and display system for creating and manipulating electronic images includes input devices permitting a user to manipulate elements of electronic images received from various image input sources. A processor, connected to the system, receives requests for various image editing operations and also accesses a memory structure. The system memory structure includes a user interaction module, which allows a user to enter new image material or select and modify existing image material to form primary image objects, as well as a grouping module, which maintains an unrestricted grouping structure, an output module, and data memory.",['G06T11/60']
CN115797336A,"Fault detection method and device of photovoltaic module, electronic equipment and storage medium","The application discloses a fault detection method and device of a photovoltaic module, electronic equipment and a storage medium, and belongs to the technical field of image processing. The fault detection method of the photovoltaic module comprises the following steps: performing image segmentation on a target infrared image corresponding to a photovoltaic component to be detected to obtain a sub-image corresponding to a target sub-component in the photovoltaic component to be detected; carrying out hot spot feature identification on the sub-image, marking a hot spot area and obtaining a target fault image; the target fault image is used for representing hot spot fault information of the photovoltaic module to be tested. According to the fault detection method of the photovoltaic module, the module edge segmentation and hot spot target detection are carried out on the infrared image of the photovoltaic panel by using a deep learning method, the identification precision is high, the missing detection risk can be effectively reduced, the fault identification effect of the photovoltaic module is obviously improved, the fault detection method is suitable for hot spot identification of any form, and the universality is high.",['Y02E10/50']
US6134354A,Apparatus for the identification of free-lying cells,"A free-lying cell classifier. An automated microscope system comprising a computer and high speed processing field of view processors identifies free-lying cells. An image of a biological specimen is obtained and the image is segmented to create a set of binary masks. The binary masks are used by a feature calculator to compute the features that characterize objects of interest including free-lying cells, artifacts and other biological objects. The objects are classified to identify their type, their normality or abnormality or their identification as an artifact. The results are summarized and reported. A stain evaluation of the slide is performed as well as a typicality evaluation. The robustness of the measurement is also quantified as a classification confidence value. The free-lying cell evaluation is used by an automated cytology system to classify a biological specimen slide.","['G01N15/1433', 'G06V20/69', 'G01N2015/1497']"
US10420523B2,Adaptive local window-based methods for characterizing features of interest in digital images and systems for practicing same,"Provided are methods for characterizing a feature of interest in a digital image. In certain aspects, the methods use an adaptive local window and include obtaining an initial contour for a feature of interest, defining a region of interest around the contour, and segmenting the feature of interest by iteratively selecting a size of a local window surrounding each point on the contour. Non-transitory computer readable media and systems that find use in practicing the methods of the present disclosure are also provided.","['A61B6/5217', 'A61B5/055', 'A61B6/12', 'A61B6/469', 'G06T7/10', 'G06T7/12', 'G06T7/174', 'G06T7/194', 'G16H50/30', 'A61B5/725', 'A61B5/7267', 'A61B6/032', 'A61B6/502', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10116', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/30096']"
CN107656613B,Human-computer interaction system based on eye movement tracking and working method thereof,"The invention discloses a human-computer interaction system based on eye movement tracking and a working method thereof, wherein the system comprises a processor, a video acquisition device and an AR/VR head display device, wherein the processor is respectively connected with the AR/VR head display device and the video acquisition device; the AR/VR head display device is provided with an eye movement tracking sensor and an angular motion sensor, and the eye movement tracking sensor and the angular motion sensor are respectively used for capturing eye movement information in real time and collecting the current movement state of the AR/VR head display device in real time and transmitting the current movement state to the processor; the video acquisition device is used for acquiring a scene image in the eye sight range and transmitting the scene image to the processor. The invention can improve the interactive experience of AR/VR in each engineering application field.",['G06F3/013']
US20230102467A1,"Method of detecting image, electronic device, and storage medium","A method of detecting an image, an electronic device, and a storage medium are provided, which relate to a field of an artificial intelligence technology, in particular to fields of computer vision and deep learning technologies, and may be applied to a smart city and an intelligent cloud. The method includes: performing a feature extraction on an image to be detected, so as to obtain a feature map of the image to be detected; generating a prediction box in the feature map according to the feature map; generating a mask for the prediction box according to a key region of a target object; and classifying the prediction box using the mask as a classification enhancement information, so as to obtain a category of the prediction box.","['G06F18/214', 'G06V10/25', 'G06V10/255', 'G06N3/045', 'G06N3/08', 'G06V10/44', 'G06V10/454', 'G06V10/764', 'G06V10/771', 'G06V10/82', 'G06V2201/07']"
US10296099B2,"Operating environment with gestural control and multiple client devices, displays, and users","Embodiments described herein includes a system comprising a processor coupled to display devices, sensors, remote client devices, and computer applications. The computer applications orchestrate content of the remote client devices simultaneously across the display devices and the remote client devices, and allow simultaneous control of the display devices. The simultaneous control includes automatically detecting a gesture of at least one object from gesture data received via the sensors. The detecting comprises identifying the gesture using only the gesture data. The computer applications translate the gesture to a gesture signal, and control the display devices in response to the gesture signal.","['G06F3/017', 'G06F3/0425', 'G06F3/147', 'G06K9/00201', 'G06K9/00335', 'G06K9/00362', 'G06K9/00375', 'G06V20/64', 'G06V40/10', 'G06V40/107', 'G06V40/20', 'G09G3/002', 'H04L67/025']"
US10491895B2,Fast and robust human skin tone region detection for improved video coding,"Techniques related to improved video coding based on skin tone detection are discussed. Such techniques may include selecting from static skin probability histograms and/or a dynamic skin probability histogram based on a received video frame, generating a skin tone region based on the selected skin probability histogram and a face region of the video frame, and encoding the video frame based on the skin tone region to generate a coded bitstream.","['H04N19/124', 'G06T7/11', 'G06T7/143', 'H04N19/136', 'H04N19/167', 'H04N19/186', 'H04N19/587', 'H04N19/80', 'H04N19/85']"
DK3161787T3,DETECTING EDGE OF A CELL CEREALS USING CAR ANALYSIS,"Systems and methods for generating a locally adaptive threshold image for foreground detection performing operations including creating a saliency edge strength image or layer indicating edge or border pixels of the nuclei by performing tensor voting on pixels neighboring the initial edge pixels within an image region to refine true edges are featured. Further, for each of a plurality of regions or blocks of the image, an adaptive threshold image is determined by sampling a foreground pixel and a background pixel for each initial edge pixel or refined edge pixel, generating histograms for both background and foreground saliency (or gradient magnitude) modulated histograms, determining a threshold range for each block of the image, and interpolating the threshold at each pixel based on the threshold range at each block. Comparing the input image with the resulting locally adaptive threshold image enables extraction of significantly improved foreground.","['G06T7/12', 'G06T7/0012', 'G06T7/13', 'G06T7/136', 'G06T7/194', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/20021', 'G06T2207/20036', 'G06T2207/20076', 'G06T2207/20164', 'G06T2207/30024']"
US7298881B2,"Method, system, and computer software product for feature-based correlation of lesions from multiple images","A method, system, and computer software product for correlating medical images, comprising: obtaining first image data representative of a first medical image including a first abnormality; obtaining second image data representative of a second medical image including a second abnormality; determining at least one feature value for each of the first and second abnormalities using the first and second image data; calculating, based on the determined feature values, a likelihood value indicative of a likelihood that the first and second abnormalities are a same abnormality; and outputting the determined likelihood value.","['G06T7/0012', 'G06T7/33', 'G06T7/35', 'G06T7/97', 'G06T2207/10116', 'G06T2207/10132', 'G06T2207/30068', 'Y10S128/922']"
US8358840B2,Methods and systems for representation and matching of video content,"The described methods and systems provide for the representation and matching of video content, including spatio-temporal matching of different video sequences. A particular method of determining temporal correspondence between different sets of video data inputs the sets of video data and represents the video data as ordered sequences of visual nucleotides. Temporally corresponding subsets of video data are determined by aligning the sequences of visual nucleotides.","['H04N21/8456', 'G06F16/783', 'G06V20/48', 'H04N21/4334', 'H04N21/44008', 'H04N21/4402', 'H04N21/4532', 'H04N21/845', 'H04N21/8455']"
CN108229591A,"Neural network adaptive training method and apparatus, equipment, program and storage medium","The embodiment of the invention discloses a kind of neural network adaptive training method and apparatus, electronic equipment, computer program and storage mediums.Wherein, method includes：Amplify the second image；To amplified second image of first nerves network inputs as currently neural network to be trained, obtain after the first nerves network processes and be adjusted to the third image big with second image etc.；Based on second image and using the third image as monitoring data, the first nerves network is trained.The embodiment of the present invention can obtain more effective adaptive effect.","['G06F18/214', 'G06F18/29', 'G06N3/02']"
US12175787B2,Three-dimensional human pose estimation method and related apparatus,"This application discloses a three-dimensional human pose estimation method performed by a computer device. An initialization pose estimation result of a single video frame in a video frame sequence of n views is extracted based on a neural network model. Single-frame and single-view human pose estimation is performed on the initialization pose estimation result for each video frame, to obtain n single-view pose estimation sequences respectively corresponding to the n views. Single-frame and multi-view human pose estimation is performed according to single-view pose estimation results with the same timestamp in the n single-view pose estimation sequences, to obtain a multi-view pose estimation sequence. Multi-frame and multi-view human pose estimation is performed on a multi-view pose estimation result in the multi-view pose estimation sequence, to obtain a multi-view and multi-frame pose estimation result. Therefore, accuracy of human pose estimation is improved.","['G06N3/0464', 'G06V40/10', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06V10/267', 'G06V10/803', 'G06V10/806', 'G06V10/82', 'G06V20/41', 'G06V20/46', 'G06V20/64', 'G06V40/103']"
US8582864B2,Fault inspection method,"A fault inspection method and apparatus in which the scattergram is separated or objects of comparison are combined in such a manner as to reduce the difference between an inspection object image and a reference image. As a result, the difference between images caused by the thickness difference in the wafer can be tolerated and the false information generation prevented without adversely affecting the sensitivity.","['G06T7/001', 'G06F2218/18']"
US10592002B2,"Gesture sequence recognition using simultaneous localization and mapping (SLAM) components in virtual, augmented, and mixed reality (xR) applications","Systems and methods for gesture sequence recognition using Simultaneous Localization and Mapping (SLAM) components in virtual, augmented, and mixed reality (xR) applications are described. In an illustrative, non-limiting embodiment, an Information Handling System (IHS) may include: a processor; and a memory coupled to the processor, the memory having program instructions stored thereon that, upon execution, cause the IHS to: detect a gesture performed by a user wearing a Head-Mounted Device (HMD) using a first image capture device and a second image capture device concurrently; evaluate the first and second image capture devices; identify the gesture based upon the evaluation; and execute a command associated with the gesture.","['G06F3/0346', 'G06F3/017', 'G02B27/017', 'G09G3/003', 'G02B2027/0138', 'G09G2360/144']"
CN112785705B,Pose acquisition method and device and mobile equipment,"The application discloses a pose acquisition method, a pose acquisition device and mobile equipment, wherein the pose acquisition method comprises the following steps: obtaining a current frame image; obtaining characteristic points with matched characteristic points in a history image contained in a sliding window in a current frame image; respectively obtaining a first pixel model of the matched characteristic points in each current frame image; respectively comparing the first pixel model of the matched characteristic points in each current frame image with the corresponding second pixel model to obtain a model comparison result, wherein the model comparison result represents whether the spatial points corresponding to the matched characteristic points in the current frame image belong to a moving object or not; the second pixel model is a pixel model of the characteristic points matched with the characteristic points in the current frame image in the historical image; screening out target feature points in the current frame image according to the model comparison result, wherein the target feature points are feature points of which the corresponding space points do not belong to a moving object; and obtaining the current pose of the mobile equipment according to the reprojection error corresponding to the target feature point.","['G06T17/05', 'G06T19/00', 'G06T7/33', 'G06T7/73']"
US11941799B2,Machine-learning based camera image triggering for quality assurance inspection processes,"Data is received that includes a feed of images of a plurality of objects passing in front of an inspection camera module forming part of a quality assurance inspection system. Within each image, it is detected whether an object is present within the image. Instance identifiers are assigned to each object. A single image is identified in which the object is optimally represented for each object using the corresponding instance identifier. These identified images are provided to a consuming application or process for quality assurance analysis.","['G06T7/0008', 'G06T7/001', 'G06F18/2113', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/0895', 'G06N3/09', 'G06N3/096', 'G06T7/20', 'G06T7/248', 'G06T7/66', 'G06T7/70', 'G06V10/25', 'G06V10/28', 'G06V10/462', 'G06V10/82', 'G06V10/993', 'G06T2207/10016', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30168', 'G06T2207/30241', 'G06V2201/06']"
US20240070214A1,Image searching method and apparatus,"Apparatus for performing searching of a plurality of reference images, the apparatus including one or more electronic processing devices that search the plurality of reference images to identify first reference images similar to a sample image, identify image tags associated with at least one of the first reference image, search the plurality of reference images to identify second reference images using at least one of the image tags and provide search results including at least some first and second reference images.","['G06F16/9538', 'G06F16/2228', 'G06F16/24578', 'G06F16/248', 'G06F16/51', 'G06F16/58', 'G06F16/583', 'G06F16/5838', 'G06F16/5846', 'G06F16/5854', 'G06F16/951', 'G06F18/10', 'G06F18/2178', 'G06T3/40', 'G06T5/002', 'G06T5/70', 'G06T7/11', 'G06T7/90', 'G06V2201/09', 'G06V2201/10', 'G06V30/10']"
CN106127148B,A method for detecting abnormal behavior of escalator passengers based on machine vision,"The invention discloses a kind of escalator passenger's anomaly detection method based on machine vision, comprising steps of 1) video image acquisition；2) Canny edge detection is carried out to the image of acquisition；3) circle of the class in edge image region is found out using Hough circle transformation；4) judge whether such circle region is the number of people by number of people grey level histogram template；5) number of people in image is tracked using Mean Shift method；6) moving direction, the movement speed of the central point in each head of passenger class circle region are calculated；7) behavior of passenger is judged by the moving direction and movement speed of analysis classes circle regional center point.The safe mass that can effectively improve escalator by the method for the invention, the situation after avoiding emergency event further deteriorate, and the personal damage that passenger on escalator falls down is preferably minimized.",['G06V20/42']
US7894641B2,Method and apparatus for acquiring and organizing ink information in pen-aware computer systems,"An ink manager running at a computer system receives ink information entered at a pen-based input/display device and accumulates the ink information into ink strokes. The ink manager communicates with a handwriting recognition engine and includes an ink phrase termination engine that is configured to detect the occurrence of one or more ink phrase termination events by examining the ink information. Upon the occurrence of an ink phrase termination event, the ink manager notifies the handwriting recognition engine and organizes the preceding ink strokes into an ink phrase data structure. The ink manager may also pass the ink phrase to an application executing on the computer system that is associated with the ink information, and it, in response, may return a reference pointer and a recognition context to the ink manager. The reference pointer and recognition context are then appended to the ink phrase data structure.","['G06F3/038', 'G06V30/1423']"
US6665841B1,Transmission of subsets of layout objects at different resolutions,"In a document search and retrieval system, document images are segmented into layout objects. Each layout object identifies different structural elements in a document image. In addition, the system computes attributes and features for each segmented layout object. Before any document images are transmitted between a client and a server, users specify which document image attributes and features are most relevant to their browsing or searching tasks. Transmission (and/or display) of document images is then divided into two stages. During the first stage, those layout objects which are identified as having the specified features or attributes are transmitted at a first or high resolution; the remaining layout objects in an image are transmitted at a second or lower resolution (or in the form of bounding polygons). If the second stage is invoked, those remaining layout objects are re-transmitted at the first or high resolution. The second stage of transmission may be invoked when either a user request is received or when there is a system timeout.","['G06F16/5838', 'G06V30/414']"
US20240296528A1,Denoising method based on multiscale distribution score for point cloud,"A denoising method based on a multiscale distribution score for a point cloud includes: constructing a two-layer network model based on multiscale perturbation and point cloud distribution, where the two-layer network model includes a feature extraction module for extracting a feature of the point cloud and a displacement prediction module for predicting a displacement of a noise point; constructing a point cloud noise model for improving a denoising effect and retaining a sharp feature and avoiding reducing quality of point cloud data; extracting a global feature h by inputting the point cloud data into the feature extraction module; iteratively learning the displacement of the noise point by the displacement prediction module according to a feature obtained by the feature extraction unit; and defining a loss function of network training, and completing convergence under the condition that the loss function reaches a set threshold or a maximum number of iterations.","['G06N3/0464', 'G06T5/60', 'G06N3/048', 'G06N3/084', 'G06T5/70', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084']"
US6873723B1,Segmenting three-dimensional video images using stereo,The present invention is a method and apparatus to segment a three-dimensional scene having foreground and background regions. Regions in stereo images of the 3-D scene are matched. The stereo images include left and right images. Ranges of matched regions are computed. The foreground region is separated from the background region using the computed ranges of the matched regions.,"['G06T7/593', 'G06T7/11', 'G06T7/194', 'G06V10/28', 'G06T2207/10012', 'G06T2207/10021']"
US11730387B2,Method for detection and diagnosis of lung and pancreatic cancers from imaging scans,"A method of detecting and diagnosing cancers characterized by the presence of at least one nodule/neoplasm from an imaging scan is presented. To detect nodules in an imaging scan, a 3D CNN using a single feed forward pass of a single network is used. After detection, risk stratification is performed using a supervised or an unsupervised deep learning method to assist in characterizing the detected nodule/neoplasm as benign or malignant. The supervised learning method relies on a 3D CNN used with transfer learning and a graph regularized sparse MTL to determine malignancy. The unsupervised learning method uses clustering to generate labels after which label proportions are used with a novel algorithm to classify malignancy. The method assists radiologists in improving detection rates of lung nodules to facilitate early detection and minimizing errors in diagnosis.","['A61B5/055', 'A61B5/7267', 'A61B6/032', 'A61B6/037', 'A61B6/12', 'A61B6/4417', 'A61B6/5217', 'A61B8/481', 'A61B8/5223', 'G06F18/2148', 'G06F18/23', 'G06F18/2411', 'G06N20/10', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/0495', 'G06N3/08', 'G06N3/09', 'G06N3/096', 'G06N5/04', 'G06T7/0012', 'G06V10/764', 'G06V10/82', 'G16H30/40', 'G16H50/20', 'G16H70/60', 'A61B8/085', 'G06T2207/10081', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30064', 'G06T2207/30096', 'G06V2201/03', 'G06V2201/032']"
US7692664B2,Closed form method and system for matting a foreground object in an image having a background,"In a method and system for matting a foreground object F having an opacity α constrained by associating a characteristic with selected pixels in an image having a background B, weights are determined for all edges of neighboring pixels for the image and used to build a Laplacian matrix L. The equation α is solved where α=arg min αT Lα s.t.αi=si, ∀iεS, S is the group of selected pixels, and si is the value indicated by the associated characteristic. The equation Ii=αiFi+(1−αi)Bi is solved for F and B with additional smoothness assumptions on F and B; after which the foreground object F may be composited on a selected background B′ that may be the original background B or may be a different background, thus allowing foreground features to be extracted from the original image and copied to a different background.","['H04N5/272', 'H04N5/275']"
US8903167B2,Synthesizing training samples for object recognition,"An enhanced training sample set containing new synthesized training images that are artificially generated from an original training sample set is provided to satisfactorily increase the accuracy of an object recognition system. The original sample set is artificially augmented by introducing one or more variations to the original images with little to no human input. There are a large number of possible variations that can be introduced to the original images, such as varying the image's position, orientation, and/or appearance and varying an object's context, scale, and/or rotation. Because there are computational constraints on the amount of training samples that can be processed by object recognition systems, one or more variations that will lead to a satisfactory increase in the accuracy of the object recognition performance are identified and introduced to the original images.","['G06K9/6256', 'G06V10/772', 'G06F18/214', 'G06F18/28', 'G06K9/6255']"
CN109741280B,"Image processing method, image processing device, storage medium and electronic equipment","The embodiment of the application discloses an image processing method, an image processing device, a storage medium and electronic equipment. The method comprises the following steps: when the image to be processed comprises a human face, determining a human face contour in the image to be processed; dividing the image to be processed into at least two sub-images according to the face contour; determining the image processing mode of each sub-image, respectively carrying out image processing on the at least two sub-images according to each image processing mode, and carrying out image fusion on the at least two processed images to obtain the processed images. By adopting the technical scheme, the image to be processed is segmented by recognizing the face contour to obtain two or more sub-images, the proper image processing mode of each sub-image is determined, the image processing is carried out, the integral image processing of the image to be processed is avoided, and the image processing effect is improved.",[]
CN111325851B,"Image processing method and device, electronic equipment and computer readable storage medium","The embodiment of the disclosure provides an image processing method and device, electronic equipment and a computer readable storage medium. The method comprises the following steps: acquiring an image to be processed, wherein the image to be processed comprises an object to be processed; extracting features of the image to be processed to obtain the target key point position of the object to be processed; acquiring a training set, wherein the training set comprises a training sample with a target design style type, and the training sample comprises known key point positions and set slide bar parameters of a rendering engine corresponding to the target design style type; obtaining a weight coefficient of a training sample according to the target key point position of the object to be processed and the known key point position of the training sample; and obtaining target slide bar parameters of the object to be processed according to the weight coefficient of the training sample and the corresponding set slide bar parameters, and generating the virtual image of the object to be processed based on the target slide bar parameters. The scheme provided by the embodiment of the disclosure can adaptively migrate the design style of the training sample to the avatar of the image to be processed.","['G06T19/006', 'G06F18/214', 'G06F18/22', 'G06T19/20', 'G06T3/02', 'G06T7/10', 'G06V40/162', 'G06V40/171', 'G06T2207/30201', 'G06T2219/2024']"
US7706610B2,Segmentation of objects by minimizing global-local variational energy,"An “Image Segmenter” provides a variational energy formulation for segmentation of natural objects from images. In general, the Image Segmenter operates by adopting Gaussian mixture models (GMM) to capture the appearance variation of objects in one or more images. A global image data likelihood potential is then computed and combined with local region potentials to obtain a robust and accurate estimation of pixel foreground and background distributions. Iterative minimization of a “global-local energy function” is then accomplished by evolution of a foreground/background boundary curve by level set, and estimation of a foreground/background model by fixed-point iteration, termed “quasi-semi-supervised EM.” In various embodiments, this process is further improved by providing general object shape information for use in rectifying objects segmented from the image.","['G06T7/11', 'G06F18/2321', 'G06T7/143', 'G06T7/149', 'G06V10/28', 'G06V10/763', 'G06T2207/20116']"
US9247129B1,Self-portrait enhancement techniques,"Systems and approaches are provided for optimizing self-portraiture. The background of the self-portrait can be enhanced by image registration or stitching techniques of images captured using one or more conventional cameras. Multiple standard resolution images can be stitched together to generate a panoramic or a composite image of a higher resolution. Foreground elements, such as one or more representations of users, can also be enhanced in various ways. The representations of the users can be composited to exclude undesirable elements, such as image data of one of the users extending her arm to capture the self-portrait. An ideal pose of the users can automatically be selected and other image enhancements, such as histogram optimization, brightness and contrast optimization, color-cast correction, or reduction or removal of noise, can automatically be performed to minimize user effort in capturing self-portraits.","['H04N5/23222', 'H04N5/272', 'G06F16/5838', 'G06T11/60', 'G06T5/30', 'G06T7/11', 'G06T7/194', 'G06V40/16', 'H04N23/64', 'G06T2207/10004', 'G06T2207/20221', 'G06T2207/30201', 'H04N23/698']"
US12172310B2,Systems and methods for picking objects using 3-D geometry and segmentation,"A method for controlling a robotic system includes: capturing, by an imaging system, one or more images of a scene; computing, by a processing circuit including a processor and memory, one or more instance segmentation masks based on the one or more images, the one or more instance segmentation masks detecting one or more objects in the scene; computing, by the processing circuit, one or more pickability scores for the one or more objects; selecting, by the processing circuit, an object among the one or more objects based on the one or more pickability scores; computing, by the processing circuit, an object picking plan for the selected object; and outputting, by the processing circuit, the object picking plan to a controller configured to control an end effector of a robotic arm to pick the selected object.","['B25J9/1697', 'B25J9/1612', 'B25J19/023', 'B25J9/161', 'G06N3/08', 'G06T7/10', 'G06T7/50', 'G06T7/75', 'G05B2219/40053', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084']"
CN114596242A,"Defect detection method, apparatus, electronic device, and computer-readable storage medium","The application is applicable to the technical field of industrial automation, and provides a defect detection method, a defect detection device, electronic equipment and a computer-readable storage medium, wherein the method comprises the following steps: acquiring an image to be detected; inputting a to-be-detected image into a pre-stored attention to generate an anti-network for processing, and outputting a target image; a defective area in the target image is determined. The attention generation method is characterized in that an attention mechanism is introduced into an anti-network, so that foreground images and background images which are paid attention by the attention mechanism are obviously distinguished, the images to be detected are processed by the attention generation method through the anti-network, the obtained target images can be images for enhancing, modifying or removing the foreground images, the defects of the detected objects in the images to be detected are automatically detected by detecting the defect areas through the target images, the link of manual marking can be omitted, the detection efficiency is improved, and the labor cost is saved.","['G06T7/001', 'G06F18/22', 'G06T7/11', 'G06T7/194', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30148', 'Y02P90/30']"
US10733754B2,Generating a graphical user interface model from an image,"Techniques are disclosed for generating a GUI model for an application based upon requirements information for an application. The requirements information may include an image of a GUI screen (or multiple images of multiple screens) included in the GUI for the application. The GUI model generated based upon the input image may include information about the type and placement (layout) of GUI components in the GUI screen, and may also include information indicative of one or more functions to be assigned to one or more of the GUI components in the GUI screen. The GUI model may then be used to generate an implementation of the application GUI, including generating executable code that implements the GUI screen.","['G06T7/73', 'G06F8/35', 'G06F8/38']"
US11328173B2,Switchable propagation neural network,"A temporal propagation network (TPN) system learns the affinity matrix for video image processing tasks. An affinity matrix is a generic matrix that defines the similarity of two points in space. The TPN system includes a guidance neural network model and a temporal propagation module and is trained for a particular computer vision task to propagate visual properties from a key-frame represented by dense data (color), to another frame that is represented by coarse data (grey-scale). The guidance neural network model generates an affinity matrix referred to as a global transformation matrix from task-specific data for the key-frame and the other frame. The temporal propagation module applies the global transformation matrix to the key-frame property data to produce propagated property data (color) for the other frame. For example, the TPN system may be used to colorize several frames of greyscale video using a single manually colorized key-frame.","['G06V10/82', 'G06K9/6215', 'G06F18/214', 'G06F18/22', 'G06K9/6256', 'G06N3/04', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T5/003', 'G06T5/009', 'G06T5/50', 'G06T5/60', 'G06T5/73', 'G06T5/92', 'G06T7/10', 'G06T7/90', 'G06V10/454', 'G06V10/56', 'G06V20/46', 'G06N3/084', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20208']"
US20210118136A1,Artificial intelligence for personalized oncology,"Techniques performed by a data processing system for operating a personalized oncology system herein include accessing a first histopathological image of a histopathological slide of a sample taken from a first patient; analyzing the first histopathological image using a first machine learning model configured to extract first features from the first histopathological image; searching a histological database that includes a plurality of second histopathological images and corresponding clinical data for a plurality of second patients to generate search results; analyzing the plurality of third histopathological images and the corresponding clinical data associated with the plurality of third histopathological images using statistical analysis techniques to generate associated statistics and metrics associated with mortality, morbidity, time-to-event, or a combination thereof for the plurality of third patients associated with the third histopathological images; and presenting an interactive visual representation of the associated statistics and metrics including information for the personalized therapeutic plan for treating the first patient.","['G16H50/70', 'G02B21/365', 'G06F16/535', 'G06T7/0012', 'G06T7/0014', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/698', 'G16B20/20', 'G16B40/00', 'G16B50/00', 'G16H20/40', 'G16H50/20', 'G16H50/30', 'G02B21/34', 'G06F18/214', 'G06K9/00147', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024', 'G06T2207/30096', 'G16H10/60', 'G16H30/20', 'G16H30/40']"
CN111316291B,Segmenting and Denoising Depth Images with Generative Adversarial Neural Networks for Recognition Applications,"A method of removing noise from a depth image includes presenting real world depth images in real time to a first generated antagonistic neural network (GAN), the first GAN being trained by a composite image generated from Computer Aided Design (CAD) information of at least one object to be identified in the real world depth images. The first GAN subtracts the background in the real-world depth image and segments the foreground in the real-world depth image to generate a cleaned real-world depth image. Using the cleaned image, an object of interest in the real-world depth image may be identified via a first GAN trained from the composite image and the cleaned real-world depth image. In an embodiment, the cleaned real world depth image from the first GAN is provided to the second GAN, which provides additional noise cancellation and restoration of functionality removed by the first GAN.","['G06T5/70', 'G06F18/2413', 'G06F30/23', 'G06F30/27', 'G06N3/0464', 'G06N3/0475', 'G06N3/08', 'G06N3/09', 'G06N3/094', 'G06T7/11', 'G06T7/194', 'G06T7/55', 'G06V10/30', 'G06V10/764', 'G06V10/82', 'G06T2207/10028']"
US6674877B1,System and method for visually tracking occluded objects in real time,"The present invention is embodied in a system and method for digitally tracking objects in real time. The present invention visually tracks three-dimensional (3-D) objects in dense disparity maps in real time. Tracking of the human body is achieved by digitally segmenting and modeling different body parts using statistical models defined by multiple size parameters, position and orientation. In addition, the present invention is embodied in a system and method for recognizing mutual occlusions of body parts and filling in data for the occluded parts while tracking a human body. The body parts are preferably tracked from frame to frame in image sequences as an articulated structure in which the body parts are connected at the joints instead of as individual objects moving and changing shape and orientation freely.","['G06T7/251', 'G06V10/24', 'G06V40/103', 'G06T2207/10016', 'G06T2207/30196']"
US7864989B2,Method and apparatus for adaptive context-aided human classification,"A method and an apparatus process digital images. The method according to one embodiment accesses digital data representing a plurality of digital images including a plurality of persons; performs face recognition to determine first scores relating to similarity between faces of the plurality of persons; performs clothes recognition to determine second scores relating to similarity between clothes of the plurality of persons; provides a plurality of formulas for estimating a probability of a face from the faces and a clothes from the clothes to belong to a person from the plurality of persons, wherein at least one formula of the plurality of formulas utilizes a first score and a second score, and at least one formula of the plurality of formulas utilizes only one score of a first score and a second score; and selects a formula from the plurality of formulas based on availability of a first score from the first scores for two persons from the plurality of persons, and availability of a second score from the second scores for the two persons, the selected formula estimating a probability relating to similarity of identities of the two persons.","['G06V40/103', 'G06F18/2323', 'G06F18/24155', 'G06V10/7635', 'G06V10/764']"
US11037010B2,Compositional model for text recognition,"Embodiments relate to a two-stage end-to-end text recognition system. The text recognition system includes a text detection stage and a text recognition stage. Images inputted to the text recognition system are provided to both the text detection stage and to the text recognition stage. The text detection stage detects text regions in the images and provides the detected regions to the text recognition stage. The text recognition stage is trained to perform geometric rectification on the text regions using the images. There is end-to-end alignment between the text detection stage and the text recognition stage. Additionally, the text detection stage and text recognition stage are each trained independent of the other.","['G06K9/03', 'G06V10/82', 'G06F18/2148', 'G06F18/24', 'G06K9/2054', 'G06K9/325', 'G06K9/6257', 'G06K9/6267', 'G06T5/002', 'G06T5/006', 'G06T5/70', 'G06T5/80', 'G06V20/62', 'G06V30/18057', 'G06K2209/01', 'G06T2207/20081', 'G06T2207/20084', 'G06V30/10']"
CN106290392A,A kind of little micro-bearing surface pitting defects online test method and system thereof,"The invention belongs to bearing surface defect detection field, especially relate to a kind of little micro-bearing surface pitting defects online test method and system thereof.It is an object of the invention to break through the deficiency of existing detection method, it is according to the feature gathering bearing image, realize micro bearing external defects based on ccd image identification technology and automatically detect screening, it is according to the feature of the collection bearing image of the different parts adopted, design one and can detect bearing both ends of the surface by parallelism recognition, bearing top circle surface and little micro-bearing surface pitting defects on-line detecting system of end face image character, bearing different parts can be carried out parallel parsing detection simultaneously, treatment effeciency is high, micro bearing surface defects detection operation is made to realize automatization, with the method substituting the commonly used manual detection of current enterprise, improve precision and efficiency of detecting, make defect recognition rate, the clearance of substandard product reaches higher level, improve product quality, improve the credit worthiness of enterprise, improve the level of modernization of enterprise.","['G01N21/95', 'G01N21/8851', 'G01N21/952', 'G01N2021/8887']"
US12288414B2,Systems and methods for performing fingerprint based user authentication using imagery captured using mobile devices,"Technologies are presented herein in support of a system and method for performing fingerprint recognition. Embodiments of the present invention concern a system and method for capturing a user's biometric features and generating an identifier characterizing the user's biometric features using a mobile device such as a smartphone. The biometric identifier is generated using imagery captured of a plurality of fingers of a user for the purposes of authenticating/identifying the user according to the captured biometrics and determining the user's liveness. The present disclosure also describes additional techniques for preventing erroneous authentication caused by spoofing. In some examples, the anti-spoofing techniques may include capturing one or more images of a user's fingers and analyzing the captured images for indications of liveness.","['G06V40/1371', 'G06F21/32', 'G06V10/32', 'G06V10/82', 'G06V30/194', 'G06V40/11', 'G06V40/13', 'G06V40/1353', 'G06V40/1359', 'G06V40/1388', 'G06V40/70']"
CN114581662B,"Brain tumor image segmentation method, system, device and storage medium","The invention discloses a brain tumor image segmentation method, a brain tumor image segmentation system, a brain tumor image segmentation device and a brain tumor image storage medium, wherein the brain tumor image segmentation method comprises the following steps: preprocessing brain tumor images and labels and carrying out data amplification; convoluting and downsampling the brain tumor image, extracting context semantic information in the brain tumor image, and obtaining a feature map; upsampling the feature map, and feature fusion is carried out on the upsampled map and features in the same level encoder module; the feature graphs are aggregated through a feature pyramid fusion module and input into a learning global context information in an expected maximization self-attention module; and aggregating the features and the maximum level feature graphs to obtain a final semantic segmentation result. The invention is based on a multi-scale channel attention mechanism, extracts the characteristics and performs characteristic fusion, adopts a characteristic pyramid and a desired maximization attention mechanism to extract global context information, improves the precision of semantic segmentation, and can be widely applied to the fields of computer vision and image processing.","['G06F18/253', 'G06N3/045', 'G06N3/048', 'G06N3/08']"
US7577314B2,Method and apparatus for generating a panorama background from a set of images,A method of generating a panorama background from a set of images comprises dividing each of the images into blocks and comparing color-space features of corresponding blocks of the images to select background blocks. The selected background blocks are merged to generate the panorama background. An apparatus and computer readable medium embodying a computer program for generating a panorama background are also provided.,"['G06T3/4038', 'G06T7/11', 'G06T7/187', 'G06T7/194', 'G06T7/254', 'G06T7/33', 'G06T7/90', 'G06V10/50', 'G06V10/56', 'G06V20/00', 'G06T2207/10016', 'G06T2207/20021']"
US8645819B2,Detection and extraction of elements constituting images in unstructured document files,"A method and a system for detecting and extracting images in an electronic document are disclosed. The method includes receiving an electronic document and identifying elements of a page. The identified elements include a set of graphical elements and a set of text elements. The method may include identifying and excluding elements which serve as graphical page constructs and/or text formatting elements. The page can then be segmented, based on (remaining) graphical elements and identified white spaces, to generate a set of image blocks. Text elements that are associated with a respective image block are identified as captions. Overlapping candidate images are then grouped to form a new image. The new image can thus include candidate images which would, without the identification of their caption(s), each be treated as a respective image.",['G06F40/103']
US11880939B2,Embedding complex 3D objects into an augmented reality scene using image segmentation,"Techniques related to embedding a 3D object model within a 3D scene are discussed. Such techniques include determining two or more object mask images for two or more corresponding cameras trained on the 3D scene, projecting 3D points from the 3D object model to the image planes of the two or more cameras, and determining a position and orientation of the 3D object model in the scene using the object mask images and the projected 3D points.","['G06T7/73', 'G06T17/20', 'G06T15/20', 'G06T19/006', 'G06T7/11', 'G06T7/75', 'G06T2207/10012', 'G06T2207/20084', 'G06T2207/30196', 'G06T2207/30244']"
US8150098B2,Grouping images by location,"A method of grouping images captured in a common location, including receiving a collection of images; classifying the images into a set of events, where each image in the collection belongs to no more than one event; analyzing background region(s) of images from each event to determine one or more features that represent the event; and comparing features from at least two events to determine which events occurred in a common location.","['G06F16/583', 'G06F16/58', 'G06V10/757']"
US11586336B2,Private control interfaces for extended reality,"Systems, methods, and non-transitory media are provided for generating private control interfaces for extended reality (XR) experiences. An example method can include determining a pose of an XR device within a mapped scene of a physical environment associated with the XR device; detecting a private region in the physical environment and a location of the private region relative to the pose of the XR device, the private region including an area estimated to be within a field of view (FOV) of a user of the XR device and out of a FOV of a person in the physical environment, a recording device in the physical environment, and/or an object in the physical environment; based on the pose of the XR device and the location of the private region, mapping a virtual private control interface to the private region; and rendering the virtual private control interface within the private region.","['G06F3/04815', 'G02B27/0093', 'G02B27/017', 'G06F21/32', 'G06F21/604', 'G06F21/6254', 'G06F21/84', 'G06F3/011', 'G06F3/014', 'G06F3/017', 'G06F3/0304', 'G06F3/0421', 'G06F3/0426', 'G06T19/006', 'G06V20/20', 'G02B2027/0138', 'G06F2203/04101', 'G06T2200/24', 'G06T2219/024']"
CN111612008B,Image segmentation method based on convolution network,"The invention discloses an image segmentation method based on a convolution network. The invention discloses an image segmentation method based on a convolution network, which comprises the following steps: step 1: preprocessing data; step 2: design of convolutional network model the convolutional network is called LBNet network, and is mainly improved based on the ene network; step 3: model training and verification; step 4: model optimization and improvement processing, namely continuously adjusting the super parameters of the model according to the measurement result on the test set in the step 3, and realizing parameter optimization on the convolutional network model established in the step 2; step 5: and (3) using a model, and testing according to the finally optimized model obtained in the step (4). The invention has the beneficial effects that: the invention provides an image segmentation method and a process based on a convolution network, wherein the convolution network is improved by taking an ENT network as a main network, and the original ENT network structure is modified in the implementation process.","['G06V10/267', 'G06F18/214', 'G06F18/253', 'G06N3/045', 'Y02T10/40']"
TWI754741B,System and method for detecting white spot or white spot mura defects in display panel and method for training the system,"A method for detecting one or more defects such as a white spot MURA defect of an image in a display panel includes receiving the image of the display panel, dividing the image into a plurality of patches, each one of the plurality of patches corresponding to an m pixel by n pixel area of the image (wherein m and n are integers greater than or equal to one), generating a plurality of feature vectors for the plurality of patches, each of the plurality of feature vectors corresponding to one of the plurality of patches and including one or more image texture features and one or more image moment features, and classifying each one of the plurality of patches based on a respective one of the plurality of feature vectors by utilizing a multi-class support vector machine to detect the one or more defects.","['G01N21/88', 'G09G3/006', 'G06F18/2411', 'G06N20/10', 'G06T7/0004', 'G06T7/45', 'G09G2330/08', 'G09G2330/10', 'G09G2354/00', 'G09G2360/14', 'G09G2360/145']"
US8538117B2,Accurate pelvic fracture detection for X-ray and CT images,"Accurate pelvic fracture detection is accomplished with automated X-ray and Computed Tomography (CT) images for diagnosis and recommended therapy. The system combines computational methods to process images from two different modalities, using Active Shape Model (ASM), spline interpolation, active contours, and wavelet transform. By processing both X-ray and CT images, features which may be visible under one modality and not under the other are extracted and validates and confirms information visible in both. The X-ray component uses hierarchical approach based on directed Hough Transform to detect pelvic structures, removing the need for manual initialization. The X-ray component uses cubic spline interpolation to regulate ASM deformation during X-ray image segmentation. Key regions of the pelvis are first segmented and identified, allowing detection methods to be specialized to each structure using anatomical knowledge. The CT processing component is able to distinguish bone from other non-bone objects with similar visual characteristics, such a blood and contrast fluid, permitting detection and quantification of soft tissue hemorrhage. The CT processing component draws attention to slices where irregularities are detected, reducing the time to fully examine a pelvic CT scan. The quantitative measurement of bone displacement and hemorrhage area are used as input for a trauma decision-support system, along with physiological signals, injury details and demographic information.","['G06T7/0012', 'A61B6/032', 'A61B6/505', 'A61B6/5217', 'A61B6/5235', 'G06T7/11', 'G16H50/30', 'G06T2207/10081', 'G06T2207/10116', 'G06T2207/20212', 'G06T2207/30008']"
US10509954B2,Method and system of image segmentation refinement for image processing,"A system, article, and method of image segmentation refinement for image processing.","['G06K9/00362', 'G06T7/12', 'G06K9/6207', 'G06T7/149', 'G06T7/194', 'G06V10/755', 'G06V40/10', 'G06T2207/20116']"
US7822274B2,Banded graph cut segmentation algorithms with laplacian pyramids,"A process for segmenting an object of interest from background, comprising: obtaining a master, high-resolution image of an object disposed within a background; applying a first band graph cut process to the master image generating a second image with the object being segmented from the background to a first approximation; and, comparing the second image with the master image to produce a comparison image with pixels identified by the comparison to be background images being removed from the comparison image.","['G06T7/11', 'G06F18/2323', 'G06T7/162', 'G06T7/194', 'G06V10/267', 'G06V10/7635', 'G06T2207/20016', 'G06T2207/20072', 'G06T2207/20101']"
US6134343A,System or method for detecting defect within a semi-opaque enclosure,"A method and system detect for the presence of a defect or a non-conforming object of unknown shape, configuration, and location within a semi-opaque enclosure. The semi-opaque enclosure has a pattern which is visible on at least a first of its external surfaces. The semi-opaque enclosure may comprise, for example, a labeled plastic or glass bottle, and the visible pattern may be print and graphical information provided on the bottle's label. A first digital image is captured from first channel light reflected off the first external surface of the enclosure; and a second digital image is captured from second channel light navigating the object inside the enclosure and emanating from the first external surface of the enclosure. A difference image is formed substantially devoid of information representing the visible pattern on the first external surface, by subtracting one of the first and second digital images from the other. Additional segmentation processing may be performed on the difference image in order to better identify the presence of a defect or non-conforming object within the semi-opaque enclosure.","['G06T7/0004', 'G01N21/88', 'G01N21/909', 'G06T7/70']"
US10977827B2,Multiview estimation of 6D pose,The accuracy of that detection and estimation can be iteratively improved by retraining the CNN with increasingly hard ground truth examples. The additional images are detected and annotated by an automatic process of pose estimation and verification.,"['G06T15/20', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/75', 'G06T2200/04', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221']"
CN112949398B,"Lane line detection method, ranging method and corresponding device","The application relates to the technical field of image processing, and provides a lane line detection method, a distance measurement method and a corresponding device. The lane line detection method comprises the following steps: determining an interested area image containing a lane line from a road image acquired by a vehicle-mounted camera; processing the region of interest image by utilizing a semantic segmentation network, obtaining a segmentation mask of a lane line in the image, and converting the segmentation mask into a corresponding top view; determining a lane line pixel point set corresponding to different lane lines in the top view, and transforming coordinates of the lane line pixel points in the set from coordinates in the top view to coordinates in a road image or coordinates in a world coordinate system; and forming a lane line equation according to the lane line pixel point set fitting after the coordinate transformation. The lane line detection method is beneficial to improving the accuracy of lane line detection.","['G06V20/588', 'G06F18/253', 'G06N3/045', 'G06N3/08', 'G06V10/25', 'Y02T10/40']"
CN115240121B,Joint modeling method and device for enhancing local features of pedestrians,"The invention discloses a joint modeling method and a joint modeling device for enhancing local characteristics of pedestrians, wherein the joint modeling method comprises the following steps: s1: acquiring an original monitoring video image data set, and dividing the original monitoring video image data set into a training set and a test set according to a proportion; s2: cutting a monitoring video image training set to obtain an image block vector sequence; the method adopts a multi-head attention neural network to extract the pedestrian local features of the video image, learns image channel weight parameters by using a channel convolution kernel, scans the spatial features on the image by utilizing the spatial convolution, enhances the pedestrian local features and improves the pedestrian recognition rate, adopts a feedforward neural network and an activation function, inputs the feedforward neural network to pass through linear layer transformation, adopts the activation function to map pedestrian probability distribution into classes, recognizes the pedestrians, outputs position coordinates of the pedestrians in the image and selects the pedestrians in a frame, realizes pedestrian re-recognition, and enables available face images to be obtained.","['G06V20/41', 'G06N3/08', 'G06V10/44', 'G06V10/82', 'G06V20/46', 'G06V20/49', 'G06V20/52', 'G06V40/10']"
CN113806613A,"Training image set generation method and device, computer equipment and storage medium","The invention relates to the technical field of artificial intelligence, and discloses a training image set generation method, a training image set generation device, training image set generation equipment and a training image set generation medium, wherein the training image set generation method comprises the following steps: acquiring a to-be-generated image set and a generation quantity; crawling historical tags similar to the target tags and the target description by using a text similarity technology, and determining the category to be migrated; acquiring a corresponding model to be migrated, identifying each image to be generated, acquiring a target area corresponding to each image to be generated, and recording the target area as an image to be processed; carrying out image enhancement processing based on factor enhancement proportion on each image to be processed to generate a training image; and associating each training image; and determining all images to be processed and training images as a training image set. Therefore, the invention realizes the automatic generation of the training image set based on the zero-label small image set to be generated, and improves the accuracy and efficiency of the generation of the training image set. The method is suitable for the field of artificial intelligence, and can further promote the construction of smart cities.","['G06F16/951', 'G06F18/214', 'G06F40/194', 'G06N3/04', 'G06N3/08', 'G06T3/60', 'G06T5/90', 'G06T7/11', 'G06T7/12', 'G06T7/13', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30176', 'Y02D10/00']"
CN110765951B,Aircraft Target Detection Method Based on Bounding Box Correction Algorithm in Remote Sensing Image,"The invention provides a remote sensing image airplane target detection method based on a boundary box correction algorithm, which solves the problems of inaccuracy of a detection boundary box caused by directly selecting a highest-score boundary box by a traditional non-maximum inhibition algorithm and false alarm caused by multi-box labeling of a segmented airplane during detection. The method comprises the following steps: generating a training and testing sample set; preprocessing a training sample; overlapping and slicing the test sample to obtain a pretreated test sample; training an airplane target detection model; detecting an aircraft target in the slice; carrying out boundary frame correction on the detection result in the test sample by using a boundary frame correction method; and generating an airplane target detection result graph. The method comprises the steps of performing overlapping correction on intersecting boundary frames on the same target by using an overlapping correction method to obtain more accurate detection frames; and performing fusion improvement on parallel adjacent bounding boxes on the same target by using an adjacent fusion method, and removing multi-box false alarms on the same target from the detection result. The method is used for detecting the airplane target by using the remote sensing image.","['G06V20/13', 'G06F18/214']"
CN109214282B,A kind of three-dimension gesture critical point detection method and system neural network based,"The invention discloses a kind of three-dimension gesture critical point detection methods neural network based, comprising the following steps: obtains the gesture data collection comprising gesture area information and gesture two dimension, three-dimensional key point location information；Training Hand Gesture Segmentation network, the network are input with the RGB image comprising gesture, can detecte out the gesture area in RGB image；The gesture area that Hand Gesture Segmentation network detects is intercepted, is up-sampled or down-sampling；Training two dimension gesture critical point detection network, the network can detecte out multiple two-dimentional gesture key points in gesture area image；The absolute coordinate of three-dimension gesture key point is converted into relative coordinate；Multiple two-dimentional gesture key points can be mapped in three-dimensional space by the gesture key point mapping network of training 2 d-to-3 d, the network, form three-dimension gesture key point.The present invention quickly and effectively can accurately detect three-dimension gesture key point from the RGB image comprising gesture.","['G06V20/64', 'G06N3/045', 'G06V40/28']"
US7340077B2,Gesture recognition system using depth perceptive sensors,"Three-dimensional position information is used to identify the gesture created by a body part of interest. At one or more instances of an interval, the posture of a body part is recognized, based on the shape of the body part and its position and orientation. The posture of the body part over each of the one or more instances in the interval are recognized as a combined gesture. The gesture is classified for determining an input into a related electronic device.","['G06F3/017', 'G06V40/10', 'G06V40/11', 'G06V40/20', 'A63F2300/1093', 'H04L2012/2841']"
US10846566B2,Method and system for multi-scale cell image segmentation using multiple parallel convolutional neural networks,"An artificial neural network system for image classification, formed of multiple independent individual convolutional neural networks (CNNs), each CNN being configured to process an input image patch to calculate a classification for the center pixel of the patch. The multiple CNNs have different receptive field of views for processing image patches of different sizes centered at the same pixel. A final classification for the center pixel is calculated by combining the classification results from the multiple CNNs. An image patch generator is provided to generate the multiple input image patches of different sizes by cropping them from the original input image. The multiple CNNs have similar configurations, and when training the artificial neural network system, one CNN is trained first, and the learned parameters are transferred to another CNN as initial parameters and the other CNN is further trained. The classification includes three classes, namely background, foreground, and edge.","['G06K9/6256', 'G06V10/82', 'G06F18/214', 'G06F18/241', 'G06F18/24143', 'G06F18/254', 'G06K9/4628', 'G06K9/6268', 'G06K9/6274', 'G06K9/6292', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/09', 'G06N3/096', 'G06T3/4046', 'G06V10/454', 'G06V10/764', 'G06V10/809']"
CN112528878B,"Method and device for detecting lane line, terminal equipment and readable storage medium","The application is applicable to the technical fields of computer vision and image processing, and provides a method, a device, terminal equipment and a readable storage medium for detecting lane lines, wherein the method comprises the following steps: acquiring a road image of a current scene; inputting the road image into a trained neural network model for processing, and outputting a detection result of a lane line in the road image of the current scene; the trained neural network model is obtained by training according to sample images in a training set and a semantic segmentation model, wherein the sample images in the training set comprise collected road images of a plurality of scenes and marker images corresponding to the road images of the scenes. The method and the device can solve the problems that most of current deep learning models for lane recognition are large in calculated amount, complex in model and unfavorable for real-time requirements in actual application scenes of automatic driving tasks.","['G06V20/588', 'G06F18/214', 'G06F18/24', 'G06N3/045', 'G06V10/40', 'Y02T10/40']"
US12367260B2,Multimodality image processing techniques for training image data generation and usage thereof for developing mono-modality image inferencing models,"Techniques are described for generating mono-modality training image data from multi-modality image data and using the mono-modality training image data to train and develop mono-modality image inferencing models. A method embodiment comprises generating, by a system comprising a processor, a synthetic 2D image from a 3D image of a first capture modality, wherein the synthetic 2D image corresponds to a 2D version of the 3D image in a second capture modality, and wherein the 3D image and the synthetic 2D image depict a same anatomical region of a same patient. The method further comprises transferring, by the system, ground truth data for the 3D image to the synthetic 2D image. In some embodiments, the method further comprises employing the synthetic 2D image to facilitate transfer of the ground truth data to a native 2D image captured of the same anatomical region of the same patient using the second capture modality.","['G06F18/214', 'G06T15/10', 'A61B5/055', 'A61B5/7267', 'A61B6/032', 'A61B6/5223', 'G06F18/2178', 'G06F18/22', 'G06F18/28', 'G06N3/045', 'G06N3/0464', 'G06N3/0475', 'G06N3/09', 'G06N3/094', 'G06N5/04', 'G06T5/50', 'G06T7/30', 'G06V10/7715', 'G06V10/772', 'G06V10/774', 'G06V20/64', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'G16H50/50', 'G16H50/70', 'G06T2200/04', 'G06T2207/10081', 'G06T2207/10116', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20212', 'G06T2207/30004', 'G06T2210/41', 'G06V2201/03']"
US4837842A,Character and pattern recognition machine and method,"A pattern recognition system, particularly a handprint character recognition system in which electrical binary black/white ""image"" of one or more handprinted characters is formed and a plurality of centers of recognition (CORs) within said binary black/white images are selected as reference points for measuring the characteristic enclave of the black/white image immediately surrounding the CORs. A library of templates of said measurements around the CORs for a plurality of known exemplary character images is stored in a memory for comparison with corresponding measurements made around the CORs of images whose class is unknown to produce ""template scores"" proportional to the similarity of the enclaves of the known image to the enclaves measured by templates. The generic shape of a character is expressed as a ""character equation"" involving template scores developed on an unknown image, and each character equation is evaluated including comparing the values of such equations, and selecting the best value to determine the generic name of the unknown character.",['G06V30/191']
US11417124B2,System for real-time automated segmentation and recognition of vehicle's license plates characters from vehicle's image and a method thereof,The present invention discloses a system for automated vehicles license plates characters segmentation and recognition comprising an imaging processor connected to at least one image grabber module or camera. The image grabber module captures images of the vehicles and forwards it to said connected imaging processor and the imaging processor segments and recognizes the vehicles license plates character region including the region with deformed license plates characters in the captured vehicle images by involving binarization of maximally stable external regions corresponding to probable license plate region in the captured vehicle images.,"['G06V20/63', 'G06T3/608', 'G06T7/11', 'G06T7/136', 'G06T7/155', 'G06V10/273', 'G06V10/28', 'G06V10/30', 'G06V30/1478', 'G06V30/414', 'G06T2207/30248', 'G06T2207/30252', 'G06V20/625']"
CN105574534B,Conspicuousness object detection method based on sparse subspace clustering and low-rank representation,"The invention discloses a kind of conspicuousness object detection method based on sparse subspace clustering and low-rank representation.It the steps include: 1, super-pixel segmentation and cluster carried out to input picture；2, color, texture and the edge feature of each super-pixel in cluster are extracted, cluster feature matrix is constructed；3, all super-pixel features are ranked up according to the size of color contrast, construct dictionary；4, joint low-rank representation model is constructed according to dictionary, solves the model and the eigenmatrix of cluster is decomposed to obtain low-rank representation coefficient, and calculates the Significance factors of cluster；5, each saliency value clustered is mapped in input picture according to its spatial position, obtains the notable figure of input picture.The present invention completely can consistently detect larger-size conspicuousness target in image, and can inhibit the noise in background, promote the robustness of conspicuousness target detection in complex background image.It can be used for image segmentation, target identification, image recovery and adapting to image compression.","['G06V10/462', 'G06T2207/10004', 'G06T2207/10024']"
WO2020047738A1,Automatic pest counting method based on combination of multi-scale feature fusion network and positioning model,"An automatic pest counting method based on the combination of a multi-scale feature fusion network and a positioning model, comprising the following steps: step 1, establishment of training samples: obtaining several pest images in a natural field environment as training images, and marking pests in the images to obtain training samples; step 2, construction of a pest detection and counting model: constructing a positioning model and a multi-scale feature fusion network, extracting a candidate region of the training samples by using the positioning model, extracting features in the candidate region by means of the multi-scale feature fusion network and then classifying the features, and outputting coordinate values in corresponding images; step 3, obtaining of images to be counted: obtaining pest images captured in the field and preprocessing said images to obtain images to be counted; and step 4, obtaining of the number of pests: inputting the images to be counted into the pest detection and counting model to obtain the number of pests in the images. The method can precisely determine the number and specific locations of pests, and has high robustness and accuracy.","['G06F18/2411', 'G06F18/214', 'G06F18/253', 'G06V10/40', 'G06V10/56']"
CN109948510B,Document image instance segmentation method and device,"The invention discloses a document image instance segmentation method and device, and belongs to the field of computer vision. The method comprises the following steps: inputting an original image into a full convolution neural network, wherein the original image comprises n classes of example objects, and n is larger than or equal to 1; outputting a character direction score map, n example object score maps and corresponding pixel link maps of the original image; segmenting n example object region outlines according to the n example object score images and the pixel link images, and describing example objects; and outputting the example object segmentation result, including the category and the position of the example object. The method greatly improves the speed of document image layout analysis and greatly improves the precision of document image layout analysis by segmenting and concentrating a plurality of object examples in a neural network.",[]
US20210365699A1,Geometry-aware instance segmentation in stereo image capture processes,"A system detects multiple instances of an object in a digital image by receiving a two-dimensional (2D) image that includes a plurality of instances of an object in an environment. For example, the system may receive the 2D image from a camera or other sensing modality of an autonomous vehicle (AV). The system uses a first object detection network to generate a plurality of predicted object instances in the image. The system then receives a data set that comprises depth information corresponding to the plurality of instances of the object in the environment. The data set may be received, for example, from a stereo camera of an AV, and the depth information may be in the form of a disparity map. The system may use the depth information to identify an individual instance from the plurality of predicted object instances in the image.","['G06K9/00805', 'G06T7/11', 'B60R1/00', 'G06T7/174', 'G06T7/593', 'G06V10/25', 'G06V10/82', 'G06V20/58', 'G06V20/64', 'B60R2300/107', 'G06T2207/10004', 'G06T2207/10012', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30261']"
US9342930B1,Information aggregation for recognized locations,"An image or video of a location can be captured using an electronic device. A physical location associated with the image or video can be determined by analyzing the image data, and information about the physical location can be presented to a user of the electronic device. Images or video may include multiple locations, and information about each of the multiple locations can be presented to the user or a single location can be automatically determined and information about the single location can be displayed. Data from other sensors of the electronic device, such as GPS, digital compasses, accelerometers, gyroscopes, barometers, or altimeters, can also be acquired and combined with image analysis to determine a location. The information presented to the user regarding the recognized location may be based on user preferences or user behavior of the specific user of the electronic device, or a community of users who share demographic traits with the specific user.","['G06Q30/0261', 'G06T19/006', 'G06F1/1643', 'G06F1/1686', 'G06F1/1694', 'G06F16/434', 'G06F3/013', 'G06F3/04842', 'G06F3/0487', 'G06F3/0488', 'G06F3/147', 'G06Q30/0267', 'G06Q30/0271', 'G06T11/60', 'G06T19/00', 'G09G5/14', 'G06F2203/04806']"
US10540055B2,Generating interactive content items based on content displayed on a computing device,"Methods and apparatus directed to generating one or more interactive content items based on content displayed on a computing device are described herein. In various implementations, content displayed to a user by a display of a computing device may be segmented into semantic region(s) based on respective content of the semantic region(s). User input may be received, e.g., via a biometric sensor of the computing device. The user input may indicate a desire of the user to act upon the semantic region(s). In response to the user input, interactive content item(s) corresponding to the semantic region(s) may be generated and provided for presentation to the user via output device(s) of the computing device. User interaction with a given interactive content item may cause the computing device to perform action(s) that are tailored to the semantic region that corresponds to the given interactive content item.","['G06F3/04817', 'G06F16/434', 'G06F16/9032', 'G06F3/0484', 'G06F40/166', 'G06V30/413', 'G06F2203/04803', 'G06V10/267']"
AU2020101581A4,Lymph node metastases detection from ct images using deep learning,"LYMPH NODE METASTASES DETECTION FROM CT IMAGES USING DEEP LEARNING ABSTRACT Due to advances in roller medical equipment and reduced costs in information storage, the absolute digital technology of the anatomical analysis of contaminated, infected tissue of computed tomography is becoming possible in recent decades. The current growth of online diagnosis includes online diagnoses, prompt access to oral history incidents, and faster communication with specialist surgeons. Detection of the existence of lymph node metastases is crucial to clinical pacing, diagnosis, and medication advice in females with cervical cancer. This proposal promotes a novel technique to detect the lymph node metastases of breast cancer using deep learning models to improve diagnostic accuracy and efficiency. The Deep Active Lesion Segmentation (DALS), a fully automatic segmentation mechanism without human intervention that optimizes a healthy non - linear extraction of features to obtain ultimately before Convolutional Neural Networks (CNNs). The convolutional network called inception v4 is proposed for generating the feature maps, and a deep neural network model like autoencoder is employed for the detection process. This invention offers essential treatment information of individual cancer patient's diagnoses and health care, and deep learning method provides excellent prediction capability and lymph node detection accuracy. 1| P a g e LYMPH NODE METASTASES DETECTION FROM CT IMAGES USING DEEP LEARNING Drawings CT scanner preprocessed automated segmented imag< Inception v4 for feature maps image segmentation feature extraction meatssscore ~ ~ 2? 1'e~ Figure 1: Lymph node detection using DNN - ----------- fl - - --- Figure 2: Inception v4 Architecture for feature extraction 11 P a g e","['A61B6/5217', 'A61B5/418', 'A61B5/7203', 'A61B6/03', 'G06N20/00', 'G06N3/0464', 'G06T7/0012', 'G06T7/10', 'A61B2576/02', 'A61B5/7275', 'G06Q50/20', 'G06T2207/10081', 'G06T2207/20036', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30068']"
CN116597168A,"Matching method, device, equipment and medium of vehicle-mounted laser point cloud and panoramic image","The invention provides a matching method, a device, equipment and a medium of a vehicle-mounted laser point cloud and a panoramic image, and relates to the technical field of visual positioning, wherein the method comprises the following steps: acquiring first vehicle-mounted laser point cloud data and a panoramic image; performing downsampling processing on the first vehicle-mounted laser point cloud data to obtain second vehicle-mounted laser point cloud data; projecting the second vehicle-mounted laser point cloud data into a panoramic image, and filtering out target projection pixels projected into the panoramic image; and determining the corresponding relation between the second point cloud point in the second vehicle-mounted laser point cloud data and the pixel point in the panoramic image, and completing the matching of the first vehicle-mounted laser point cloud data and the panoramic image. According to the invention, the calculation burden of projecting the point cloud data to the panoramic image can be reduced, and the obtained matching result can truly reflect the corresponding relation between the pixels of the panoramic image and the point cloud, so that the accuracy of visual positioning can be improved.","['G06V10/443', 'G06V10/26', 'G06V20/58', 'Y02T10/40']"
CN113590796B,Training method and device for ranking model and electronic equipment,"The invention discloses a training method and device for a sequencing model and electronic equipment, and relates to the technical fields of natural language processing, intelligent searching and the like. The specific implementation scheme is as follows: when training the sequencing model, firstly acquiring a plurality of first sample pairs and label information thereof; inputting a first search text, a first title text of a first candidate text and a first target abstract corresponding to the first candidate text into an initial language model aiming at each first sample pair to obtain a second relevance score corresponding to the first sample pair; therefore, the first candidate text is replaced by the first target abstract to participate in training of the ranking model, and the network parameters of the initial language model are updated according to the label information and the second relevance score of each first sample pair.","['G06F16/3329', 'G06F40/30', 'G06F16/313', 'G06F16/3347', 'G06F16/338', 'G06F18/2113', 'G06F18/214', 'G06F40/284', 'G06N20/00', 'Y02D10/00']"
CN111178236B,Parking space detection method based on deep learning,"The invention discloses a parking space detection method based on deep learning, which comprises the steps of shooting images by a plurality of fish-eye cameras on a vehicle, and splicing the images into 360-degree looking-around images after deformation correction; firstly, detecting a parking space in an image of a look-around map through a target detection network for deep learning training; intercepting the detected parking spaces, inputting the intercepted parking spaces into an image segmentation network for deep learning training, and segmenting out parking space lines of the intercepted parking spaces to obtain masks corresponding to images of the intercepted parking spaces; and obtaining four corner coordinates of the parking space on the output mask through image processing and logic processing, and mapping the four corner coordinates back to the original image, thereby detecting the position of the parking space. The parking space detection method provides accurate parking space position information for an automatic parking system; the influence of inaccurate detection caused by other noise in the parking space detection process is reduced, and the robustness of a parking space detection system is improved; under the condition of ensuring the parking space detection accuracy, the parking space detection speed is improved as much as possible.","['G06V20/586', 'G06V10/267', 'G06V10/44', 'Y02T10/40']"
CN116051730A,"Method, device and equipment for constructing three-dimensional blood vessel model","The application discloses a method, a device and equipment for constructing a three-dimensional blood vessel model, and belongs to the technical field of computers. The method comprises the following steps: acquiring a first initial blood vessel center line in a first blood vessel image and a second initial blood vessel center line in a second blood vessel image; acquiring a first target blood vessel central line corresponding to the first initial blood vessel central line and a second target blood vessel central line corresponding to the second initial blood vessel central line; determining a central line of a blood vessel to be updated passing through a target space point; iteratively updating the blood vessel centerline to be updated based on the first target blood vessel centerline and the second target blood vessel centerline; taking a blood vessel central line obtained when the iteration update termination condition is met as a three-dimensional blood vessel central line; based on the three-dimensional blood vessel center line, constructing a three-dimensional blood vessel model corresponding to the blood vessel. The method is beneficial to improving the accuracy of the constructed three-dimensional blood vessel model.","['G06T17/00', 'G06T7/12', 'G06T7/13', 'G06T7/66', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/30101']"
CN114419327A,Image detection method and training method and device of image detection model,"The disclosure provides an image detection method, an image detection model training device, electronic equipment and a storage medium, and relates to the field of artificial intelligence, in particular to the field of deep learning and the field of computer vision. The image detection method is specifically realized in the following manner: extracting the features of an image to be processed to obtain a plurality of image features under a plurality of scales, wherein each image feature comprises at least two pixel-level features; determining respective importance of a plurality of pixel level features for a plurality of pixel level features included in a plurality of image features; decoding the plurality of image features according to the importance degree to obtain a plurality of decoding features respectively corresponding to the plurality of image features; and determining a detection result for the image to be processed according to the plurality of decoding characteristics.","['G06F18/214', 'G06N20/10', 'G06N3/045', 'G06N3/084']"
EP3916614A1,"Method and apparatus for training language model, electronic device, readable storage medium and computer program product","A method and apparatus for training a language model, an electronic device, a readable storage medium and a computer program product, which relate to the field of natural language processing technologies in artificial intelligence, are disclosed. The method may include pre-training the language model using preset text language materials in a corpus; replacing at least one word in a sample text language material with a word mask respectively to obtain a sample text language material including at least one word mask; inputting the sample text language material including the at least one word mask into the language model, and outputting a context vector of each of the at least one word mask via the language model; determining a word vector corresponding to each word mask based on the context vector of the word mask and a word vector parameter matrix; and training the language model based on the word vector corresponding to each word mask. Introduction of semantic information representation with greater granularity enhances the capacity of the language model to learn word meaning information and may effectively avoid an information leakage risk possibly caused by character-based full word coverage.","['G06N20/00', 'G06F17/16', 'G06F17/18', 'G06F40/20', 'G06F40/30', 'G06F40/284']"
US20220270215A1,Method for applying bokeh effect to video image and recording medium,"A method for applying a bokeh effect to a video image in a user terminal is provided. The method for applying a bokeh effect includes extracting characteristic information of an image from the image included in the video image, analyzing the extracted characteristic information of the image, selecting a bokeh effect to be applied to the image based on the analyzed characteristic information of the image, and applying the determined bokeh effect to the image.","['G06T5/002', 'G06T11/00', 'G06T5/70', 'G06T3/40', 'G06T5/50', 'G06T7/11', 'G06T7/194', 'G06T7/536', 'G06T7/60', 'G06T7/70', 'G06T2207/10016', 'G06T2207/20084', 'G06T2207/20212', 'G06T2207/30201']"
US8873856B1,Determining a class associated with an image,"The technology is directed to determining a class associated with an image. In some examples, a method determines the class associated with an image. The method can include determining a segmentation score for an image segment based on a comparison of the image segment and a region of an image. The region of the image can be associated with the image segment. The method further includes determining a confidence score for the image segment based on the segmentation score and a classification score. The classification score can be indicative of a similarity between the image segment and at least one class. The method further includes determining a class associated with the image based on the confidence score. The method further includes outputting the class associated with the image.","['G06K9/344', 'G06V30/153', 'G06V30/10']"
CN111651038A,ToF-based gesture recognition control method and control system,"The invention provides a gesture recognition control method based on ToF, which comprises the following steps: s1, acquiring a scene depth image shot by a ToF module in real time, and processing the acquired scene depth image into a depth image frame sequence in real time; s2, identifying and judging whether the scene depth image contains a hand model, if so, executing the next step, otherwise, returning to execute the step S1; s3, extracting a gesture feature subsequence from the depth image frame sequence collected in the step S1 based on a sliding window method; and S4, comparing the extracted gesture feature subsequence with gesture information in a gesture library to identify control information of the gesture feature subsequence so as to realize corresponding control. According to the gesture recognition control method based on the ToF provided by the invention, the capture of fine hand interaction is realized based on the acquisition rate of hundreds of frames to thousands of frames per second of the ToF camera, so that the rapid hand action is detected and divided, and the accuracy of gesture recognition is improved. The invention also provides a gesture recognition control system based on the ToF.","['G06F3/017', 'G06N20/00', 'G06N3/045', 'G06V40/113', 'G06V40/28']"
US11334975B2,Pose synthesis in unseen human poses,"Techniques related to synthesizing an image of a person in an unseen pose are discussed. Such techniques include detecting a body part occlusion for a body part in a representation of the person in a first image and, in response to the detected occlusion, projecting a representation of the body part from a second image having a different view into the first image. A geometric transformation based on a source pose of the person and a target pose is then applied to the merged image to generate a synthesized image comprising a representation of the person in the target pose.","['G06T5/50', 'G06V40/23', 'G06T1/20', 'G06T7/0002', 'G06T7/11', 'G06T7/194', 'G06T7/40', 'G06T7/70', 'G06T7/90', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30168', 'G06T2207/30196', 'G06T3/0093', 'G06T3/18', 'G06T3/20']"
CN111311666B,Monocular vision odometer method integrating edge features and deep learning,"The invention discloses a monocular vision odometer method fusing edge features and deep learning, and relates to the technical field of vision odometers. The invention has the innovation point that a monocular vision odometer method integrating edge characteristics and deep learning is adopted, firstly, the edge enhancement algorithm is designed based on a Canny edge detection algorithm, an image data set after edge enhancement is used as the input of a convolutional neural network and characteristic extraction is carried out, the output of the convolutional neural network is input into the convolutional neural network again for calculation, and finally, the whole model is output to estimate the pose of a camera, and the characteristic extraction is optimized. The experimental result shows that the algorithm can learn more image features during model training, the pose estimation accuracy is improved, and the superior performance is shown in a low-texture scene.","['G06T7/55', 'G06F18/241', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084']"
CN112883962B,"Fundus image recognition method, device, equipment, storage medium and program product","The application discloses a fundus image identification method, a fundus image identification device, fundus image identification equipment, a fundus image identification storage medium and a fundus image identification program product, and relates to the technical field of artificial intelligence such as computer vision, deep learning and intelligent medical treatment. One embodiment of the method comprises the following steps: acquiring the position of a fundus focus, the position of a macula fovea, the lesion level of retina and the lesion probability value of a macula area in a fundus image; establishing a correlation of each feature with the lesion type of the macular region based on the position of the fundus focus, the position of the macular fovea, the lesion level of the retina and the lesion probability value of the macular region; feature screening is carried out based on the correlation of each feature and the lesion type of the macular region; and inputting the screened characteristics into a pre-trained macular region classification decision tree to obtain the categories of the macular region. The embodiment utilizes the computer to assist the fundus image recognition, thereby greatly reducing the labor cost.","['G06V10/25', 'G06F18/2415', 'G06F18/24323', 'G06N3/045', 'G06N3/08', 'G06T7/0012', 'G06T7/136', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30041']"
US12299920B2,"Method for detecting hand key points, method for recognizing gesture, and related devices","Provided is a method for detecting hand key points. The method includes: acquiring a hand image to be detected; acquiring heat maps of the hand key points by inputting the hand image into a pre-trained heat map model, wherein the heat maps include two-dimensional coordinates of the hand key points; acquiring hand structured connection information by inputting the heat maps and the hand image into a pre-trained three-dimensional information prediction model; and determining, based on the hand structured connection information and the two-dimensional coordinates in the heat maps, three-dimensional coordinates of the hand key points in a world coordinate system.","['G06V40/113', 'G06T7/73', 'G06F18/214', 'G06V40/11', 'G06V40/20', 'G06T2207/20076', 'G06T2207/30196']"
US11983817B2,Systems and methods for labeling 3D models using virtual reality and augmented reality,"A computer-implemented method for labeling a three-dimensional (3D) model using virtual reality (VR) techniques implemented by a computer system including a processor is provided herein. The method may include (i) receiving a 3D model including an environmental feature that is unlabeled, (ii) displaying, through a VR device in communication with the processor, a VR environment to a user representing the 3D model, (iii) prompting a user to input labeling data for the environmental feature displayed within the VR environment of the VR device by prompting the user to select the environmental feature through user interaction with the VR device, and input labeling data for the environmental feature, wherein the labeling data identifies the environmental feature, and (iv) generating a labeled 3D model by embedding the labeling data associated with the selected environmental feature into the 3D model.","['G06T17/205', 'G06F16/5866', 'G06T19/00', 'G06T19/006', 'G06V10/454', 'G06V10/82', 'G06V20/20', 'H04W4/021', 'G06T2219/004', 'G06V20/58', 'G06V20/582']"
US8766982B2,Vectorization of line drawings using global topology and storing in hybrid form,"An animation system can vectorize an image by generating, from an input drawing, a dataset corresponding to vector and digital representations of the input drawing such that a rendering engine could render an image having features in common with the input drawing from the representations, as a collection of strokes and/or objects rather than merely a collection of pixels having pixel color values. A vectorizer might receive an input image, generate a particle clustering data structure from a digitization of the input image, generate a stroke list, wherein strokes in the stroke list correspond to clusters of particles represented in the particle clustering data structure, generate a graph structure that represents connections between strokes on the stroke list, and determine additional characteristics of a stroke beyond the path of the stroke, additional characteristics being stored such that they correspond to strokes. The strokes might be generated using global topology information.","['G06V30/347', 'G06V10/469']"
US12165292B2,Generating an image mask for a digital image by utilizing a multi-branch masking pipeline with neural networks,"Methods, systems, and non-transitory computer readable storage media are disclosed for utilizing a plurality of neural networks in a multi-branch pipeline to generate image masks for digital images. Specifically, the disclosed system can classify a digital image as a portrait or a non-portrait image. Based on classifying a portrait image, the disclosed system can utilize separate neural networks to generate a first mask portion for a portion of the digital image including a defined boundary region and a second mask portion for a portion of the digital image including a blended boundary region. The disclosed system can generate the mask portion for the blended boundary region by utilizing a trimap generation neural network to automatically generate a trimap segmentation including the blended boundary region. The disclosed system can then merge the first mask portion and the second mask portion to generate an image mask for the digital image.","['G06T7/12', 'G06T5/75', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T3/4046', 'G06T7/194', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132']"
CN114092665B,Reconstructing 3D objects from videos,"Reconstructing a three-dimensional object from video is disclosed. A three-dimensional (3D) object reconstruction neural network system learns 3D shape representations of predicted objects from video including the objects. 3D reconstruction techniques may be used for content creation such as generating 3D characters for games, movies, and 3D printing. When generating 3D characters from video, the content may also include movements of the characters, as predicted based on the video. 3D object construction techniques utilize temporal consistency to reconstruct dynamic 3D representations of objects from unlabeled video. Specifically, objects in a video have consistent shapes and consistent textures across multiple frames. Texture, base shape, and site correspondence invariance constraints may be applied to a fine-tuning neural network system. The versatility of reconstruction techniques is good-especially for non-rigid objects.","['G06T15/04', 'G06T7/40', 'G06N3/02', 'G06N3/08', 'G06T15/20', 'G06T17/20', 'G06T17/205', 'G06T3/08', 'G06T7/251', 'G06T7/50', 'G06T7/579', 'G06T7/70', 'G06T2200/08', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30244']"
US11393100B2,Automatically generating a trimap segmentation for a digital image by utilizing a trimap generation neural network,"Methods, systems, and non-transitory computer readable storage media are disclosed for utilizing a plurality of neural networks in a multi-branch pipeline to generate image masks for digital images. Specifically, the disclosed system can classify a digital image as a portrait or a non-portrait image. Based on classifying a portrait image, the disclosed system can utilize separate neural networks to generate a first mask portion for a portion of the digital image including a defined boundary region and a second mask portion for a portion of the digital image including a blended boundary region. The disclosed system can generate the mask portion for the blended boundary region by utilizing a trimap generation neural network to automatically generate a trimap segmentation including the blended boundary region. The disclosed system can then merge the first mask portion and the second mask portion to generate an image mask for the digital image.","['G06T7/194', 'G06T3/40', 'G06T7/11', 'G06T2207/10024', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/20221']"
CN110135455B,"Image matching method, device and computer readable storage medium","The invention relates to the technical field of artificial intelligence, and discloses an image matching method, which comprises the following steps: generating an image imaging image according to a scene image shot by the aerial camera, and performing primary image matching on the image imaging image by using a scale-invariant feature transformation method to generate a primary image matching set; generating epipolar line images based on the primary image matching set, calculating the overlapping degree between the epipolar line images, completing secondary image matching, and generating a secondary image matching set; and based on the second image matching set, establishing dense matching of all pixel points among the images, generating a third image matching set, and executing three-dimensional reconstruction to obtain a reconstructed scene image. The invention also provides an image matching device and a computer readable storage medium. The invention provides a novel image matching scheme applied to aerial dense stereoscopic scenes, and the image matching efficiency can be improved.","['G06F18/22', 'G06T17/00']"
US10699491B2,Virtually representing spaces and objects while maintaining physical properties,"Systems and techniques from displaying virtual representations of real-world spaces and objects in various environments are disclosed. A source environment at a first location can be scanned by a head-mounted display (HMD) device to generate three-dimensional datasets corresponding to the physical environment at the first location. The three-dimensional datasets can include detected physical properties associated with the physical environment. At a second location, the HMD can re-create the source environment, and render for display a virtual representation of the physical environment based on the three-dimensional datasets, where the virtual representation of the source environment is rendered to maintain any one of the detected physical properties associated with the physical environment. Further, at the second location, the HMD can enable a user to view one or more physical objects within the virtual representation of the physical environment such that the physical object is perceived to be within the source environment.","['G06F3/03', 'G06F3/011', 'G02B27/017', 'G02B27/0172', 'G06F3/012', 'G06F3/0304', 'G06F3/1454', 'G06K9/00671', 'G06T15/40', 'G06T19/006', 'G06T19/20', 'G06V20/20', 'G02B2027/0138', 'G02B2027/0141', 'G06T2219/2016']"
US9483709B2,Visual saliency estimation for images and video,"Methods and apparatus for deriving a saliency measure for images and video are described. In an embodiment, a process includes decomposing, by a processor, an image into elements, wherein elements cluster the image and each element consists of spatially connected pixels. The processor then calculates a first image measure indicative of each element's uniqueness in the image on a per element basis, and calculates a second image measure indicative of each element's spatial distribution in the image on a per element basis. The processor then provides a per element saliency measure by combining the first image measure and the second image measure.","['G06V10/758', 'G06K9/4676', 'G06F18/22', 'G06K9/4652', 'G06K9/4671', 'G06K9/6212', 'G06K9/6215', 'G06T7/0081', 'G06T7/11', 'G06T7/194', 'G06T7/2033', 'G06T7/246', 'G06T7/403', 'G06T7/408', 'G06T7/44', 'G06T7/90', 'G06V10/462', 'G06V10/56', 'H04N19/117', 'G06T2207/10004', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20144']"
US12373958B2,Real time perspective correction on faces,"Apparatus and methods related to image processing are provided. A computing device can determine a first image area of an image, such as an image captured by a camera. The computing device can determine a warping mesh for the image with a first portion of the warping mesh associated with the first image area. The computing device can determine a cost function for the warping mesh by: determining first costs associated with the first portion of the warping mesh that include costs associated with face-related transformations of the first image area to correct geometric distortions. The computing device can determine an optimized mesh based on optimizing the cost function. The computing device can modify the first image area based on the optimized mesh.","['G06T5/80', 'G06T7/174', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/09', 'G06T17/20', 'G06T5/60', 'G06V40/165', 'G06T2207/20084', 'G06T2207/30201']"
US10580140B2,Method and system of real-time image segmentation for image processing,"Techniques related to a system, article, and method of real-time image segmentation for image processing.","['G06T7/143', 'G06T7/12', 'G06T7/162', 'G06T7/168', 'G06T7/174', 'G06T7/194', 'G06T7/246', 'G06T2207/10016', 'G06T2207/20072', 'G06T2207/20112', 'G06T2207/20116', 'G06T2207/30196']"
US10068316B1,Tilts as a measure of user engagement for multiview digital media representations,"Various embodiments of the present invention relate generally to systems and methods for analyzing and manipulating images and video. According to particular embodiments, the spatial relationship between multiple images and video is analyzed together with location information data, for purposes of creating a representation referred to herein as a multi-view interactive digital media representation for presentation on a device. Once a multi-view interactive digital media representation is generated, a user can provide navigational inputs, such via tilting of the device, which alter the presentation state of the multi-view interactive digital media representation. The navigational inputs can be analyzed to determine metrics which indicate a user's interest in the multi-view interactive digital media representation.","['G06T3/60', 'G06F3/04815', 'G06F3/04842', 'G06F3/0485', 'G06F3/0487', 'G06T11/001', 'G06T15/205', 'G06T19/20', 'G06T2219/2016']"
US6901155B2,Wavelet-enhanced automated fingerprint identification system,A method and system for performing automated biological identification. The system including a preprocessing module with a histogram transform for locally and globally enhancing biological data such as fingerprints. An enhancement module with a fast smoothing and enhancement function. A feature extraction module with a fingerprint oriented thinning function. A matching module with a resolution-enhanced Hough transform function for fingerprint registration and matching score function.,"['G06V40/1347', 'G06V40/1365']"
US8249343B2,Representing documents with runlength histograms,"An apparatus and method are provided for generating a representation of an image which may be used in tasks such as classification, clustering, or similarity determination. An image, such as a scanned document, in which the pixel colorant values are quantized into a plurality of colorant quantization levels, is partitioned into regions, optionally at a plurality of different scales. For each region, a runlength histogram is computed, which may be a combination of sub-histograms for each of the colorant quantization levels and optionally each of plural directions. The runlength histograms, optionally normalized, can then be combined to generate a representation of the document image.","['G06V30/40', 'G06V30/18086', 'G06V30/10']"
US7254268B2,Object extraction,"A video sequence is processed to extract one or more objects. The extracted objects are used to create a new video sequence. In particular, a user is able to select a background and/or an audio recording. Alternatively, the object extraction system automatically selects a background and/or audio recording. The extracted objects are integrated into the selected background and/or the audio recording is added to form a new video sequence.","['G06T7/11', 'G06T7/194', 'G06T7/215', 'G06V10/40', 'G06T2207/10016', 'G06T2207/20216']"
CN114757913A,A kind of display screen defect detection method,"The invention discloses a method for detecting defects of a display screen, which comprises the steps of firstly, converting a to-be-detected graph from a spatial domain to a frequency domain by using Fourier transform, carrying out low-pass filtering on a frequency spectrogram, and removing periodic textures on the surface of an LCD display screen; secondly, detecting image feature points by using a Harris operator, and matching the feature points according to Euclidean distance to obtain an accurately registered image pair; and then carrying out difference operation on the accurately registered image pair to obtain a defect image with a residual image, and finally calculating an optimal threshold value t by using a Niblack method, and carrying out defect background binarization to obtain a defect detection binary image. The method of the invention realizes the high-accuracy detection of the surface defects of the LCD display screen by utilizing the improved difference image method and through conversion detection, has robustness under geometric distortion and chromatic aberration, and has better effect on the detection of the defects of the display screen.","['G06T7/0004', 'G06T7/136', 'G06T7/344', 'G09G3/006', 'G06T2207/30121']"
US8811771B2,Content aware slideshows,"A method, system, and computer-readable storage medium for performing content based transitions between images. Image content within each image of a set of images are analyzed to determine at least one respective characteristic metric for each image. A respective transition score for each pair of at least a subset of the images is determined with respect to each transition effect of a plurality of transition effects based on the at least one respective characteristic metric for each image. Transition effects implementing transitions between successive images for a sequence of the images are determined based on the transition scores. An indication of the determined transition effects is stored. The determined transition effects are useable to present the images in a slideshow or other image sequence presentation.","['G06T7/38', 'G06T13/80', 'G06T3/02', 'G06T3/10', 'G06V20/41', 'G06T2207/20036']"
CN110543848B,Driver action recognition method and device based on three-dimensional convolutional neural network,"The embodiment of the invention provides a driver action recognition method and device based on a three-dimensional convolutional neural network. The target model is obtained by training a constructed three-dimensional convolutional neural network, the three-dimensional convolutional neural network comprises a plurality of sequentially connected combined layer structures, and each combined layer structure comprises a convolutional layer and a pooling layer. By improving the structure of the three-dimensional convolutional neural network, the trained target model has a more accurate recognition result on the action of the driver. On the other hand, compared with the method for collecting facial features, the method for collecting the driver actions is not easily interfered by the environment, and the feature data contains optical flow features reflecting the changes of the driver actions along with the time, so that the accuracy of the identification result is further improved by the data of action continuity.","['G06N3/045', 'G06N3/08', 'G06V20/46', 'G06V20/52', 'G06V40/172', 'G06V40/20']"
US10902598B2,Automated segmentation utilizing fully convolutional networks,"Systems and methods for automated segmentation of anatomical structures (e.g., heart). Convolutional neural networks (CNNs) may be employed to autonomously segment parts of an anatomical structure represented by image data, such as 3D MRI data. The CNN utilizes two paths, a contracting path and an expanding path. In at least some implementations, the expanding path includes fewer convolution operations than the contracting path. Systems and methods also autonomously calculate an image intensity threshold that differentiates blood from papillary and trabeculae muscles in the interior of an endocardium contour, and autonomously apply the image intensity threshold to define a contour or mask that describes the boundary of the papillary and trabeculae muscles. Systems and methods also calculate contours or masks delineating the endocardium and epicardium using the trained CNN model, and anatomically localize pathologies or functional characteristics of the myocardial muscle using the calculated contours or masks.","['G06T7/10', 'G06T7/136', 'G06N3/045', 'G06N3/0454', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06N3/096', 'G06N3/0985', 'G06T7/0012', 'G06T7/11', 'G06T7/143', 'G06T7/149', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004', 'G06T2207/30048']"
WO2021185016A1,Methods and systems for controlling device using hand gestures in multi-user environment,"Methods and apparatus for gesture-based control of a device in a multi-user environment are described. The methods prioritize users or gestures based on a predetermined priority ruleset. A first-user-in-time ruleset prioritizes gestures based on when in time they were begun by a user in the camera FOV. An action-hierarchy ruleset prioritizes gestures based on the actions they correspond to, and the relative positions of those actions within an action hierarchy. A designated-master-user ruleset prioritizes gestures performed by an explicitly designated master user. Methods for designating a new master user and for providing gesture-control-related user feedback in a multi-user environment are also described.","['G06F3/017', 'G06F3/005', 'G06F3/0304']"
US8175412B2,Method and apparatus for matching portions of input images,"A method and apparatus for finding correspondence between portions of two images that first subjects the two images to segmentation by weighted aggregation (10), then constructs directed acylic graphs (16,18) from the output of the segmentation by weighted aggregation to obtain hierarchical graphs of aggregates (20,22), and finally applies a maximally weighted subgraph isomorphism to the hierarchical graphs of aggregates to find matches between them (24). Two algorithms are described; one seeks a one-to-one matching between regions, and the other computes a soft matching, in which is an aggregate may have more than one corresponding aggregate. A method and apparatus for image segmentation based on motion cues. Motion provides a strong cue for segmentation. The method begins with local, ambiguous optical flow measurements. It uses a process of aggregation to resolve the ambiguities and reach reliable estimates of the motion. In addition, as the process of aggregation proceeds and larger aggregates are identified, it employs a progressively more complex model to describe the motion. In particular, the method proceeds by recovering translational motion at fine levels, through affine transformation at intermediate levels, to 3D motion (described by a fundamental matrix) at the coarsest levels. Finally, the method is integrated with a segmentation method that uses intensity cues. The utility of the method is demonstrated on both random dot and real motion sequences.","['G06T7/593', 'G06T7/11', 'G06T7/162', 'G06T7/174', 'G06T7/215', 'G06V10/26', 'G06V10/267', 'G06V30/1988', 'G06T2207/10016', 'G06T2207/20016', 'G06V10/759']"
US11922318B2,System and method of character recognition using fully convolutional neural networks with attention,Embodiments of the present disclosure include a method that obtains a digital image. The method includes extracting a word block from the digital image. The method includes processing the word block by evaluating a value of the word block against a dictionary. The method includes outputting a prediction equal to a common word in the dictionary when a confidence factor is greater than a predetermined threshold. The method includes processing the word block and assigning a descriptor to the word block corresponding to a property of the word block. The method includes processing the word block using the descriptor to prioritize evaluation of the word block. The method includes concatenating a first output and a second output. The method includes predicting a value of the word block.,"['G06N3/084', 'G06F18/2413', 'G06F40/242', 'G06N3/044', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G06N3/092', 'G06V10/764', 'G06V10/82', 'G06V30/1478', 'G06V30/18057', 'G06V30/347', 'G06V30/36', 'G06V30/414', 'G06V30/10']"
US8983200B2,Object segmentation at a self-checkout,"Techniques for segmenting an object are provided. The techniques include capturing an image of an object, dividing the image into one or more blocks, computing a confidence value for each of the one or more blocks, and eliminating one or more blocks from consideration based on the confidence value for each of the one or more blocks.","['G06T7/0079', 'G06T7/11', 'G06K9/34', 'G06T7/0081', 'G06T7/136', 'G06T2207/10024', 'G06T2207/20012', 'G06T2207/20021', 'G06T2207/20148']"
CA2658727C,Interactive segmentation of images with single scribbles,A computer-implemented assigns attributes to an image by processing pixels of the image containing a single marked area spanning more than a single pixel that defines for a current iteration a target attribute so as to determine an optimal function that defines a respective attribute of pixels in the image. Respective attributes are assigned to pixels in the image according to the optimal function; and the attributes of the pixels are displayed.,"['G06T7/162', 'G06T7/11', 'G06T7/194', 'G06T2207/10016', 'G06T2207/20072', 'G06T2207/20096']"
US11989518B2,"Normalized processing method and apparatus of named entity, and electronic device",A normalized processing method of a named entity includes: obtaining first text data; recognizing a named entity from the first text data; determining whether a first standard named entity exists in a standard named entity database according to the named entity; determining the first standard named entity as a normalized representation of the named entity in response to determining that the first standard named entity exists in the standard named entity database; and obtaining a second standard named entity from the standard named entity database and determining an obtained second standard named entity as the normalized representation of the named entity in response to determining that the first standard named entity does not exist in the standard named entity database.,"['G06F40/295', 'G06F16/3344', 'G06F40/247', 'G06F40/279', 'G06N3/044', 'G06N3/0442', 'G06N3/09', 'G16H50/70', 'G16H70/60', 'G06N3/045', 'G06N3/048']"
US8340360B2,System and method for fusing vector data with imagery,"Automatic conflation systems and techniques which provide vector-imagery conflation and map-imagery conflation. Vector-imagery conflation is an efficient approach that exploits knowledge from multiple data sources to identify a set of accurate control points. Vector-imagery conflation provides automatic and accurate alignment of various vector datasets and imagery, and is appropriate for GIS applications, for example, requiring alignment of vector data and imagery over large geographical regions. Map-imagery conflation utilizes common vector datasets as “glue” to automatically integrate street maps with imagery. This approach provides automatic, accurate, and intelligent images that combine the visual appeal and accuracy of imagery with the detailed attribution information often contained in such diverse maps. Both conflation approaches are applicable for GIS applications requiring, for example, alignment of vector data, raster maps, and imagery. If desired, the conflated data generated by such systems may be retrieved on-demand.","['G06T7/75', 'G01C21/3807', 'G01C21/3833', 'G06T2207/10032', 'G06T2207/30184']"
US5790692A,Method and means of least squares designed filters for image segmentation in scanning cytometry,"In an image segmentation system that processes image objects by digital filtration, a digital filter is defined. The digital filter includes a neighborhood operator for processing intensity values of neighborhoods of pixels in a pixel array. A first pixel array is received defining a pixelated image including one or more objects and a background and a second pixel array is received that defines a reference image. The reference image includes at least one object included in the pixelated image in a background. In the reference image, pixels included in the at least one object are distinguished from pixels included in the background by a predetermined amount of contrast. Pixels of the first and second images are compared to determine a merit value; the merit value is used to compute neighborhood operator values; and, the neighborhood operator is applied to images in order to create or enhance contrast between objects and background in the images.","['G01N15/147', 'G06T7/0012', 'G06T7/11', 'G06T7/136', 'G06V20/69', 'G06T2207/10056', 'G06T2207/10064', 'G06T2207/30024']"
US8755569B2,Methods for recognizing pose and action of articulated objects with collection of planes in motion,"The invention comprises an improved system, method, and computer-readable instructions for recognizing pose and action of articulated objects with collection of planes in motion. The method starts with a video sequence and a database of reference sequences corresponding to different known actions. The method identifies the sequence from the reference sequences such that the subject in performs the closest action to that observed. The method compares actions by comparing pose transitions. The cross-homography invariant may be used for view-invariant recognition of human body pose transition and actions.",['G06V40/20']
US9773196B2,Utilizing deep learning for automatic digital image segmentation and stylization,"Systems and methods are disclosed for segregating target individuals represented in a probe digital image from background pixels in the probe digital image. In particular, in one or more embodiments, the disclosed systems and methods train a neural network based on two or more of training position channels, training shape input channels, training color channels, or training object data. Moreover, in one or more embodiments, the disclosed systems and methods utilize the trained neural network to select a target individual in a probe digital image. Specifically, in one or more embodiments, the disclosed systems and methods generate position channels, training shape input channels, and color channels corresponding the probe digital image, and utilize the generated channels in conjunction with the trained neural network to select the target individual.","['G06T7/90', 'G06K9/66', 'G06T7/73', 'G06K9/4604', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/0081', 'G06T7/11', 'G06V40/161', 'G06V40/169', 'G06T2207/20084', 'G06T2207/30201']"
US8396328B2,Minimal artifact image sequence depth enhancement system and method,"Motion picture scenes to be colorized/depth enhanced (2D->3D) are broken into separate elements, backgrounds/sets or motion/onscreen-action. Background and motion elements are combined into composite frame which becomes a visual reference database that includes data for all frame offsets used later for the computer controlled application of masks within a sequence of frames. Masks are applied to subsequent frames of motion objects based on various differentiating image processing methods, including automated mask fitting/reshaping. Colors and/or depths are automatically applied to masks throughout a scene from the composite background and to motion objects. Areas never exposed by motion or foreground objects in a series of images may be partially or fully realistically drawn or rendered and applied to the occluded areas of the background and then automatically applied throughout the images to generate of minimal artifact or artifact-free secondary viewpoints when translating foreground objects horizontally during 2D->3D conversion.","['G06T11/001', 'G06T5/77', 'H04N13/257', 'H04N13/266', 'H04N9/43', 'G06T2207/10028']"
US10325176B2,"Methods and systems for assessing retinal images, and obtaining information from retinal images","A method of assessing the quality of an retinal image (such as a fundus image) includes selecting at least one region of interest within a retinal image corresponding to a particular structure of the eye (e.g. the optic disc or the macula), and a quality score is calculated in respect of the, or each, region-of-interest. Each region of interest is typically one associated with pathology, as the optic disc and the macula are. Optionally, a quality score may be calculated also in respect of the eye as a whole (i.e. over the entire image, if the entire image corresponds to the retina).","['G06T7/0012', 'G06K9/4676', 'A61B3/0025', 'A61B3/12', 'A61B3/14', 'G06F18/2411', 'G06K9/0061', 'G06K9/036', 'G06K9/6269', 'G06T7/0014', 'G06V10/464', 'G06V10/764', 'G06V10/993', 'G06V40/193', 'A61B2576/02', 'G06K2009/00932', 'G06K2209/05', 'G06T2207/20081', 'G06T2207/30041', 'G06T2207/30168', 'G06V2201/03', 'G06V40/14']"
US8929600B2,Action recognition based on depth maps,"A plurality of depth maps corresponding to respective depth measurements determined over a respective plurality of time frames may be obtained. A plurality of skeleton representations respectively corresponding to the respective time frames may be obtained. Each skeleton representation may include joints associated with an observed entity. Local feature descriptors corresponding to the respective time frames may be determined, based on the depth maps and the joints associated with the skeleton representations. An activity recognition associated with the observed entity may be determined, based on the obtained skeleton representations and the determined local feature descriptors.","['G06V40/20', 'G06K9/00335', 'G06V40/23']"
CN112618026B,Remote surgical data fusion interactive display system and method,"The invention provides a remote operation data fusion interactive display system and a method, comprising the following steps: the system comprises a preoperative acquisition and processing module, a depth camera scene acquisition module, a multi-modal data integration and naked eye three-dimensional fusion display module, a data communication module, a multi-dimensional interaction control and feedback module and a surgical instrument operation and sensing module; the method comprises the steps of multi-modal data integration, real-time image information fusion and three-dimensional display, and multi-dimensional interaction control and feedback. The method utilizes the dynamic opacity fusion algorithm to fully fuse the preoperative and intraoperative multimodal data of the remote operation, generates intraoperative three-dimensional display multi-viewpoint images in real time through three-dimensional winding and hole filling, realizes real-time naked eye three-dimensional display of multimodal information and natural remote interactive control, can be applied to remote operations or medical teaching scenes, enables relevant application to be good in real-time performance, low in time delay, rich in information, accurate and convenient to operate, and further reduces operation difficulty and improves operation success rate.","['A61B34/35', 'A61B34/10', 'A61B34/25', 'A61B34/70', 'A61B34/76', 'G16H40/67', 'A61B2034/105', 'A61B2034/107']"
US9445716B2,"Obtaining data for automatic glaucoma screening, and screening and diagnostic techniques and systems using the data","A non-stereo fundus image is used to obtain a plurality of glaucoma indicators. Additionally, genome data for the subject is used to obtain genetic marker data relating to one or more genes and/or SNPs associated with glaucoma. The glaucoma indicators and genetic marker data are input into an adaptive model operative to generate an output indicative of a risk of glaucoma in the subject. In combination, the genetic indicators and genome data are more informative about the risk of glaucoma than either of the two in isolation. The adaptive model may be a two-stage model, having a first stage in which individual genetic indicators are combined with respective portions of the genome data by first adaptive model modules to form respective first outputs, and a second stage in which the first outputs are combined by a second adaptive mode. Texture analysis is performed on the fundus images to classify them based on their quality, and only images which are determined to meet a quality criterion are subjected to an analysis to determine if they exhibit glaucoma indicators. Also, the images are put into a standard format. The system may include estimating the position of the optic cup by combining results from multiple optic cup segmentation techniques. The system may include estimating the position of the optic disc by applying edge detection to the funds image, excluding edge points that are unlikely to be optic disc boundary points, and estimating the position of an optic disc by fitting an ellipse to the remaining edge points.","['A61B3/0025', 'A61B3/12', 'A61B5/02007', 'A61B5/7264', 'A61B5/7267', 'A61B5/7275', 'G06T7/0012', 'G06T7/40', 'G06T2207/30041', 'G16H50/20', 'G16H50/70']"
US6167146A,Method and system for segmentation and detection of microcalcifications from digital mammograms,"A method and system for detecting and displaying clustered microcalcifications in a digital mammogram, wherein a single digital mammogram is first automatically cropped to a breast area sub-image which is then processed by means of an optimized Difference of Gaussians filter to enhance the appearance of potential microcalcifications in the sub-image. The potential microcalcifications are thresholded, clusters are detected, features are computed for the detected clusters, and the clusters are classified as either suspicious or not suspicious by means of a neural network. Thresholding is preferably by sloping local thresholding, but may also be performed by global and dual-local thresholding. The locations in the original digital mammogram of the suspicious detected clustered microcalcifications are indicated. Parameters for use in the detection and thresholding portions of the system are computer-optimized by means of a genetic algorithm. The results of the system are optimally combined with a radiologist's observation of the original mammogram by combining the observations with the results, after the radiologist has first accepted or rejected individual detections reported by the system.","['B25J15/04', 'G06F18/41', 'G06T5/20', 'G06T7/0012', 'G06V10/28', 'G06V10/443', 'A61B6/502', 'B29C2791/006', 'B29C2791/007', 'B29L2023/18', 'G06V2201/03', 'Y10S128/922']"
CN118527377B,Debris detection and removal device and method,"The invention discloses a device and a method for detecting and removing incomplete fragments, wherein the method comprises the steps of obtaining an image sequence of a preset area from a high-definition camera which is illuminated by reticulate pattern projection; preprocessing and primarily segmenting the image sequence to obtain a two-dimensional image mask of the segmented tablet, and constructing two-dimensional and three-dimensional data sets of the tablet by combining the two-dimensional image mask; based on the two-dimensional data set and the three-dimensional data set, performing image registration and fusion, and performing fine segmentation, texture analysis, shape analysis and spectral feature extraction to obtain a tablet comprehensive data set; reading the comprehensive data set, calling a preconfigured comprehensive defect feature vector, finely classifying the fine module by adopting the integrated learning module, evaluating the defect severity according to the fine classification result, and giving out the overall quality score of the tablet; and if the overall quality score is smaller than the threshold value, sending a rejection instruction to a rejection mechanism. The problem that the photoelectric counter is easy to be shielded by dust and causes counting and detection errors is solved by removing the fragments through pure vision.","['G06V20/64', 'B07C5/34', 'B07C5/362', 'G06N20/20', 'G06T7/0004', 'G06T7/33', 'G06V10/26', 'G06V10/449', 'G06V10/52', 'G06V10/774', 'G06V10/806', 'B07C2501/0081', 'G06T2207/20081', 'G06T2207/30108']"
US11244203B2,Automated generation of structured training data from unstructured documents,"Methods, systems and computer program products for automatically generating structured training data based on an unstructured document are provided. Aspects include receiving an unstructured document and a corresponding structured document that includes labeled portions. Aspects also include generating a parsed document that has one or more extracted objects by applying a parsing tool to the unstructured document. Aspects also include identifying one or more matching extracted objects by applying a matching algorithm to the structured document and the parsed document. Each matching extracted object is an extracted object of the parsed document that corresponds to a labeled portion of the structured document. Aspects also include annotating a region of the unstructured document that corresponds to the bounding box of the respective matching extracted object with a respective label of the corresponding labeled portion of the unstructured document.","['G06F16/355', 'G06K9/6259', 'G06F16/3329', 'G06F16/93', 'G06F18/2155', 'G06F18/22', 'G06F18/23', 'G06F18/24', 'G06F40/205', 'G06K9/3233', 'G06K9/6201', 'G06K9/6218', 'G06K9/6267', 'G06V10/25', 'G06V10/82', 'G06V30/19147', 'G06V30/19173', 'G06V30/414', 'G06F40/143']"
CN113724216B,Method and system for detecting wave crest welding spot defects,"The invention provides a wave crest welding spot defect detection method and a system, which solve the problem of unsatisfactory detection accuracy of the current wave crest welding spot defect detection method based on automatic optical detection.","['G06T7/001', 'G06F17/16', 'G06T5/70', 'G06T7/11', 'G06T7/136', 'G06T7/62', 'G06T7/90', 'G06T2207/30141', 'Y02P90/30']"
US10958891B2,Visual annotation using tagging sessions,"Various embodiments of the present invention relate generally to systems and methods for analyzing and manipulating images and video. In particular, a multi-view interactive digital media representation (MVIDMR) of an object can be generated from live images of an object captured from a camera. After the MVIDMR of the object is generated, a tag can be placed at a location on the object in the MVIDMR. The locations of the tag in the frames of the MVIDMR can vary from frame to frame as the view of the object changes. When the tag is selected, media content can be output which shows details of the object at location where the tag is placed. In one embodiment, the object can be car and tags can be used to link to media content showing details of the car at the locations where the tags are placed.","['G06F3/04815', 'G06F18/251', 'G06F3/04842', 'G06K9/00671', 'G06T5/002', 'G06T5/70', 'G06V10/803', 'G06V20/20', 'H04N13/183', 'H04N13/207', 'H04N23/90', 'G06V10/16']"
CN115620118A,Saliency target detection method based on multi-scale expansion convolutional neural network,"The invention discloses a method for detecting a salient target based on a multi-scale expanded convolutional neural network, which comprises the following steps: extracting multi-scale features of the input image; inputting the multi-scale features into an expansion residual convolution module to obtain fusion features of context information including the multi-scale features; inputting the fusion features into a plurality of channel attention modules respectively to obtain a plurality of significance features; and (3) performing dimensionality reduction activation on each displayed saliency characteristic to generate a saliency map, and performing deep supervision training by adopting a mixed loss function fusing cross entropy and cross-over ratio loss. The method of the invention fully captures the abundant global semantic information and local semantic information in the image by using the expansion residual convolution module based on the multi-scale expansion convolution neural network, solves the problems of shallow depth of the encoder and insufficient information extraction, and simultaneously, leads the network to focus on the target area by the designed channel attention module, thereby effectively improving the precision of target detection.","['G06V20/00', 'G06N3/08', 'G06V10/22', 'G06V10/26', 'G06V10/77', 'G06V10/774', 'G06V10/806', 'G06V10/82', 'G06V2201/07']"
US10319096B2,Automated tattoo recognition techniques,"In some implementations, a computer-implemented method is capable of automatically segmenting and detecting a tattoo within an image. An image may be initially obtained. A block coverage pattern that identifies multiple blocks within the obtained image may be determined. A set of processing operations may then be performed for each block. The processing operations may include calculating a plurality of statistical features. A confidence score reflecting a likelihood that at least a portion of the block includes a predetermined graphical attribute associated with tattoos may be calculated. A subset of the multiple blocks of the image that have a respective confidence score greater than a predetermined threshold value may be identified. A portion of the image that includes one or more blocks from among the subset of the multiple blocks may then be determined to correspond to a tattoo.","['G06V10/764', 'G06F18/2413', 'G06K9/00', 'G06K9/00362', 'G06K9/0061', 'G06K9/00885', 'G06K9/3241', 'G06K9/4642', 'G06K9/66', 'G06T7/0002', 'G06T7/0012', 'G06T7/11', 'G06T7/337', 'G06T7/35', 'G06T7/41', 'G06T7/73', 'G06T7/90', 'G06V10/255', 'G06V10/50', 'G06V20/60', 'G06V40/10', 'G06V40/193']"
US11612311B2,System and method of otoscopy image analysis to diagnose ear pathology,Disclosed herein are systems and methods to detect a wide range of eardrum abnormalities by using high-resolution otoscope images and report the condition of the eardrum as “normal” or “abnormal.”,"['G06V10/56', 'G06T7/0014', 'A61B5/0082', 'A61B1/000094', 'A61B1/227', 'A61B5/004', 'A61B5/7267', 'A61B5/7282', 'G06K9/00', 'G06N20/20', 'G06N7/02', 'G06V10/70', 'G06V10/74', 'G06V10/765', 'G06V10/774', 'G16H30/00', 'G16H50/20', 'G16H70/60', 'A61B2503/06', 'A61B2576/02', 'A61B5/12', 'A61B5/7264', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004', 'G06T7/0012', 'G06V10/764', 'G06V10/82', 'G06V40/10']"
US7769772B2,Mixed media reality brokerage network with layout-independent recognition,"A Mixed Media Reality (MMR) system associated techniques are disclosed. The MMR system provides mechanisms for forming a mixed media document that includes media of at least two types (e.g., printed paper as a first medium and digital content as a second medium. The MMR system of the present invention provides mechanisms for forming a mixed media document that includes media of at least two types, such as printed paper as a first medium and a digital photograph, digital movie, digital audio file, or web link as a second medium. The present invention also includes a number of novel methods including: a method for layout independent MMR recognition, a strip fragment candidate generation process, and a page candidate accumulation process.","['G06F16/5846', 'G06V30/414', 'G06V30/10', 'G06V30/18133', 'H04N2201/3249', 'H04N2201/3264', 'H04N2201/3267', 'H04N2201/3269', 'H04N2201/3271']"
AU2006252025B2,Recognition of parameterised shapes from document images,"Abstract RECOGNITION OF PARAMETERISED SHAPES FROM DOCUMENT IMAGES 5 A method (1100) of creating a document comprising a modifiable shape, is disclosed. The method analyses an image to detect at least a graphical object. The method matches the detected graphical object with at least one of a plurality of predetermined modifiable closed-form non-textual template shapes (e.g., 420) comprising control parameters for modifying the closed-form non-textual template shape in a non-affine 10 manner. The number of control parameters of the predetermined modifiable closed-form non-textual template shape is less than the number of sections making up the modifiable closed-form non-textual template shape. The method creates a document comprising the at least one modifiable closed-form non-textual template shape. ( Start Segment bitmap image into one or more regions 1110 Identify regions at various document content sites 1120 Detect graphical objects Within graphics regions L1130 Match each detected graphical object with one of a plurality of predetermined I 1140 template shapes Analyse line objects to determine if the line objects form connectors ,1150 Create editable output document ,1160 F End Fig. 11","['G06V30/413', 'G06V10/752']"
CN109859233B,"Image processing method and system, and training method and system of image processing model","The invention discloses an image processing method and a training system of an image processing model, wherein the image processing method comprises the following steps: acquiring an image to be detected; inputting the image to be detected into a neural network model for processing to obtain a bone segmentation result, a bone central line segmentation result and a bone fracture detection result; wherein the neural network model is determined by machine training learning based on training images. The invention adopts the trained deep learning network to realize the functions of bone segmentation, bone center line segmentation and bone fracture detection at the same time, can shorten the total time consumption by fifty percent, saves the memory space by forty percent by a model, and can help doctors to reduce the burden of film reading, accelerate the film reading time, reduce the missed diagnosis probability and reduce the contradiction between doctors and patients.",[]
US8023726B2,Method and system for markerless motion capture using multiple cameras,"Completely automated end-to-end method and system for markerless motion capture performs segmentation of articulating objects in Laplacian Eigenspace and is applicable to handling of the poses of some complexity. 3D voxel representation of acquired images are mapped to a higher dimensional space ( k), where k depends on the number of articulated chains of the subject body, so as to extract the 1-D representations of the articulating chains. A bottom-up approach is suggested in order to build a parametric (spline-based) representation of a general articulated body in the high dimensional space followed by a top-down probabilistic approach that registers the segments to an average human body model. The parameters of the model are further optimized using the segmented and registered voxels.","['G06T7/596', 'G06T7/12', 'G06T7/251', 'G06T7/277', 'G06V10/426', 'G06V40/23', 'G06T2200/08', 'G06T2207/10021', 'G06T2207/20044', 'G06T2207/30196']"
CN109151501B,"Video key frame extraction method and device, terminal equipment and storage medium","The invention discloses a video key frame extraction method, a video key frame extraction device, terminal equipment and a storage medium. The method comprises the following steps: preprocessing a video data stream, and extracting candidate key frames; determining a feature vector corresponding to each candidate key frame; clustering each feature vector to obtain at least two clustering clusters; and extracting key frames from each clustering cluster according to the staticities of the candidate key frames. By the method, the key frames in the video can be extracted quickly and accurately.","['H04N21/234', 'H04N21/23418', 'H04N21/44', 'H04N21/44008']"
US9519641B2,Photography recognition translation,Methods are described for efficient and substantially instant recognition and translation of text in photographs. A user is able to select an area of interest for subsequent processing. Optical character recognition (OCR) may be performed on the wider area than that selected for determining the subject domain of the text. Translation to one or more target languages is performed. Manual corrections may be made at various stages of processing. Variations of translation are presented and made available for substitution of a word or expression in the target language. Translated text is made available for further uses or for immediate access.,"['G06F17/289', 'G06F40/58', 'G06F16/583', 'G06F17/30247', 'G06K2209/01', 'G06V30/10']"
US8200011B2,Context processor for video analysis system,"Embodiments of the present invention provide a method and a system for mapping a scene depicted in an acquired stream of video frames that may be used by a machine-learning behavior-recognition system. A background image of the scene is segmented into plurality of regions representing various objects of the background image. Statistically similar regions may be merged and associated. The regions are analyzed to determine their z-depth order in relation to a video capturing device providing the stream of the video frames and other regions, using occlusions between the regions and data about foreground objects in the scene. An annotated map describing the identified regions and their properties is created and updated.","['G06V20/52', 'G06T7/11', 'G06V40/20', 'G06T2207/10016']"
US10552952B2,GPU-based TFT-LCD Mura defect detection method,"The present disclosure discloses a GPU-based TFT-LCD Mura defect detection method, comprising: (1) establishing a studentized residual based double-second-order regression diagnosis model based original image data to obtain double-second-order regression background data; (2) obtaining influence quantities of respective data points on fitted values according to the original image data and the double-second-order regression background image data; (3) excluding outliers and influential points in the original image data according to the influence quantities to obtain a new pixel point set; (4) establishing a double-N-order polynomial surface fitting model according to the new pixel point set to obtain double-N-order background image data; (5) obtaining a residual image R according to the double-N-order background image data and the original image data, and performing threshold segmentation on the residual image to obtain a threshold segmentation image; and (6) performing morphological processing on the threshold segmentation image to obtain an eroded and dilated image, thereby achieving effective segmentation of Mura defects with uneven brightness distribution.","['G09G3/006', 'G06T7/001', 'G01N21/8851', 'G02F1/1309', 'G02F1/1368', 'G06T1/20', 'G06T5/002', 'G06T5/20', 'G06T5/70', 'G06T7/0004', 'G06T7/11', 'G06T7/136', 'G06T7/155', 'G09G3/3648', 'H04N25/677', 'H04N25/683', 'G01N2021/8887', 'G02F2203/69', 'G06T2207/20032', 'G06T2207/20036', 'G06T2207/30121']"
CN107493488B,Method for intelligent implantation of video content based on Faster R-CNN model,"The method for intelligently implanting video contents based on the Faster R-CNN model comprises the following steps: the frames are decoded to obtain a video sound file, a video frame image and an implanted content frame image; detecting and identifying the content contained in each video frame, and performing shot segmentation on the video frame file; selecting a content object as a content implantation area for each lens; accurately positioning the vertex of the content implantation area, tracking the movement of the vertex, and determining the position of the content implantation area in each frame of image in a single lens; carrying out shielding detection on the implantation area of the content in the lens, and automatically and accurately segmenting the motion foreground if shielding exists; inserting the implanted contents into the position of the content implantation area in the video frame, and if a motion foreground is obtained in the previous step, supplementing the motion foreground back to the video frame; and synthesizing the video frame obtained in the last step with the obtained video and sound file to obtain the video embedded with the content material. The invention can greatly improve the video implantation efficiency of the content.","['H04N21/2668', 'H04N21/23418', 'H04N21/23424', 'H04N21/812']"
US10810745B2,Method and apparatus with image segmentation,"A processor-implemented learning method for an image segmentation includes training first duplicate layers, as duplications of trained first layers of a pre-trained model, so that a second feature extracted from a target image by the trained first duplicate layers is matched to a first feature extracted from a training image by the trained first layers; regularizing the trained first duplicate layers so that a similarity between the first feature and a third feature extracted from the training image by the regularized first duplicate layers meets a threshold; and training second duplicate layers, as duplications of trained second layers of the pre-trained model, to be configured to segment the target image based on the regularized first duplicate layers, the trained second layer being configured to segment the training image.","['G06T7/10', 'G06N3/084', 'G06F18/214', 'G06K9/6256', 'G06N3/045', 'G06N3/0464', 'G06N3/048', 'G06N3/0481', 'G06N3/08', 'G06N3/0895', 'G06N3/09', 'G06N3/094', 'G06N3/096', 'G06T7/136', 'G06V10/764', 'G06V10/776', 'G06V10/82', 'G06V30/274', 'G06N20/10', 'G06T2207/20081', 'G06T2207/20084']"
US10579875B2,Systems and methods for object identification using a three-dimensional scanning system,"A method for identifying and tracking objects includes: capturing one or more 3-D models of one or more objects in a scene using a three-dimensional (3-D) scanning system, the one or more 3-D models including color and geometry information of the one or more objects; and computing, by an analysis agent, one or more descriptors of the one or more 3-D models, each descriptor corresponding to a fixed-length feature vector; and retrieving metadata identifying the one or more objects based on the one or more descriptors.","['G06K9/00664', 'G06V10/82', 'G06F18/24', 'G06K9/6267', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/084', 'G06N3/09', 'G06N3/094', 'G06N3/096', 'G06Q20/208', 'G06T15/10', 'G06T17/00', 'G06T7/0004', 'G06T7/174', 'G06T7/285', 'G06V10/764', 'G06V20/10', 'G06V20/52', 'G06V20/64', 'G07G1/0063', 'G06T2207/10012', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/10048', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084']"
CN106778664B,Iris image iris area segmentation method and device,"The invention discloses a segmentation method of an iris region in an iris image, which comprises the following steps: the first step is as follows: establishing a multi-scale convolutional neural network; the second step is that: pre-labeling a pre-selected preset key point in the iris image; the third step: inputting the iris image labeled with preset key points into the convolutional neural network, and training the convolutional neural network to ensure that a model of the convolutional neural network is converged; the fourth step: inputting the iris image to be tested, which needs to be subjected to iris region segmentation, into the trained convolutional neural network to obtain a binary code image of the iris image to be tested, namely the iris region to be segmented finally. The iris region segmentation method and device in the iris image disclosed by the invention can be used for timely and accurately segmenting the iris region in the iris image acquired in a non-controllable scene, meet the requirement of a user on iris segmentation and improve the working efficiency of the user.","['G06V40/193', 'G06N3/02', 'G06V10/462']"
US10088899B2,Eye gaze tracking utilizing surface normal identification,"A gaze tracking system captures images of a vehicle operator. The gaze tracking system may detect facial features in the images and track the position of the facial features over time. The gaze tracking system may detect a triangle in an image, wherein the vertices of the triangle correspond to the facial features. The gaze tracking system may analyze the detected triangle to identify a surface normal for the triangle, and may track the surface normal (e.g., across multiple images) to track the eye gaze direction of the driver over time. The images may be captured and analyzed in near-real time. By tracking movement of the driver's head and eyes over time, the gaze analysis system may predict or estimate head position and/or gaze direction when one or more facial features are not detectable.","['G06V20/597', 'G06F3/013', 'G06K9/00268', 'G06K9/00597', 'G06K9/00845', 'G06V40/168', 'G06V40/18']"
US11538164B2,Coupled multi-task fully convolutional networks using multi-scale contextual information and hierarchical hyper-features for semantic image segmentation,"Techniques related to implementing fully convolutional networks for semantic image segmentation are discussed. Such techniques may include combining feature maps from multiple stages of a multi-stage fully convolutional network to generate a hyper-feature corresponding to an input image, up-sampling the hyper-feature and summing it with a feature map of a previous stage to provide a final set of features, and classifying the final set of features to provide semantic image segmentation of the input image.","['G06V10/82', 'G06F16/55', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N5/046', 'G06T7/10', 'G06T7/11', 'G06T7/143', 'G06V10/26', 'G06V10/454', 'G06V10/94', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084']"
US9235775B2,Entrance detection from street-level imagery,"Architecture that detects entrances on building facades. In a first stage, scene geometry is exploited and the multi-dimensional problem is reduced down to a one-dimensional (1D) problem. Entrance hypotheses are generated by considering pairs of locations along lines exhibiting strong gradients in the transverse direction. In a second stage, a rich set of discriminative image features for entrances is explored according to constructed designs, specifically focusing on properties such as symmetry and color consistency, for example. Classifiers (e.g., random forest) are utilized to perform automatic feature selection and entrance classification. In another stage, a joint model is formulated in three dimensions (3D) for entrances on a given facade, which enables the exploitation of physical constraints between different entrances on the same facade in a systematic manner to prune false positives, and thereby select an optimum set of entrances on a given facade.","['G06V20/38', 'G06K9/46', 'G06F18/24', 'G06F18/2415', 'G06K9/4604', 'G06K9/4652', 'G06K9/6267', 'G06T7/408', 'G06T7/90', 'G06K2009/4666']"
US6556704B1,Method for forming a depth image from digital image data,"A computer vision/image processing method generates a depth map useful in producing a foreground depth mask for 2D/3D image editing. The method uses image data from a plurality of scenes. Feature points on each of the vertical scan lines in each of the scene images are used to search for corresponding feature points on the corresponding vertical lines in other images. The corresponding feature-point search is performed by using a bipartite match network with a feature-point-ordering constraint and a disparity-limit constraint, and produces an individual feature-point depth map for each input image. A sparse feature-point depth map of the scene is obtained after applying a consistency test to all the individual depth maps. A complete feature-point depth map is produced by applying a color property assisted depth propagation process to the sparse feature-point depth map. Foreground and background separation is then conducted in the depth domain by using the order statistics of the depth data extracted the feature-point depth map. A foreground feature-point depth map is obtained from the separation operation. The final foreground depth mask is generated by applying a color aided eight-nearest-neighbor LMS interpolation process to the foreground feature-point depth map.","['G06T7/593', 'G06T7/12', 'G06T7/194', 'G06V10/10', 'G06V10/24', 'G06T2207/10012', 'G06V2201/12']"
US7409092B2,Method and apparatus for the surveillance of objects in images,"Object detection and tracking operations on images that may be performed independently are presented. A detection module receives images, extracts edges in horizontal and vertical directions, and generates an edge map where object-regions are ranked by their immediacy. Filters remove attached edges and ensure regions have proper proportions/size. The regions are tested using a geometric constraint to ensure proper shape, and are fit with best-fit rectangles, which are merged or deleted depending on their relationships. Remaining rectangles are objects. A tracking module receives images in which objects are detected and uses Euclidean distance/edge density criterion to match objects. If objects are matched, clustering determines whether the object is new; if not, a sum-of-squared-difference in intensity test locates matching objects. If this succeeds, clustering is performed and rectangles are applied to regions and merged or deleted depending on their relationships and are considered objects; if not the regions are rejected.","['G06V20/58', 'G06V10/255', 'G06V10/62']"
CN117392066A,"Defect detection method, device, equipment and storage medium","The embodiment of the specification provides a defect detection method, a device, equipment and a storage medium, wherein the defect detection method comprises the following steps: acquiring an initial defect image, and performing optimization treatment on the initial defect image to obtain a target defect image; extracting features based on the target defect image to obtain an initial defect feature image; performing feature enhancement processing based on the initial defect feature image to obtain a target defect feature image; extracting an image based on the target defect characteristic image to obtain a local defect image; and carrying out image analysis on the local defect image to determine a defect area. By optimizing the initial defect image, performing feature enhancement processing based on the initial defect feature image, and performing image extraction based on the target defect feature image, the detection accuracy is improved, the detection time is reduced, and therefore higher detection efficiency is achieved.","['G06T7/0004', 'G06T5/20', 'G06T5/30', 'G06T7/11', 'G06T7/136', 'G06V10/44', 'G06T2207/20036', 'G06T2207/30108']"
US8300949B2,Edge detection technique having improved feature visibility,A method for determining edge features of an image comprising filtering at least a portion of the image to attenuate high frequency signals of the image to an extent greater than low frequency signals of the image. Performing a one-dimensional search in a two different horizontal directions relative to a particular pixel of the image to determine horizontal direction local maximums. Calculating a horizontal gradient based upon the horizontal direction local maximums. Performing a one-dimensional search in a two vertical horizontal directions relative to a particular pixel of the image to determine vertical direction local maximums. Calculating a vertical gradient based upon the vertical direction local maximums. Calculating a gradient for the particular pixel based upon the horizontal gradient and the vertical gradient.,['G06T7/13']
CN111444889B,Fine granularity action detection method of convolutional neural network based on multistage condition influence,"Establishing a multi-level condition influence convolutional neural network based on a fine granularity action detection method of the multi-level condition influence convolutional neural network, fusing additional explicit knowledge in a visual scene with multi-level visual characteristics, generating multi-level visual characteristics by taking a multi-branch convolutional neural network structure of the multi-level condition influence convolutional neural network as a main body, simultaneously encoding additional spatial semantic information of human body structure and object context information as conditions, dynamically influencing CNN characteristic extraction through affine transformation and attention mechanism, and finally fusing and modulating multi-mode characteristics to distinguish various interactive actions; and performing model training on the convolutional neural network influenced by the multistage conditions, and outputting a fine-granularity action detection result by the obtained model. The method is evaluated on two most commonly used references, namely HICO-DET and V-COCO, and experimental results show that the method is superior to the existing method.","['G06V40/20', 'G06F18/241', 'G06F18/2415', 'G06F18/253', 'G06N3/045', 'G06N3/08']"
CN106446896B,Character segmentation method and device and electronic equipment,"The invention discloses a character segmentation method, a character segmentation device and electronic equipment, a character recognition system, an image separation method, an image separation device and electronic equipment, and an italic character correction method, an italic character correction device and electronic equipment. The character segmentation method comprises the following steps: acquiring character segmentation points of the character image to be segmented as candidate segmentation points through a preset segmentation point generation algorithm; the character image to be segmented is a foreground character image which is obtained by separating an original gray character image and is removed of a background image; screening and obtaining correct segmentation points from candidate segmentation points according to the original gray character image and a pre-generated segmentation point classifier; and according to the correct segmentation point, performing character segmentation on the character image to be segmented. By adopting the method provided by the application, the candidate segmentation points can be filtered to obtain the correct segmentation points, and the character image with the phenomena of character fracture and the like is prevented from being excessively segmented, so that the effect of improving the character segmentation accuracy is achieved.","['G06V10/26', 'G06V10/267', 'G06V30/153', 'G06V30/158', 'G06F2218/12']"
US20240029283A1,"Image depth prediction method, electronic device, and non-transitory storage medium","An image depth prediction method acquires image frames of containing a dynamic object by a monocular camera, extracts a continuous of object frames and reference frames from the image frames, reconstructs the object frames to obtain reconstructed frames according to the reference frames and a preset depth estimation model, obtains a reconstruction error between the object frames and the reconstructed frames, processes the image frames to obtain point cloud data and instance segmentation data, fuses the point cloud data with the instance segmentation data to obtain mask data, obtains a loss function according to the reconstruction error and the mask data, and trains the depth estimation model based on the loss function until the loss function converges. The method can obtain more accurate depth estimation results for dynamic scenes. An electronic device and a non-transitory storage recording the method are also disclosed.","['G06T7/30', 'G06T7/55', 'G06T3/0031', 'G06T3/06', 'G06T7/10', 'G06T7/11', 'G06T7/70', 'G06T7/80', 'G06V10/80', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/30244']"
US12118455B2,"Systems, methods, and/or media, for selecting candidates for annotation for use in training a classifier","Systems for selecting candidates for labelling and use in training a convolutional neural network (CNN) are provided, the systems comprising: a memory device; and at least one hardware processor configured to: receive a plurality of input candidates, wherein each candidate includes a plurality of identically labelled patches; and for each of the plurality of candidates: determine a plurality of probabilities, each of the plurality of probabilities being a probability that a unique patch of the plurality of identically labelled patches of the candidate corresponds to a label using a pre-trained CNN; identify a subset of candidates of the plurality of input candidates, wherein the subset does not include all of the plurality of candidates, based on the determined probabilities; query an external source to label the subset of candidates to produce labelled candidates; and train the pre-trained CNN using the labelled candidates.","['G06N3/08', 'G06F18/2148', 'G06F18/217', 'G06F18/2413', 'G06F18/28', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/09', 'G06N3/091', 'G06N3/096', 'G06V10/454', 'G06V10/764', 'G06V10/772', 'G06V10/7747', 'G06V10/776', 'G06V10/82']"
US8317325B2,Apparatus and method for two eye imaging for iris identification,"The apparatus represents a device having one or two sensors for capturing a single image or two images having the subject's eyes, and processor(s) in a housing with the one or two sensors and/or in a computer system which receives the single image or two images. Such processor(s) determine a head tilt angle between a virtual line extending between the two eyes of the subject in accordance with a predefined features associated with the eyes and a dimension characterizing zero head tilt in the single image or two images, segment left and right iris images, and rotate the segmented left and right iris images in accordance with the angle to substantially remove head tilt when present. The apparatus may also determine head tilt using predefined features associated with a single eye in the image. The resulting iris image(s) are utilized for enrollment or identification.","['A61B3/1216', 'G06V40/193']"
CN110659664B,A method for recognizing small objects with high precision based on SSD,"The invention discloses a high-precision small object identification method based on an SSD. The improved network structure of the invention is as follows: (1) Based on an original SSD network, a waterfall decreasing structure same as that of the SSD network is reserved, and then Conv8_2 and Conv9_2 behind Conv7 are replaced by RFB modules to form a network I; (2) The network I is improved into a characteristic pyramid structure network II similar to FPN, the high-level characteristics can be fused through the characteristic pyramid structure network II through upsampling and low-level characteristics, a fused characteristic diagram is obtained, and therefore the characteristics of each layer of network are used through the characteristic diagram; (3) For the fused feature map, positioning a target by using a new NMS algorithm; a new PrROI posing was introduced to map the ROI thereto to correct the border. The invention utilizes the FPN network to fully utilize the characteristic information of each layer of network and the effect that RFB can simulate the eccentricity in the visual cortex of human beings to improve the receptive field.","['G06F18/29', 'G06F18/253', 'G06N3/045', 'G06N3/08']"
CN111340814B,RGB-D image semantic segmentation method based on multi-mode self-adaptive convolution,"The invention relates to an RGB-D image semantic segmentation method based on multi-mode self-adaptive convolution, which comprises the following steps: the coding module extracts RGB image characteristics and depth image characteristics respectively; sending the RGB features and the depth features into a fusion module for fusion; firstly, inputting the multi-modal characteristics into a multi-modal self-adaptive convolution generating module, and calculating multi-modal self-adaptive convolution kernels with two different scales; then, the multi-mode feature fusion module respectively carries out depth separable convolution operation on the RGB features and the depth features and the self-adaptive convolution kernel to obtain self-adaptive convolution fusion features; splicing the three features with the RGB features and the depth features to obtain a final fusion feature; the decoding module continuously upsamples the final fusion features, and semantic segmentation results are obtained through convolution operation; the invention enables the multi-mode characteristics to be interacted in a cooperative way through the self-adaptive convolution, and the convolution kernel parameters of the multi-mode characteristics are dynamically adjusted according to the input multi-mode images, so that the method is more flexible compared with the traditional convolution kernel with fixed parameters.","['G06T7/10', 'G06F18/241', 'G06N3/045', 'G06N3/084', 'G06T2207/10004', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084']"
AU2013245477A1,"Method, system and apparatus for determining a contour segment for an object in an image captured by a camera","- 38 Abstract METHOD, SYSTEM AND APPARATUS FOR DETERMINING A CONTOUR SEGMENT FOR AN OBJECT IN AN IMAGE CAPTURED BY A CAMERA A method of determining a contour segment for an object in an image captured by a camera is disclosed. A plurality of corner points located on a part of an object in the image is determined. Pairs of corner points are selected from the determined corner points. A path for at least one of the pairs of corner points is determined, the path connecting the corner points 0 of the at least one pair of corner points along the boundary of the object. The contour segment is determined for the object using the determined path. 7937988vl (PO86350_SpeciAs Filed) 11240a 20a 1260a0 12970a Time T Fig. 12A 1250b1240b 1290b Time T+1 Fie. 12B","['G06T7/12', 'G06T7/149', 'G06V10/457', 'G06V10/46', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20072', 'G06T2207/30232']"
US10460033B2,"Structured knowledge modeling, extraction and localization from images","Techniques and systems are described to model and extract knowledge from images. A digital medium environment is configured to learn and use a model to compute a descriptive summarization of an input image automatically and without user intervention. Training data is obtained to train a model using machine learning in order to generate a structured image representation that serves as the descriptive summarization of an input image. The images and associated text are processed to extract structured semantic knowledge from the text, which is then associated with the images. The structured semantic knowledge is processed along with corresponding images to train a model using machine learning such that the model describes a relationship between text features within the structured semantic knowledge. Once the model is learned, the model is usable to process input images to generate a structured image representation of the image.","['G06N3/08', 'G06F17/2785', 'G06F16/5838', 'G06F40/30', 'G06K9/4685', 'G06K9/6232', 'G06N3/042', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G06N5/022', 'G06V10/424', 'G06V10/7715', 'G06V10/84', 'G06V20/70', 'G06N3/044']"
US11632533B2,System and method for generating combined embedded multi-view interactive digital media representations,"Various embodiments describe systems and processes for capturing and generating multi-view interactive digital media representations (MIDMRs). In one aspect, a method for automatically generating a MIDMR comprises obtaining a first MIDMR and a second MIDMR. The first MIDMR includes a convex or concave motion capture using a recording device and is a general object MIDMR. The second MIDMR is a specific feature MIDMR. The first and second MIDMRs may be obtained using different capture motions. A third MIDMR is generated from the first and second MIDMRs, and is a combined embedded MIDMR. The combined embedded MIDMR may comprise the second MIDMR being embedded in the first MIDMR, forming an embedded second MIDMR. The third MIDMR may include a general view in which the first MIDMR is displayed for interactive viewing by a user on a user device. The embedded second MIDMR may not be viewable in the general view.","['H04N13/221', 'G06T17/00', 'G06T7/285', 'H04N13/117', 'H04N13/178', 'H04N13/189', 'H04N13/243', 'H04N13/275', 'H04N13/279', 'H04N13/344', 'H04N13/366', 'G06T2207/10021', 'G06T2207/30252', 'G06T2219/2008']"
CN111695488B,"Interest area identification method, device, equipment and storage medium","The application discloses a method, a device, equipment and a storage medium for identifying an interest surface, and relates to artificial intelligence and deep learning. The specific implementation scheme is as follows: acquiring target function category information corresponding to a target interest surface in a target area to be identified and geographic attribute related data of the target area; acquiring input data of a corresponding preset computer vision model according to the target function class information and the geographic attribute related data of the target area; and identifying the outline of the target interest surface in the target area according to the input data and the corresponding preset computer vision model. The application realizes the identification of the interest surface in the target area by utilizing the computer vision technology, does not need to carry out manual labeling based on street view images, can reduce the cost, improve the efficiency, has higher accuracy, can be suitable for different scenes, has larger application range, can be applied to the identification of massive interest surfaces, and further enables the description of the surface-shaped information of massive interest points to be possible.","['G06V20/176', 'G06F16/9537', 'G06F18/23', 'G06F18/24', 'G06N3/045', 'G06N3/08', 'G06Q30/0631', 'G06Q30/0639', 'G06V10/267']"
US10642369B2,"Distinguishing between one-handed and two-handed gesture sequences in virtual, augmented, and mixed reality (xR) applications","Systems and methods for distinguishing between one-handed and two-handed gesture sequences in virtual, augmented, and mixed reality (xR) applications are described. In an illustrative, non-limiting embodiment, an Information Handling System (IHS) may include a processor and a memory coupled to the processor, the memory having program instructions stored thereon that, upon execution, cause the IHS to: receive a gesture sequence from a user wearing a Head-Mounted Device (HMD) coupled to the IHS, where the HMD is configured to display an xR application, and identify the gesture sequence as: (i) a one-handed gesture sequence, or (ii) a two-handed gesture sequence.","['G06F3/017', 'G06F1/163', 'G06F1/1686', 'G06F3/011', 'G06F3/012', 'G06F3/0304', 'G06F3/0482', 'G06F3/04845', 'G06K9/00355', 'G06K9/00382', 'G06K9/00389', 'G06V40/11', 'G06V40/113', 'G06V40/28']"
CN110276356B,Fundus image microaneurysm identification method based on R-CNN,"An R-CNN architecture-based fundus microaneurysm target detection model for realizing detection and identification of fundus microaneurysm focuses, the method comprises the following steps: preprocessing the fundus image; performing blood vessel segmentation on the preprocessed image; the method comprises the steps of carrying out local self-adaptive threshold segmentation, blood vessel removal and area screening on a preprocessed image to obtain a real microaneurysm candidate region; adopting data enhancement to expand the number of training samples; performing feature extraction on the sample by using a pre-trained VGG16 network by adopting a transfer learning method, and adding a microaneurysm classifier after the feature extraction network for joint training; the scheme provides a new method for detecting the diabetic retina image fundus microaneurysm target.","['G06F18/214', 'G06T7/0012', 'G06T7/136', 'G06V10/20', 'G06V10/267', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/30096', 'G06T2207/30101']"
US10198823B1,Segmentation of object image data from background image data,"Devices and techniques are generally described for segmentation of object image data from background image data. In some examples, the segmentation of object image data may comprise capturing image data comprising color data and depth data. In some examples, the segmentation of object image data may further include separating the depth data into a plurality of clusters of image data, wherein each cluster is associated with a respective range of depth values. In various examples, the segmentation of object image data may comprise selecting a main cluster of image data as corresponding to an object of interest in the image data. In various other examples, the segmentation of object image data may comprise identifying pixels of the main cluster that correspond to the object of interest.","['G06T7/194', 'G06T11/60', 'G06T7/11', 'G06T7/90', 'G06T2207/10024', 'G06T2207/10028', 'G06T2210/22']"
CN113518996B,Damage detection from multi-view visual data,"The plurality of images may be analyzed to determine an object model. The object model may have a plurality of components, and each of the images may correspond to one or more of the components. Component condition information for one or more of the components may be determined based on the image. The component condition information may indicate damage caused by the object portion corresponding to the component.","['G01C21/32', 'G06F16/29', 'G06F17/18', 'G06F18/2414', 'G06F30/15', 'G06F9/453', 'G06N3/02', 'G06Q30/0278', 'G06Q40/08', 'G06T15/10', 'G06T15/205', 'G06T17/00', 'G06T19/003', 'G06T19/006', 'G06T7/0002', 'G06T7/0004', 'G06T7/001', 'G06T7/593', 'G06T7/70', 'G06V10/82', 'G06V20/64', 'H04N13/243', 'H04N13/271', 'H04N23/633', 'G06T2200/08', 'G06T2200/24', 'G06T2207/10016', 'G06T2207/10028', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20224', 'G06T2207/30108', 'G06T2207/30156', 'G06T2207/30244', 'G06T2207/30248', 'H04N2013/0081']"
US9317128B2,Remote devices used in a markerless installation of a spatial operating environment incorporating gestural control,"Embodiments described herein includes a system comprising a processor coupled to display devices, sensors, remote client devices, and computer applications. The computer applications orchestrate content of the remote client devices simultaneously across the display devices and the remote client devices, and allow simultaneous control of the display devices. The simultaneous control includes automatically detecting a gesture of at least one object from gesture data received via the sensors. The detecting comprises identifying the gesture using only the gesture data. The computer applications translate the gesture to a gesture signal, and control the display devices in response to the gesture signal.","['G06F3/017', 'G06F3/1423', 'G06K9/00355', 'G06K9/00375', 'G06V40/107', 'G06V40/28']"
US11514244B2,Structured knowledge modeling and extraction from images,"Techniques and systems are described to model and extract knowledge from images. A digital medium environment is configured to learn and use a model to compute a descriptive summarization of an input image automatically and without user intervention. Training data is obtained to train a model using machine learning in order to generate a structured image representation that serves as the descriptive summarization of an input image. The images and associated text are processed to extract structured semantic knowledge from the text, which is then associated with the images. The structured semantic knowledge is processed along with corresponding images to train a model using machine learning such that the model describes a relationship between text features within the structured semantic knowledge. Once the model is learned, the model is usable to process input images to generate a structured image representation of the image.","['G06F40/30', 'G06N5/022', 'G06F16/5866', 'G06N20/00', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/0895', 'G06N7/01']"
WO2019223274A1,Methods and systems for improved quality inspection,"A method of performing automated object inspection includes obtaining a plurality of test images. For each of the plurality of test images, the method includes performing independent object inspection on each of two or more sub-portions of the test image. The method further includes segmenting the test image into at least a first sub-portion of the test image and a second sub-portion of the test image; performing object inspection on the first sub-portion of the test image using a first subset of information channels of the test image and a first model trained on a first set of training images containing the first component; and performing object inspection on the second sub-portion of the test image using a second subset of information channels of the test image, and a second model trained on a second set of training images containing the second component.","['G06T7/0004', 'G06F18/214', 'G06T7/11', 'G06V10/46', 'G06V10/50', 'G06V10/758', 'G06V10/945', 'G06T2207/20081']"
WO2022007685A1,Method and device for text-based image generation,"A method and device for image generation are provided. The method includes: obtaining a text describing a content of an image to be generated; extracting, using a text encoder, a text feature vector from the text; determining a semantic mask as spatial constraints of the image to be generated; and automatically generating the image using a generative adversarial network (GAN) model according to the semantic mask and the text feature vector.","['G06T11/203', 'G06T11/00', 'G06F18/213', 'G06F40/126', 'G06F40/20', 'G06F40/279', 'G06F40/289', 'G06F40/30', 'G06N3/02', 'G06N3/045', 'G06N3/047', 'G06N3/0475', 'G06N3/088', 'G06V10/70', 'G06V10/768', 'G06V10/82', 'H04N21/8153', 'G06N3/044', 'G06T2207/20084']"
US12130964B2,Use of eye tracking to adjust region-of-interest (ROI) for compressing images for transmission,Gaze tracking data representing a user's gaze is analyzed to determine one or more regions of interest. One or more gaze tracking parameters are determined from the gaze tracking data. Adjusted foveation data is determined representing an adjusted size and/or shape of one or more regions of interest in one or more images to be subsequently presented to the user based on the one or more gaze tracking parameters. The compression of the one or more transmitted images is adjusted so that fewer bits are needed to transmit data for portions of an image outside the one or more regions of interest than for portions of the image within the one or more regions of interest. Adjusting compression includes eliminating the region(s) of interest from images that are presented to the user during the saccade or blink.,"['G06F3/013', 'G02B27/017', 'G06F1/163', 'G06F1/1686', 'G06F1/325', 'G06F3/011', 'G06F3/0304', 'G06F3/147', 'H04N19/124', 'H04N19/167', 'G02B2027/014', 'G02B2027/0178', 'G02B2027/0187', 'G09G2310/04', 'G09G2340/0428', 'G09G2350/00']"
US7660445B2,Method for selecting an emphasis image from an image collection based upon content recognition,"A method for selecting an emphasis image from a collection of images based on facial identification comprises the steps of: (a) obtaining a collection of digital images; (b) detecting image patterns indicative of the presence of one or more faces in the digital images, thereby identifying one or more detected faces for each image in which a face is detected; (c) recognizing one or more faces from the detected faces for each of the images in which a face is detected; and (d) scoring an image based on the relative frequency of occurrence of a recognized face within the collection of images, thereby producing an emphasis image characteristic of the most frequently occurring face in the collection of images.","['G06F16/5854', 'G06F16/5838']"
US9152292B2,Image collage authoring,"A user interface that includes a catalog area, a collage mock-up area, and a mode select interface control operable to select an operational state of the user interface is displayed. Thumbnails of respective images are shown in the catalog area. A layout of a subset of the images is presented in the collage mock-up area. In response to the receipt of a user input gesture and a determination that the user interface is in a first operational state, a first action type is performed based on the type of the received user input gesture and the object type of the target object. In response to the receipt of the user input gesture and a determination that the user interface is in a second operational state, a second action type is performed based on the type of the received user input gesture and the object type of the target object.","['G06F3/0481', 'G06T11/60', 'G06F16/9577', 'G06F17/30905', 'G06F3/017', 'G06F3/04817', 'G06F3/0482', 'G06F3/0484', 'G06F3/04883', 'G06F3/1296', 'G06F9/4443', 'G06F9/451', 'H04N2201/0082']"
CN105447459B,A kind of unmanned plane detects target and tracking automatically,"A kind of unmanned plane detects target and tracking automatically, including unmanned plane, the sonar range finder being assemblied on unmanned plane, illumination compensation module, image denoising module, face detection module, fuselage face recognition module, distal end face recognition module, target tracking module, flight control modules, console module；The illumination compensation module carries out illumination compensation to image；The image denoising module carries out Denoising disposal to image；The image received is carried out Face datection by the face detection module；The facial image that the fuselage face recognition module will test out is identified, the recognition result of distal end face recognition module is added；The facial image that the distal end face recognition module can not handle fuselage identifies；The target tracking module tracks target；The flight control modules control the motion profile of unmanned plane；The console module is by being manually monitored and issuing various orders.","['G06V40/161', 'G06V40/172']"
US6064749A,Hybrid tracking for augmented reality using both camera motion detection and landmark tracking,"Systems, methods and computer program products which have the registration accuracy of vision-based tracking systems and the robustness of magnetic tracking systems. Video tracking of landmarks is utilized as the primary method for determining camera position and orientation but is enhanced by magnetic or other forms of physical tracking camera movement and orientation. A physical tracker narrows the landmark search area on images, speeding up the landmark search process. Information from the physical tracker may also be used to select one of several solutions of a non-linear equation resulting from the vision-based tracker. The physical tracker may also act as a primary tracker if the image analyzer cannot locate enough landmarks to provide proper registration, thus, avoiding complete loss of registration. Furthermore, if 1 or 2 landmarks (not enough for a unique solution) are detected, several may be utilized heuristic methods are used to minimize registration loss. Catastrophic failure may be avoided by monitoring the difference between results from the physical tracker and the vision-based tracker and discarding corrections that exceed a certain magnitude. The hybrid tracking system is equally applicable to see-through and video augmented reality systems.","['G06F3/011', 'G06T19/006', 'G06T7/73', 'G06V10/245']"
CN114821105B,Optical flow calculation method combining image pyramid subnet guidance and circulating cross attention,"The invention provides an optical flow calculation method combining image pyramid subnet guidance and circulating cross attention. The method comprises the following steps of 1) inputting two continuous frames of images of an image sequence into an image pyramid subnet and a feature pyramid subnet respectively, 2) processing the images by using the image pyramid subnet, 3) adding and fusing the features extracted by the image pyramid subnet and the features extracted by the feature pyramid at the same layer to serve as the input of the feature pyramid at the next layer, and 4) inputting the fused features into a circulating cross attention module at the last three layers of the feature pyramid to extract contextual information. According to the optical flow calculation method combining image pyramid subnet guidance and circulating cross attention, the feature information of the motion edge and the large displacement area of the image sequence is extracted through the supplement of shallow information and the accurate extraction capability of context information, so that the accuracy and the robustness of optical flow estimation are remarkably improved.","['G06V10/44', 'G06N3/045', 'G06V10/82']"
US7062093B2,System and method for object recognition,"A system and method recognize a user-defined model object within an image. The system and method recognize the model object with occlusion when the model object to be found is only partially visible. The system and method also recognize the model object with clutter when there may be other objects in the image, even within the model object. The system and method also recognize the model object with non-linear illumination changes as well as global or local contrast reversals. The model object to be found may have been distorted, when compared to the user-defined model object, from geometric transformations of a certain class such as translations, rigid transformations by translation and rotation, arbitrary affine transformations, as well as similarity transformations by translation, rotation, and scaling.",['G06V10/7515']
US9547908B1,Feature mask determination for images,"Implementations relate to feature mask determination for images. In some implementations, a computer-implemented method to determine a feature mask for an image includes estimating one or more prior regions in the image that define a feature in the image. The method determines superpixels based on multiple pixels of the image similar in color. The method constructs a graph, each node of the graph corresponding to a superpixel, and determines a superpixel score for each superpixel based on a number of pixels of the superpixel. The method determines one or more segmented regions in the image based on applying a graph cut technique to the graph based at least on the superpixel scores, and determines the feature mask based on the segmented regions. The feature mask indicates a degree to which pixels of the image depict the feature. The method modifies the image based on the feature mask.","['G06T7/11', 'G06T7/0081', 'G06K9/00228', 'G06T3/40', 'G06T5/20', 'G06T7/162', 'G06T7/194', 'G06T7/408', 'G06T7/90', 'G06V10/426', 'G06V40/10', 'G06V40/161', 'G06V40/162', 'G06T2207/10024', 'G06T2207/20028', 'G06T2207/30196']"
CA2880054C,Virtual controller for visual displays,"Virtual controllers for visual displays are described. In one implementation, a camera captures an image of hands against a background. The image is segmented into hand areas and background areas. Various hand and finger gestures isolate parts of the background into independent areas, which are then assigned control parameters for manipulating the visual display. Multiple control parameters can be associated with attributes of multiple independent areas formed by two hands, for advanced control including simultaneous functions of clicking, selecting, executing, horizontal movement, vertical movement, scrolling, dragging, rotational movement, zooming, maximizing, minimizing, executing file functions, and executing menu choices.","['G06F3/04845', 'G06F3/017', 'G06F3/0304', 'G06F3/04815', 'G06F3/0487']"
US6282307B1,Method and system for the automated delineation of lung regions and costophrenic angles in chest radiographs,"A method, system, and computer product for the automated segmentation of the lung fields and costophrenic angle (CP) regions in posteroanterior (PA) chest radiographs wherein image segmentation based on gray-level threshold analysis is performed by applying an iterative global gray-level thresholding method to a chest image based on the features of a global gray-level histogram. Features of the regions in a binary image constructed at each iteration are identified and analyzed to exclude regions external to the lung fields. The initial lung contours that result from this global process are used to facilitate a local gray-level thresholding method. Individual regions-of-interest (ROIs) are placed along the initial contour. A procedure is implemented to determine the gray-level thresholds to be applied to the pixels within the individual ROIs. The result is a binary image, from which final contours are constructed. Smoothing processes are applied, including a unique adaptation of a rolling ball method. CP angles are identified and delineated by using the lung segmentation contours as a means of placing ROIs that capture the CP angle regions. Contrast-based information is employed on a column-by-column basis to identify initial diaphragm points, and maximum gray-level information is used on a row-by-row basis to identify initial costal points. Analysis of initial diaphragm and costal points allows for appropriate adjustment of CP angle ROI positioning. Polynomial curve-fitting is used to combine the diaphragm and costal points into a continuous, smooth CP angle delineation. This delineation is then spliced into the final lung segmentation contours. In addition, quantitative information derived from the CP angle delineations is used to assess the presence of abnormal CP angles.","['G06T7/11', 'G06T7/12', 'G06T7/66', 'G06V10/28', 'A61B6/50', 'G06T2207/10116', 'G06T2207/20012', 'G06T2207/30061']"
US6771262B2,System and method for volume rendering-based segmentation,"A three-dimensional (3D) imaging system and method include a processor for generating a volume-rendered 3D image on a display using a plurality of voxels from a 3D image dataset; and a view selector which responds to user inputs for determining a first set of voxels corresponding to a boundary in the volume-rendered 3D image using a predetermined boundary-specifying criteria applied to a function of the opacity of the plurality of voxels. The processor responds to the first set and to a user function selection by performing the selected function to modify the volume-rendered 3D image relative to the detected boundaries. The selected function is a display function, a measurement function, or a segmentation function. The view selector determines the boundary from voxels having opacities greater than a predetermined threshold, or determines the boundary from voxels having a gradient in opacities greater than a predetermined threshold. Alternatively, the view selector determines the boundary from voxels having a greatest contribution to a total intensity which is a function of the opacities of the voxels, or determines the boundary from voxels having respective contributions greater than a predetermined threshold, with such contributions being to a total intensity which is a function of the opacities of the voxels.","['G06T15/08', 'G06T19/00', 'G06T7/12', 'G06T2200/04', 'G06T2207/30004', 'G06T2210/41', 'G06T2219/028']"
CN113591967B,"Image processing method, device, equipment and computer storage medium","The embodiment of the application provides an image processing method, an image processing device, image processing equipment and a computer storage medium, relates to the field of image detection, and is used for improving the accuracy of positioning detection of graphic codes in images under complex scenes. The method comprises the following steps: acquiring an image to be detected; inputting an image to be detected into a pre-trained key point detection model, determining the reference confidence coefficient of an object in the image to be detected and a plurality of groups of reference angular point coordinate information of the object, wherein the key point detection model is obtained by training according to a training sample set, the training sample set comprises a plurality of sample graphic codes and label information of each sample graphic code, and the label information comprises a plurality of angular point coordinate information and central point coordinate information of the sample graphic codes; under the condition that the reference confidence coefficient is larger than a preset confidence coefficient threshold value, determining that the object is the to-be-positioned graphic code; and screening a plurality of reference frames determined by the plurality of groups of reference angular point coordinate information to determine the frame position information of the graphic code to be positioned.","['G06F18/214', 'G06K19/06009', 'G06N3/045', 'G06N3/08']"
US10970844B2,"Image segmentation method and device, computer device and non-volatile storage medium","An image segmentation method and device, a computer device and a non-volatile storage medium are provided. The image segmentation method includes: performing super-pixel segmentation on an image to be segmented to obtain a super-pixel image, and binarizing the image to be segmented to obtain a binary image; combining the super-pixel image and the binary image to obtain a binary super-pixel image; performing distance transformation on the binary super-pixel image, to obtain a grayscale super-pixel image; marking seed points in the grayscale super-pixel image, to obtain a seed point super-pixel image in which grayscale values of the seed points are greater than a first value, and grayscale values of pixel blocks other than the seed points in target regions are the first value; and marking and filling the pixel blocks, grayscale values of which are the first value in the seed point super-pixel image, to obtain a segmented image.","['G06T7/11', 'G06T7/136', 'G06T7/155', 'G06T7/187', 'G06T7/194', 'G06T2207/10004', 'G06T2207/20041', 'G06T2207/20152']"
US10019655B2,Deep-learning network architecture for object detection,"Systems and methods are disclosed herein for automatically identifying a query object within a visual medium. The technique generally involves receiving as input to a neural network a query object and a visual medium including the query object. The technique also involves generating, by the neural network, representations of the query object and the visual medium defining features of the query object and the visual medium. The technique also involves generating, by the neural network, a heat map using the representations. The heat map identifies a location of pixels corresponding to the query object within the visual medium and is usable to generate an updated visual medium highlighting the query object.","['G06K9/6269', 'G06V10/82', 'G06F16/583', 'G06F16/9538', 'G06F18/214', 'G06F18/2411', 'G06F18/2414', 'G06F18/28', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06V10/454', 'G06V10/764', 'G06V10/772', 'G06V10/774', 'G06F16/50', 'G06F16/9535', 'G06F17/30244', 'G06F17/30867']"
US12315084B2,System and method for extracting a region of interest from volume data,"The present disclosure relates to a system and method for extracting a region of interest. Image data in a first sectional plane may be acquired. The image data in the first sectional plane may include at least one first slice image and one second slice image. A first region of interest (ROI) in the first slice image may be determined. A second ROI in the second slice image may be determined. A first volume of interest (VOI) may be determined based on the first ROI, the second ROI, and characteristic information of the image data in the first sectional plane.","['G06T19/00', 'G06T15/20', 'G06T19/20', 'G06T3/08', 'G06T7/11', 'G06T7/12', 'G06T7/13', 'G06T7/187', 'G06V10/25', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'A61B2034/105', 'A61B2034/107', 'A61B2576/00', 'G06T2207/20104', 'G06T2207/30056', 'G06T2207/30096', 'G06T2207/30101', 'G06T2210/41', 'G06T2219/028', 'G06T2219/2021', 'G06V2201/031']"
US10521026B2,Passive optical and inertial tracking in slim form-factor,Systems are provided that include a wireless hand-held inertial controller with passive optical and inertial tracking in a slim form-factor. These systems are configured for use with a head mounted virtual or augmented reality display device (HMD) that operates with six degrees of freedom by fusing (i) data related to the position of the controller derived from a forward-facing optical sensor located in the HMD with (ii) data relating to the orientation of the controller derived from an inertial measurement unit located in the controller.,"['G06F3/0346', 'A63F13/211', 'A63F13/24', 'G02B27/0093', 'G02B27/0172', 'G06F3/011', 'G06F3/013', 'G06F3/017', 'G06F3/038', 'G06T19/006', 'H04N13/204', 'H04N13/257', 'H04N13/344', 'H04N13/383', 'H04N13/398', 'A63F2300/105', 'A63F2300/8082', 'G02B2027/0134', 'G02B2027/0138', 'G02B2027/014', 'H04N2213/001']"
US11188965B2,Method and apparatus for recommending customer item based on visual information,"An apparatus for recommending a customer item identifies a purchase tendency of a customer based on an image, determines a recommended item for the customer by selecting a purchase tendency model corresponding to the purchase tendency, and provides information associated with the determined recommended item.","['G06Q30/0631', 'G06F16/583', 'G06F16/9535', 'G06Q30/0643', 'G06T7/11']"
CN119006469B,Automatic detection method and system for surface defects of substrate glass based on machine vision,"The invention provides a method and a system for automatically detecting defects on the surface of a substrate glass based on machine vision, which relate to the technical field of defect detection and comprise the steps of acquiring multi-mode data on the surface of the substrate glass, preprocessing, performing cross-mode feature fusion on extracted image features, acoustic features and thermal imaging features, and inputting the fused feature images into a pre-constructed image neural network to obtain a topological feature image representing a defect structure; the method comprises the steps of carrying out cascade connection on a topological feature diagram and a fused feature diagram, inputting the cascade connection to a multi-task learning network to obtain a segmented suspected defect region and a corresponding defect type, carrying out defect positioning by utilizing a pre-trained target detection model, determining the accurate position of a defect through bounding box regression to obtain a target defect region image, uploading the target defect region image to a cloud, and carrying out accurate identification on the defect type of the target defect region by utilizing the defect detection model deployed at the cloud to obtain a final defect detection result.","['G06T7/0004', 'G06N3/042', 'G06N3/0464', 'G06N3/08', 'G06V10/40', 'G06V10/764', 'G06V10/766', 'G06V10/806', 'G06V10/82', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30121', 'Y02P90/30']"
CN110008932B,Vehicle violation line-pressing detection method based on computer vision,"The invention discloses a vehicle violation line pressing detection method based on computer vision, which comprises the following steps of 1: the obtained image is filtered and equalized, so that the noise influence is weakened, and the gray level of the image is uniformly distributed; step 2: carrying out edge detection on the image processed in the step 1 by adopting a Canny edge detection algorithm, and carrying out image segmentation by adopting a threshold value; and step 3: performing background modeling including background learning, background pixel matching, discontinuous contour processing and foreground hole area filling by using a Gaussian mixture background model; and 4, step 4: acquiring straight line information in the model obtained in the step 3 by using Hough transformation, and carrying out AND operation on the straight line image and a contour image detected by an original image Canny operator; and 5: using the moment of the image to extract features, and screening out a target straight line according to the length and the angle of the straight line to be obtained in the detection of the lane line by using the characteristics of the moment; and 6: and judging whether to press the line violated by the shielding condition of the lane solid line and the profile characteristic of the line pressing object.","['G06V10/267', 'G06V10/44', 'G06V10/48', 'G06V20/54']"
CN112749344B,"Information recommendation method, device, electronic device, storage medium and program product","The application discloses an information recommendation method, an information recommendation device, electronic equipment, a storage medium and a program product, relates to the computer technology, and particularly relates to the technical field of big data and intelligent search. Comprising the following steps: acquiring a user searching intention, determining an intention keyword in the user searching intention, determining a text to be recommended corresponding to an entity word with an association relation with the intention keyword according to the intention keyword and a preset text library, wherein the preset text library comprises at least one text, each text in the preset text library is marked with at least one entity word, pushing the text to be recommended to a user, and determining the text to be recommended through the marked at least one entity word and the intention keyword, so that the text to be recommended is highly attached to the intention keyword, and the accuracy and reliability of recommendation are improved.","['G06F16/9535', 'G06F16/3344', 'G06F16/335', 'G06F40/295']"
US20140309986A1,Word breaker from cross-lingual phrase table,"Automatically creating word breakers which segment words into morphemes is described, for example, to improve information retrieval, machine translation or speech systems. In embodiments a cross-lingual phrase table, comprising source language (such as Turkish) phrases and potential translations in a target language (such as English) with associated probabilities, is available. In various examples, blocks of source language phrases from the phrase table are created which have similar target language translations. In various examples, inference using the target language translations in a block enables stem and affix combinations to be found for source language words without the need for input from human-judges or prior knowledge of source language linguistic rules or a source language lexicon.","['G06F17/2755', 'G06F40/268', 'G06F17/28', 'G06F40/44', 'G06F40/45']"
US20240331413A1,Associating two dimensional label data with three-dimensional point cloud data,"An image processing device is provided that can easily generate label data used for an object recognition technology using three-dimensional point cloud data. The image processing device (1) includes a three-dimensional point cloud obtaining unit (2), a two-dimensional label obtaining unit (4), and a label conversion unit (6). The three-dimensional point cloud obtaining unit (2) obtains three-dimensional point cloud data that represents a three-dimensional point cloud of an object. The two-dimensional label obtaining unit (4) obtains two-dimensional label data corresponding to a two-dimensional image of the object. The label conversion unit (6) associates the two-dimensional label data with the three-dimensional point cloud data, and converts the two-dimensional label data into three-dimensional point cloud label data that indicates a label of the three-dimensional point cloud.","['G06V20/653', 'G06V20/647', 'G06T7/174', 'G06V10/143', 'G06V10/26', 'G06V10/762', 'G06V10/766', 'G06V20/70']"
CN114299086A,"Image segmentation processing method, electronic equipment and system for low-contrast imaging","The embodiment of the invention discloses an image segmentation processing method for low-contrast imaging, electronic equipment and a system. The method comprises the steps of obtaining mask outline information of a bonding pad from an original image; obtaining a classification characteristic contour map according to the mask contour information; acquiring a group of refined feature contour maps according to the mask contour information; acquiring a fitted circular contour of a welding spot and a first positioning area of the welding spot according to the classification characteristic contour map; acquiring a second positioning area of the welding spot according to the refined feature profile and the first positioning area; according to the second positioning area, obtaining a final welding point position and a final welding point outline in a group of corresponding refined feature outline maps in the first positioning area; and comparing the final contour of the welding spot with the contour of the welding spot corresponding to the final position of the welding spot in the sample to obtain the deviation value of the welding spot. Through the mode, the embodiment of the invention can distinguish the bonding pad and the welding point more quickly.",['Y02P90/30']
US20210034860A1,System and method of handwriting recognition in diagrams,"A system, method and computer program product for hand-drawing diagrams including text and non-text elements on a computing device are provided. The computing device has a processor and a non-transitory computer readable medium for detecting and recognizing hand-drawing diagram element input under control of the processor. Display of input diagram elements in interactive digital ink is performed on a display device associated with the computing device. One or more of the diagram elements are associated with one or more other of the diagram elements in accordance with a class and type of each diagram element. The diagram elements are re-displayed based on one or more interactions with the digital ink received and in accordance with the one or more associations.","['G06K9/00476', 'G06F3/04883', 'G06F16/353', 'G06F40/205', 'G06K9/00402', 'G06K9/00416', 'G06K9/00436', 'G06K9/222', 'G06T11/206', 'G06V30/1423', 'G06V30/32', 'G06V30/347', 'G06V30/387', 'G06V30/422']"
US9373029B2,Invisible junction feature recognition for document security or annotation,"The present invention uses invisible junctions which are a set of local features unique to every page of the electronic document to match the captured image to a part of an electronic document. The present invention includes: an image capture device, a feature extraction and recognition system and database. When an electronic document is printed, the feature extraction and recognition system captures an image of the document page. The features in the captured image are then extracted, indexed and stored in the database. Given a query image, the features in the query image are extracted and compared against those stored in the database to identify the query image. The feature extraction and recognition system of the present invention is integrated into a multifunction peripheral. This allows the feature extraction and recognition system to be used in conjunction with other modules to provide security and annotation applications.","['G06V30/412', 'G06K9/00449', 'G06K9/00463', 'G06V30/414']"
US10496884B1,Transformation of textbook information,"Methods and systems for transforming image data and training or testing neural networks. Images from textbooks can contain valuable related text. Transforming the related text into discrete determinate labels can be performed using natural language processing. Once transformed, the images and the labels can be advantageously used together to train neural networks.","['G06T7/0012', 'G06K9/00671', 'G06N3/045', 'G06N3/0464', 'G06N3/084', 'G06N3/0895', 'G06N3/09', 'G06V10/82', 'G06V20/20', 'G06V30/1916', 'G06V30/19173', 'G06V30/413', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30176', 'G06V2201/03']"
US12073574B2,Structuring visual data,"Mappings are determined between viewpoints of an object and an object model representing the object. Each mapping identifies a location on the object model corresponding with a portion of the object captured in one of the viewpoints. Tags for the object model are created based on the mappings, where each tag links one of the viewpoints to one of the locations on the object model. A user interface that includes the object model and the tags is provided for presentation on a display screen in a user interface. One of the viewpoints is presented in the user interface when the corresponding tag is selected in the object model.","['G06T7/38', 'G06T15/20', 'G06F3/011', 'G06N3/04', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T15/205', 'G06T17/20', 'G06T19/003', 'G06T19/006', 'G06T7/33', 'G06T7/75', 'G06T7/97', 'H04N23/64', 'G06T2200/08', 'G06T2200/24', 'G06T2207/10016', 'G06T2207/30244', 'G06T2219/004']"
US11861927B1,Generating tracklets from digital imagery,"Actors may be detected and tracked within a scene using multiple imaging devices provided in a network that are aligned with fields of view that overlap at least in part. Processors operating on the imaging devices may evaluate the images using one or more classifiers to recognize body parts within the images, and to associate the body parts with a common actor within the scene. Each of the imaging devices may generate records of the positions of the body parts and provide such records to a central server, that may correlate body parts appearing within images captured by two or more of the imaging devices and generate a three-dimensional model of an actor based on positions of the body parts. Motion of the body parts may be tracked in subsequent images, and the model of the actor may be updated based on the motion.","['G06V40/10', 'G06T7/73', 'G06F18/2411', 'G06F18/24133', 'G06F18/2415', 'G06T7/246', 'G06T7/292', 'G06V10/34', 'G06V10/42', 'G06V10/426', 'G06V20/52', 'H04N7/188', 'G06T2207/20084', 'G06T2207/30196', 'G06T2207/30241', 'G06T7/74', 'G06V10/56', 'G06V10/751', 'H04N7/181']"
US7660436B2,Stereo-vision based imminent collision detection,"A stereo vision based collision avoidance systems having stereo cameras that produce imagery that is processed to produce a depth map of a scene. A potential threat is detected in the depth map. The size, position, and velocity of the detected potential threat are then estimated, and a trajectory analysis of the detected potential threat is determined using the estimated position and the estimated velocity. A collision prediction based on the trajectory analysis is determined, and then a determination is made as to whether a collision is imminent based on the collision prediction and on the estimated size of the potential threat.","['G06V20/58', 'G06V10/255']"
CN107341517B,Multi-scale small object detection method based on feature fusion between deep learning levels,"The invention relates to an object verification technology in the field of machine vision, in particular to a multi-scale small object detection method based on deep learning inter-level feature fusion, aiming at solving the problems that the existing object detection has very low detection precision in a real scene and is constrained by scale size, for the defect that the detection of small objects is very difficult, the invention provides a multi-scale small object detection method based on deep learning inter-level feature fusion, which takes an image in a real scene as a research object, the method comprises the steps of extracting features of an input image by constructing a convolutional neural network, generating a network by utilizing a candidate region to generate fewer candidate regions, mapping each candidate region to a feature map generated by the convolutional neural network to obtain the features of each candidate region, obtaining the features with fixed size and fixed dimensionality after pooling, inputting the features into a full connection layer, and respectively outputting an identification category and a position after regression by two branches behind the full connection layer. The invention is suitable for object verification in the field of machine vision.","['G06F18/24', 'G06F18/253', 'G06V20/41']"
JP2023073231A,Method and device for image processing,"To provide a method for image processing, an image processor, an electronic device, and a computer readable storage medium.SOLUTION: The method for image processing includes the steps of: acquiring two-dimensional feature information and three-dimensional feature information on the basis of a color image and a depth image; fusing the two-dimensional feature information and three-dimensional feature information on the basis of an attention mechanism and acquiring fusion feature information; and performing image processing on the basis of the fusion feature information.SELECTED DRAWING: Figure 1","['G06V10/806', 'G06T17/00', 'G06T7/50', 'G06T7/62', 'G06T7/70', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20016']"
US9530195B2,Interactive refocusing of electronic images,"A method is performed to refocus a digital photographic image comprising a plurality of pixels. In the method, a set of images is computed corresponding to the digital photographic image and focused at different depths. Refocus depths for at least a subset of the pixels are identified and stored in a look-up table. At least a portion of the digital photographic image is refocused at a desired refocus depth determined from the look-up table.","['G06T5/50', 'G06T5/002', 'G06T5/003', 'G06T5/20', 'G06T5/70', 'G06T5/73', 'G02B27/0075', 'G06T2200/24', 'G06T2207/20092', 'G06T2207/20104', 'G06T2207/20108', 'H01L27/14627', 'H10F39/8063']"
US8885941B2,System and method for estimating spatially varying defocus blur in a digital image,"An image editing application (or a blur classification module thereof) may automatically estimate a coherent defocus blur map from a single input image. The application may represent the blur spectrum as a differentiable function of radius r, and the optimal radius may be estimated by optimizing the likelihood function through a gradient descent algorithm. The application may generate the spectrum function over r through polynomial-based fitting. After fitting, the application may generate look-up tables to store values for the spectrum and for its first and second order derivatives, respectively. The use of these tables in the likelihood optimization process may significantly reduce the computational costs of a given blur estimation exercise. The application may minimize an energy function that includes a data term, a smoothness term, and a smoothness parameter that is adaptive to local image content. The output blur map may be used for image object depth estimation.","['G06T5/73', 'G06T5/003', 'G06T7/0081', 'G06T7/11', 'G06T2207/10024', 'G06T2207/20064', 'G06T2207/20076']"
US11699236B2,Systems and methods for the segmentation of multi-modal image data,"There is provided a computer implemented method of automatic segmentation of three dimensional (3D) anatomical region of interest(s) (ROI) that includes predefined anatomical structure(s) of a target individual, comprising: receiving 3D images of a target individual, each including the predefined anatomical structure(s), each 3D image is based on a different respective imaging modality. In one implementation, each respective 3D image is inputted into a respective processing component of a multi-modal neural network, wherein each processing component independently computes a respective intermediate, and the intermediate outputs are inputted into a common last convolutional layer(s) for computing the indication of segmented 3D ROI(s). In another implementation, each respective 3D image is inputted into a respective encoding-contracting component a multi-modal neural network, wherein each encoding-contracting component independently computes a respective intermediate output. The intermediate outputs are inputted into a single common decoding-expanding component for computing the indication of segmented 3D ROI(s).","['G06T7/11', 'A61B6/501', 'A61B34/20', 'A61B34/32', 'A61B5/0042', 'A61B5/055', 'A61B5/4381', 'A61B5/7267', 'A61B6/032', 'A61B6/037', 'A61B6/466', 'A61B6/50', 'A61B8/5207', 'G01R33/5602', 'G01R33/5608', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/082', 'G06N3/09', 'G06T2200/04', 'G06T2207/10088', 'G06T2207/10092', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/20156', 'G06T2207/30016', 'G06T2207/30081', 'G06T2207/30096']"
US5625711A,Method and apparatus for producing a hybrid data structure for displaying a raster image,"A system for producing a raster image derived from coded and non-coded portions of a hybrid data structure from an input bitmap including (1) a data processing apparatus, (2) a recognizer which performs recognition on an input bitmap to the data processing apparatus to detect identifiable objects within the input bitmap, (3) a mechanism for producing a hybrid data structure including coded data corresponding to the identifiable objects and non-coded data derived from portions of the input bitmap which do not correspond to the identifiable objects, and (4) an output device capable of developing a visually perceptible raster image derived from the hybrid data structure. The raster image includes raster images of the identifiable objects and raster images derived from portions of the input bitmap that do not correspond to the identifiable objects. The invention includes a method for producing a hybrid data structure for a bitmap of an image having the steps of: (1) inputting a signal comprising a bitmap into a digital processing apparatus, (2) partitioning the bitmap into a hierarchy of lexical units, (3) assigning labels to a label list for each lexical unit of a predetermined hierarchical level, where labels in the label list have an associated confidence level, and (4) storing each lexical unit in a hybrid data structure as either an identifiable object or a non-identifiable object.","['H04N1/4115', 'G06V30/127', 'G06V30/268', 'G06V30/40', 'G06V30/10']"
US8004576B2,Histogram methods and systems for object recognition,"A multidimensional histogram is used to characterize an image (or object), and is used to identify candidate matches with one or more reference images (or objects). An exemplary implementation employs hue information for two of the dimensions, and a second derivative function based on luminance for a third dimension. The simplicity and speed of the detailed arrangements make them well suited for use with cell phones and other mobile devices, which can use the technology for image/object recognition, e.g., in visual search applications.","['G06V10/40', 'G06V10/50', 'G06F16/5838', 'G06F18/22', 'G06T7/11', 'G06V10/56', 'H04M2250/52']"
US10254845B2,Hand gesture recognition for cursor control,"A system for hand gesture recognition is described herein. The system includes a display, camera, memory, and processor. The memory that is to store instructions and is communicatively coupled to the camera and the display. The processor is communicatively coupled to the camera, the display, and the memory. When the processor is to execute the instructions, the processor is to estimate one or more motion vectors of an object using a pair of consecutive frames and estimate an average motion vector of the object. The processor is also to obtain a descriptor based on histogram values from a histogram of optical flow (HOOF) of the one or more motion vectors and the average motion vector and classify the descriptor as a gesture.","['G06F3/017', 'G06F3/04812', 'G06F3/04883', 'G06K9/00355', 'G06V40/28', 'G06T2207/20072', 'G06T2207/20081', 'G06T2207/30196']"
CN113158828B,Facial emotion calibration method and system based on deep learning,"The invention provides a facial emotion calibration method and system based on deep learning. The method comprises the following steps: preprocessing an original image containing a human face; sending the preprocessed original image into a trained first convolution neural network, and detecting the specific position and the specific size of the hand in the original image by combining with a skin color model to serve as limb characteristics; sending the preprocessed original image into a trained second convolutional neural network, and extracting expression features; and fusing the extracted limb characteristics with the expression characteristics to determine final emotion attributes and energy levels. The invention provides a facial emotion calibration system based on deep learning, which can capture more accurate, real and rich facial emotion information through the steps of face detection, key point detection, face alignment, limb characteristics and expression characteristics extraction, final fusion and the like, and has wide application prospect.","['G06V40/70', 'G06F18/253', 'G06F18/256', 'G06N3/045', 'G06N3/084', 'G06V40/11', 'G06V40/168', 'G06V40/174', 'Y02D10/00']"
CN111080622B,"Neural network training method, workpiece surface defect classification and detection method and device","The application discloses a neural network training method, a workpiece surface defect classifying and detecting method and a device, which belong to the field of machine vision industrial detection. The defect classification results can effectively provide more information in the detected images and can be used to evaluate test samples and to suggest influencing factors for the manufacturing process; the defect quantification result can rapidly judge the industrial processing production process, and is beneficial to improving the production quality of industrial processing parts.","['G06T7/0004', 'G01N21/8851', 'G06F18/241', 'G06T7/73', 'G01N2021/8861', 'G01N2021/8887', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30108']"
CN109615016B,Target detection method of convolutional neural network based on pyramid input gain,"The invention relates to a target detection method of a convolutional neural network based on pyramid input gain, and belongs to the technical field of computer vision and target detection. The target detection method is based on a convolutional neural network model PiaNet comprising a feature extraction module and a multi-task prediction module; the target detection method comprises a training stage and a testing stage; the training stage adopts a two-stage transfer learning strategy, which comprises the following steps: step (1), data enhancement and data preprocessing are carried out to generate a training set of first-stage training, a training set of second-stage training and a test set; step (2) performing a first stage training in a two-class network; step (3) performing second-stage training to obtain a trained PiaNet network; the testing stage is to accurately detect the target, and specifically comprises the following steps: and inputting the test set into the trained PiaNet network, and outputting the position of the detection frame and the classification result through a multi-task loss function. The method has wide application range and high robustness.","['G06F18/214', 'G06F18/24', 'G06N3/045', 'G06T7/0012', 'G06T2207/10012', 'G06T2207/10081', 'G06T2207/30064']"
US10706535B2,Tissue staining quality determination,"The invention relates to the automated determination of the staining quality of an IHC stained biological sample. A plurality of features is extracted from a digital IHC stained tissue image. The features are input into a first classifier configured to identify the extended tissue type of the depicted tissue as a function of the extracted features. An extended tissue type is a tissue type with a defined expression level of the tumor marker. In addition, the extracted features are input into a second classifier configured to identify a contrast level of the depicted tissue as a function of at least some second ones of the extracted features. The contrast level indicates the intensity contrast of pixels of the stained tissue. Then, a staining quality score of the image is computed as a function of the identified extended tissue type and the identified contrast level.","['G06V20/695', 'G06F18/214', 'G06K9/0014', 'G06K9/00147', 'G06K9/036', 'G06K9/4647', 'G06K9/6256', 'G06T7/0012', 'G06T7/194', 'G06V10/993', 'G06V20/698', 'G01N1/30', 'G06K9/6298', 'G06T2200/24', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/30024', 'G06T2207/30096', 'G06T2207/30168', 'G06T2207/30204']"
CN113822314B,"Image data processing method, device, equipment and medium","The embodiment of the application provides an image data processing method, an image data processing device, image data processing equipment and a medium. The method comprises the following steps: m object feature graphs with different sizes are obtained from a source image, classification confidence degrees corresponding to pixel points in each object feature graph are obtained, and M is a positive integer; acquiring initial prediction polar diameters corresponding to pixel points in each object feature map, acquiring contour sampling points in each object feature map according to the initial prediction polar diameters, and determining the sum of polar diameter deviation of the contour sampling points and the initial prediction polar diameters as a target prediction polar diameter corresponding to the pixel points in each object feature map; and determining the shape of the object edge used for representing the outline of the target object in the source image according to the classification confidence coefficient and the target prediction polar diameter corresponding to the pixel points in each object feature map. By adopting the embodiment of the application, the segmentation accuracy of the image can be improved.","['G06T7/12', 'G06F18/24', 'G06F18/25', 'G06V10/26', 'G06V10/44', 'G06V10/454', 'G06V10/764', 'G06V10/806', 'G06V10/82']"
CN110210363B,Vehicle-mounted image-based target vehicle line pressing detection method,"The invention relates to a vehicle-mounted image-based target vehicle line pressing detection method, which is characterized in that the position of a vehicle in an image is obtained based on an image instance segmentation method, the result is represented by a vehicle detection segmentation graph, the position of a lane line area in the image is obtained based on a lane line detection method, and the result is represented by a lane line segmentation graph or a lane line curve; generating a region of interest, ROI, for a single target vehicle; estimating the positions of front and rear wheels of the vehicle in the ROI to obtain two line segments of the front and rear wheels of the vehicle, which are respectively contacted with the ground; meanwhile, if the lane line detection result is represented by a segmentation graph, performing curve fitting on the lane line to obtain a fitting curve of each lane line; and judging whether the vehicle is pressed according to the estimated positions of the front wheel and the rear wheel of the vehicle and the lane line curve. The invention improves the vehicle line pressing detection accuracy and has lower processing complexity.","['G06T7/13', 'G06T7/70', 'G06V10/25', 'G06V20/584', 'G06V20/588', 'G06T2207/30252']"
CN110232337B,Chinese character image stroke extraction method and system based on full convolution neural network,"The invention belongs to the field of computer vision and mode recognition, and particularly relates to a Chinese character image stroke extraction method and system based on a full convolution neural network, aiming at solving the problem of difficulty in extracting free-written handwritten character strokes. The method comprises the following steps: extracting the region of the acquired Chinese character image; performing skeletonization operation on the overlapped region and the non-overlapped region; calculating the consistency between any stroke segments in the skeletonized overlapped area; and connecting the stroke segments belonging to the same stroke in the overlapped area, and combining the directly connected stroke segments in the non-overlapped area into a complete skeleton-shaped stroke. On one hand, the invention can still realize the stroke extraction of the handwritten Chinese character under the condition that the strokes of the handwritten Chinese character which is freely written are overlapped, on the other hand, the invention adopts a character synthesis method to obtain the training sample and attaches different marking information of the training sample in different tasks, thereby greatly saving the labor cost.","['G06N3/045', 'G06V30/2268']"
US8983142B1,Programmatic silhouette attribute determination,"A set of silhouette attributes are determined for a class of objects, where each of the silhouette attribute corresponds to a discriminative feature that is not associated with any other silhouette attribute in the set. An image content item depicting an object of the class is analyzed. A discriminative feature is identified for the object. The silhouette attribute associated with the determined discriminative feature is associated with the object as provided in the image content item.","['G06F16/954', 'G06Q30/0278']"
CN113362441A,"Three-dimensional reconstruction method and device, computer equipment and storage medium","The application relates to a three-dimensional reconstruction method, a three-dimensional reconstruction device, a computer device and a storage medium. The method comprises the following steps: acquiring a plurality of depth videos acquired by a depth camera on a reconstructed scene; performing point cloud registration based on each depth video to obtain complete point cloud of a reconstructed scene; carrying out coordinate correction on the complete point cloud to obtain a corrected point cloud; dividing the corrected point cloud along the vertical direction to obtain a plurality of layers of point cloud slices with the same thickness and different heights; respectively identifying each layer of point cloud slice to obtain a goods shelf area and a wall surface area of each layer of point cloud slice; screening and combining the goods shelf areas of the point cloud slices of each layer to obtain a complete goods shelf area; and performing three-dimensional reconstruction based on the wall surface area and the complete shelf area to obtain a three-dimensional reconstruction image. The method can improve the precision of three-dimensional reconstruction.","['G06T15/10', 'G06T7/30', 'G06T2207/10012']"
US8995725B2,On-site composition and aesthetics feedback through exemplars for photographers,"A comprehensive system to enhance the aesthetic quality of the photographs captured by mobile consumers provides on-site composition and aesthetics feedback through retrieved examples. Composition feedback is qualitative in nature and responds by retrieving highly aesthetic exemplar images from the corpus which are similar in content and composition to the snapshot. Color combination feedback provides confidence on the snapshot to contain good color combinations. Overall aesthetics feedback predicts the aesthetic ratings for both color and monochromatic images. An algorithm is used to provide ratings for color images, while new features and a new model are developed to treat monochromatic images. This system was designed keeping the next generation photography needs in mind and is the first of its kind. The feedback rendered is guiding and intuitive in nature. It is computed in situ while requiring minimal input from the user.","['G06K9/6267', 'G06V20/00', 'G06F18/211', 'G06F18/24', 'G06K9/00624', 'G06K9/46', 'G06K9/4652', 'G06K9/4671', 'G06K9/6228', 'G06V10/462', 'G06V10/56', 'G06V10/771', 'H04N23/64', 'H04N5/23222']"
US10452899B2,Unsupervised deep representation learning for fine-grained body part recognition,"A method and apparatus for deep learning based fine-grained body part recognition in medical imaging data is disclosed. A paired convolutional neural network (P-CNN) for slice ordering is trained based on unlabeled training medical image volumes. A convolutional neural network (CNN) for fine-grained body part recognition is trained by fine-tuning learned weights of the trained P-CNN for slice ordering. The CNN for fine-grained body part recognition is trained to calculate, for an input transversal slice of a medical imaging volume, a normalized height score indicating a normalized height of the input transversal slice in the human body.","['G06T7/74', 'G06K9/00362', 'G06F18/2148', 'G06F18/2155', 'G06F18/24', 'G06F18/24133', 'G06F18/25', 'G06K9/46', 'G06K9/4628', 'G06K9/6257', 'G06K9/6259', 'G06K9/6267', 'G06K9/6271', 'G06K9/6288', 'G06K9/6298', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V40/10', 'G06K2209/05', 'G06T2200/04', 'G06T2207/10072', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004', 'G06T7/60', 'G06V2201/03']"
US9378431B2,Method of matching image features with reference features and integrated circuit therefor,"The invention is related to a method of matching image features with reference features, comprising the steps of providing a current image captured by a capturing device, providing reference features (r), wherein each of the reference features comprises at least one reference feature descriptor (d(r)), determining current features (c) in the current image and associating with each of the current features at least one respective current feature descriptor (d(c)), and matching the current features with at least some of the reference features by determining a respective similarity measure (D(c, r)) between each respective current feature descriptor (d(c)) and each respective reference feature descriptor (d(r)). According to the invention, the determination of the similarity measure is performed on an integrated circuit by hardwired logic or configurable logic which processes logical functions for determining the similarity measure. The invention is also concerned with an integrated circuit for matching of image features with reference features.","['G06V10/955', 'G06K9/6215', 'G06F16/50', 'G06F16/53', 'G06F16/5838', 'G06F17/30244', 'G06F17/30256', 'G06F18/22', 'G06K9/00986', 'G06K9/46', 'G06K9/4671', 'G06K9/6202', 'H04N23/651', 'G02B27/0101', 'G06T7/004', 'H04N5/23241']"
US8078006B1,Minimal artifact image sequence depth enhancement system and method,"Motion picture scenes to be colorized/depth enhanced (2D→3D) are broken into separate elements, backgrounds/sets or motion/onscreen-action. Background and motion elements are combined into composite frame which becomes a visual reference database that includes data for all frame offsets used later for the computer controlled application of masks within a sequence of frames. Masks are applied to subsequent frames of motion objects based on various differentiating image processing methods, including automated mask fitting/reshaping. Colors and/or depths are automatically applied to masks throughout a scene from the composite background and to motion objects. Areas never exposed by motion or foreground objects in a series of images may be partially or fully realistically drawn or rendered and applied to the occluded areas of the background and then automatically applied throughout the images to generate of minimal artifact or artifact-free secondary viewpoints when translating foreground objects horizontally during 2D→3D conversion.","['H04N9/43', 'G06T11/001']"
CN113128610B,Industrial part pose estimation method and system,"The invention provides a method and a system for estimating the pose of an industrial part, comprising the following steps: acquiring part point clouds of the part to be tested; classifying the part point cloud based on the part posture classification model to obtain a part classification result; determining a target part template according to the part classification result; and matching the part point cloud by using the target part template to acquire pose information of the part to be detected. According to the method and the system for estimating the pose of the industrial part, the pose information of the part to be detected is determined by classifying and matching the point cloud of the part to be detected, so that the efficiency and the accuracy of estimating the pose of the industrial part are improved, support is provided for judging the position and the angle of the pose offset of the part in the industrial scene, and the problem of estimating the pose of the part grabbing by using a mechanical arm and other devices in the industrial scene is solved.","['G06F18/24', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T7/80', 'G06V10/30']"
US12205317B2,Light-weight pose estimation network with multi-scale heatmap fusion,"Embodiments identify joints of a multi-limb body in an image. One such embodiment unifies depth of a plurality of multi-scale feature maps generated from an image of a multi-limb body to create a plurality of feature maps each having a same depth. In turn, for each of the plurality of feature maps having the same depth, an initial indication of one or more joints in the image is generated. The one or more joints are located at an interconnection of a limb to the multi-limb body or at an interconnection of a limb to another limb. To continue, a final indication of the one or more joints in the image is generated using each generated initial indication of the one or more joints.","['G06T7/70', 'G06F18/24133', 'G06N3/045', 'G06N3/0464', 'G06N3/0495', 'G06N3/084', 'G06N3/09', 'G06T7/74', 'G06V10/454', 'G06V10/7715', 'G06V10/82', 'G06V20/64', 'G06V40/10', 'G06V40/103', 'G06T2207/10004', 'G06T2207/20084', 'G06T2207/30196']"
US7616807B2,System and method for using texture landmarks for improved markerless tracking in augmented reality applications,"A method of tracking a pose of a moving camera includes receiving a first image from a camera, receiving a sequence of digitized images from said camera, recording, for each of said sequence of digitized images, the pose and 2D correspondences of landmarks, reconstructing a location and appearance of a 2-dimensional texture patch from 2D correspondences of the landmarks by triangulation and optimization, computing back-projection errors by comparing said reconstructed texture patch with said first received image; and reconstructing said location and appearance of said 2-dimensional texture patch from the 2D correspondences of the landmarks of said sequence of digitized images by triangulation and optimization after eliminating those landmarks with large back-projection errors.","['G06T7/246', 'G06T7/74']"
US11003940B2,System and methods for automatic solar panel recognition and defect detection using infrared imaging,"Methods and systems are provided for detecting a defect in a solar panel. The method includes initially imaging, via an infrared camera, a group of solar panels. Then, identifying, via a computer system configured for solar panel defect detection, the individual solar panels in the group of solar panels. Finally, identifying, via evaluation of an infrared image obtained by the infrared camera, a defect in at least one of the group of solar panels.","['G06K9/3233', 'G06T7/0004', 'G06K9/4638', 'G06T7/0008', 'G06T7/001', 'G06T7/13', 'G06V10/25', 'G06V10/457', 'H04N23/23', 'H04N5/33', 'G06T2207/10048', 'G06T2207/30108']"
US10424065B2,Systems and methods for performing three-dimensional semantic parsing of indoor spaces,"Systems and methods for performing three-dimensional semantic parsing of indoor spaces in accordance with embodiments of the invention are disclosed. In one embodiment, a method includes receiving input data representing a three-dimensional space, determining disjointed spaces within the received data by generating a density histogram on each of a plurality of axes, determining space dividers based on the generated density histogram, and dividing the point cloud data into segments based on the determined space dividers, and determining elements in the disjointed spaces by aligning the disjointed spaces within the point cloud data along similar axes to create aligned versions of the disjointed spaces normalizing the aligned version of the disjointed spaces into the aligned version of the disjointed spaces, determining features in the disjointed spaces, generating at least one detection score, and filtering the at least one detection score to determine a final set of determined elements.","['G06T7/11', 'G01C21/206', 'G01C21/32', 'G01C21/383', 'G01C21/3867', 'G06F18/245', 'G06K9/42', 'G06K9/4642', 'G06K9/6285', 'G06T11/206', 'G06T19/00', 'G06T7/162', 'G06T7/187', 'G06V20/10', 'G06V20/653', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20072', 'G06T2207/20221', 'G06T2210/04', 'G06T2210/56', 'G06V10/32', 'G06V10/50']"
CN115631117B,"Image enhancement method, device, detection system and storage medium for defect detection","The application provides an image enhancement method, an image enhancement device, an image enhancement detection system and a storage medium for defect detection. The method comprises the following steps: preprocessing an acquired original image of an object to be detected to determine an interested area; performing improved contrast-limited adaptive histogram equalization processing on the region of interest to obtain a first image; carrying out noise reduction processing on the first image to obtain a second image; carrying out multi-scale image information enhancement on the region of interest to obtain a third image; fusing the second image and the third image to obtain a fused image; and detecting the defects according to the fused image. Therefore, the method has a good enhancement effect on some weak defects with local contrast only having a few gray differences, and can improve the processing efficiency of images and the accuracy of defect detection.","['G06T5/70', 'G06T5/40', 'G06T5/50', 'G06T7/0004', 'G06T7/136', 'G06T2207/10024', 'G06T2207/20021', 'G06T2207/20032', 'G06T2207/20104', 'G06T2207/20132', 'G06T2207/20221']"
CN111581976B,"Medical term standardization method, device, computer equipment and storage medium","The application discloses a medical term standardization method, a device, a computer device and a storage medium, wherein the method comprises the following steps: acquiring medical text data, and performing data cleaning on the medical text data to obtain an initial text; performing word segmentation processing on the initial text by using a word segmentation engine to obtain word segmentation units corresponding to the initial text; identifying medical characteristic words in the word segmentation unit by means of deep learning entity identification of medical knowledge to obtain target word segmentation; performing inverted indexing on the target word, and confirming the medical term text containing the target word and the occurrence frequency of the target word in the medical term text to obtain the medical term text as a candidate text; and selecting the candidate text with the maximum similarity value as a standard medical term text. The method and the device are beneficial to effectively improving the accuracy of medical term standardization, and further improving the usability of data of medical term texts.","['G06F40/295', 'G06F16/319', 'G06F40/30']"
US7796822B2,Foreground/background segmentation in digital images,An analysis and classification tool compares at least a portion of a captured image and a reference image of nominally the same scene. One of the captured and reference images is taken with flash and the other is taken without flash. The tool provides a measure of the difference in illumination between the captured image and the reference image. The tool compares the measure with a threshold and segments a foreground region from a background region based on the measure.,"['G06T7/0002', 'G06T7/11', 'G06T7/174', 'G06T7/194', 'G06T7/40', 'G06V10/28', 'G06V20/35', 'G06V40/193', 'H04N1/624', 'G06T2207/10152', 'G06T2207/20224', 'G06T2207/30216']"
US11783491B2,"Object tracking method and apparatus, storage medium, and electronic device","An object tracking method includes: obtaining a video stream acquired by a camera, and decoding the video stream to obtain a plurality of image frames; and obtaining position information of a target object in a current image frame. Obtaining the position information of the target object in the current image frame includes: performing, in response to that the current image frame is a first-type image, object detection on the current image frame to obtain the position information of the target object in the current image frame; and predicting, in response to that the current image frame is a second-type image, the position information of the target object in the current image frame based on motion vectors of a previous image frame and a position the target object in the previous image frame.","['G06T7/246', 'G06T7/215', 'G06T7/251', 'G06V10/25', 'G06V20/40', 'G06V40/10', 'G06V40/25', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196', 'G06T2207/30232']"
CN110220917B,Crown cap surface defect online detection method based on image processing,"The invention discloses an online detection method for surface defects of crown caps based on image processing, which belongs to the technical field of image processing, and is characterized by collecting images of the inner surfaces of crown caps and realizing center positioning and region extraction by utilizing an image segmentation technology; detecting defects on the inner surface of the bottle cap by using image processing algorithms such as threshold segmentation, morphological processing, connected region characteristic analysis, region comparison and the like; an automatic training learning algorithm is adopted to establish a pattern template of the outer surface of the crown cap in an off-line manner; and collecting an image of the outer surface of the crown cap, and completing the detection of the defects of the outer surface of the bottle cap by using a characteristic template matching method, an affine transformation method and a region comparison method. The method can be used for rapidly detecting various types of defects on the surface of the crown cap on line, can detect 300 bottle caps per minute, has the advantages of high defect detection accuracy, strong robustness and the like, and can be attached to a crown cap production line to realize defect detection and type judgment of the crown cap.",['G01N21/95']
US10007336B2,"Apparatus, system, and method for mobile, low-cost headset for 3D point of gaze estimation","An apparatus, system, and method for a mobile, low-cost headset for 3D point of gaze estimation. A point of gaze apparatus may include an eye tracking camera configured to track the movements of a user's eye and a scene camera configured to create a three-dimensional image and a two-dimensional image in the direction of the user's gaze. The point of gaze apparatus may include an image processing module configured to identify a point of gaze of the user and identify an object located at the user's point of gaze by using information from the eye tracking camera and the scene camera.","['G06F3/013', 'G06K9/00604', 'G06K9/6211', 'G06K9/6212', 'G06V10/757', 'G06V10/758', 'G06V40/19']"
US7508990B2,Apparatus and method for processing video data,"An apparatus and methods for processing video data are described. The invention provides a representation of video data that can be used to assess agreement between the data and a fitting model for a particular parameterization of the data. This allows the comparison of different parameterization techniques and the selection of the optimum one for continued video processing of the particular data. The representation can be utilized in intermediate form as part of a larger process or as a feedback mechanism for processing video data. When utilized in its intermediate form, the invention can be used in processes for storage, enhancement, refinement, feature extraction, compression, coding, and transmission of video data. The invention serves to extract salient information in a robust and efficient manner while addressing the problems typically associated with video data sources.","['G06T9/00', 'G06T7/251', 'G06V10/24', 'G06V10/32', 'G06V10/7515', 'G06V20/40', 'H04N19/53', 'H04N19/54', 'G06T2207/10016']"
EP3913499A1,"Method and apparatus for processing dataset, electronic device and storage medium","The present disclosure discloses a method and apparatus for processing a dataset. The method includes: obtaining a first text set meeting a preset similarity matching condition with a target text from multiple text blocks provided by a target user; obtaining a second text set from the first text set, in which each text in the second text set does not belong to a same text block as the target text; generating a negative sample set of the target text based on content of a candidate text block to which each text in the second text set belongs; generating a positive sample set of the target text based on content of a target text block to which the target text belongs; and generating a dataset of the target user based on the negative sample set and the positive sample set, and training a matching model based on the dataset.","['G06F40/30', 'G06F16/35', 'G06F16/2237', 'G06F16/243', 'G06F16/24556', 'G06F16/332', 'G06F16/3347', 'G06F16/355', 'G06F18/214', 'G06F18/2148', 'G06F18/22', 'G06F40/284', 'G06F40/289', 'G06N20/00', 'G06N5/00', 'G06N5/02', 'G06V30/414']"
CN111369576B,"Training method of image segmentation model, image segmentation method, device and equipment","The application discloses a training method of an image segmentation model, an image segmentation method, an image segmentation device and image segmentation equipment, and belongs to the field of image segmentation. The method comprises the following steps: acquiring a sample image, wherein the sample image is an image with an annotation area; performing superpixel division on the sample image to obtain at least two superpixel areas; obtaining a hard label of the pixel according to whether the pixel in the sample image belongs to the labeling area; obtaining a soft label of the pixel according to the super-pixel area to which the pixel belongs and the hard label of the pixel, wherein the soft label is used for representing the pseudo probability that the pixel belongs to the labeling area; and training an image segmentation model according to the hard label of the pixel and the soft label of the pixel. The image segmentation model is trained by using the hard labels and the soft labels of the pixels, so that the trained image segmentation model can accurately segment the segmentation region in the input image, and meanwhile, the training efficiency of the image segmentation model is improved.","['G06T7/11', 'G06F18/214', 'G06N3/045', 'G06T2207/30204']"
CN108986136B,Binocular scene flow determination method and system based on semantic segmentation,"The invention discloses a binocular scene flow determination method and system based on semantic segmentation. In the process of optimizing motion, the initial scene flow is obtained by superpixel segmentation, and then optimization is carried out inside the semantically segmented labels, so that the motion of superpixel blocks inside the semantically labels tends to be consistent, and meanwhile, the edge information of moving objects is well protected. According to the invention, semantic information is added into the optical flow information, so that the edge of the object is protected, and the reasoning process of the shielding problem is greatly simplified; in addition, the motion reasoning on the semantic label layer enables the scene flow of the surface pixel points of the same moving object to be approximately consistent, and finally the purpose of optimizing the scene flow is achieved.","['G06T7/207', 'G06T7/11', 'G06T7/246', 'G06T7/285', 'G06T2207/10016']"
EP0757544B1,Computerized detection of masses and parenchymal distortions,A method and system for the automated detection of lesions such as masses and/or tissue (parenchymal) distortions in medical images such as mammograms. Dense regions and subcutaneous fat regions within a mammogram are segmented. A background correction may be performed within the dense regions. Hough spectrum within ROIs placed in the breast region of a mammogram are calculated and thresholded using the intensity value eta in order to increase sensitivity and reduce the number of false-positive detections. Lesions are detected based on the thresholded Hough spectra. The thresholded Hough spectra are also used to differentiate between benign and malignant masses.,"['G06T7/0012', 'G06T7/11', 'G06T2207/10116', 'G06T2207/20061', 'G06T2207/20132', 'G06T2207/30068', 'G06T2207/30096']"
US11593585B2,Object detection and image cropping using a multi-detector approach,"Computer-implemented methods for detecting objects within digital image data based on color transitions include: receiving or capturing a digital image depicting an object; sampling color information from a first plurality of pixels of the digital image, wherein each of the first plurality of pixels is located in a background region of the digital image; optionally sampling color information from a second plurality of pixels of the digital image, wherein each of the second plurality of pixels is located in a foreground region of the digital image; assigning each pixel a label of either foreground or background using an adaptive label learning process; binarizing the digital image based on the labels assigned to each pixel; detecting contour(s) within the binarized digital image; and defining edge(s) of the object based on the detected contour(s). Corresponding systems and computer program products configured to perform the inventive methods are also described.","['G06V30/147', 'G06K9/6227', 'G06F18/217', 'G06F18/23', 'G06F18/254', 'G06F18/285', 'G06K9/6262', 'G06N20/00', 'G06N7/00', 'G06N7/01', 'G06T7/0002', 'G06T7/11', 'G06T7/12', 'G06T7/13', 'G06T7/194', 'G06T7/70', 'G06T7/90', 'G06V10/44', 'G06V10/762', 'G06V10/809', 'G06V20/40', 'G06V30/162', 'G06V30/18105', 'G06V30/19147', 'G06V30/19173', 'G06V30/413', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/30176', 'G06V10/28', 'G06V30/10', 'G06V30/2253']"
US11295522B2,Three-dimensional (3D) model creation and incremental model refinement from laser scans,"A method, system, and apparatus create a 3D CAD model. Scan data from two or more structured scans of a real-world scene are acquired and each scan processed independently by segmenting the scan data into multiple segments, filtering the scan data, and fitting an initial model that is used as a model candidate. Model candidates are clustered into groups and a refined model is fit onto the model candidates in the same group. A grid of cells representing points is mapped over the refined model. Each of the grid cells is labeled by processing each scan independently, labeling each cell located within the refined model as occupied, utilizing back projection to label remaining cells as occluded or empty. The labels from multiple scans are then combined. Based on the labeling, model details are extracted to further define and complete the refined model.","['G06T15/10', 'G01S17/42', 'G01S17/89', 'G06T15/06', 'G06T17/10', 'G06T5/00', 'G06T7/11', 'G06T7/13', 'G06T2200/04', 'G06T2207/10028', 'G06T2207/20072', 'G06T2210/04']"
US8836768B1,Method and system enabling natural user interface gestures with user wearable glasses,"User wearable eye glasses include a pair of two-dimensional cameras that optically acquire information for user gestures made with an unadorned user object in an interaction zone responsive to viewing displayed imagery, with which the user can interact. Glasses systems intelligently signal process and map acquired optical information to rapidly ascertain a sparse (x,y,z) set of locations adequate to identify user gestures. The displayed imagery can be created by glasses systems and presented with a virtual on-glasses display, or can be created and/or viewed off-glasses. In some embodiments the user can see local views directly, but augmented with imagery showing internet provided tags identifying and/or providing information as to viewed objects. On-glasses systems can communicate wirelessly with cloud servers and with off-glasses systems that the user can carry in a pocket or purse.","['G06F3/017', 'G02B27/017', 'G06F3/0304', 'G02B2027/0178']"
US8510283B2,Automatic adaption of an image recognition system to image capture devices,"A MMR system for newspaper publishing includes a plurality of mobile devices, an MMR gateway, an MMR matching unit and an MMR publisher. The MMR matching unit receives an image query from the MMR gateway and sends it to one or more of the recognition units to identify a result including a document, the page and the location on the page. The image registration unit includes an indexing unit for generating images adapted to the environment and capabilities of the image capture device. The indexing unit also automatically adapts the configuration of the plurality of recognition units and index tables based upon image queries applied to the plurality of recognition and index tables. The plurality of recognition units and index tables are configured based on content they reference, recognition algorithm used or other factors.","['G06F16/955', 'G06F16/583', 'G06F18/214', 'G06F18/217', 'G06F18/285', 'G06V10/96', 'G06V30/19113', 'G06V30/19147', 'G06V30/1916', 'G06V30/414']"
US8401225B2,Moving object segmentation using depth images,"Moving object segmentation using depth images is described. In an example, a moving object is segmented from the background of a depth image of a scene received from a mobile depth camera. A previous depth image of the scene is retrieved, and compared to the current depth image using an iterative closest point algorithm. The iterative closest point algorithm includes a determination of a set of points that correspond between the current depth image and the previous depth image. During the determination of the set of points, one or more outlying points are detected that do not correspond between the two depth images, and the image elements at these outlying points are labeled as belonging to the moving object. In examples, the iterative closest point algorithm is executed as part of an algorithm for tracking the mobile depth camera, and hence the segmentation does not add substantial additional computational complexity.","['G06T7/215', 'G06T7/194', 'G06T2207/10028', 'G06T2207/30244']"
CN112364827B,"Face recognition method, device, computer equipment and storage medium","The application relates to a face recognition method, a face recognition device, computer equipment and a storage medium. The method comprises the steps of obtaining an image to be identified; extracting a complete face image and a face image of a specific area in an image to be identified; acquiring a complete face image and face features to be recognized corresponding to the face image in a specific area based on a preset face recognition network; and carrying out face recognition on the image to be recognized according to the face features to be recognized. In the application, in the face recognition process, the corresponding features of the whole face image and the face image of the specific area are respectively extracted, then the feature fusion is carried out to obtain the face features to be recognized, and the face recognition is assisted based on the face features to be recognized, so that the face recognition performance under the partial shielding of the face can be improved while the normal face recognition performance is not basically influenced.","['G06V40/164', 'G06Q20/3674', 'G06V40/168', 'G06V40/169', 'G06V40/171']"
CN108596184B,"Training method of image semantic segmentation model, readable storage medium and electronic device","A training method for image semantic segmentation comprises the following steps: inputting a training image with pre-labeled semantic segmentation information into an image semantic segmentation model to obtain a semantic segmentation result comprising a feature map and prediction; inputting the feature map into a difficult sample mining unit to calculate a difficult sample of the training image; counting pixels with wrong prediction according to the predicted semantic segmentation result and pre-labeled semantic segmentation information, and taking the pixels with wrong prediction in the predicted semantic segmentation result as reference difficult samples; and correcting parameters of the basic image semantic segmentation model and the difficult sample mining unit according to the predicted semantic segmentation result, the pre-labeled semantic segmentation information, the difficult sample and the reference difficult sample. The invention also provides a readable storage medium and electronic equipment applying the training method for image semantic segmentation. The training method of the image semantic segmentation model, the readable storage medium and the electronic equipment can improve the accuracy of semantic segmentation results.","['G06V10/267', 'G06F18/214']"
CN110111334B,"Crack segmentation method and device, electronic equipment and storage medium","The invention discloses a crack segmentation method, a crack segmentation device, electronic equipment and a storage medium, wherein the method comprises the following steps: inputting the image into a pre-trained crack segmentation model; performing convolution processing and pooling processing on the image based on the crack segmentation model to obtain a first characteristic image; respectively carrying out average pooling, maximum pooling and void convolution on the first characteristic images to obtain each second characteristic image; and connecting each second characteristic image in series to obtain a third characteristic image, performing residual convolution processing on the third characteristic image, and outputting a crack segmentation image. The global characteristic information of the cracks can be extracted through average pooling processing, the edge characteristic information of the cracks can be reserved through maximum pooling processing, and the local characteristic information of the cracks can be extracted through cavity convolution processing. The three processing modes are combined, so that the characteristic information of the crack can be kept as much as possible, and the finally output crack segmentation image is more accurate.","['G06T5/50', 'G06T7/0002', 'G06T7/10', 'G06T2207/20081', 'G06T2207/20221']"
US8620026B2,Video-based detection of multiple object types under varying poses,"Training data object images are clustered as a function of motion direction attributes and resized from respective original into same aspect ratios. Motionlet detectors are learned for each of the sets from features extracted from the resized object blobs. A deformable sliding window is applied to detect an object blob in input by varying window size, shape or aspect ratio to conform to a shape of the detected input video object blob. A motion direction of an underlying image patch of the detected input video object blob is extracted and motionlet detectors selected and applied that have similar motion directions. An object is thus detected within the detected blob and semantic attributes of an underlying image patch extracted if a motionlet detectors fires, the extracted semantic attributes available for use for searching for the detected object.","['G06V20/47', 'G06T7/248', 'G06V10/44']"
US9626766B2,Depth sensing using an RGB camera,"A method of sensing depth using an RGB camera. In an example method, a color image of a scene is received from an RGB camera. The color image is applied to a trained machine learning component which uses features of the image elements to assign all or some of the image elements a depth value which represents the distance between the surface depicted by the image element and the RGB camera. In various examples, the machine learning component comprises one or more entangled geodesic random decision forests.","['G06T7/0051', 'G06T7/50', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20072', 'G06T2207/20081']"
WO2020238054A1,"Method and apparatus for positioning chart in pdf document, and computer device","Provided are a method and apparatus for positioning a chart in a PDF document, and a computer device and a computer-readable storage medium. The embodiments of the present application belong to the technical field of image processing. The method comprises: when carrying out positioning on a chart in a PDF document, acquiring the PDF document, and converting, in a pre-set manner and according to the position in the PDF document of each page of the document, each page of the document in the PDF document into a picture carrying a pre-set position identifier; identifying, by means of a pre-set target detection model, a picture, among all the pictures, containing the chart as a target picture; extracting the chart in each target picture by means of the target detection model, so as to identify the position, in each corresponding target picture, of the chart; and combining, according to a pre-set sequence, the position, in the PDF document, of each target picture and the position, in each corresponding target picture, of the chart, so as to generate the position, in the PDF document, of the chart. A chart in a PDF document is accurately positioned.","['G06T7/10', 'G06T7/73', 'G06V30/413', 'G06T2207/20081', 'G06T2207/20084']"
US9269003B2,Diminished and mediated reality effects from reconstruction,"Disclosed embodiments pertain to apparatus, systems, and methods for mixed reality. In some embodiments, a camera pose relative to a tracked object in a live image may be determined and used to render synthetic images from keyframes in a 3D model without the tracked object. Optical flow magnitudes for pixels in a first mask region relative to a subset of the synthetic images may be determined and the optical flow magnitudes may be used to determine pixels in each of the subset of synthetic images that correspond to pixels in the first mask. For each pixel in the first mask, a corresponding replacement pixel may be determined as a function of pixels in the subset of synthetic images that correspond to the corresponding pixel in the first mask.","['G06K9/00624', 'G06T7/20', 'G06T19/006']"
US11756318B2,Convolutional neural networks for locating objects of interest in images of biological samples,"Convolutional neural networks for detecting objects of interest within images of biological specimens are disclosed. Also disclosed are systems and methods of training and using such networks, one method including: obtaining a sample image and at least one of a set of positive points and a set of negative points, wherein each positive point identifies a location of one object of interest within the sample image, and each negative point identifies a location of one object of no-interest within the sample image; obtaining one or more predefined characteristics of objects of interest and/or objects of no-interest, and based on the predefined characteristics, generating a boundary map comprising a positive area around each positive point the set of positive points, and/or a negative area around each negative point in the set of negative points; and training the convolutional neural network using the sample image and the boundary map.","['G06V20/69', 'G06F18/214', 'G06F18/24143', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/0895', 'G06N3/09', 'G06N3/091', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/698']"
US20180300937A1,System and a method of restoring an occluded background region,"A system and method of restoring an occluded background region includes detecting surfaces of a point cloud, thereby resulting in a surface map; substantially enhancing edges between detected surfaces according to a gradient map and the surface map, thereby generating an edge map; inpainting a depth image, thereby generating in an inpainted depth image; and inpainting a color image, thereby generating an inpainted color image.","['G06T17/00', 'G06T15/04', 'G06T5/77', 'G06T7/13', 'G06T7/194', 'G06T2200/04', 'G06T2200/08', 'G06T2207/10024', 'G06T2207/10028']"
US11581087B2,"Method, system and computer readable medium for automatic segmentation of a 3D medical image","A method, a system and a computer readable medium for automatic segmentation of a 3D medical image, the 3D medical image comprising an object to be segmented, the method characterized by comprising: carrying out, by using a machine learning model, in at least two of a first, a second and a third orthogonal orientation, 2D segmentations for the object in slices of the 3D medical image to derive 2D segmentation data; determining a location of a bounding box (10) within the 3D medical image based on the 2D segmentation data, the bounding box (10) having predetermined dimensions; and carrying out a 3D segmentation for the object in the part of the 3D medical image corresponding to the bounding box (10).","['G16H30/40', 'G06T7/11', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/0012', 'G06T7/10', 'G16H50/20', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004', 'G06T2210/12']"
US8331637B2,System and method of automatic prioritization and analysis of medical images,"A system for analyzing a source medical image of a body organ. The system comprises an input unit for obtaining the source medical image having three dimensions or more, a feature extraction unit that is designed for obtaining a number of features of the body organ from the source medical image, and a classification unit that is designed for estimating a priority level according to the features.","['G06T7/0012', 'G06T7/149', 'G06T7/75', 'G06V10/24', 'G06V10/40', 'G06V10/754', 'G16H15/00', 'G16H30/40', 'G16H50/20', 'G16H70/60', 'G06T2207/20128', 'G06T2207/20161', 'G06T2207/30016', 'G06V2201/032']"
US20250069204A1,Restoring degraded digital images through a deep learning framework,"The present disclosure relates to systems, methods, and non-transitory computer readable media for accurately, efficiently, and flexibly restoring degraded digital images utilizing a deep learning framework for repairing local defects, correcting global imperfections, and/or enhancing depicted faces. In particular, the disclosed systems can utilize a defect detection neural network to generate a segmentation map indicating locations of local defects within a digital image. In addition, the disclosed systems can utilize an inpainting algorithm to determine pixels for inpainting the local defects to reduce their appearance. In some embodiments, the disclosed systems utilize a global correction neural network to determine and repair global imperfections. Further, the disclosed systems can enhance one or more faces depicted within a digital image utilizing a face enhancement neural network as well.","['G06T5/77', 'G06N3/045', 'G06N3/0455', 'G06N3/0475', 'G06N3/08', 'G06N3/094', 'G06T3/18', 'G06T5/50', 'G06T5/60', 'G06T5/70', 'G06T5/73', 'G06T7/11', 'G06N3/0464', 'G06N3/09', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20092', 'G06T2207/20221', 'G06T2207/30201']"
US7623712B2,Image processing method and apparatus,"Disclosed is an image processing apparatus in which vector data from which noise has been removed and amount of data reduced can be generated in ideal fashion with regard to an illustration area selected from a document image. The document image is input to the apparatus and is segmented into a plurality of areas. A clip-art image is selected from these areas and a plurality of clusters are generated with regard to this image area. When a small area included in the plurality of clusters is determined as a noise, the noise is eliminated by combining the small area with a adjacent cluster. After noise is removed, the cluster group eventually obtained is converted to vector data.","['G06V30/413', 'G06V10/993']"
US9449239B2,Credit card auto-fill,"Differing embodiments of this disclosure may employ one or all of the several techniques described herein to perform credit card recognition using electronic devices with integrated cameras. According to some embodiments, the credit card recognition process may comprise: obtaining a first representation of a first image, wherein the first representation comprises a first plurality of pixels; identifying a first credit card region within the first representation; extracting a first plurality of sub-regions from within the identified first credit card region, wherein a first sub-region comprises a credit card number, wherein a second sub-region comprises an expiration date, and wherein a third sub-region comprises a card holder name; generating a predicted character sequence for the first, second, and third sub-regions; and validating the predicted character sequences for at least the first, second, and third sub-regions using various credit card-related heuristics, e.g., expected character sequence length, expected character sequence format, and checksums.","['G06K9/186', 'G06V30/142', 'G06K9/2081', 'G06K9/228', 'G06K9/325', 'G06K9/72', 'G06V10/768', 'G06V20/62', 'G06V30/2253']"
CN112652016B,"Generation method of point cloud prediction model, pose estimation method and device thereof","The application discloses a point cloud prediction model generation method, a pose estimation device, equipment and a storage medium, and relates to the technical fields of computer vision, automatic driving, robots and the like. The method for generating the point cloud prediction model comprises the following steps: acquiring training data, the training data comprising: two-dimensional image data of a target object and six-degree-of-freedom pose data of the target object; the two-dimensional image data of the target object are obtained by shooting with a calibrated camera, and the two-dimensional image data comprise two-dimensional coordinates of a plurality of pixels of the target object in the two-dimensional image; the six-degree-of-freedom pose data of the target object are obtained by carrying out three-dimensional modeling on the real size of the target object; and training the first neural network by using the training data, and completing training after the stopping condition is reached to obtain a point cloud prediction model, wherein the output of the point cloud prediction model comprises three-dimensional coordinates of a plurality of pixels of the target object.","['G06T7/73', 'G06T17/00', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30252']"
US8503760B2,System and method for real-time object recognition and pose estimation using in-situ monitoring,"Provided are a system and method for real-time object recognition and pose estimation using in-situ monitoring. The method includes the steps of: a) receiving 2D and 3D image information, extracting evidences from the received 2D and 3D image information, recognizing an object by comparing the evidences with model, and expressing locations and poses by probabilistic particles; b) probabilistically fusing various locations and poses and finally determining a location and a pose by filtering inaccurate information; c) generating ROI by receiving 2D and 3D image information and the location and pose from the step b) and collecting and calculating environmental information; d) selecting an evidence or a set of evidences probabilistically by receiving the information from the step c) and proposing a cognitive action of a robot for collecting additional evidence; and e) repeating the steps a) and b) and the steps c) and d) in parallel until a result of object recognition and pose estimation is probabilistically satisfied.","['G06T7/60', 'G06T7/35', 'G06F18/254', 'G06T1/00', 'G06T7/20', 'G06T7/73', 'G06T7/77', 'G06V10/809', 'G06V20/64', 'G06T2207/10028']"
CN112101160B,Binocular semantic SLAM method for automatic driving scene,"The invention discloses a binocular semantic SLAM method for an automatic driving scene, which can accurately track the pose of a vehicle in the environment in the automatic driving scene, and detect and track other objects such as vehicles, pedestrians and the like in the scene. Compared with the previous method which can only use the quadric surface as a map for a static object or a road sign, the method can use the quadric surface to express the state of the object at different moments, both for the static object in a scene and the object in motion. Aiming at the condition of insufficient observation of a semantic object, the invention fully utilizes the neural network and the prior information to carry out constraint, constructs an error item for object observation, and realizes nonlinear optimization solution of object state parameters.","['G06V20/56', 'G06F18/2415', 'G06N3/045', 'G06N3/082', 'G06V10/25', 'G06V20/41']"
CN111582409B,"Training method of image tag classification network, image tag classification method and device","The application discloses a training method of an image tag classification network, an image tag classification method and equipment, and relates to the field of artificial intelligence, wherein the method comprises the following steps: acquiring a sample image; extracting features of the sample image through a feature extraction network to obtain a sample feature image output by the feature extraction network; inputting the sample feature images into a graph network classifier to obtain sample label classification results output by the graph network classifier, wherein the graph network classifier is constructed based on a target graph network, graph nodes in the target graph network correspond to image labels, and edges between different graph nodes are used for representing co-occurrence probabilities between different image labels; and training a feature extraction network and a graph network classifier according to the error between the sample label classification result and the sample image label. In the embodiment of the application, when the image network classifier is used for classifying the labels, the relevance among different image labels can be integrated, and the efficiency and the accuracy of image label classification can be improved.","['G06F18/2415', 'G06F18/214', 'G06F18/253', 'G06N3/045', 'G06N3/048', 'G06N3/084']"
CN112970056B,Human-computer interface using high-speed and accurate user interaction tracking,"Embodiments described herein relate to systems, devices, and methods for use in the implementation of human-computer interfaces that use high-speed and efficient tracking of user interactions with user interfaces/user experiences that are strategically presented to a user. Embodiments described herein also relate to the implementation of a hardware agnostic human-computer interface that uses neural, eye movement, and/or electromyographic signals to convey user manipulation of machines and devices.","['G06F3/011', 'G02B27/0093', 'G02B27/0172', 'G02B27/0179', 'G06F3/012', 'G06F3/013', 'G06F3/015', 'G06F3/016', 'G06F3/04815', 'G02B2027/014', 'G02B2027/0141', 'G02B2027/0187']"
US9767371B2,Systems and methods for identifying traffic control devices and testing the retroreflectivity of the same,"Systems and methods for identifying traffic control devices from images, and systems and methods for assessing the retro reflectivity of traffic control devices. The identification of traffic control devices can be accomplished using a lighting-dependent statistical color model. The identification of traffic control devices can be accomplished using an active contour or active polygon method.","['G06K9/00818', 'G06F13/16', 'G01S17/42', 'G01S17/931', 'G06F13/20', 'G06F18/24', 'G06K9/4604', 'G06K9/4652', 'G06K9/6267', 'G06T7/11', 'G06V10/26', 'G06V10/44', 'G06V10/56', 'G06V20/582', 'G06V30/194', 'G08G1/00', 'G09F2009/3055']"
CN113205568B,"Image processing method, device, electronic equipment and storage medium","The disclosure relates to an image processing method, an image processing device, electronic equipment and a storage medium, and belongs to the technical field of computers. The method comprises the following steps: smoothing the skin area of the target object to obtain a first image for an original image containing the target object; determining skin texture material corresponding to the face area based on the face area of the target object; rendering the skin texture material to obtain a face texture image corresponding to the target object, wherein the face key point information and the face posture information of the face texture image are matched with the target object; and fusing the face texture image and the first image to obtain a second image. The method and the device can restore the face texture details after the original image is ground, and weaken the false face feeling of the grinding belt on the basis of not introducing image noise, so that the image processing effect is improved.","['G06T5/50', 'G06T5/70', 'G06T11/001', 'G06T11/00', 'G06T15/04', 'G06T5/20', 'G06T5/77', 'G06T7/73', 'G06T2200/04', 'G06T2207/10004', 'G06T2207/20221', 'G06T2207/30201']"
US6185320B1,Method and system for detection of lesions in medical images,"A method and system for the automated detection of lesions in medical images. Medical images, such as mammograms are segmented and optionally processing with peripheral enhancement and/or modified median filtering. A modified morphological open operation and filtering with a modified mass filter are performed for the initial detection of circumscribed lesions. Then, the lesions are matched using a deformable shape template with Fourier descriptors. Characterization of the match is done using simulated annealing, and measuring the circularity and density characteristics of the suspected lesion. The procedure is performed iteratively at different spatial resolution in which at each resolution step a specific lesion size is detected. The detection of the lesion leads to a localization of a suspicious region and thus the likelihood of cancer.","['G06T7/0012', 'G06T7/11', 'G06T7/12', 'G06T7/149', 'G06T7/155', 'G06T7/187', 'G06V10/478', 'G06T2207/20056', 'G06T2207/20156', 'G06T2207/30068', 'G06T2207/30096']"
CN112418216B,Text detection method in complex natural scene image,"The invention discloses a text detection method in a complex natural scene image, belongs to the field of computer vision and pattern recognition, relates to the technical field of neural networks and computer vision, and particularly relates to a text detection method based on deep learning under a complex scene. The character detection method based on character labeling and the character detection method based on word labeling are combined, the combination characteristics among characters are learned, the false detection rate of characters can be reduced, the redundancy of a detection frame is reduced, and the capability of flexibly coping with characters of any shape is achieved. A text detection method under complex scene comprises the following steps: preprocessing image data, constructing a network frame, pre-training a model and training the network frame; and the text real tag generation and input module is used for generating and inputting a text image, feature extraction, image judgment and text correction under a natural scene to be detected.","['G06V20/63', 'G06F18/214', 'G06N3/045', 'G06N3/08', 'G06V10/267', 'G06V30/1478', 'G06V30/153', 'G06V30/10']"
US11455525B2,Method and apparatus of open set recognition and a computer readable storage medium,"A method and apparatus of open set recognition, and a computer-readable storage medium are disclosed. The method comprises acquiring auxiliary data and training data of known categories for open set recognition, training a neural network alternately using the auxiliary data and the training data, until convergence; extracting a feature of data to be recognized for open set recognition, using the trained neural network; and recognizing a category of data to be recognized, based on the feature of the data to be recognized.","['G06V10/82', 'G06F18/214', 'G06K9/6256', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/0475', 'G06N3/08', 'G06N3/09', 'G06N3/094', 'G06N5/046', 'G06V30/19147', 'G06V30/244', 'G06N20/10', 'G06N3/044', 'G06V30/287']"
CN109377485B,Machine vision detection method for instant noodle packaging defects,"The invention discloses a machine vision detection method for defects of instant noodle packaging, which mainly comprises the following steps: establishing an automatic detection device model for the packaging defects of the instant noodles, collecting an image of the packaging of the instant noodles on a conveyor belt, preprocessing the image, and enhancing the image characteristics; selecting image segmentation center pixel points, establishing a similarity measurement standard according to the continuity between pixel grayscales, and segmenting all pixels into different regions; convolving the packaging image by a Gaussian kernel function to obtain a Gaussian difference image, and extracting the defect characteristics of the packaging image of the instant noodle by solving the gray extreme value of the image; classifying by measuring the similarity between the defect characteristics and the sample, detecting the defect category and kicking off, and completing the automatic detection of the defects of the instant noodle packaging. The method has the advantages of good stability and robustness, low omission factor and false detection rate, high detection efficiency, high detection speed, nondestructive detection, labor resource saving, cost reduction for enterprises and more fine instant noodle packaging.","['G06T7/0004', 'G06T5/40', 'G06T7/11', 'G06T7/187', 'G06T2207/10024', 'G06T2207/20076', 'G06T2207/30128', 'Y02P90/30']"
CN108090919B,A Kernel Correlation Filter Tracking Method Based on Superpixel Optical Flow and Adaptive Learning Factor Improvement,"The invention discloses a kernel correlation filtering tracking method based on superpixel optical flow and self-adaptive learning factor improvement, which realizes the appearance reconstruction of a target through a superpixel analysis strategy, divides the target into superpixel blocks and clusters the superpixel blocks into superpixel centers, calculates the displacement change of an optical flow analysis pixel point of each superpixel center, and predicts the motion offset and the scale change of the target; based on the predicted parameters, after cyclic sampling is carried out in a new frame of image, each sample adopts a related filtering target tracking method based on a Gaussian kernel after introducing self-adaptive learning factors for improvement, and the accurate position and the scale of the target are detected; and finally, detecting and correcting the detection result through an online double-SVM detection model, correcting the position with low confidence coefficient, and finally accurately positioning the target position and obtaining the accurate scale of the target. The invention overcomes the tracking problems of scale change, shielding, deformation, motion blur and the like in the target tracking process and realizes real-time high-precision target tracking.","['G06T7/262', 'G06F18/23213', 'G06F18/2411', 'G06T7/246', 'G06T2207/10016', 'G06T2207/20056', 'G06T2207/20081', 'G06V2201/07']"
CN111951221B,A glomerular cell image recognition method based on deep neural network,"The application discloses a glomerular cell image recognition method based on a deep neural network, which is used for acquiring a pathological image to be detected based on artificial intelligence and a deep learning technology; preprocessing the pathological image to obtain a plurality of slice images; inputting each slice image into a preset neural network model for identification and segmentation to obtain a glomerular region map; cell counting was performed on glomerular area map; the method can quickly and accurately divide the sub-image of the glomerulus in the kidney in the pathological image, count the cells in the glomerulus by using the traditional method and the deep learning fusion model, and solve the problems of high workload and low efficiency and high misdiagnosis rate of manually identifying the glomerulus in the pathological image; the application relates to the field of biomedical image processing, in particular to a method for optimizing glomerular sub-image segmentation and glomerular cell counting algorithms in pathological images, improving segmentation and counting accuracy by using more data training algorithms.","['G06T7/0012', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T7/136', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024', 'G06T2207/30084', 'G06T2207/30242', 'Y02A90/10']"
CN110032942B,Action recognition method based on time domain segmentation and feature difference,"The invention discloses a motion identification method based on time domain segmentation and feature difference, which comprises the following steps: s1, dividing the action video in a training set into a plurality of segments at equal intervals, and randomly extracting a frame of RGB image and an optical flow image in each segment; s2, constructing a double-current network; s3, correspondingly inputting all RGB images and optical flow images into a double-current network respectively for training; and S4, inputting the target action video into the trained dual-flow network for action recognition, and fusing results obtained by all network flows to obtain a recognition result of the action video. The motion video is segmented on a time domain, the characteristics of different time periods in the motion video are integrated, and the difference fusion characteristics of the motion video are obtained by performing difference fusion on the characteristics, so that long-term dynamic information is effectively extracted; and meanwhile, space-time correlation fusion is carried out on the space flow characteristics and the time flow characteristics in the double-flow network, and important local information with space-time consistency is further extracted while original space-time information is kept.","['G06F18/214', 'G06F18/253', 'G06V40/20']"
US8731255B2,Computer aided diagnostic system incorporating lung segmentation and registration,"A computer aided diagnostic system and automated method diagnose lung cancer through tracking of the growth rate of detected pulmonary nodules over time. The growth rate between first and second chest scans taken at first and second times is determined by segmenting out a nodule from its surrounding lung tissue and calculating the volume of the nodule only after the image data for lung tissue (which also includes image data for a nodule) has been segmented from the chest scans and the segmented lung tissue from the chest scans has been globally and locally aligned to compensate for positional variations in the chest scans as well as variations due to heartbeat and respiration during scanning. Segmentation may be performed using a segmentation technique that utilizes both intensity (color or grayscale) and spatial information, while registration may be performed using a registration technique that registers lung tissue represented in first and second data sets using both a global registration and a local registration to account for changes in a patient's orientation due in part to positional variances and variances due to heartbeat and/or respiration.","['G06T7/0016', 'G06T7/11', 'G06T7/143', 'G06T7/33', 'G06T7/35', 'G06T2207/10081', 'G06T2207/20016', 'G06T2207/30064']"
CN111971752B,Display of medical image data,"A system and method for displaying medical image data is provided, wherein the display of the medical image data is determined based on schematic image data of a schematic representation of an anatomical structure. The schematic representation may provide a specific view of the anatomical structure. The type of anatomical structure and the view of the anatomical structure provided by the schematic representation may be determined based on one or more image features in the schematic representation image data. The view may be characterized as showing a geometrically defined perspective at the anatomical structure in a schematic representation. An output image displaying the anatomical structure in the medical image data may be generated from the determined geometrically defined view angle. Thus, the user may be provided with a display of medical image data that is easier to interpret after considering the schematic representation.","['G16H30/40', 'A61B5/7267', 'A61B5/743', 'A61B5/7475', 'G06F18/214', 'G06F18/24', 'G06N20/00', 'G06T15/20', 'G06T7/0014', 'G06T7/10', 'G06T7/70', 'G16H15/00', 'G16H30/20', 'G16H50/50', 'A61B2576/00', 'G06T2200/24', 'G06T2207/20081', 'G06T2207/30004']"
US8401292B2,Identifying high saliency regions in digital images,"A method for identifying high saliency regions in a digital image, comprising: segmenting the digital image into a plurality of segmented regions; determining a saliency value for each segmented region, merging neighboring segmented regions that share a common boundary in response to determining that one or more specified merging criteria are satisfied; and designating one or more of the segmented regions to be high saliency regions. The determination of the saliency value for a segmented region includes: determining a surround region including a set of image pixels surrounding the segmented region; analyzing the image pixels in the segmented region to determine one or more segmented region attributes; analyzing the image pixels in the surround region to determine one or more corresponding surround region attributes; determining a region saliency value responsive to differences between the one or more segmented region attributes and the corresponding surround region attributes.","['G06T7/11', 'G06T7/162', 'G06V10/462', 'G06T2207/20072', 'G06T2207/20164', 'G06V10/26']"
US9911055B2,Method and system for detection and classification of license plates,"Methods, systems, and processor-readable media for the detection and classification of license plates. In an example embodiment, an image of a vehicle can be captured with an image-capturing unit. A license plate region can then be located in the captured image of the vehicle by extracting a set of candidate regions from the image utilizing a weak classifier. A set of candidate regions can be ranked utilizing a secondary strong classifier. The captured image can then be classified according to a confidence driven classification based on classification criteria determined by the weak classifier and the secondary strong classifier.","['G06K9/325', 'G06V10/245', 'G06F18/24', 'G06K9/3216', 'G06K9/6267', 'G06V10/454', 'G06V20/62', 'G06V30/2504', 'G06K2209/15', 'G06V20/625']"
US9489743B2,Determining dimension of target object in an image using reference object,Systems and methods for determining one or more dimensions of an object using a digital image are described.,"['G06T7/12', 'G06T7/0087', 'G06T7/0032', 'G06T7/0071', 'G06T7/0083', 'G06T7/0093', 'G06T7/143', 'G06T7/162', 'G06T7/194', 'G06T7/344', 'G06T7/579', 'G06T7/60', 'G06T2207/10024', 'G06T2207/20076', 'G06T2207/20144', 'G06T2207/30196', 'G06T2207/30201', 'G06T2207/30204']"
US8009880B2,Recovering parameters from a sub-optimal image,"A subregion-based image parameter recovery system and method for recovering image parameters from a single image containing a face taken under sub-optimal illumination conditions. The recovered image parameters (including albedo, illumination, and face geometry) can be used to generate face images under a new lighting environment. The method includes dividing the face in the image into numerous smaller regions, generating an albedo morphable model for each region, and using a Markov Random Fields (MRF)-based framework to model the spatial dependence between neighboring regions. Different types of regions are defined, including saturated, shadow, regular, and occluded regions. Each pixel in the image is classified and assigned to a region based on intensity, and then weighted based on its classification. The method decouples the texture from the geometry and illumination models, and then generates an objective function that is iteratively solved using an energy minimization technique to recover the image parameters.","['G06T7/11', 'G06V10/60', 'G06V40/168', 'G06T2207/30201']"
CN111462054B,A kind of dispensing quality detection method,"The invention discloses a dispensing quality detection method, which comprises the following steps: s1, acquiring a standard qualified workpiece image, and determining an edge groove based on a HALCON image analysis processing technology, wherein the edge groove refers to a glue area in a workpiece; s2, determining a glue overflow area in the standard qualified workpiece image by using the obtained glue area; s3, creating a matching template by using the standard qualified workpiece drawing; s4, acquiring a workpiece image to be detected, and matching the workpiece image with the matching template established in the S3; and S5, determining the dispensing quantity detection result according to the matching result of the step S4.","['G06T7/0004', 'G01B11/28', 'G06T7/13', 'G06T7/136', 'G06T7/187', 'G06V10/751', 'G06T2207/10004', 'G06T2207/30108', 'Y02P90/30']"
WO2024130776A1,Three-dimensional lidar point cloud semantic segmentation method and apparatus based on deep learning,"A three-dimensional LiDAR point cloud semantic segmentation method based on deep learning, comprising: constructing a lightweight deep full convolutional neural network; preprocessing LiDAR original point cloud; converting three-dimensional space features into two-dimensional spatial features; training the lightweight deep full convolutional neural network; performing two-dimensional point cloud semantic segmentation; performing two-dimensional semantic tag post-processing optimization; and recovering three-dimensional semantic tags. The present invention further provides a three-dimensional LiDAR point cloud semantic segmentation apparatus based on deep learning. The present invention achieves feature dimension reduction by using a polar coordinate system transformation mode; and using the characteristic of cross-stage feature fusion of the lightweight full convolutional neural network (CSPRangeNet) based on deep learning not only ensures the accuracy of a segmentation result, but also reduces the space consumption, thus reducing the computational load of a network feature extraction stage, and greatly improving the segmentation rate. The present invention is applicable to three-dimensional LiDAR point cloud semantic segmentation having a high real-time requirement.","['G06V10/26', 'G06N3/084', 'G06V10/20', 'G06V10/82']"
US8219438B1,Method and system for measuring shopper response to products based on behavior and facial expression,"The present invention is a method and system for measuring human response to retail elements, based on the shopper's facial expressions and behaviors. From a facial image sequence, the facial geometry—facial pose and facial feature positions—is estimated to facilitate the recognition of facial expressions, gaze, and demographic categories. The recognized facial expression is translated into an affective state of the shopper and the gaze is translated into the target and the level of interest of the shopper. The body image sequence is processed to identify the shopper's interaction with a given retail element—such as a product, a brand, or a category. The dynamic changes of the affective state and the interest toward the retail element measured from facial image sequence is analyzed in the context of the recognized shopper's interaction with the retail element and the demographic categories, to estimate both the shopper's changes in attitude toward the retail element and the end response—such as a purchase decision or a product rating.","['G06Q30/0201', 'G06F18/295', 'G06Q30/0202', 'G06Q30/0204', 'G06V10/85', 'G06V20/52', 'G06V40/174', 'G06V40/193']"
US5097517A,"Method and apparatus for processing bank checks, drafts and like financial documents","Banking apparatus for reading numeric information on bank checks, drafts and like documents. An electronic black/white pixel image of the numeric information is analyzed by a system capable of reading unconstrained, constrained, printed and typed numeric characters, locating the division between dollar and cents amounts. The invention also reads overlapping touching and not touching characters. A check is fed (330) to a scanner (331) and converted to a black and white image. A courtesy amount field (CAF) on the check is located by a CAF locator (332) and is provided to a recognition system (333).",['G06V30/19093']
US9336541B2,"Augmented reality product instructions, tutorials and visualizations","In a system for augmented reality product instructions, tutorials and visualizations a method may include receiving a request for information from a client device, the request including image data and a request type; converting, using at least one processor, the image data into a digital fingerprint; comparing the digital fingerprint to a plurality of stored fingerprints to identify an object in the image data; and generating an augmented reality view of the identified object based on the request type; and transmitting the augmented reality view to the client device.","['G06Q30/0281', 'G06F3/011', 'G06F3/147', 'G06Q30/0261', 'G06Q30/0631', 'G06T19/006', 'G06V40/12', 'G09G2370/022', 'G09G2370/027', 'G09G2370/04', 'G09G2380/14']"
US5504822A,Character recognition system,"Banking apparatus for reading numeric information on bank checks, drafts and like documents. An electronic black/white pixel image of the numeric information is analyzed by a system capable of reading unconstrained, constrained, printed and typed numeric characters, locating the division between dollar and cents amounts. The invention also reads overlapping, touching and not touching characters.",['G06V30/18']
US10922572B2,Adaptive auto meter detection method based on character segmentation and cascade classifier,"An adaptive automobile meter detection method based on character segmentation cascade classifier which processes threshold segmentation, morphology and connected components analysis on the original image; extracts the pointer based on the contour analysis method, establishes a pointer information list; constructs a character segmentation cascade classifier by combining a HOG/SVM character segmentation classifier, a character filter and a CNN digit classifier. The character segmentation cascade classifiers is used to identify the digit character area of the automobile meter. Region analysis is performed to extract tick marks based on the center of the digit character area. The angular position corresponding to the tick mark is determined. The response value corresponding to the pointer is determined by establishing the Newton interpolation linear description relationship between the pointer angle and the response value and then the meter is classified as pass or fail. The invention is suitable for the field of visual inspection of automobile meter pointers.","['G06K9/32', 'G06V10/82', 'B60K35/00', 'B60K35/10', 'B60K35/28', 'B60K35/90', 'B60K37/02', 'G01D18/002', 'G06N3/0464', 'G06N3/09', 'G06V30/153', 'G06V30/19173', 'B60K2360/176', 'B60K2360/21', 'B60K2360/60', 'B60K2370/176', 'B60K2370/21', 'B60K2370/60', 'B60K2370/95', 'G06K2209/01', 'G06K2209/03', 'G06N20/00', 'G06N3/045', 'G06N3/0454', 'G06T2207/20084', 'G06V2201/02', 'G06V30/10']"
CN109635685B,"Target object 3D detection method, device, medium and equipment","The embodiment of the application discloses a target object 3D detection method, a target object 3D detection device, electronic equipment, a computer readable storage medium and a computer program, wherein the target object 3D detection method comprises the following steps: extracting characteristic information of point cloud data of the acquired scene; performing semantic segmentation on the point cloud data according to the feature information of the point cloud data to obtain first semantic information of a plurality of points in the point cloud data; predicting at least one foreground point of a corresponding target object in a plurality of points according to the first semantic information; generating a 3D initial frame corresponding to each of the at least one foreground point according to the first semantic information; and determining a 3D detection frame of the target object in the scene according to the 3D initial frame.","['G06V20/56', 'G06F18/24', 'G06N3/045', 'G06N3/08']"
CN112881419B,"Chip detection method, electronic device and storage medium","The application relates to a chip detection method, electronic equipment and a storage medium. The method comprises the following steps: shooting a rough detection image of the chip to be detected under a low-magnification objective lens by using a black-and-white high-resolution camera; using a color high-resolution camera to shoot an upper surface image and a lower surface image on a chip to be detected under a high-magnification objective lens; the upper surface image and the lower surface image are fused into an image by utilizing an image fusion algorithm, and a reinspection image with superimposed depth information is obtained; and performing defect analysis on the recheck image by using a defect analysis algorithm, and identifying the defect type corresponding to the target defect image. According to the scheme, coarse inspection can be completed under the low-magnification objective lens by using the black-white high-resolution camera, recheck can be completed under the high-magnification objective lens by using the color high-resolution camera, the data acquisition cost of acquiring the image to be analyzed in the early stage is reduced, and the efficiency of the whole defect analysis processing process is improved.","['G01N21/8851', 'G01N2021/8887']"
US10627915B2,Visual collaboration interface,"Embodiments described herein includes a system comprising a processor coupled to display devices, sensors, remote client devices, and computer applications. The computer applications orchestrate content of the remote client devices simultaneously across the display devices and the remote client devices, and allow simultaneous control of the display devices. The simultaneous control includes automatically detecting a gesture of at least one object from gesture data received via the sensors. The detecting comprises identifying the gesture using only the gesture data. The computer applications translate the gesture to a gesture signal, and control the display devices in response to the gesture signal.","['G06F3/017', 'G06F3/0304', 'G06F3/0346', 'G06F3/03547', 'G06F3/1423', 'G06F3/1454', 'G06F9/451', 'G06K9/00355', 'G06V40/28', 'G09G5/34', 'G06F2203/0381', 'G06F2203/0383', 'G09G2340/045', 'G09G2354/00']"
US6351573B1,Imaging device and method,"A method and apparatus for obtaining and displaying in real time an image of an object obtained by one modality such that the image corresponds to a line of view established by another modality. In a preferred embodiment, the method comprises the following steps: obtaining a follow image library of the object via a first imaging modality; providing a lead image library obtained via the second imaging modality; referencing the lead image library to the follow image library; obtaining a lead image of the object in real time via the second imaging modality along a lead-view; comparing the real time lead image to lead images in the lead image library via digital image analysis to identify a follow image line of view corresponding to the real time lead view; transforming the identified follow image to correspond to the scale, rotation and position of the lead image; and displaying the transformed follow image, the comparing, transforming and displaying steps being performed substantially simultaneously with the step of obtaining the lead image in real time.","['G16H30/20', 'G16H20/40', 'A61B2090/374', 'A61B2090/378', 'A61B90/10', 'A61B90/36']"
EP4109350A1,"Neural network compression method, data processing method, and related apparatuses","A neural network compression method, a data processing method, and a related apparatus are provided, and relate to the field of artificial intelligence, and in particular, to the field of computer vision. The method includes: performing the following operations on a first three-dimensional convolution kernel included in a to-be-compressed three-dimensional convolutional neural network: performing orthogonal transform on the first three-dimensional convolution kernel in a time dimension of the first three-dimensional convolution kernel (S310); and pruning the time dimension of the first three-dimensional convolution kernel based on sparsity of the orthogonally transformed first three-dimensional convolution kernel in the time dimension, to obtain a compressed three-dimensional convolutional neural network (S320). In the foregoing method, orthogonal transform is performed on the three-dimensional convolution kernel in the time dimension of the three-dimensional convolution kernel of the to-be-compressed three-dimensional convolutional neural network, and the sparse time dimension of the orthogonally transformed three-dimensional convolution kernel is pruned. In this way, a parameter quantity and a calculation amount of a three-dimensional convolutional neural network can be further reduced.","['G06N3/082', 'G06F17/147', 'G06N3/045', 'G06N3/0464', 'G06N3/0495']"
CN115115570B,"Medical image analysis method and device, computer equipment and storage medium","The present disclosure relates to a medical image analysis method and apparatus, a computer device, and a computer-readable storage medium. The medical image analysis method includes receiving a medical image acquired by a medical imaging device, performing a navigation process on the medical image based on analysis requirements to obtain a navigation trajectory, the analysis requirements including a disease to be analyzed, extracting a set of image blocks along the navigation trajectory, extracting image features based on the set of image blocks using a first learning network, and feeding the extracted image features to a second learning network configured to determine an analysis result based on the image features and the navigation trajectory. The method and the device unify the model framework by utilizing the navigation design, and can realize more accurate and targeted analysis on the medical image based on the navigation track and combined with the first learning network and the second learning network, so that the complexity of medical image processing is reduced, the efficiency and the accuracy of medical image analysis are improved, and the labor cost is reduced.","['G06F40/30', 'G06F18/21', 'G06F18/24', 'G06F18/253', 'G06F40/20', 'G06N3/084', 'G06T7/0012', 'G06T7/10', 'G06T7/11', 'G06V10/454', 'G06V10/50', 'G06V10/82', 'G16H10/60', 'G16H30/20', 'G16H30/40', 'G16H50/70', 'G06F40/216', 'G06N3/044', 'G06N3/045', 'G06N3/0464', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10116', 'G06T2207/10132', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30016', 'G06T2207/30056', 'G06T2207/30084', 'G06T2207/30096', 'G06T2207/30101', 'G06T2207/30172', 'G06T2207/30241', 'G06V2201/031']"
US11176443B1,Application control and text detection from application screen images,"Automation controls and associated text in images displayed by a computer application are automatically detected by way of region-based R-FCN and Faster R-CNN engines. Datasets comprising images containing application controls, where the application controls include images of application where width is greater than height, width is equal to height and height is greater than width are retrieved and each dataset is processed with the R-FCN and Faster R-CNN engines to generate a software robot configured to recognize corresponding application controls. Text is recognized by an optical character recognition system that employs a deep learning system trained to process a plurality of images to identify images representing text within each image and to convert the images representing text to textually encoded data. The deep learning system is trained with training data generated from a corpus of real-life text segments that are generated by a plurality of OCR modules.","['G06V30/414', 'G06N3/0454', 'G06F18/2148', 'G06F18/217', 'G06F18/24', 'G06F18/25', 'G06K9/00442', 'G06K9/3241', 'G06K9/46', 'G06K9/6257', 'G06K9/6262', 'G06K9/6267', 'G06K9/6288', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/0895', 'G06N3/09', 'G06V10/82', 'G06V30/19147', 'G06V30/19173', 'G06K2209/01', 'G06N20/00', 'G06N3/044', 'G06V30/10']"
CN109670591B,A neural network training method and image matching method and device,"The application discloses a training method of a neural network, an image matching method and an image matching device, which at least comprise the following steps: annotating annotation information for a first garment instance and a second garment instance, the first garment instance and the second garment instance derived from a first garment image and a second garment image, respectively; in response to the first garment instance and the second garment instance matching, pairing the first garment image and the second garment image; training a neural network to be trained based on the paired first garment image and second garment image.","['G06V20/20', 'G06F18/214', 'G06F18/22', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06V10/25', 'G06V10/44', 'G06V10/454', 'G06V10/774', 'G06V10/82']"
US10915788B2,Optical character recognition using end-to-end deep learning,"Disclosed herein are system, method, and computer program product embodiments for optical character recognition using end-to-end deep learning. In an embodiment, an optical character recognition system may train a neural network to identify characters of pixel images and to assign index values to the characters. The neural network may also be trained to identify groups of characters and to generate bounding boxes to group these characters. The optical character recognition system may then analyze documents to identify character information based on the pixel data and produce a segmentation mask and one or more bounding box masks. The optical character recognition system may supply these masks as an output or may combine the masks to generate a version of the received document having optically recognized characters.","['G06N3/08', 'G06K9/6256', 'G06F18/214', 'G06F40/279', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/09', 'G06V10/82', 'G06V30/19173', 'G06V30/413', 'G06K2209/01', 'G06V30/10']"
US20230281807A1,Mixed-format labels for pathology detection and localization in magnetic resonance (mr) imaging,"Described are systems, techniques, and processes for pathology detection in radiological images. A process can include obtaining a radiological image corresponding to an imaged anatomical area. Based on processing the radiological image using a semantic segmentation neural network, a target map can be generated corresponding to a plurality of candidate anatomical defect locations within the cropped radiological image. At least one volume of interest (VOI) can be generated centered around a particular candidate anatomical defect location within the cropped radiological image. A classification neural network can be used to classify the particular candidate anatomical defect location within the cropped radiological image, wherein classifying the particular candidate anatomical defect location includes determining a pathology associated with the particular candidate anatomical defect location.","['G06T7/0012', 'G06T7/11', 'G06V10/25', 'G06V10/267', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06V20/64', 'G06T2207/10116', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30004', 'G06V2201/03']"
CN113312973B,A method and system for extracting key point features of gesture recognition,"The invention relates to a gesture recognition key point feature extraction method and a gesture recognition key point feature extraction system, wherein the gesture recognition key point feature extraction method comprises the following steps: performing feature extraction and region segmentation on the input RGB three-channel image to obtain an example segmentation and mask of the hand; performing target matching on the example segmentation of the hand and the mask, and marking key points; and carrying out data smoothing on the marked key points, and recalibrating the skeleton points, so as to obtain stable gesture extraction characteristics. The method can remove environmental interference to the greatest extent, accurately extract key points, and obviously improve the precision and the robustness compared with the traditional method and single Mask R-CNN extraction; the invention can be widely applied to the technical field of feature extraction.","['G06V40/113', 'G06N3/045', 'G06N3/08', 'G06V10/34', 'G06V10/44']"
CN106803244B,Defect identification method and system,"A kind of defect identification method and system, wherein method includes: to obtain image to be detected and template image；Standard picture is determined in template image, the set in the region that the external geometric figure of minimum that the external geometric figure of minimum in region, test object that standard picture is covered by the external geometric figure of minimum of test object expands the region or each element that are covered after predetermined size outward is covered；Found out in image to be detected according to standard picture with the maximum region of standard picture similarity, and the region is determined as area to be tested；Area to be tested is transformed to and the consistent image of standard picture；Standard picture and area to be tested are compared, determine defective region.Above method can not only identify the whether defective presence of area to be tested, can also obtain the specific location of defect.","['G06T7/001', 'G06T2207/10004', 'G06T2207/30108', 'G06T2207/30144']"
US6798897B1,"Real time image registration, motion detection and background replacement using discrete local motion estimation","A method for registration between first and second images, the method including defining, for each individual location from among a plurality of locations sparsely distributed over the first image, a local probability matrix in which each element represents the probability of a possible displacement between the individual location in the first image and its corresponding location within the second image, defining a combined probability matrix by combining corresponding elements over the plurality of probability matrices, and computing an alignment of the first and second images in accordance with a combination of at least one of the largest of the elements of the combined probability matrix; and further, a method for detecting motion within a scene including at least one moving objects by comparing first and second time-separated images of the scene, the method comprising defining, for each individual location from among a plurality of locations distributed over the first image, a local probability matrix in which each element represents the probability of a possible displacement between said individual location in the first image, representing an individual portion of the scene in the first image, and its corresponding location within the second image, and ranking the local probability matrices into a plurality of ranks of matrices, differing in the probability that the individual location corresponding to a matrix belonging to a rank was displaced between the first and second images, relative to what is known regarding camera motion between the first and second images.","['G06T7/35', 'G06T7/33']"
CN111626190B,Water level monitoring method for scale recognition based on clustering partition,"The invention relates to a water level monitoring method for scale recognition based on clustering partition, and belongs to the technical field of water level monitoring. Comprising the following steps: 1) Acquiring an original image at the moment t from a real-time monitoring video; 2) Intercepting a water gauge area in an original image, and taking the tail end of the water gauge as the position of a water line; 3) Binarization processing is carried out on the water gauge area image, and the processed water gauge area image is divided into a plurality of subareas by adopting a clustering method according to the three edges of the E; 4) Identifying the content of each sub-area to obtain the numerical value of the last area containing the number of the area where the water line is located; 5) And calculating the water level according to the height of the subareas and the numerical value obtained in the step 4) of identification and displaying. The invention avoids complex characteristic extraction and data reconstruction processes in the traditional recognition algorithm, can rapidly and efficiently recognize the water level of the water gauge, and controls the error within a certain range.","['G06V20/52', 'G06F18/23213', 'G06F18/241', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T7/136', 'G06T7/194', 'G06V10/30', 'G06T2207/10004', 'Y02A90/30']"
US10606364B2,"Two-handed gesture sequences in virtual, augmented, and mixed reality (xR) applications","Systems and methods for enabling two-handed gesture sequences in virtual, augmented, and mixed reality (xR) applications are described. In an illustrative, non-limiting embodiment, an Information Handling System (IHS) may include a processor and a memory coupled to the processor, the memory having program instructions stored thereon that, upon execution, cause the IHS to: receive a gesture sequence from a user wearing an HMD coupled to the IHS, where the HMD is configured to display an xR application; identify the gesture sequence as a two-handed sequence; and execute a command in response to the two-handed sequence.","['G06F3/017', 'G06F3/0346', 'G06F3/167', 'G06K9/00355', 'G06K9/00382', 'G06K9/00389', 'G06K9/00744', 'G06V20/46', 'G06V40/11', 'G06V40/113', 'G06V40/28']"
US10356395B2,Tilts as a measure of user engagement for multiview digital media representations,"Various embodiments of the present invention relate generally to systems and methods for analyzing and manipulating images and video. According to particular embodiments, the spatial relationship between multiple images and video is analyzed together with location information data, for purposes of creating a representation referred to herein as a multi-view interactive digital media representation for presentation on a device. Once a multi-view interactive digital media representation is generated, a user can provide navigational inputs, such via tilting of the device, which alter the presentation state of the multi-view interactive digital media representation. The navigational inputs can be analyzed to determine metrics which indicate a user's interest in the multi-view interactive digital media representation.","['H04N13/279', 'G06F1/1694', 'G06F3/017', 'G06F3/04815', 'G06F3/04842', 'G06F3/04845', 'G06F3/0485', 'G06F3/0487', 'G06T19/20', 'H04N13/117', 'H04N13/221', 'G06T2219/2016']"
CN106485215B,Face occlusion detection method based on deep convolutional neural network,"The invention discloses a face shielding detection method based on a deep convolutional neural network, which comprises the following steps: partitioning an input image to obtain a target preselection area; constructing a first deep convolutional neural network, obtaining required parameters by training the first deep convolutional neural network comprising the first deep convolutional network and a first multilayer perceptron connected with the first deep convolutional network, extracting the characteristics of a target preselection area and classifying; predicting the head position by the second multilayer perceptron according to the extracted features; removing overlapped repeated detection frames of the credibility of the head and the predicted head position of the classification type through non-maximum suppression filtering; and obtaining a human head block by combining with original image segmentation, constructing a second deep convolutional neural network based on a multitask learning strategy, and judging whether the left eye, the right eye, the nose and the mouth of the human head block are shielded. The method can accurately detect the shielded face and judge the specific shielding part of the shielded face, and is mainly used for the crime early warning of the video of a camera in front of an automatic teller machine.","['G06V40/172', 'G06F18/214', 'G06F18/24', 'G06V40/165', 'G06V40/171']"
US10846571B2,System and method for recognizing logos,"A system, a method, and a computer program product for recognizing a logo from an image is provided. The method comprises receiving and preprocessing an image that contains the logo to be recognized. The method further comprises detecting text of the logo to recognize the logo. The method furthermore comprises extracting local and global features of a region of interest associated with the logo to recognize the logo if the text is not detected in the logo. The method further comprises detecting one or more signs in proximity of the logo to recognize the logo if the logo is not recognized by extracting the local and global features of the logo. The method finally comprises identifying a region where the logo is present in the pre-processed image to recognize the logo if the logo is not recognized by detecting the one or more signs in proximity of the logo.","['G06K9/6267', 'G06V10/757', 'G06F16/5862', 'G06F18/23213', 'G06F18/24', 'G06K9/3233', 'G06K9/6223', 'G06T7/13', 'G06V30/248', 'G06K2209/25', 'G06V10/462', 'G06V10/56', 'G06V10/768', 'G06V2201/09']"
CN111738169B,Handwriting formula recognition method based on end-to-end network model,"The invention relates to the field of handwriting formula recognition, and discloses a handwriting formula recognition method based on an end-to-end network model, which comprises the following steps: collecting a handwriting formula identification picture data set, and preprocessing the handwriting formula identification picture data set; establishing an encoder-decoder model, wherein the encoder-decoder model comprises a feature extraction layer, an encoder and a decoder; training the encoder-decoder model by utilizing the preprocessed handwriting formula identification picture data set to obtain the trained encoder-decoder model; and identifying the hand-written formula picture to be identified by using the trained encoder-decoder model to obtain an identification result of the hand-written formula picture to be identified. According to the invention, the size characteristics of the handwritten formula pictures are extracted by using different receptive fields, the word vector coding and the butting and decoding operations with the labels are completed for different characteristics, the network super-parameters are optimized, the problem of partial text deletion caused by lower resolution of the characteristic pictures after multiple convolutions is avoided, the handwritten formula can be better identified, and the identification accuracy is high.","['G06V30/36', 'G06F18/214', 'G06F18/2411', 'G06N3/045', 'G06N3/08', 'G06V30/333']"
US10467502B2,Surface defect detection,A method is provided of forming a neural network for detecting surface defects in aircraft engine components. The method includes: providing (i) a pre-trained deep learning network and (ii) a learning machine network; providing a set of pixelated training images of aircraft engine components exhibiting examples of different classes of surface defect; training the trainable weights of the learning machine network on the set of training images.,"['G06K9/6256', 'G06T7/0004', 'G06F18/214', 'G06N3/04', 'G06N3/0464', 'G06N3/084', 'G06N3/0895', 'G06N3/09', 'G06N3/096', 'G06T7/001', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30164']"
CN118613826A,Analyzing liver lesions in medical images,"According to one aspect, a method of determining whether a liver lesion has one or more lesion characteristics in a first contrast image of the liver is provided. The method comprises the following steps: a first attribute of an interior region of the lesion is determined in the first contrast image and a second attribute of a region outside the lesion is determined in the first contrast image. The method further includes obtaining an indication of whether the lesion has the one or more lesion characteristics based on the first attribute and the second attribute using a model trained using a machine learning process.","['G06T7/0016', 'G06T7/0014', 'G06T2207/10076', 'G06T2207/10081', 'G06T2207/10096', 'G06T2207/10132', 'G06T2207/20036', 'G06T2207/20081', 'G06T2207/30056', 'G06T2207/30096']"
US9691155B2,Image processing apparatus and image processing method for finding background regions in an image,"An image processing apparatus comprises a video input unit, a region division unit configured to divide an image acquired by the video input unit into a plurality of regions each including pixels of similar attributes, a feature extraction unit configured to extract a feature from each divided region, a background model storage unit configured to store a background model generated from a feature of a background in advance, and a feature comparison unit configured to compare the extracted feature with a feature in the background model and determine for each of the regions whether the region is the background.","['G06T7/0081', 'G06T7/254', 'G06K9/4609', 'G06K9/4642', 'G06K9/4652', 'G06T7/11', 'G06T7/194', 'G06T7/30', 'G06T2207/10016', 'G06T2207/20021', 'G06T2207/30232']"
US10311574B2,"Object segmentation, including sky segmentation","A digital medium environment includes an image processing application that performs object segmentation on an input image. An improved object segmentation method implemented by the image processing application comprises receiving an input image that includes an object region to be segmented by a segmentation process, processing the input image to provide a first segmentation that defines the object region, and processing the first segmentation to provide a second segmentation that provides pixel-wise label assignments for the object region. In some implementations, the image processing application performs improved sky segmentation on an input image containing a depiction of a sky.","['G06T7/11', 'G06F18/2411', 'G06F18/295', 'G06K9/00697', 'G06K9/34', 'G06K9/6269', 'G06K9/6297', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N7/005', 'G06N7/01', 'G06V10/26', 'G06V10/82', 'G06V10/85', 'G06V20/38', 'G06T2207/20016', 'G06T2207/20076', 'G06T2207/20084', 'G06T2207/20092']"
AU2018101336A4,Building extraction application based on machine learning in Urban-Suburban-Integration Area,"Abstract This is a technology based on computer learning, which realizes the automatic recognition of buildings in semantic segmentation by the means of deep learning.The invention consists of the following steps: Inputing the improved DCNN (with hole convolution and ASPP module) and getting a rough prediction result - expanding to the original size by bilinear interpolation refining the prediction result by fully connected CRF - getting the final output . It does not require manual selection of features and can accurately and quickly identify different shapes and heights of buildings on macroscopic remote sensing images. Conv1 rate=2 rate=4 rate=8 rate=16 Poolt Blockl Block2 Block3 Block4 Block5 Block6 Block Image "",rid, 4 8 16 16 16 16 16 16 figure 3 f~i, j) figure 4 P(C) figure 5","['G06T1/20', 'G06N3/0464', 'G06V20/176', 'G06T2207/10032', 'G06T2207/20084', 'G06T2207/30184']"
US11043011B2,"Image processing method, apparatus, terminal, and storage medium for fusing images of two objects","The present disclosure discloses an image processing method, including: recognizing basic feature points of a source object in a source image and basic feature points of a base object in a base image; determining, according to distribution of the basic feature points in each of the images, auxiliary feature points that meet a filling condition according to the distribution in a corresponding image; determining feature points of a same type in the source object and the base object, the feature points of the same type including basic feature points of a same type and auxiliary feature points of a same type in the source object and the base object; determining average feature points according to positions of the feature points of the same type; deforming the source object and the base object according to the average feature points; and fusing the deformed source object with the deformed base object. The present disclosure further discloses an image processing apparatus, a terminal, and a storage medium.","['G06T11/60', 'G06F18/211', 'G06F18/22', 'G06K9/00281', 'G06K9/6202', 'G06T11/00', 'G06T3/0093', 'G06T3/18', 'G06V10/17', 'G06V10/25', 'G06V10/75', 'G06V10/771', 'G06V40/161', 'G06V40/171']"
US11886892B2,Machine learned retraining for detection of user interface controls via variance parameters,"Computerized detection of one or more user interface objects is performed by processing an image file containing one or more user interface objects of a user interface generated by an application program. Sub-control objects can be detected in the image file, where each sub-control object can form a portion of a user interface object that receives user input. Extraneous sub-control objects can be detected. Sub-control objects that overlap with or that are within a predetermined vicinity of an identified set of sub-control objects can be removed. Sub-control objects in the identified set of sub-control objects can be correlated to combine one or more of the sub-control objects in the identified set of sub-control objects to generate control objects that correspond to certain of the user interface objects of the user interface generated by the application program.","['G06F3/04855', 'G06F9/451', 'G06F3/0481', 'G06N3/045', 'G06N3/0464', 'G06N3/082', 'G06N3/09', 'G06N20/10']"
CN112902953B,An autonomous pose measurement method based on SLAM technology,"The invention relates to the field of remote situation awareness, in particular to an autonomous pose measurement method based on SLAM technology, which is realized based on a vision sensor, an IMU inertial navigation sensor and a GPS sensor, and comprises the following steps: receiving point cloud data, image information, attitude information and target position information of a scene to be built; performing object-level motion scene segmentation on a high dynamic scene based on an improved polar geometric constraint 3D point cloud motion segmentation algorithm; adopting a multi-source data positioning and attitude determining algorithm for point cloud data, image information, attitude information and target position information, namely adopting a 2D-3D matching algorithm for image information of a static part and adopting a dynamic target and point position association estimation algorithm for a dynamic target to realize positioning; and (3) performing three-dimensional point cloud scene construction on a static part in the scene to obtain accurately matched point cloud data, performing dynamic object construction on a dynamic target in the scene, performing global optimization for the positioning and attitude determination result in an auxiliary manner, and further completing attitude measurement.","['G01C21/165', 'G01C21/20']"
CN108345869B,Driver gesture recognition method based on depth image and virtual data,"The invention discloses a driver posture identification method based on a depth image and virtual data, which comprises the following steps of: taking the depth image of the driver and the joint point label as the input of a depth learning frame Caffe, and training a depth convolution neural network model; and detecting the joint point position of the driver in the image by using the deep learning model. Constructing a virtual driver head posture data set: constructing a driver model by utilizing a Modeling function Modeling of three-dimensional Modeling software; setting the head movement of a driver by using an Animation function; rendering the driver images in batch by using Rendering function Rendering; and performing head segmentation processing on the driver image to construct a virtual driver head posture data set. Analyzing the head posture of the driver: training a deep migration learning model by using virtual driver head attitude data acquired in the virtual driver head attitude data set construction process and adopting a migration learning method; and estimating the head posture of the driver in the target domain image according to the depth migration learning model, and detecting the position of the facial feature point of the driver.","['G06V20/597', 'G06F18/2413', 'G06T13/40', 'G06T15/005', 'G06N3/045', 'G06N3/08']"
US5999664A,System for searching a corpus of document images by user specified document layout components,"A document search system provides a user with a programming interface for dynamically specifying features of documents recorded in a corpus of documents. The programming interface operates at a high-level that is suitable for interactive user specification of layout components and structures of documents. In operation, a bitmap image of a document is analyzed by the document search system to identify layout objects such as text blocks or graphics. Subsequently, the document search system computes a set of attributes for each of the identified layout objects. The set of attributes which are identified are used to describe the layout structure of a page image of a document in terms of the spatial relations that layout objects have to frames of reference that are defined by other layout objects. After computing attributes for each layout object, a user can operate the programming interface to define unique document features. Each document feature is a routine defined by a sequence of selections operations which consume a first set of layout objects and produce a second set of layout objects. The second set of layout objects constitutes the feature in a page image of a document. Using the programming interface, a user flexibly defines a genre of document using the user-specified document features.","['G06F16/5838', 'G06F16/58', 'G06F16/5846', 'G06F16/5854', 'G06V30/1444', 'G06V30/414', 'G06V30/10']"
US10805543B2,"Display method, system and computer-readable recording medium thereof","A display method, a system and a computer-readable recording medium thereof are provided, where the method includes the following steps. An image of a user in front of a screen is captured by using an image capturing device to generate a user image. A foreground region and a background region in the user image are distinguished by using a depth sensor. A virtual window is set on a display area of the screen according to its display size. Human face detection is performed on the foreground region to obtain plane information of the human face, and depth information of the human face is obtained by using the depth sensor. A background image is generated based on the background region according to the virtual window, the plane information and the depth information of the human face. The foreground region is combined with the background image to generate an output image.","['H04N5/2226', 'H04N23/63', 'G06F3/011', 'H04N5/23293', 'G06F3/1407', 'G06K9/00268', 'G06T11/60', 'G06T5/006', 'G06T5/80', 'G06T7/11', 'G06T7/194', 'G06T7/50', 'G06T7/60', 'G06T7/70', 'G06V40/168', 'H04N23/611', 'H04N23/698', 'H04N23/81', 'H04N25/61', 'H04N5/217', 'H04N5/23219', 'H04N5/23238', 'H04N5/3572', 'G06T2207/10028', 'G06T2207/20221', 'G06T2207/30201']"
US11762462B2,Eye-tracking using images having different exposure times,"An eye tracking system can include an eye-tracking camera configured to obtain images of the eye at different exposure times or different frame rates. For example, longer exposure images of the eye taken at a longer exposure time can show iris or pupil features, and shorter exposure, glint images can show peaks of glints reflected from the cornea. The shorter exposure glint images may be taken at a higher frame rate (than the longer exposure images) for accurate gaze prediction. The shorter exposure glint images can be analyzed to provide glint locations to subpixel accuracy. The longer exposure images can be analyzed for pupil center or center of rotation. The eye tracking system can predict future gaze direction, which can be used for foveated rendering by a wearable display system.","['G06F3/013', 'G02B27/017', 'G02B27/0093', 'G06F3/011', 'G06F3/0304', 'G06F3/0346', 'G06F3/147', 'G06V40/19', 'G06V40/193', 'G09G5/36', 'G09G2354/00']"
US8913847B2,Replacement of a person or object in an image,"Disclosed herein are a system and a method that use a background model to determine and to segment target content from an image and replace them with different content to provide a composite image. The background model can be generated based on image data representing images of a predetermined area that does not include traversing content. The background model is compared to image data representing a set of captured images of the predetermined area. Based on the comparison, portions of an image that differs from the background model are determined as the traversing content. A target content model is used to determine the target content in the traversing content. The target content determined in the images is replaced with different content to provide a composite image.","['G06T7/97', 'G06T7/0022', 'G06T11/60', 'H04N5/272']"
CA2848233C,Methods of chromogen separation-based image analysis,"Methods for chromogen separation-based image analysis are provided, with such methods being directed to quantitative video-microscopy techniques in cellular biology and pathology applications. Specifically, the invention relates to a method of determining optical density data for at least one dye staining a sample.","['G01N1/30', 'G01N33/48', 'A61B5/00', 'G01N1/31', 'G06T7/0012', 'G06T7/11', 'G06T7/136', 'G06T7/155', 'G06V20/695', 'G06T2207/10016', 'G06T2207/10056', 'G06T2207/20152', 'G06T2207/30024']"
US11488309B2,Robust machine learning for imperfect labeled image segmentation,"To improve the performance and accuracy of an image segmentation neural network, a cascaded robust learning framework for the segmentation of noisy labeled images includes two stages: a sample selection stage, and a joint optimization stage with label correction. In the first stage, the clean annotated samples are selected for network updating, so that the influence of noisy sample can be interactively eliminated. In the second stage, the label correction module works together with the joint optimization scheme to revise the imperfect labels. Thus, the training of the whole network is supervised by the corrected labels and the original ones.","['G06T7/11', 'G06T7/10', 'G06N3/045', 'G06N3/0454', 'G06N3/0455', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06T5/003', 'G06T5/73', 'G06T7/0012', 'G06V20/70', 'G06T2207/10116', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30041', 'G06T2207/30064', 'G06T2207/30088', 'G06T2207/30096']"
US10492723B2,Predicting immunotherapy response in non-small cell lung cancer patients with quantitative vessel tortuosity,"Embodiments classify a region of tissue demonstrating non-small cell lung cancer using quantified vessel tortuosity (QVT). One example apparatus includes annotation circuitry configured to segment a lung region from surrounding anatomy in the region of tissue represented in a radiological image and segment a nodule from the lung region by defining a nodule boundary; vascular segmentation circuitry configured to generate a three dimensional (3D) segmented vasculature by segmenting a vessel associated with the nodule, and to identify a center line of the 3D segmented vasculature; QVT feature extraction circuitry configured to extract a set of QVT features from the radiological image; and classification circuitry configured to compute a probability that the region of tissue will respond to immunotherapy and generate a classification that the region of tissue is a responder or a non-responder based, at least in part, on the probability.","['G16H30/40', 'A61B5/02007', 'A61B5/4839', 'A61B5/4848', 'A61B6/032', 'A61B6/504', 'A61B6/5217', 'G06F18/217', 'G06F18/241', 'G06K9/6262', 'G06K9/6268', 'G06T7/0012', 'G06T7/0016', 'G06T7/11', 'G16H20/40', 'G16H50/20', 'G16H50/30', 'G16H50/50', 'A61B5/055', 'A61B5/4842', 'A61B5/7264', 'G06K2009/00932', 'G06K2009/6213', 'G06K2209/051', 'G06K2209/053', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/30064', 'G06T2207/30101', 'G06T2207/30172', 'G06V2201/031', 'G06V2201/032', 'G06V40/14']"
CN110097109B,A road environment obstacle detection system and method based on deep learning,"The invention provides a road environment obstacle detection system and method based on deep learning. The system of the invention comprises: the intelligent vehicle comprises a vehicle front camera, a vehicle rear camera, a left rearview mirror camera, a right rearview mirror camera, a first intelligent processing unit, a second intelligent processing unit, a third intelligent processing unit, a fourth intelligent processing unit, a switch, a main control single page, a display screen, a buzzer and a power module. The method comprises the steps of collecting video sequences around a vehicle in real time through a camera, and transmitting images in the obtained sequences to corresponding intelligent processing units; the intelligent processing unit acquires a region of interest in a received image; and (3) creating a road obstacle data set offline training YOLO neural network, inputting the pictures classified into the non-road surface clusters into a trained YOLO network model by using an intelligent processing unit as a verification set, and outputting a regression frame and a category of the obstacle. The invention reduces the complex image preprocessing process and can quickly distinguish the detection targets in the background and the foreground of the image.","['G06F18/214', 'G06F18/23', 'G06F18/23213', 'G06V10/25', 'G06V20/39', 'G06V20/58']"
US9940724B2,Method and system for detecting multiple moving objects from real-time aerial images,"In accordance with various embodiments of the disclosed subject matter, a method and a system for detecting multiple objects from real-time images are provided. The method comprises: performing, using a CPU host, an image segmentation process to divide real-time input images into a plurality of image partitions; performing, by multiple GPUs, a fast block-wise registration process, a mark setting process, a background generation process, a foreground generation process based on a Hyper-Q computation infrastructure, and a support vector machine classification process; and generating, by the CPU host, visualization classification images.","['G06T7/11', 'G06T3/4038', 'G06F18/2411', 'G06T7/194', 'G06T7/33', 'G06V10/28', 'G06V10/462', 'G06V10/50', 'G06V10/764', 'G06V10/955', 'G06V20/13', 'G06T1/20', 'G06T2207/10032', 'G06T2207/30184', 'G06T2207/30236', 'G06V10/757']"
US9495752B2,Multi-bone segmentation for 3D computed tomography,"Multiple object segmentation is performed for three-dimensional computed tomography. The adjacent objects are individually segmented. Overlapping regions or locations designated as belonging to both objects may be identified. Confidence maps for the individual segmentations are used to label the locations of the overlap as belonging to one or the other object, not both. This re-segmentation is applied for the overlapping local, and not other locations. Confidence maps in re-segmentation and application just to overlap locations may be used independently of each other or in combination.","['G06T7/11', 'G06T7/0079', 'G06T7/0012', 'G06T7/0081', 'G06T7/0087', 'G06T7/143', 'G06T2207/10072', 'G06T2207/20041', 'G06T2207/20076']"
US11037335B2,Deep learning based virtual hair dyeing method and method for providing virtual hair dyeing service using the same,A deep learning-based virtual hair dyeing method is performed by an image editing server including a deep learning neural network and generative adversarial networks (GANs). The method comprises obtaining an original image and a desired hair color; generating a semantic map by providing the original image to the deep learning neural network; generating a trimap automatically with respect to the original image based on the semantic map; generating an alpha mask by performing alpha matting on the original image based on the trimap; obtaining a transformed image by applying the original image to the GANs; and obtaining a virtual dyeing image by synthesizing the original image and the obtained transformed image through the alpha mask.,"['G06T11/001', 'G06N3/088', 'G06Q50/10', 'G06N3/02', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/094', 'G06Q30/0643', 'G06T11/60', 'G06T5/30', 'G06T7/11', 'G06T7/155', 'G06T7/194', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/30196']"
US20200395117A1,Adaptive image processing method and system in assisted reproductive technologies,"Adaptive image processing, image analysis, pattern recognition, and time-to-event prediction in various imaging modalities associated with assisted reproductive technology. The reference image may be processed according to one or more adaptive processing frameworks for de-speckling or noise processing of ultrasound images. The subject image is processed according to various computer vision techniques for object detection, recognition, annotation, segmentation, and classification of reproductive anatomy, such as follicles, ovaries and the uterus. An image processing framework may also analyze secondary data along with subject image data to analyze time-to-event progression of the subject image.","['G16H30/20', 'G06N20/00', 'G06N3/045', 'G06N3/084', 'G06N3/088', 'G16H10/60', 'G16H20/10', 'G16H30/40', 'G16H50/20', 'G16H50/30', 'A61B10/0012', 'A61B8/5223', 'G06N3/082', 'Y02A90/10']"
CN112967243B,Deep learning chip packaging crack defect detection method based on YOLO,"A deep learning chip packaging crack defect detection method based on YOLO comprises the following steps: firstly, acquiring a chip unit image; secondly, marking the acquired image with defect type information and defect target coordinates; thirdly, enhancing data and making a data set for training; fourthly, constructing a deep learning network model for defect detection based on a YOLOv4 network; fifthly, training a deep learning network by using the pre-trained parameters as initial weights; sixthly, predicting by using the trained network, wherein the picture to be detected is input into the network after being standardized to obtain the output of a network head, the head output is decoded, and the result after decoding is inhibited by adopting an optimized non-maximum value to filter the decoded result by NMS to obtain a network prediction result; and seventhly, further filtering the network prediction result of the sixth step by adopting a confidence coefficient threshold value and a crack boundary threshold value to obtain a final result. The invention has good detection effect on the crack defects of the chip.","['G06T7/0004', 'G06F18/24', 'G06N3/045', 'G06N3/08', 'G06T7/13', 'G06T7/136', 'G06T7/62', 'G06T7/73', 'G06T2207/10004', 'G06T2207/30148']"
CN109299716B,"Neural network training method, image segmentation method, device, equipment and medium","The embodiment of the application discloses a training method of a neural network, an image segmentation method, an image segmentation device, electronic equipment, a computer readable storage medium and a computer program, wherein the training method of the neural network mainly comprises the following steps: extracting characteristic information of an input source domain image with labeling information through an encoder of a neural network to be trained; performing image segmentation processing on the source domain image according to the characteristic information through an image splitter of the neural network, and outputting an image segmentation prediction result; determining the loss of the image segmentation prediction result according to the probability value corresponding to the pixel type in the labeling information in the image segmentation prediction result, and adjusting the network parameters of the neural network according to the loss of the image segmentation prediction result.","['G06V10/267', 'G06F18/2413']"
US11222224B2,Automatic image synthesizing apparatus and method,An automatic image synthesizing apparatus including an image search unit configured to search for and extract a frame image having an object to be learned or replaced from a plurality of frame images inputted through a vehicle camera; an instance object detection unit configured to detect an instance object to be learned or replaced from the extracted frame image; an image extraction and object class information detection unit configured to extract mask-segmented objects from the extracted frame image and form a data-structure; a standard image mapping unit configured to map a standard image matching a class value of the mask-segmented object; and a generative adversarial network (GAN) network unit configured to receive the selected standard image and the mask segmentation image and learn a replacement image or produce a replacement image based on a learning result.,"['G06T11/001', 'G06K9/2054', 'G06T5/77', 'G06F16/51', 'G06F16/532', 'G06F18/214', 'G06F18/22', 'G06F18/24', 'G06K9/00791', 'G06K9/6201', 'G06K9/6256', 'G06K9/6267', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/08', 'G06N3/088', 'G06N3/094', 'G06T11/60', 'G06T5/50', 'G06T5/60', 'G06T7/10', 'G06T7/11', 'G06V20/56', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30252']"
US10692221B2,Automatic trimap generation and image segmentation,"A digital medium environment is described to automatically generate a trimap and segment a digital image, independent of any user intervention. An image processing system receives an image and a low-resolution mask for the image, which provides a probability map indicating a likelihood that a pixel in the image mask corresponds to a foreground object in the image. The image processing system analyzes the image to identify content in the image's foreground and background portions, and adaptively generates a trimap for the image based on differences between the identified foreground and background content. By identifying content of the image prior to generating the trimap, the techniques described herein can be applied to a wide range of images, such as images where foreground content is visually similar to background content, and vice versa. Thus, the image processing system can automatically generate trimaps for images having diverse visual characteristics.","['G06V10/50', 'G06K9/38', 'G06K9/6212', 'G06T7/194', 'G06T7/90', 'G06V10/758', 'G06T2207/20084']"
US11854202B2,Image registration using a fully convolutional network,"Methods and systems for analyzing images are disclosed. An example method may comprise inputting one or more of a first image or a second image into a fully convolutional network, and determining an updated fully convolutional network by optimizing a similarity metric associated with spatially transforming the first image to match the second image. The one or more values of the fully convolutional network may be adjusted to optimize the similarity metric. The method may comprise registering one or more of the first image or the second image based on the updated fully convolutional network.","['G06T7/0014', 'G06F18/22', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/088', 'G06N3/0895', 'G06T7/0012', 'G06T7/33', 'G06V10/25', 'G06V10/7515', 'G06V10/82', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30016']"
CN106663309B,Method and storage medium for user-guided bone segmentation in medical imaging,"A semi-automatic approach is used for user-guided bone segmentation (132) in medical imaging. A user indicates (126) a plurality of landmarks on the bone image. The processor morphs (128) the same landmarks on the bone model to user input landmarks on the image, resulting in a transformation. The transformation is then applied (130) to the model to roughly segment the bone. The user may edit (134) the resulting fit, and the processor then refines (136) the edited fit, such as fitting a model of the bone to the scan data and avoiding any overlap with other bones. This user-guided segmentation may avoid the need for many samples to train the classifier for a fully automated approach, while also avoiding the tedious contouring in a manual approach.","['G06T3/18', 'A61B5/7475', 'G06T11/003', 'G06T19/20', 'G06T7/12', 'G06T7/149', 'G06T7/162', 'G06T2200/04', 'G06T2207/10081', 'G06T2207/20101', 'G06T2207/30008', 'G06T2219/2021']"
CN117115147B,Textile detection method and system based on machine vision,"The invention relates to the technical field of image data processing, in particular to a textile detection method and system based on machine vision, comprising the following steps: based on the textile image, the characteristic extraction is carried out on the image by utilizing a convolutional neural network, and a primary recognition image characteristic library is generated. In the invention, a textile detection system is formed by a convolutional neural network, a cyclic neural network, a multispectral image acquisition technology, a computer vision technology, a blockchain technology and the like, a high-accuracy basis is provided for textile flaw detection by utilizing the combination of the convolutional neural network and the cyclic neural network, the multispectral image acquisition technology enables hidden defects to be detected more accurately, and the generation and the application of a real-time quality control instruction and intelligent defect analysis ensure the real-time monitoring in the production process and the timely adjustment of the product quality, and a reliable and untampered traceability system is provided for the quality information of textiles by combining the blockchain technology and the Internet of things equipment.","['G06T7/001', 'G06N3/0442', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/048', 'G06N3/08', 'G06Q30/0185', 'G06Q50/04', 'G06T5/40', 'G06V10/30', 'G06V10/36', 'G06V10/42', 'G06V10/44', 'G06V10/52', 'G06V10/54', 'G06V10/58', 'G06V10/62', 'G06V10/751', 'G06V10/761', 'G06V10/764', 'G06V10/82', 'G06T2207/10036', 'G06T2207/30124', 'G06V2201/06', 'Y02P90/30']"
US8204308B2,"Image processing apparatus, method, and program","An image processing apparatus generates a 3-value image from a 2-value image, wherein the first and second values from the 2-value image indicate foreground and background regions of an input image, and the third value indicates an unknown region of predetermined width at the boundary therebetween. A ratio image is generated from the input image and the 3-value image, having a fourth value indicating the ratio of the first value. The 3-value image is updated by defining a predetermined range near a pixel in the 3-value image corresponding to a pixel in the ratio image whose fourth value is greater than a minimum value and less than a maximum value, and then sets all pixels within the defined range to the third value. If the updated 3-value image is determined to be identical or nearly identical to the pre-update 3-value image, then the updated 3-value image is output.","['G06T7/11', 'G06T7/194']"
EP3518145A1,Cognitive indexing of images in digital video content,A method for indexing and cognitively labeling image elements from a plurality of video frames is implemented in a special purpose computer system. Video files are ingested and stored on a storage device within the computer system. The video files comprise the plurality of video frames. A subset of the video frames indicative of substantial motion in context with adjacent video frames is identified. The subset of the video frames is separated as a sample data set. The subset of video frames is filtered to identify discrete image elements within the video frames. The image elements are segmented from the video frames in the subset. A multi-dimensional feature vector is calculated for each image element. The feature vectors are clustered based upon similarities in feature vector values. The clusters of feature vectors are processed with a support vector classifier machine to refine clusters into predicted classes.,"['G06V10/764', 'G06F16/71', 'G06F18/214', 'G06F18/23', 'G06F18/2411', 'G06N5/02', 'G06V20/41', 'G06V20/46', 'G06V20/49']"
CN112949458A,Training method of target tracking segmentation model and target tracking segmentation method and device,"The disclosure relates to a training method of a target tracking segmentation model, and a target tracking segmentation method and device. The training method comprises the following steps: acquiring image sample data, wherein each image sample data comprises a target image, a tracking image and a target mask image, the target image is an image comprising a target to be tracked, the tracking image is an image of a target to be tracked, and the target mask image is a mask image of a pre-marked target true value; inputting the target image and the tracking image into a target tracking segmentation model to obtain a plurality of characteristic graphs with different resolutions; processing each feature map in the plurality of feature maps respectively to generate a plurality of target foreground probability maps with the same resolution; performing fusion on the plurality of target foreground probability maps to generate a final target foreground probability map; determining a loss function based on each target foreground probability map and the target mask image to obtain a plurality of loss functions; parameters of the target tracking segmentation model are adjusted according to a plurality of loss functions.","['G06V20/41', 'G06F18/214', 'G06F18/253', 'G06N3/045', 'G06N3/048', 'G06N3/084', 'G06V20/46', 'G06V20/49']"
CN105144239B,"Image processing apparatus, image processing method","Image processing apparatus includes：Input receiving unit (110) accepts the input of study image and normal solution label；Processing unit (120) carries out classifier data and deals with objects the generation processing of image；And storage unit (130), store classifier data.Processing unit (120) generates process object image, the process object image is the general image or parts of images for learning image, and the characteristic quantity of calculation processing object images, classifier data is generated according to characteristic quantity and to the group, that is, training data for the normal solution label that study image corresponding with characteristic quantity assigns, image group is generated according to study image or process object image, using classifier data by each image classification of image group, the classification score for calculating each image generates process object image according to classification score and image group again.","['G06V10/50', 'G06F18/214', 'G06F18/217', 'G06V10/774', 'G06V10/776', 'G06V10/464', 'G06V10/768']"
CN102682287A,Pedestrian detection method based on saliency information,"The invention provides a pedestrian detection method based on saliency information. The method comprises an offline training step and an online detection step; the online detection step comprises calculating a salient map of an image to be detected, extracting a detection child window from the image and calculating the corresponding saliency of the detection child window according to the salient map, calculating corresponding features in the detection child window, detecting the corresponding features in the detection child window through a cascade classifier, and simultaneously distributing adjustment coefficients for the cascade classifier according to the corresponding saliency of the detection child window. According to the method, the saliency information is introduced to be served as auxiliary information for pedestrian detection to participate in the process of image identification on the basis of the existing AdaBoost classifier. In most cases, pedestrians are different from the surrounding environment in terms of color, shape and profile, the saliency information of the child window is adopted for correcting detection results of the classifier, the detection rate can be effectively improved, and the false detecting rate can be reduced.",[]
US8538081B2,Contextual boost for object detection,"Aspects of the present invention includes systems and methods for generating detection models that consider contextual information of an image patch and for using detection models that consider contextual information. In embodiments, a multi-scale image context descriptor is generated to represent the contextual cues in multiple parameters, such as spatial, scaling, and color spaces. In embodiments, a classification context is defined using the contextual features and is used in a contextual boost classification scheme. In embodiments, the contextual boost propagates contextual cues to larger coverage through iterations to improve the detection accuracy.","['G06V10/50', 'G06V10/462', 'G06V10/52']"
US8045770B2,System and method for three-dimensional image rendering and analysis,The present invention relates to methods and systems for conducting three-dimensional image analysis and diagnosis and possible treatment relating thereto. The invention includes methods of handling signals containing information (data) relating to three-dimensional representation of objects scanned by a scanning medium. The invention also includes methods of making and analyzing volumetric measurements and changes in volumetric measurements which can be used for the purpose of diagnosis and treatment.,"['G06T5/77', 'G06T11/008', 'G06T15/08', 'G06T17/10', 'G06T5/30', 'G06T5/70', 'G06T7/0012', 'G06T7/11', 'G06T7/155', 'G06T7/62', 'G06T2200/04', 'G06T2207/10081', 'G06T2207/30061', 'G06T2207/30096']"
CN112836713B,Mesoscale convective system recognition and tracking method based on image anchor-free frame detection,"The invention discloses a Mesoscale convection system (Mesoscale ConvectiveSystem, MCS) identification and tracking method based on image anchor-free frame detection, which comprises the following steps: step 1, preprocessing original static satellite infrared bright temperature data, marking a mesoscale convection system on an infrared cloud picture obtained after processing, and then randomly dividing a training set, a verification set and a test set; step 2, an example segmentation network based on an anchor-free frame is constructed, and the network is used for extracting image characteristics, detecting a mesoscale convection system and segmenting specific examples; step 3, training set image enhancement, and using a transfer learning supervision training example to segment a convolutional neural network, and automatically learning network parameters; step 4, detecting and dividing a mesoscale convection system of the static satellite infrared cloud picture at the adjacent moment by using the trained model; and step 5, tracking the mesoscale convection system according to a related target matching principle.","['G06V10/44', 'G06F18/241', 'G06N3/045', 'G06N3/048', 'G06N3/08', 'G06T7/11', 'G06T7/136', 'G06T7/62', 'G06V10/25', 'G06T2207/10048', 'G06T2207/20081', 'G06T2207/20104', 'G06T2207/30192', 'Y02A90/10']"
US8873812B2,Image segmentation using hierarchical unsupervised segmentation and hierarchical classifiers,"An image segmentation method includes generating a hierarchy of regions by unsupervised segmentation of an input image. Each region is described with a respective region feature vector representative of the region. Hierarchical structures are identified, each including a parent region and its respective child regions in the hierarchy. Each hierarchical structure is described with a respective hierarchical feature vector that is based on the region feature vectors of the respective parent and child regions. The hierarchical structures are classified according to a set of predefined classes with a hierarchical classifier component that is trained with hierarchical feature vectors of hierarchical structures of training images. The training images have semantic regions labeled according to the set of predefined classes. The input image is segmented into a plurality of semantic regions based on the classification of the hierarchical structures and optionally also on classification of the individual regions.","['G06V10/7715', 'G06K9/00', 'G06T7/11', 'G06T7/162', 'G06T7/194', 'G06T2207/20076']"
US9031317B2,Method and apparatus for improved training of object detecting system,"An adequate solution for computer vision applications is arrived at more efficiently and, with more automation, enables users with limited or no special image processing and pattern recognition knowledge to create reliable vision systems for their applications. Computer rendering of CAD models is used to automate the dataset acquisition process and labeling process. In order to speed up the training data preparation while maintaining the data quality, a number of processed samples are generated from one or a few seed images.","['G06T15/20', 'G06K9/6219', 'G06F18/214', 'G06F18/231', 'G06F18/285', 'G06F18/41', 'G06K9/6227', 'G06K9/6254', 'G06K9/6256', 'G06T15/50', 'G06V10/7625', 'G06V10/774', 'G06V10/7788']"
US10366492B2,Segmentation and identification of layered structures in images,"Disclosed herein are systems and method for segmentation and identification of structured features in images. According to an aspect, a method may include representing an image as a graph of nodes connected together by edges. For example, the image may be an ocular image showing layered structures or other features of the retina. The method may also include adding, to the graph, nodes adjacent to nodes along first and second sides of the graph. The added nodes may have edge weights less than the nodes along the first and second sides of the graph. Further, the method may include assigning start and end points to any of the added nodes along the first and second sides, respectively. The method may also include graph cutting between the start and end points for identifying a feature in the image.","['G06T7/11', 'A61B3/0025', 'A61B3/102', 'A61B5/0066', 'A61B5/7203', 'G06K9/34', 'G06T7/0012', 'G06T7/162', 'G06V10/26', 'A61B2576/02', 'G06T2207/10004', 'G06T2207/10101', 'G06T2207/30041']"
US7835578B2,Automated video-to-text system,"A method for transforming Video-To-Text is disclosed that automatically generates text descriptions of the content of a video. The present invention first segments an input video sequence according to predefined semantic classes using a Mixture-of-Experts blob segmentation algorithm. The resulting segmentation is coerced into a semantic concept graph and based on domain knowledge and a semantic concept hierarchy. Then, the initial semantic concept graph is summarized and pruned. Finally, according to the summarized semantic concept graph and its changes over time, text and/or speech descriptions are automatically generated using one of the three description schemes: key-frame, key-object and key-change descriptions.","['G06T7/00', 'G06V10/426', 'G06V20/52', 'G06T2207/10016', 'G06T2207/20072']"
US11386897B2,Method and system for extraction of key-terms and synonyms for the key-terms,"This disclosure relates generally to a method and a system for extraction of key-terms and synonyms for the key-terms. The method discloses to extract the key-terms and further determine a set of relevant synonyms for the extracted key-terms, wherein the key-terms is a collection of small sub-set of words and phrases which are individually meaningful and collectively provide a generic context of a given input text. The method discloses techniques for extraction of key-terms that is independent of any specific supervised training, does not require a huge amount of training data and computationally less intensive. The key-terms are determined in several stages using frequency based techniques based on a relevancy scoring of the key-terms. Further a set of synonyms are determined for the identified key-terms based on a language-based approach or domain specific approach.","['G10L15/22', 'G06F3/167', 'G06F40/247', 'G06F40/30', 'G06F16/93', 'G06F40/284']"
US8582037B2,System and method for hand gesture recognition for remote control of an internet protocol TV,"Gesture recognition system for remote controlling a TV is disclosed. The system comprises: (a) a webcam for capturing a video sequence of a user which is composed of video frames (b) a hand tracker for receiving the video sequence, and when a hand is determined within a frame of the video sequence, the hand tracker calculates the hand centroid; (c) a trajectory buffer for receiving plurality of the hand centroids as determined from plurality of frames respectively; (d) trajectory segmentation module for continuously inspecting the buffer for possible inclusion of relevant trajectories that relate to a hand gesture pattern, and whenever a relevant trajectory is detected, it extracts a respective trajectory segment, and conveys the segment into a gesture classifier; and (e) the gesture classifier for verifying whether the segment relates to a specific gesture command, and when affirmative, the gesture classifier transfers the respective command to the TV.","['G06F3/017', 'G06T7/254', 'G06T7/66', 'G06V40/28', 'G06T2207/10024', 'G06T2207/20076', 'G06T2207/30196', 'G06T2207/30241']"
US11195329B2,Terrain modeling method that fuses geometric characteristics and mechanical characteristics and computer readable storage medium thereof,"A terrain modeling method that fuses geometric characteristics and mechanical characteristics, and a terrain modeling system are provided in the present invention. The method includes steps of: obtaining color images and depth images of the detected region, performing a terrain semantic segmentation on the color images, and fusing ground semantic information obtained by the semantic segmentation and depth information contained in the depth images at the same time to generate the point clouds; mapping the point clouds to a raster map in a map coordinate system to generate corresponding rasters, and updating the elevation values to the corresponding rasters; and calculating the input images in terms of terra-mechanical characteristics, and updating calculation results to the corresponding rasters to generate a terrain model. A mechanical characteristic parameter is added to a terrain factor, and terrain characteristics are innovatively performed from two dimensions of geometric characteristic and mechanical characteristics.","['G06N3/08', 'G06F18/251', 'G06K9/6289', 'G06N3/0464', 'G06N3/09', 'G06N7/005', 'G06N7/01', 'G06T17/05', 'G06T7/10', 'G06T7/11', 'G06V10/764', 'G06V10/82', 'G06V20/38', 'G06N3/045', 'G06N3/047', 'G06T2200/08', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30244', 'G06V2201/12']"
CA2820950C,Optimized focal area for augmented reality displays,"A method and system that enhances a user's experience when using a near eye display device, such as a see-through display device or a head mounted display device is provided. An optimized image for display relative to the a field of view of a user in a scene is created. The user's head and eye position and movement are tracked to determine a focal region for the user. A portion of the optimized image is coupled to the user's focal region in the current position of the eyes, a next position of the head and eyes predicted, and a portion of the optimized image coupled to the user's focal region in the next position.","['G02B27/017', 'G02B27/0093', 'G06F3/013', 'G06F3/0304', 'G02B2027/014', 'G02B2027/0178', 'G02B2027/0187']"
US10413180B1,System and methods for automatic processing of digital retinal images in conjunction with an imaging device,"Systems and methods of obtaining and recording fundus images by minimally trained persons, which includes a camera for obtaining images of a fundus of a subject's eye, in combination with mathematical methods to assign real time image quality classification to the images obtained based upon a set of criteria. The classified images will be further processed if the classified images are of sufficient image quality for clinical interpretation by machine-coded and/or human-based methods. Such systems and methods can thus automatically determine whether the quality of a retinal image is sufficient for computer-based eye disease screening. The system integrates global histogram features, textural features, and vessel density, as well as a local non-reference perceptual sharpness metric. A partial least square (PLS) classifier is trained to distinguish low quality images from normal quality images.","['A61B3/152', 'A61B3/0025', 'A61B3/0033', 'A61B3/12', 'A61B3/14']"
WO2020048308A1,"Multimedia resource classification method and apparatus, computer device, and storage medium","Disclosed in the present application are a multimedia resource classification method and apparatus, a computer device, and a storage medium. Said method comprises: acquiring multimedia resources, and extracting a plurality of pieces of feature information of the multimedia resources; clustering the plurality of pieces of feature information, to obtain at least one cluster set, and determining cluster description information of each cluster set, said each cluster set comprising at least one piece of feature information, each piece of cluster description information being used to indicate features of one cluster set; determining, on the basis of the cluster description information of each cluster set, at least one piece of target feature description information of the multimedia resources, each piece of target feature description information being used to represent an association between one piece of cluster description information and the remaining pieces of cluster description information; and classifying the multimedia resources on the basis of the at least one piece of target feature description information of the multimedia resources, to obtain a classification result of the multimedia resources.","['G06V20/41', 'G06F16/435', 'G06F16/45', 'G06F16/48', 'G06F18/23', 'G06F18/2321', 'G06V10/763', 'G06V10/764', 'G06V10/7715', 'G06V10/82']"
CN110712202B,"Special-shaped component grabbing method, device and system, control device and storage medium","The embodiment of the application is suitable for the technical field of automation, and discloses a method, a device, a system, a control device and a computer readable storage medium for grabbing a special-shaped component, wherein the method comprises the following steps: acquiring a first image, wherein the first image comprises a special-shaped component and/or a grabbing device; and generating a control instruction according to the first image, and sending the control instruction to a grabbing device, wherein the control instruction is used for indicating the grabbing device to grab the target special-shaped component. According to the embodiment of the application, the first image comprising the special-shaped component and/or the grabbing device is obtained, the mechanical arm is controlled to grab the target special-shaped component according to the first image, the automatic grabbing of the special-shaped component is achieved, the traditional manual grabbing is replaced, a large amount of labor cost is saved, the production cost is reduced, and the production efficiency is improved.","['B25J9/1697', 'B65G47/90']"
US10867117B2,Optimized document views for mobile device interfaces,"Portions of document contents are separated into individually controlled sections on a user interface of a smaller size client device display. A document viewed on a mobile device may include different content portions such as textual content, tables, slides and graphics. Due to a smaller user interface of the mobile device, some portions of the content may extend outside of the user interface and may not all be visible at the same time. The user may use gestures to scroll through and resize the document to view all of the contents. The system may separate each of the different content portions into individual sections and enable the user to control each section separately, such that the user may navigate, resize, and reposition each individual section without affecting the size and position of the remaining sections of the document for optimally viewing the document on the user interface.","['G06F40/106', 'G06F3/0485', 'G06F16/9577', 'G06F3/0483', 'G06F3/04842', 'G06F3/04845', 'G06F3/04847', 'G06F3/04883', 'G06F3/04886']"
CA3029411C,Video to data,"A method and system can generate video content from a video, The method and system can include a coordinator, an image detector, and an object recognizer. The coordinator can be communicatively coupled to a splitter and/or to a plurality of demultiplexer nodes. The splitter can be configured to segment the video. The demultiplexer nodes can be configured to extract audio files from the video and/or to extract still frame images from the video. The image detector can be configured to detect images of objects in the still frame images. The object recognizer can be configured to compare an image of an object to a fractal. The recognizer can be further configured to update the fractal with the image. The coordinator can be configured to embed metadata about the object into the video.","['G06F40/40', 'G06F18/214', 'G06V10/774', 'G06V20/41', 'G06V20/64', 'G06V40/167', 'G10L15/26', 'H04N21/23439', 'H04N21/23608', 'H04N21/8456', 'G06V2201/09', 'G06V2201/10']"
US12020398B2,3D reconstruction method and apparatus,"Provided are a 3D reconstruction method and apparatus, an electronic device and a storage medium. The 3D reconstruction method comprises: using a plurality of cameras with different viewing angles to image a symbol to obtain a symbol image, a reference object for camera calibration being called the symbol, the symbol including a plurality of markers, and each of the markers having a corresponding ID number; identifying the ID number of the marker in the symbol image and searching for world coordinates corresponding to the marker according to the ID number; computing an external parameter matrix of the camera according to marker coordinates of a camera coordinate system and marker coordinates of a world coordinate system, and unifying point clouds under the world coordinate system to obtain a plurality of point clouds under different viewing angles; and stitching the plurality of point clouds together to obtain a 3D reconstructed image.","['G06T3/4038', 'G06T7/80', 'G06T7/97', 'G06V10/245', 'G06V10/44', 'G06V10/462', 'G06V20/64', 'G06T2207/10028', 'G06T2207/30204']"
US9158744B2,System and method for automatically extracting multi-format data from documents and converting into XML,"A system, a computer-implemented method and a computer program product for extracting insurance data from one or more documents having one or more file formats and converting into Extensible Markup Language (XML) format is provided. The system comprises a user interface configured to facilitate one or more users to submit one or more documents related to insurance. The system further comprises a business type classification module configured to identify the one or more submitted documents based on a business type. Further, the system comprises a format classification module configured to identify file format of the one or more submitted documents. Furthermore, the system comprises an extraction and conversion module configured to match one or more headers in the one or more submitted documents with one or more pre-stored headers, extract insurance data corresponding to the one or more matched headers and convert the extracted insurance data into XML format.","['G06F17/218', 'G06F40/117']"
CN104657740B,Method and device for segmenting an occupancy grid of an environmental model,"The invention relates to a method for segmenting an occupancy grid (320) of an environment model for a driver assistance system of a vehicle. The method (1500) comprises a step of reading an occupancy grid (320) having a plurality of grid cells (322), wherein each occupancy grid (322) of the plurality of occupancy grids (322) is assigned grid cell information comprising one information on occupancy degree (324) and at least one additional information on the grid cells (322), and a step of assigning at least one object (326) and/or a plurality of objects (326) to the plurality of grid cells (322) for segmenting the occupancy grid (320) for the environment model using the grid cell information.","['G06F30/20', 'B60W40/04', 'G06F30/15']"
US7224830B2,Gesture detection from digital video images,Human gestures are detected and/or tracked from a pair of digital video images. The pair of images may be used to provide a set of observation vectors that provide a three dimensional position of a subject's upper body. The likelihood of each observation vector representing an upper body component may be determined. Initialization of the model for detecting and tracking gestures may include a set of assumptions regarding the initial position of the subject in a set of foreground observation vectors.,['G06V40/10']
CN109490316B,Surface defect detection algorithm based on machine vision,"The invention discloses a surface defect detection method based on machine vision, which is characterized in that after surface images of the surface of an object are collected, a 'maximum filtering difference' algorithm is adopted for block defects, and a 'linear enhancement detector' algorithm is adopted for scratch defects. The on-line detection requirement can be met by adopting conventional hardware configuration, and the method has good real-time performance and quantized production line operation characteristics; meanwhile, the characteristics of the shape and the like of the defect are considered, so that the method has a good detection effect and can be applied to the surfaces of different detected objects. The method can overcome the unstable, unquantized production and low efficiency state of the existing surface defect visual detection technology.","['G01N21/8851', 'G01N2021/8854']"
US10074028B2,Real-time diet assessment and food identification using thermal imaging,Systems and methods are described for automatically identifying a food item. A color image and a thermal image are received by an electronic processor with a first food item in the field of view of both the color image and the thermal image. The electronic processor identifies a region of pixels in the color image that corresponds to the first food item based at least in part on a temperature intensity of the pixels in the identified region of pixels relative to other pixels in the thermal image. At least one feature is extracted from the identified region of pixels in the color image corresponding to the first food item and the electronic processor automatically identifies a type of food corresponding to the first food item based at least in part on the at least extracted feature.,"['G06T7/11', 'G06K9/4652', 'G06F18/251', 'G06F18/253', 'G06T7/12', 'G06T7/136', 'G06T7/162', 'G06T7/174', 'G06T7/90', 'G06V10/56', 'G06V10/803', 'G06V10/806', 'G06V20/68', 'G01N25/72', 'G01N33/02', 'G06K2209/17', 'G06T2207/10024', 'G06T2207/10048', 'G06T2207/20061', 'G06T2207/20152']"
CN110599476B,"Disease grading method, device, equipment and medium based on machine learning","The application discloses disease grading method, device, equipment and medium based on machine learning, the method belongs to the field of medical artificial intelligence, and the system comprises: acquiring a mammary gland ultrasonic image; invoking a multi-task network to process the breast ultrasound image to obtain a first feature map and a semantic segmentation map of the breast ultrasound image, wherein the first feature map comprises high-level semantic features of the breast ultrasound image, and the semantic segmentation map is a map for carrying out semantic segmentation on focus areas in the breast ultrasound image; extracting features with different weights from focus areas and non-focus areas in the first feature map according to the semantic segmentation map to obtain a second feature map; and predicting and obtaining the BI-RADS grade of the breast ultrasonic image according to the second characteristic diagram. The method adopts high-level semantic features with classification guidance as main classification features, and introduces focus contours to extract different features of focus areas and non-focus areas, and regression is performed to obtain accurate BI-RADS classification results.","['G06T7/0012', 'G06T7/10', 'G06T7/13', 'G16H30/00', 'G16H50/20', 'G06T2207/10132', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30068', 'G06T2207/30096']"
US10353948B2,Content based image retrieval,"A method and non-transitory computer readable medium for content based image retrieval. The method includes selecting a query image, segmenting the selected query image by applying a segmentation technique, extracting features from the segmented query image by determining at least two feature descriptors, including color feature descriptors and texture feature descriptors, and determining a similarity of the query image to a plurality of images included in a database using the determined at least two feature descriptors of the segmented query image, features being extracted from each of the plurality of images included in the database by determining the at least two feature descriptors, the color feature descriptors and the texture feature descriptors including a simultaneous combination of different color spaces, and global and local statistical measurements being carried out on the simultaneous combination of the different color spaces.","['G06F16/5838', 'G06F16/00', 'G06F16/5854', 'G06F16/5862', 'G06K9/6212', 'G06T7/10', 'G06T2207/10004', 'G06T2207/20112']"
CN112070044B,Video object classification method and device,"The application discloses a video object classification method and a video object classification device, wherein a key frame in a target video is extracted, a preset convolutional neural network is utilized to extract image features of the key frame to obtain a multilayer feature map, local features contained in each layer of feature map are weighted and fused to obtain fusion features, dimension reduction processing is carried out on the fusion features to obtain dimension reduction results, and the dimension reduction results are clustered to obtain classification results of all pixels contained in the key frame. Because the dimension reduction processing is carried out on the fusion features after the fusion features are obtained to obtain the dimension reduction result, the fc-lstm structure is avoided from being used for classification, the time and space dependency among video data can be effectively learned, the dimension reduction result carries the space time sequence characteristic, the complex scene with multiple objects moving in a complex mode can not be effectively classified, and meanwhile, the pixel-level segmentation of a single-frame image is realized based on the algorithm characteristic of a clustering method to obtain the classification result.","['G06V20/46', 'G06F18/213', 'G06F18/22', 'G06F18/23213', 'G06F18/24', 'G06F18/253', 'G06N3/044', 'G06N3/045', 'G06N3/084', 'G06V10/267', 'G06V10/56']"
CN106548463B,Automatic dehazing method and system for sea fog images based on dark channel and Retinex,"The sea fog image automatic defogging method and system based on dark and Retinex that the invention discloses a kind of, belong to technical field of image information processing.The method of the present invention includes the following steps: that (1) seeks the dark channel image of input picture；(2) ratio shared by the lower pixel of dark channel image pixel value is sought, brightness and the contrast metric of input picture are sought；(3) image is classified automatically according to required ratio and feature；(4) image is handled according to the classification of image to be processed.The sea fog image automatic defogging system based on dark and Retinex that the invention also discloses a kind of.The present invention can classify to image according to the attribute of image, corresponding processing method is adaptive selected, the contrast and clarity of marine foggy image are greatly improved, and the complexity of algorithm is low, the speed of service is fast, it is thus possible to be applied to Maritime Intelligent Traffic System.","['G06T5/73', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20024', 'G06T2207/20036']"
US11593943B2,RECIST assessment of tumour progression,"The present invention relates to a method and system that automatically finds, segments and measures lesions in medical images following the Response Evaluation Criteria In Solid Tumours (RECIST) protocol. More particularly, the present invention produces an augmented version of an input computed tomography (CT) scan with an added image mask for the segmentations, 3D volumetric masks and models, measurements in 2D and 3D and statistical change analyses across scans taken at different time points.","['G06T7/0016', 'A61B5/0035', 'A61B5/015', 'A61B5/1073', 'A61B5/4842', 'A61B5/4887', 'A61B5/7267', 'A61B6/032', 'G06F18/211', 'G06F18/2431', 'G06K9/6228', 'G06K9/628', 'G06N3/04', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06T7/11', 'G06T7/62', 'G06T7/73', 'G16H30/20', 'G16H50/20', 'G06T2207/10081', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30096', 'G06V2201/03']"
CN112669197B,"Image processing method, device, mobile terminal and storage medium","The embodiment of the application discloses an image processing method, an image processing device, a mobile terminal and a storage medium, wherein the embodiment of the application can acquire an image to be processed, and perform face detection on the image to obtain a face area; when the face area meets the preset compliance condition, extracting a portrait area mask and a skin area mask from the image, extracting a target area which accords with a preset proportion from the image, preprocessing the skin area in the target area according to the skin area mask, and carrying out color adjustment on a non-portrait area in the target area according to the portrait area mask to obtain a target image. The method not only can carry out compliance detection on the face area in the image, but also can carry out pretreatment on the skin area in the target area and carry out color adjustment on the non-face area based on the face area mask and the skin area mask so as to obtain the target image meeting the requirements, thereby improving the efficiency and the reliability of image processing.",[]
CN115351598B,Method for detecting bearing of numerical control machine tool,"The invention relates to the field of data processing, in particular to a method for detecting a bearing of a numerical control machine tool, which comprises the steps of obtaining a panoramic image of the surface of the bearing, carrying out data processing analysis on obtained image data, namely counting the neighborhood pixel gray scale characteristics of each pixel point in a defect area, calculating the probability that the pixel point is a defect edge point, then giving weight to each gray level according to the number of the pixel points in each gray level in the defect area which is primarily segmented and the probability of the defect edge point, and further carrying out re-segmentation on the defect area which is primarily segmented by utilizing a weighted Otsu algorithm to obtain a target area; comparing the difference value between the target pixel point of the target area and the background pixel point of the current bearing surface sub-gray level image with the set value, and determining whether the target area is a defect area. Namely, the method of the invention can accurately detect the defects of the bearing.","['B23Q17/00', 'G06T7/0004', 'G06T7/11', 'G06T7/136', 'G06T7/187', 'Y02P90/30']"
CN109558810B,Target person identification method based on part segmentation and fusion,"The invention discloses a target person identification method based on part segmentation and fusion, which comprises the steps of carrying out part segmentation on a human body, segmenting the human body into four parts, namely a head part, a shoulder part, a trunk part, a leg part and a whole body, respectively training each part through a convolutional neural network to obtain four part classifiers, outputting confidence scores corresponding to identification of the target person by each part classifier, and then carrying out threshold judgment and fusion decision on all part classifiers to obtain a final identification result. The identification method integrates decision information of a plurality of body parts, can fully mine the internal association among the body parts, and has multi-scene universality. When parts with larger feature discrimination, such as the faces of people and the like, are partially shielded, effective identification can still be carried out by combining information of other body parts, and the robustness for partial shielding is better. And a part decision threshold is added on the basis of weighted combination, so that the weighted combination result is prevented from being influenced by a part extreme value.","['G06V40/103', 'G06F18/254']"
US7755608B2,Systems and methods of interfacing with a machine,"Systems and methods of interfacing with a machine are described. In one aspect, sets of contemporaneous images of an interactive space are acquired from multiple respective fields of view. An input target is detected in the acquired images. Coordinates of the input target detected in the acquired images are computed. A spatiotemporal input data structure linking input target coordinates computed from contemporaneous images to respective reference times is constructed. The spatiotemporal input data structure is processed to identify an input instruction. The identified input instruction is executed on a machine. In another aspect, an image is displayed at a display location disposed between a viewing space and an interactive space, wherein the displayed image is viewable from a perspective in the viewing space. Images of the interactive space are acquired from at least one field of view. An input target is detected in the acquired images. Coordinates of the input target detected in the acquired images are computed. An input instruction is identified based on the computed input coordinates. The identified input instruction is executed on a machine.","['G06F3/017', 'G06F3/0304', 'G06F3/0425', 'G06V40/20']"
CN114003758A,Training method and device of image retrieval model and retrieval method and device,"A training method and device of an image retrieval model and a retrieval method and device are provided. The training method comprises the following steps: acquiring a training image and a training text for describing the training image; obtaining an image region feature vector and an image global feature vector through an image information feature extraction process based on a training image; obtaining a text word segmentation feature vector and a text global feature vector through a text information feature extraction process based on a training text; obtaining similarity through an image retrieval model based on the image region feature vector, the image global feature vector, the text word segmentation feature vector and the text global feature vector; and training the image retrieval model by taking the maximum similarity as a target.","['G06F16/583', 'G06F16/5866', 'G06F18/214', 'G06F18/22', 'G06F40/284', 'G06N3/044', 'G06N3/045']"
US9087241B2,Intelligent part identification for use with scene characterization or motion capture,"A variety of methods, systems, devices and arrangements are implemented for use with motion capture. One such method is implemented for identifying salient points from three-dimensional image data. The method involves the execution of instructions on a computer system to generate a three-dimensional surface mesh from the three-dimensional image data. Lengths of possible paths from a plurality of points on the three-dimensional surface mesh to a common reference point are categorized. The categorized lengths of possible paths are used to identify a subset of the plurality of points as salient points.","['G06T7/251', 'G06K9/00718', 'G06F18/22', 'G06F18/24', 'G06K9/00342', 'G06K9/6215', 'G06K9/6267', 'G06T17/205', 'G06T7/2046', 'G06V10/761', 'G06V20/41', 'G06V40/23', 'G06T2207/10028', 'G06T2207/30196']"
CN112926654B,"Pre-labeling model training and certificate pre-labeling method, device, equipment and medium","The invention relates to the field of classification models of artificial intelligence, and provides a method, a device, equipment and a medium for training a pre-labeling model and pre-labeling a certificate, wherein the method comprises the following steps: obtaining a target annotation category, a target description, a model performance parameter and an image sample set; crawling the category to be migrated in the target classification and identification library by using a text similarity technology; searching a model to be migrated from a target classification recognition library and recognizing target areas of all image samples by a simulated target recognition technology; performing target fine adjustment to obtain a fine adjustment region, and inputting an image sample, the fine adjustment region and a target labeling category into a model to be migrated; obtaining a marked target marking area by using a transfer learning technology; determining a loss value according to the target labeling area and the fine tuning area; training the model to be migrated until training is completed to obtain the pre-labeling model. The invention realizes the automatic training of the image sample set of the zero mark annotation, obtains the pre-marking model, and reduces the manual marking time and the workload.","['G06F18/241', 'G06F18/214', 'G06F18/22']"
US11972569B2,Segmenting objects in digital images utilizing a multi-object segmentation model framework,"The present disclosure relates to a multi-model object segmentation system that provides a multi-model object segmentation framework for automatically segmenting objects in digital images. In one or more implementations, the multi-model object segmentation system utilizes different types of object segmentation models to determine a comprehensive set of object masks for a digital image. In various implementations, the multi-model object segmentation system further improves and refines object masks in the set of object masks utilizing specialized object segmentation models, which results in more improved accuracy and precision with respect to object selection within the digital image. Further, in some implementations, the multi-model object segmentation system generates object masks for portions of a digital image otherwise not captured by various object segmentation models.","['G06T7/11', 'G06T3/4046', 'G06T7/174', 'G06T7/187', 'G06T7/194', 'G06T2207/20081', 'G06T2207/20084']"
US8868555B2,Computation of a recongnizability score (quality predictor) for image retrieval,"A MMR system for newspaper publishing comprises a plurality of mobile devices, an MMR gateway, an MMR matching unit and an MMR publisher. The MMR matching unit receives an image query from the MMR gateway and sends it to one or more of the recognition units to identify a result including a document, the page and the location on the page. The MMR system also includes a quality predictor as a plug-in installed on the mobile device to filter images before they are included as part of a retrieval request or as part of the MMR matching unit. The quality predictor comprises an input for receiving recognition algorithm information, a vector calculator, a score generator and a scoring module. The quality predictor receives as inputs an image query, context information and device parameters, and generates an outputs a recognizability score. The present invention also includes a method for generating robustness features.","['G06F16/583', 'G06F17/30247', 'G06F16/955', 'G06F17/30876', 'G06F18/214', 'G06K9/00463', 'G06K9/00993', 'G06K9/6256', 'G06V10/96', 'G06V30/19147', 'G06V30/414', 'H04N1/00307', 'G06K2009/3291', 'G06V10/62', 'Y10S707/915']"
CN109086726B,Local image identification method and system based on AR intelligent glasses,"The invention belongs to the technical field of AR intelligent glasses application, and discloses a local image identification method and a system based on AR intelligent glasses, which are used for calibrating and calibrating the consistency of an intelligent glasses imaging screen, an intelligent glasses front camera picture and a surrounding real environment vision field picture; recognizing a human eye image, calculating an eyeball motion vector, and acquiring an eye movement coordinate; identifying whether a focus falling point of a human binocular vision extension line is an imaging screen or a real three-dimensional space; acquiring a mapping relation of human eyes in the surrounding real world; and respectively acquiring the coordinate positions of the human eye fixation point on the glasses imaging screen and the front camera picture according to an embedded mapping algorithm. According to the invention, the eye pattern information of human eyes is obtained through the eye tracker, the eye pattern and the scene camera are synchronously calibrated, the fixation point area of human eyes in a real scene is obtained, and only the fixation point area is subjected to image recognition during processing, so that the image processing pressure of the GPU is greatly reduced, and the image processing efficiency is improved.","['G06F3/013', 'G06T19/006', 'G06V20/64']"
US10789622B2,Generating and providing augmented reality representations of recommended products based on style compatibility in relation to real-world surroundings,"The present disclosure relates to systems, methods, and non-transitory computer readable media for generating augmented reality representations of recommended products based on style compatibility with real-world surroundings. For example, the disclosed systems can identify a real-world object within a camera feed and can utilize a 2D-3D alignment algorithm to identify a three-dimensional model that matches the real-world object. In addition, the disclosed systems can utilize a style compatibility algorithm to generate recommended products based on style compatibility in relation to the identified three-dimensional model. The disclosed systems can further utilize a color compatibility algorithm to determine product textures which are color compatible with the real-world surroundings and generate augmented reality representations of recommended products to provide as an overlay of the real-world environment of the camera feed.","['G06Q30/0261', 'G06F18/2132', 'G06K9/00671', 'G06T19/006', 'G06V10/255', 'G06V10/507', 'G06V10/56', 'G06V10/7715', 'G06V10/92', 'G06V20/20']"
WO2021150017A1,Method for interactive segmenting an object on an image and electronic computing device implementing the same,"The present invention relates generally to the fields of computer vision and computer graphics using neural networks, machine learning for interactive segmentation of objects on images, in particular, to a method for interactive segmenting an object on an image and electronic computing device implementing the method. The method comprises: inputting (S101) the image and user inputs, wherein each user input indicates either the object or a background on the image and is defined by coordinates; converting (S102) each user input into a distance map and a tensor representation including coordinates and an indication that the user input indicates either the object or the background; fusing (S103), by a trained artificial intelligence tool, the distance maps with the image into an intermediate representation; extracting (S104), by the trained artificial intelligence tool, features of the image from the intermediate representation; adjusting (S105), by the trained artificial intelligence tool, a scale to 1 and a bias to 0; rescaling (S106), by the trained artificial intelligence tool, the extracted features by using adjusted scale and adjusted bias; predicting (S107), by the trained artificial intelligence tool, a segmentation mask segmenting the object on the image by predicting that the rescaled features belong to the object or the background based on the intermediate representation; estimating (S108), by the trained artificial intelligence tool, whether a discrepancy between predicted segmentation mask and the tensor representation meet a minimum threshold value set by a user in advance; and adjusting (S109), by the trained artificial intelligence tool, the scale and the bias using an iterative optimization procedure to minimize the discrepancy between the predicted segmentation mask and the tensor representation; wherein the steps (S106) to (S109) are repeated until, on the step (S107), the segmentation mask is predicted so that the discrepancy between the predicted segmentation mask and the tensor representation meet the minimum threshold value or a number of repetitions reaches a maximum number set by the user in advance.","['G06T7/11', 'G06F18/24133', 'G06N3/045', 'G06N3/048', 'G06N3/084', 'G06T7/194', 'G06V10/235', 'G06V10/454', 'G06V10/82', 'G06T2207/10024', 'G06T2207/20016', 'G06T2207/20041', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/20132']"
JP5049356B2,Separation of directional lighting variability in statistical face modeling based on texture space decomposition,"A technique for determining a characteristic of a face or certain other object within a scene captured in a digital image including acquiring an image and applying a linear texture model that is constructed based on a training data set and that includes a class of objects including a first subset of model components that exhibit a dependency on directional lighting variations and a second subset of model components which are independent of directional lighting variations. A fit of the model to the face or certain other object is obtained including adjusting one or more individual values of one or more of the model components of the linear texture model. Based on the obtained fit of the model to the face or certain other object in the scene, a characteristic of the face or certain other object is determined.","['G06V40/171', 'G06T5/90', 'G06V10/60', 'G06V10/7557', 'G06V40/168', 'H04N23/71', 'H04N23/74']"
CN107202982B,A kind of beacon arrangement and image processing method based on UAV position and orientation calculating,"A kind of beacon arrangement and image processing method based on UAV position and orientation calculating, first the machine vision of unmanned plane is guide to fall and be designed, according to the Vision imaging system built ensure it is image sensing and to visual field distortion analyze under conditions of, the pose being likely to occur in conjunction with land process, design is for the beacon infrastructure and method for arranging needed for vision guide land, then according to designed vision guide system, for different cooperation beacon conformations, the index of three measure algorithm applicabilities is proposed to be evaluated and screened to pose computation, the ground cooperation beacon finally used is infrared LED, infrared LED imaging effect is more stablized, image-forming range is farther, imaging precision higher.This method is simple and practicable, low to the configuration requirement in landing place, and extraction feature is more convenient, quick from near-infrared image, expands the scope of application of vision system, and can detect identification all-time anf all-weather；Accurately calculate UAV Attitude information.","['G05D1/0676', 'G01S11/12', 'G01C11/34', 'G06T7/11', 'G06T7/136', 'G06T7/73']"
CN113379764B,Pathological image segmentation method based on domain antagonism self-supervision learning,"The invention relates to a pathological image segmentation method based on domain antagonism self-supervision learning. Comprising the following steps: acquiring a pathology image and establishing a pathology image self-supervision data set; establishing a domain countermeasure self-supervision model; deep learning training is carried out on the domain countermeasure self-supervision model by using the pathology image self-supervision data set; establishing a pathological image segmentation model; initializing a pathological image segmentation model by using a domain countermeasure self-supervision model after deep learning; performing pixel level labeling on a focus area in a pathological image to establish a pathological image segmentation data set; performing deep learning training on the pathological image segmentation model by using the pathological image segmentation data set; and segmenting the unknown focus area of the pathological image by using the pathological image segmentation model after the deep learning training. The method adopts a domain countermeasure self-supervision learning method, effectively relieves the dependence of the segmentation model on a large number of manual labels and solves the problem of the segmentation performance fluctuation of the model on different domains.","['G06F18/214', 'G06F17/16', 'G06T3/60', 'G06T5/70', 'G06T7/11', 'G06T7/62', 'Y02T10/40']"
US11954846B2,Explainability and complementary information for camera-based quality assurance inspection processes,A video processing pipeline receives data derived from a feed of images of a plurality of objects passing in front of an inspection camera module forming part of a quality assurance inspection system. Quality assurance metrics for the object are generated by one or more containerized image analysis inspection tools forming part of the video processing pipeline using the received data for each object. Overlay images are later generated that characterize the quality assurance metrics. These overlay images are combined with the corresponding image of the object to generate an enhanced image of each of the objects. These enhanced images are provided to a consuming application or process for quality assurance analysis.,"['G06T7/001', 'G06T11/60', 'G06T5/002', 'G06T5/003', 'G06T5/50', 'G06T5/70', 'G06T5/73', 'G06T7/0004', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30108']"
US6363160B1,Interface using pattern recognition and tracking,"A method for interfacing with an electronic system is provided. The method includes receiving images from a camera. A portion of at least one image from the camera is tracked. A pattern in the tracked portion of the at least one image is recognized. And, the electronic system is controlled based on the recognized pattern.",['G06F3/017']
US8045783B2,Method for moving cell detection from temporal image sequence model estimation,A computerized robust cell kinetic recognition method for moving cell detection from temporal image sequence receives an image sequence containing a current image. A dynamic spatial-temporal reference generation is performed to generate dynamic reference image output. A reference based object segmentation is performed to generate initial object segmentation output. An object matching and detection refinement is performed to generate kinetic recognition results output. The dynamic spatial-temporal reference generation step performs frame look ahead and the reference images contain a reference intensity image and at least one reference variation image.,"['G06V20/695', 'G06T7/254', 'G06T2207/10056', 'G06T2207/30024']"
US7933454B2,Class-based image enhancement system,"A method for image enhancement includes providing for a semantic class to be assigned to a digital image based on image content, the assigned semantic class being selected from a plurality of semantic classes. The method further includes providing for an aesthetic enhancement to be applied to the image based on image quality of the image and the assigned semantic class, the enhancement including at least one enhancement dimension selected from a plurality of enhancement dimensions.","['G06V30/413', 'G06T5/60', 'G06T2207/20008', 'G06T2207/20081']"
US9661239B2,System and method for online processing of video images in real time,"The invention is directed to real-time processing of video data. In some examples at least one image of the video data is processed utilizing reference data comprising reference image data of background within a region of interest and clutter image data indicative thereof, to determine a pixel deviation level of each pixel in the at least one image and generate pixel-deviation image data indicative thereof. The pixel-deviation image data is processed to enhance its tonal pixel distribution and generating enhanced image data, which is processed to determine a threshold level based on the tonal pixel distribution. A binary image map is then generated using the determined threshold level, the binary image map being indicative of the background and foreground components of the at least one image.","['H04N5/272', 'G06T11/60', 'G06T5/008', 'G06T5/20', 'G06T5/40', 'G06T5/50', 'G06T5/94', 'G06T7/0081', 'G06T7/11', 'G06T7/194', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20144', 'G06T2207/20221']"
CN108230337B,Semantic SLAM system implementation method based on mobile terminal,"A semantic SLAM system implementation method based on a mobile terminal relates to the fusion of SLAM system construction and point cloud semantic analysis. The method comprises the following steps: 1) solving the camera attitude based on the improved FAST and LDB characteristic feature point method; 2) the rear end is optimized to obtain accurate point cloud positions and camera postures; 3) loop detection; 4) constructing a global point cloud map; 5) realizing an augmented reality system by utilizing an SLAM system; 6) utilizing semantic segmentation to realize 3D point cloud semantic segmentation; 7) and optimizing the semantic SLAM system based on the mobile terminal. The solution with low cost and good performance is obtained according to the environment cognition and augmented reality requirements under flexible indoor and outdoor scenes, particularly in the fields of unmanned driving, path planning and the like, and the method has wide application value and expansion capability.","['G06T7/11', 'G06F18/23', 'G06T19/006', 'G06T7/73', 'G06T2207/10028']"
US11645506B2,Neural network for skeletons from input images,"A computing system is provided. The computing system includes a processor configured to execute a convolutional neural network that has been trained, the convolutional neural network including a backbone network that is a concatenated pyramid network, a plurality of first head neural networks, and a plurality of second head neural networks. At the backbone network, the processor is configured to receive an input image as input and output feature maps extracted from the input image. The processor is configured to: process the feature maps using each of the first head neural networks to output corresponding keypoint heatmaps; process the feature maps using each of the second head neural networks to output corresponding part affinity field heatmaps; link the keypoints into one or more instances of virtual skeletons using the part affinity fields; and output the instances of the virtual skeletons.","['G06N3/0454', 'G06N3/08', 'G06F17/15', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G06T7/10', 'G06V10/764', 'G06V10/82', 'G06V40/103', 'G06V40/23', 'G06T2207/10004', 'G06T2207/10028', 'G06T2207/10048', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196']"
US10488939B2,Gesture recognition,"A gesture recognition method comprises receiving at a processor from a sensor a sequence of captured signal frames for extracting hand pose information for a hand and using at least one trained predictor executed on the processor to extract hand pose information from the received signal frames. For at least one defined gesture, defined as a time sequence comprising hand poses, with each of the hand poses defined as a conjunction or disjunction of qualitative propositions relating to interest points on the hand, truth values are computed for the qualitative propositions using the hand pose information extracted from the received signal frames, and execution of the gesture is tracked, by using the truth values to determine which of the hand poses in the time sequence have already been executed and which of the hand poses in the time sequence is expected next.","['G06F3/017', 'G06F3/0304', 'G06K9/00355', 'G06K9/00389', 'G06V40/113', 'G06V40/28', 'G06K9/4604']"
US6711278B1,Tracking semantic objects in vector image sequences,"A semantic object tracking method tracks general semantic objects with multiple non-rigid motion, disconnected components and multiple colors throughout a vector image sequence. The method accurately tracks these general semantic objects by spatially segmenting image regions from a current frame and then classifying these regions as to which semantic object they originated from in the previous frame. To classify each region, the method performs a region based motion estimation between each spatially segmented region and the previous frame to compute the position of a predicted region in the previous frame. The method then classifies each region in the current frame as being part of a semantic object based on which semantic object in the previous frame contains the most overlapping points of the predicted region. Using this method, each region in the current image is tracked to one semantic object from the previous frame, with no gaps or overlaps. The method propagates few or no errors because it projects regions into a frame where the semantic object boundaries are previously computed rather than trying to project and adjust a boundary in a frame where the object's boundary is unknown.","['H04N19/80', 'G06T7/11', 'G06T7/174', 'G06T7/215', 'G06T7/246', 'G06V10/24', 'H04N19/543']"
CN109657533B,Pedestrian re-identification method and related product,"The application provides a pedestrian re-identification method and a related product, wherein the method comprises the following steps: extracting the features of the target image through a preset convolutional neural network training model to obtain a first feature set, wherein the preset convolutional neural network training model consists of a first training module and a second training module, and the features extracted by the first training module and the second training module are fused into one feature set; determining a Hamming distance between the first feature set and each second feature set in the plurality of second feature sets to obtain a plurality of Hamming distance values; calculating a similarity probability value between the input image and each image in the image library through the plurality of Hamming distance values to obtain a plurality of similarity probability values; selecting a similar probability value larger than a preset threshold value from the multiple similar probability values to obtain at least one target similar probability value; and displaying the images in the image library corresponding to the at least one target similar probability value to the user according to the sequence of the similar probability values from large to small. By the adoption of the pedestrian recognition method and device, the pedestrian recognition accuracy can be improved.","['G06V40/103', 'G06F18/22', 'G06N3/045', 'G06N3/08']"
US10424341B2,Dynamic video summarization,"Cyclical pixel-based remapping techniques are disclosed for efficiently creating and displaying a video summary. Cyclical temporal remapping at the pixel level requires less processing power and eliminates the need for object identification and tracking, allowing for dynamic adjustment of summary video parameters while a user is viewing the summary video. An operator can select an archived camera feeds or video file, specify a time compression ratio or time interval over which to review video content, specify a motion sensitivity threshold, and then view a significantly condensed summary video that depicts all activity from the source video within a much shorter timeline. Furthermore, activity co-occurrence is preserved such that any activities that co-occur in the source video will also co-occur in the summary video.","['G06F3/04847', 'G06F3/04845', 'G08B13/19682', 'G11B27/031', 'G11B27/06', 'G11B27/28', 'G11B27/3081', 'H04N21/44008', 'H04N21/47217', 'H04N21/8549', 'H04N7/18', 'H04N7/181', 'G08B13/19602']"
US9424461B1,Object recognition for three-dimensional bodies,"Various embodiments utilize two-dimensional (“2D”) and three-dimensional (“3D”) object features for purposes such as object recognition and/or image matching. For example, a user can capture an image (e.g., still images or video) of an object and can receive information about items that are determined to match the object. For example, the image can be analyzed to detect visual features (e.g., corners, edges, etc.) of the object and the detected visual features can be combined to generate a combined visual feature vector which can be used for object recognition, image matching, or other such purposes. Other approaches utilize the image to generate a 3D model of the object represented in the image, which can be used to determine at least one object or types of objects that match the object represented in the image.","['G06V20/64', 'G06V20/647', 'G06K9/00201', 'G06V20/653']"
US20110158491A1,Method and system for lesion segmentation,"A method and system for acquiring information on lesions in dynamic 3D medical images of a body region and/or organ, having the steps: applying a registration technique to align a plurality of volumetric image data of the body region and/or organ, yielding multi-phase registered volumetric image data of the body region and/or organ; applying a hierarchical segmentation on the multi-phase registered volumetric image data of the body region and/or organ, the segmentation yielding a plurality of clusters of n-dimensional voxel vectors of the multi-phase aligned volumetric image; determining from the plurality of clusters a cluster or set of clusters delineating the body and/or organ; identifying the connected region(s) of voxel vectors belonging to the body region and/or organ; refining/filling the connected region(s) corresponding to the body region and/or organ; reapplying the segmentation step to the refined/filled connected region(s) corresponding to the body region and/or organ, to obtain a more accurate segmentation; and acquiring information on the presence of lesions in cluster of set of clusters of said plurality of clusters.","['G06T7/0012', 'G06T3/153', 'G06T7/41', 'G06T2200/24', 'G06T2207/10072', 'G06T2207/20036', 'G06T2207/20096', 'G06T2207/30004', 'G06T2207/30056']"
WO2022146109A1,Infrared camera-based method and system for estimating hand position through domain transfer learning,"The present invention relates to a hand position estimation method and system for estimating a 3-dimensional hand position for quick hand movement, on the basis of domain transfer learning from a depth image to an infrared image, the hand position estimation method comprising the steps of: processing a depth image and an infrared image for hand movement; synthesizing a depth map with the infrared image by using a hand image generator (HIG), and estimating skeletal positions of hand joints from each of the depth map and an infrared map; and calculating a 3-dimensional hand position by using the skeletal positions and the center of a hand depth image.","['G06T7/74', 'G06T7/73', 'G06T7/11', 'G06T7/174', 'G06T7/248', 'G06T7/593', 'G06V40/107', 'G06T2207/10028', 'G06T2207/10048', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196']"
CN111353983B,"Defect detection identification method, device, computer readable medium and electronic equipment","The embodiment of the application provides a defect detection and identification method, a device, a computer readable medium and electronic equipment. The defect detection and identification method comprises the following steps: acquiring a target product image; matching the target product image with a template image corresponding to the target product image to obtain a first defect map of the target product image; performing defect detection processing on the target product image through a neural network model to obtain a second defect map corresponding to the target product image; and combining the first defect map and the second defect map, and identifying defects contained in the target product image. According to the technical scheme, the accuracy of the quality inspection result can be improved, meanwhile, the manual quality inspection can be replaced, and the quality inspection efficiency is effectively improved.","['G06T7/0006', 'G06T7/187', 'G06T2207/10004', 'G06T2207/30164', 'Y02P90/30']"
US11947890B2,Implementation of deep neural networks for testing and quality control in the production of memory devices,"Techniques are presented for the application of neural networks to the fabrication of integrated circuits and electronic devices, where example are given for the fabrication of non-volatile memory circuits and the mounting of circuit components on the printed circuit board of a solid state drive (SSD). The techniques include the generation of high precision masks suitable for analyzing electron microscope images of feature of integrated circuits and of handling the training of the neural network when the available training data set is sparse through use of a generative adversary network (GAN).","['G06F30/398', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/063', 'G06N3/08', 'G06N3/088', 'G06N3/09', 'G06N3/094', 'H01L22/12', 'H05K13/081', 'G06F2119/22']"
US8401336B2,System and method for rapid image sequence depth enhancement with augmented computer-generated elements,"Motion picture scenes to be colorized/depth enhanced (2D->3D) are broken into separate elements, backgrounds/sets or motion/onscreen-action. Background and motion elements are combined into composite frame which becomes a visual reference database that includes data for all frame offsets used later for the computer controlled application of masks within a sequence of frames. Masks are applied to subsequent frames of motion objects based on various differentiating image processing methods, including automated mask fitting/reshaping. Colors and/or depths are automatically applied to masks throughout a scene from the composite background and to motion objects. Areas never exposed by motion or foreground objects in a series of images may be partially or fully realistically drawn or rendered and applied to the occluded areas of the background and then automatically applied throughout the images to generate of minimal artifact or artifact-free secondary viewpoints when translating foreground objects horizontally during 2D->3D conversion.","['H04N13/257', 'H04N13/266']"
CN117853735A,Single image mixed rainwater removing method,"The invention discloses a method for removing mixed rainwater of a single image, which comprises the following steps: acquiring an image to be processed, and inputting the image to a trained generation network; in a generating network, carrying out primary decomposition on the image to be processed through wavelet transformation to obtain a high-frequency component and a low-frequency component; extracting and removing rain, fog and rain drop features based on the high-frequency components to obtain a first feature map; extracting and removing rain line features based on the low-frequency components to obtain a second feature map; performing wavelet inverse transformation according to the first characteristic diagram and the second characteristic diagram to generate a rainwater removal image; the invention introduces wavelet transformation, removes rainwater on the basis of separating high and low frequency components, and improves the definition of the rainless picture.","['G06V10/30', 'G06N3/0442', 'G06N3/0455', 'G06N3/0464', 'G06V10/765', 'G06V10/774', 'G06V10/82', 'Y02A90/10']"
US10417775B2,Method for implementing human skeleton tracking system based on depth data,"The present invention relates to a method for implementing a human skeleton tracking system based on depth data, specifically applied to a human skeleton tracking system based on depth data which is composed of a data acquisition unit, a limb segmentation unit, a skeleton point unit, and a tracking display unit. The units are in a relationship of sequential invocation. The limb segmentation unit uses scene depth data obtained after processing by the data acquisition unit to perform limb segmentation. The skeleton point unit uses the result obtained after segmentation by the limb segmentation unit to compute specific positions of respective skeleton points of a limb. The tracking display unit uses the positions of the skeleton points computed by the skeleton point unit to establish a skeleton model of the human body and perform tracking display.","['G06T7/251', 'A61B5/1128', 'G06K9/00', 'G06K9/00342', 'G06K9/00369', 'G06T7/11', 'G06T7/194', 'G06V10/255', 'G06V10/267', 'G06V20/64', 'G06V40/103', 'G06V40/23', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/30196', 'G06T2210/12', 'G06T3/40']"
CN110717872B,Method and system for extracting characteristic points of V-shaped welding seam image under laser-assisted positioning,"The invention discloses a method for extracting characteristic points of a V-shaped welding seam image under laser-assisted positioning, which comprises the following steps of: s1, collecting laser welding seam images; s2, bilateral filtering is carried out to remove noise; s3, gray level histogram equalization processing; s4, segmenting the binary image based on density peak clustering; s5, performing morphological opening operation; s6, laser pixel skeleton extraction: performing iterative processing on the processed laser welding seam binary segmentation image, and continuously reducing the boundary of a communicated region to obtain a laser framework with a single-pixel width; s7, extracting characteristic points based on curvature analysis: in the laser framework with the single pixel width, firstly finding the head characteristic point and the tail characteristic point of the laser weld joint, then calculating the curvature of the laser pixel according to the direction from head to tail, and taking the curvature extreme point as the characteristic point of the laser weld joint, thereby positioning the position of the weld joint. The method realizes the automation of the welding seam tracking, provides powerful support for the automatic welding production of the welding parts, and better ensures the product quality of the welding parts.","['G06T7/0004', 'G06F18/23', 'G06T5/30', 'G06T5/40', 'G06T5/70', 'G06T7/11', 'G06T7/155', 'G06T2207/20028', 'G06T2207/30152']"
CN110363182B,Lane detection method based on deep learning,"A lane line detection method based on deep learning belongs to the technical field of lane line detection. The invention solves the problems of time consumption and low detection precision of the traditional lane line detection method. The method comprises the steps of firstly regarding a task of detecting a lane line as a pixel-level semantic segmentation problem, dividing the lane line and a background through a full convolution neural network FCN8s, extracting coordinates of a left lane line and a right lane line on two sides of a current lane through a post-processing and DBSCAN clustering method, and finally fitting the left lane line and the right lane line through a fitting algorithm, wherein the method achieves an average of 50 frames per second for video detection, the detection precision can reach 92.3%, and the accurate and rapid detection effect can be realized. The invention can be applied to the technical field of lane line detection.","['G06F18/23', 'G06V10/28', 'G06V20/588']"
US20220092856A1,"Crack detection, assessment and visualization using deep learning with 3d mesh model","In various example embodiments, techniques are provided for crack detection, assessment and visualization that utilize deep learning in combination with a 3D mesh model. Deep learning is applied to a set of 2D images of infrastructure to identify and segment surface cracks. For example, a Faster region-based convolutional neural network (Faster-RCNN) may identify surface cracks and a structured random forest edge detection (SFRED) technique may segment the identified surface cracks. Alternatively, a Mask region-based convolutional neural network (Mask-RCNN) may identify and segment surface cracks in parallel. Photogrammetry is used to generate a textured three-dimensional (3D) mesh model of the infrastructure from the 2D images. A texture cover of the 3D mesh model is analyzed to determine quantitative measures of identified surface cracks. The 3D mesh model is displayed to provide a visualization of identified surface cracks and facilitate inspection of the infrastructure.","['G06T19/20', 'G06T19/006', 'G01C11/02', 'G01C11/04', 'G06F18/2148', 'G06K9/00671', 'G06K9/2054', 'G06K9/4604', 'G06K9/6257', 'G06T17/20', 'G06T7/0004', 'G06T7/579', 'G06V10/22', 'G06V10/44', 'G06V10/82', 'G06V20/17', 'G06V20/176', 'G06V20/20', 'G06T2207/10016', 'G06T2207/10032', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30132', 'G06T2207/30184', 'G06T2219/004', 'G06T2219/2012']"
US9159128B2,Enhanced multi-protocol analysis via intelligent supervised embedding (empravise) for multimodal data fusion,"The present invention provides a system and method for analysis of multimodal imaging and non-imaging biomedical data, using a multi-parametric data representation and integration framework. The present invention makes use of (1) dimensionality reduction to account for differing dimensionalities and scale in multimodal biomedical data, and (2) a supervised ensemble of embeddings to accurately capture maximum available class information from the data.","['G06V10/7715', 'G06F18/213', 'G06F18/2137', 'G06F18/214', 'G06K9/6232', 'G06K9/6251', 'G06K9/6256', 'G06T7/0012', 'G06K2209/051', 'G06V2201/031']"
CN108509918B,Target detection and tracking method fusing laser point cloud and image,"The invention discloses a target detection and tracking method fusing laser point cloud and an image, which comprises the following steps: s1, respectively acquiring laser point cloud data and image data of a target, performing first target detection according to the acquired point cloud data to obtain a first target detection result, and performing second target detection according to the acquired image data to obtain a second target detection result; s2, performing fusion judgment on the first target detection result and the second target detection result based on Bayesian decision to obtain a final target detection result and output the final target detection result; and S3, tracking the target according to the final target detection result. The method has the advantages of simple and efficient implementation method, high target detection and tracking precision, strong environmental adaptability and fault tolerance, stability, reliability and the like.","['G06V20/13', 'G06V10/25', 'G06V10/267', 'G06V40/10']"
US12399568B2,Dynamic configuration of user interface layouts and inputs for extended reality systems,"Systems and techniques are provided for generating virtual content. For example, a process can include obtaining an image of a physical object in a real-world environment and determining at least one attribute of the physical object in the image. The process can include determining an interface layout for virtual content based on the at least one attribute of the physical object. The process can include displaying the virtual content based on the determined interface layout. In some cases, the process can including determining an input type based on the at least one attribute of the physical object, receiving input corresponding to the input type, and controlling displayed virtual content based on the determined input type.","['G06F3/011', 'G06F3/017', 'G06F3/0304', 'G06F3/04815', 'G06F3/0485', 'G06F3/0488', 'G06T19/006', 'G06V20/10', 'G06V20/20', 'G06V40/107', 'G06V40/28', 'G06T2200/24']"
US9600711B2,Method and system for automatically recognizing facial expressions via algorithmic periocular localization,"This disclosure provides a method and system for automatically recognizing facial expressions at variable resolutions of video. According to one exemplary method, facial expressions are detected, extracted and classified from a video sequence based on an automatic localization of the periocular region associated with a detected and extracted face.","['G06K9/00228', 'G06V40/161', 'G06K9/00315', 'G06V20/52', 'G06V40/174', 'G06V40/176', 'G06K2009/4666', 'G06V10/467']"
US9158985B2,Method and apparatus for processing image of scene of interest,"A method for processing an image of a scene of interest includes receiving an original target image of a scene of interest at an image processing device from an image source device, the original target image exhibiting shadowing effects associated with the scene of interest when the original target image was captured, the original target image comprising a plurality of elements and representing an instantaneous state for the scene of interest, pre-processing the original target image using a modification identification algorithm to identify elements of the original target image to be modified, and generating a copy mask with a mask region representing the elements to be modified and a non-mask region representing other elements of the original target image. An image processing device for processing an image of a scene of interest and a non-transitory computer-readable medium are also provided.","['G06T7/194', 'G06K9/3241', 'G06K9/00711', 'G06T7/0081', 'G06T7/0097', 'G06T7/11', 'G06T7/174', 'G06V10/267', 'G06V20/54', 'G06K2209/21', 'G06T2207/10016', 'G06T2207/20112', 'G06T2207/30232', 'G06T2207/30236']"
KR20160120238A,Clothes recommendation system,"The present invention relates to a clothes recommendation system which recommends clothes based on an image file which a user inputs. The clothes recommendation system includes a separation part which recognizes clothes from a first image file, determines the extent of the display of the body part of a person on the first image file, and generates cloths region information; an extraction part which extracts first tag information including the feature vector of the clothes from the clothes region information; and a recommendation part which searches second tag information matched to the first tag information from a database, selects a second image file having the second tag information, and suggests it.","['G06F16/335', 'G06Q30/0257', 'G06Q30/0601', 'G06Q30/0631', 'G06T3/4038']"
CN114693610B,"A method, device and medium for detecting weld surface defects based on machine vision","The invention discloses a weld surface defect detection method, equipment and a medium based on machine vision, which comprise the steps of obtaining a weld image of a boiler pipeline acquired by an industrial camera, preprocessing the image, namely image restoration, image denoising, image enhancement and image rotation correction, performing binarization processing on the preprocessed image, performing morphological processing, intercepting a region of interest (ROI) in the image obtained after the morphological processing by determining the positions of the edge and the central line of the weld, intercepting the region of interest (ROI) in the image obtained after the morphological processing by determining the positions of the edge and the central line of the weld, extracting characteristic information from the image of the region of interest, inputting the characteristic information into a pre-trained weld surface defect detection model for detection, and outputting the type of the weld surface defect, wherein the structure of the weld surface defect detection model adopts a partial binary tree support vector machine. The invention improves the accuracy and the efficiency of identifying the defects on the surface of the welding line.","['G06T7/0004', 'G06F18/23213', 'G06F18/2411', 'G06T2207/30152', 'Y02P90/30']"
CN109409297B,Identity recognition method based on dual-channel convolutional neural network,"The invention discloses an identity recognition method based on a dual-channel convolutional neural network, which comprises the steps of training the neural network and identifying the identity, wherein two time synchronization images, namely a face image and a whole body attitude image, are adopted for comprehensive training and recognition, so that single factor deception is avoided, and the identity recognition method has stronger anti-jamming capability and higher recognition accuracy; the method comprises the steps of connecting the characteristic data of two channels in a weighted mode through a full connecting layer, obtaining image characteristic data through a plurality of convolution layers and pooling layers, obtaining class probability through a classifier, extracting the maximum probability, comparing the maximum probability with a set threshold value, and determining a recognition result. Through multiple times of convolution extraction of characteristic graphs, nonlinear excitation and pooling dimension reduction processing, the control data of the dual-channel convolution neural network are more flexible, and the abstract capability and the learning capability are stronger, so that the method has a better identification effect.","['G06V40/70', 'G06N3/045']"
US20180260628A1,Apparatus and method for image processing to calculate likelihood of image of target object detected from input image,"An image processing apparatus, which receives an input image and detects an image of a target object based on a detection algorithm, includes a machine learning device which performs learning by using a plurality of partial images cut out from at least one input image, based on a result of detection of the image of the target object, and calculates a likelihood of the image of the target object.","['G06K9/00664', 'G06T7/0004', 'G06N3/084', 'G06F18/214', 'G06F18/217', 'G06F18/2413', 'G06N3/088', 'G06T7/73', 'G06T7/75', 'G06V10/25', 'G06V10/40', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06V10/955', 'G06V20/10', 'B25J9/1697', 'G06N3/063', 'G06T2207/20081', 'G06T2207/20084', 'G06V2201/06']"
US11144787B2,"Object location method, device and storage medium based on image segmentation","The invention discloses an object location method, device and storage medium based on image segmentation, the object location method comprises: collecting and labeling training images to obtain a trained database; designing a fully convolutional neural network (FCNN); training the FCNN to obtain a target neural network, by inputting the trained database into the FCNN; labeling and locating object images, based on the target neural network. The method is using the training samples collected in the application scenario to train the FCNN, so it can obtain an optimized FCNN and achieve higher robustness and segmentation accuracy. Particularly, the object segmentation method in the embodiment can perform high-precision segmentation on a plurality of overlapping envelope regions when processing envelopes in a logistics system, and an envelope on the top layer is accurate singulation, allowing the robot to grab only one envelope each time, greatly improving the accuracy and experience of logistics sorting.","['G06T7/70', 'G06K9/6256', 'G06F18/214', 'G06F18/24', 'G06F18/241', 'G06K9/6267', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/11', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30164']"
US8073226B2,Automatic detection and monitoring of nodules and shaped targets in image data,"A method for detecting a nodule in image data including the steps of segmenting scanning information from an image slice to isolate lung tissue from other structures, resulting in segmented image data; extracting anatomic structures, including any potential nodules, from the segmented image data, resulting in extracted image data; and detecting possible nodules from the extracted image data, based on deformable prototypes of candidates generated by a level set method in combination with a marginal gray level distribution method. Embodiments of the invention also relate to an automatic method for detecting and monitoring a nodule in image data, where the method includes the steps of determining adaptive probability models of visual appearance of small 2D and large 3D nodules to control evolution of deformable models to get accurate segmentation of pulmonary nodules from image data; modeling a first set of nodules in image data with a translation and rotation invariant Markov-Gibbs random field (MGRF) of voxel intensities with pairwise interaction analytically identified from a set of training nodules; modeling a second subsequent set of nodules in image data by estimating a linear combination of discrete Gaussians; and integrating both models to guide the evolution of the deformable model to determine and monitor the boundary of each detected nodule in the image data.","['G06T7/0012', 'G06F18/22', 'G06V10/7553', 'G06V10/761', 'G06V20/695', 'G06T2207/30064']"
CN113177616B,"Image classification method, device, equipment and storage medium","The application discloses an image classification method, an image classification device, image classification equipment and a storage medium. The method comprises the following steps: performing image segmentation on the first sample image, and performing feature extraction on each image block obtained by segmentation to obtain an initial image feature set, wherein the initial image feature set comprises initial image features corresponding to each image block, and the first sample image is a sample image which is not subjected to annotation; rearranging and combining initial image features in the initial image feature set to obtain a first image feature set and a second image feature set, wherein the first image features in the first image feature set correspond to the second image features in the second image feature set in different rearrangement combination modes; pre-training an image classification model based on the first image feature set and the second image feature set, wherein the image classification model is used for classifying the content in the image; and fine-tuning the pre-trained image classification model based on a second sample image, wherein the second sample image is an annotated sample image.","['G06V10/764', 'G06F18/241', 'G06F18/214', 'G06F18/253', 'G06N3/08', 'G06V10/774', 'Y02T10/40']"
US9135508B2,Enhanced user eye gaze estimation,"Systems, methods, and computer media for estimating user eye gaze are provided. A plurality of images of a user's eye are acquired. At least one image of at least part of the user's field of view is acquired. At least one gaze target area in the user's field of view is determined based on the plurality of images of the user's eye. An enhanced user eye gaze is then estimated by narrowing a database of eye information and corresponding known gaze lines to a subset of the eye information having gaze lines corresponding to a gaze target area. User eye information derived from the images of the user's eye is then compared with the narrowed subset of the eye information, and an enhanced estimated user eye gaze is identified as the known gaze line of a matching eye image.","['G06V40/197', 'G06K9/00671', 'G06F3/013', 'G06K9/00617', 'G06V20/20', 'G02B27/017', 'G06K9/0061', 'G06V40/193']"
US7860311B2,Video object segmentation method applied for rainy situations,A video object segmentation method takes advantage of edge and color features in conjunction with edge detection and change detection to improve the accuracy of video object segmentation for rainy situations. The video object segmentation method of the present invention includes analyzing HSI-color information of the initially extracted objects to obtain features of the moving object; performing edge detection to obtain edges of the moving object for reducing the effect of confusing raindrops with moving objects in rainy dynamic background; performing object region detection to generate an accurate object mask for solving the uncovered-background problem and the still-object problem; and employing a bounding-box based matching method for solving the reflection problem of the moving object in the rained ground.,"['H04N7/18', 'G06T7/215', 'G06V20/40', 'H04N19/20']"
US12026892B2,Figure-ground neural radiance fields for three-dimensional object category modelling,"Systems and methods for three-dimensional object category modeling can utilize figure-ground neural radiance fields for unsupervised training and inference. For example, the systems and methods can include a foreground model and a background model that can generate an object output based at least in part on one or more learned embeddings. The foreground model and background model may process position data and view direction data in order to output color data and volume density data for a respective position and view direction. Moreover, the object category model may be trained to generate an object output, which may include an instance interpolation, a view synthesis, or a segmentation.","['G06T7/194', 'G06N3/04', 'G06N3/084', 'G06N3/088', 'G06N3/09', 'G06T17/00', 'G06V10/774', 'G06V10/776', 'G06V10/82', 'G06V20/64', 'G06T2207/20081', 'G06T2207/20084', 'G06V20/20']"
CN108520226B,Pedestrian re-identification method based on body decomposition and significance detection,"The invention discloses a pedestrian re-identification method based on body decomposition and significance detection, which comprises the steps of firstly, analyzing a pedestrian image into a region with a Deep Decomposition Network (DDN) in a semantic meaning, separating pedestrians from a cluttered environment by using a sliding window and color method for matching, then, dividing the pedestrian image into small blocks, and automatically selecting an effective picture region according to a background subtraction result and a significance region.","['G06V40/103', 'G06F18/22', 'G06F18/24147', 'G06F18/253', 'G06V10/462']"
CN110287960B,Method for detecting and identifying curve characters in natural scene image,"The invention discloses a method for detecting and identifying curve characters in a natural scene image, which is used for solving the problems of fuzzy boundary and low contrast with a background in curve character identification and improving the detection precision of the curve characters. The method comprises the following main steps of 1) training a curve character detection network based on a Mask RCNN network, detecting natural scene images by using the trained curve character detection network, and detecting character areas in the images; 2) utilizing a correction network to correct the curve characters in the character area into horizontal characters and outputting corrected images; 3) and training the curve character recognition network, extracting the convolution characteristics of the corrected image by using the trained curve character recognition network, decoding the convolution characteristics, and recognizing characters.","['G06F18/213', 'G06F18/253', 'G06V20/63', 'G06V30/1478', 'G06V30/153', 'G06V30/10']"
US11651526B2,Frontal face synthesis from low-resolution images,"An apparatus and corresponding method for frontal face synthesis. The apparatus comprises a decoder that synthesizes a high-resolution (HR) frontal-view (FV) image of a face from received features of a low-resolution (LR) non-frontal-view (NFV) image of the face. The HR FV image is of a higher resolution relative to a lower resolution of the LR NFV image. The decoder includes a main path and an auxiliary path. The auxiliary path produces auxiliary-path features from the received features and feeds the auxiliary-path features produced into the main path for synthesizing the HR FV image. The auxiliary-path features represent a HR NFV image of the face at the higher resolution. As such, an HR identity-preserved frontal face can be synthesized from one or many LR faces with various poses and may be used in types of commercial applications, such as video surveillance.","['G06T11/00', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06N3/094', 'G06V10/764', 'G06V10/82', 'G06V40/161', 'G06V40/168']"
CN110678903B,System and method for analysis of ectopic ossification in 3D images,"Provided herein are systems and methods that facilitate automatic segmentation of a 3D image of a subject to distinguish ectopic ossified (HO) normal bone regions from soft tissue regions. In certain embodiments, the method identifies discrete and distinguishable regions of a 3D image (e.g., CT or micro-CT image) of a subject, which may then be manually or automatically classified as HO or normal bone.","['G06T7/0012', 'G06T7/11', 'A61B6/032', 'A61B6/505', 'A61B6/5217', 'G06F3/04842', 'G06T5/20', 'G06T7/13', 'G06T7/136', 'G06T7/187', 'G06V10/751', 'G06T2207/10028', 'G06T2207/10081', 'G06T2207/20012', 'G06T2207/20152', 'G06T2207/30008']"
US8856108B2,Combining results of image retrieval processes,"A MMR system for newspaper publishing comprises a plurality of mobile devices, an MMR gateway, an MMR matching unit and an MMR publisher. The MMR matching unit receives an image query from the MMR gateway and sends it to one or more of the recognition units to identify a result including a document, the page and the location on the page. The MMR matching unit also includes a result combiner coupled to each of the recognition units to receive recognition results. The result combiner produces a list of most likely results and associated confidence scores. This list of results is sent by the result combiner back to the MMR gateway for presentation on the mobile device. The result combiner uses the quality predictor as an input in deciding which results are best.","['G06F17/30247', 'G06F16/583', 'G06F16/955', 'G06F17/30876', 'G06F18/21', 'G06F18/285', 'G06K9/00463', 'G06K9/6217', 'G06V30/19113', 'G06V30/414', 'H04N1/00244', 'H04N1/00312', 'H04N1/2179', 'H04N1/2191', 'H04N1/00307', 'H04N2201/0039']"
CN114847871A,"Method, system and related product for analyzing fundus variation trend of subject","The present disclosure relates to a method, system, and related product for analyzing a fundus variation trend of a subject. The method comprises the following steps: acquiring a plurality of groups of fundus images of a subject; selecting at least two sets of target fundus images of the subject from the plurality of sets of fundus images; performing a feature matching operation on the at least two groups of target fundus images to obtain difference features; calculating target characteristic parameters according to the at least two groups of target fundus images; and analyzing the fundus variation tendency of the subject based on the difference feature and the target feature parameter. By using the scheme disclosed by the invention, the change trend of the eyeground can be comprehensively analyzed through the difference characteristics and the target characteristic parameters, and an accurate analysis result is obtained.","['A61B3/14', 'A61B3/12', 'G06V10/26', 'G06V40/193', 'G06V40/197']"
CN110910391B,A Double-Module Neural Network Structure Video Object Segmentation Method,"The invention provides a method for dividing a video object with a two-module neural network structure, which is used for solving the problem of non-ideal video object dividing result caused by noise interference in the video object dividing process. The method comprises the following steps: generating an image pair in a transformation network for the first frame map and the mask input for the first frame; generating a target proposal frame for each image pair to determine whether the image pair is an happy region; the happy region adding tracker is input into the happy segmentation network to train a learning model and output; the feature images are output from the convolution of the last layer of the happy segmentation network and are respectively input to a space attention module and a channel attention module; finally, fusing the feature graphs output by the two concerned modules, and outputting a final segmentation mask result through convolution layer operation; the invention obtains better segmentation experimental results on the DAVIS video data set.","['G06T7/10', 'G06T7/207', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/20221', 'Y02T10/40']"
US9589365B2,Method and apparatus for expressing motion object,"A method and an apparatus for expressing a motion object are disclosed. The method includes obtaining a stereo image in which the motion object has been captured, the stereo image including a depth image; extracting a key point from the motion object in the stereo image; determining, based on statistical information relating to three-dimensional motion of pixels within a first predetermined region surrounding the key point, a dominant direction of the key point; determining, based on the dominant direction of the key point, motion vectors of pixels within a second predetermined region surrounding the key point to obtain rotation invariant motion vectors; and extracting, based on the determined motion vectors of the pixels within the second predetermined region surrounding the key point, a feature describing the key point. The present invention can extract features of motion object that are irrelevant to a viewing angle of a camera.","['G06T7/254', 'G06T7/2033', 'G06K9/00348', 'G06K9/4671', 'G06T7/2053', 'G06T7/246', 'G06V10/462', 'G06V40/25', 'G06T2207/10021', 'G06T2207/10028', 'G06T2207/30196']"
CN104537649B,A kind of Vehicular turn judgment method and system compared based on image blur,"The present invention provides a kind of Vehicular turn judgment method compared based on image blur, comprising: picture pick-up device acquires the traveling image at vehicle each moment in real time；It receives traveling image and is divided into a plurality of subgraphs, extract the subgraph composition ROI region of gray scale and color range variation sensitivity；ROI region is symmetrically divided into two regions in left and right and calculates respective image blur；The difference for counting image blur between the left ROI region and right ROI region of at least 300 width traveling image corresponding with friction speed, calculates the mean value and variance of each difference under corresponding speed；By under vehicle present speed between left ROI region and right ROI region under the difference of image blur and the speed each difference mean value and variance, judge whether vehicle is in turn condition and turn direction.Calculation amount of the present invention is small, can carry out turn condition judgement to vehicle real-time, quickly, not limited by day and night condition, especially suitable for rear dress portable navigation product.",['G06T2207/30248']
US11132824B2,"Face image processing method and apparatus, and electronic device","A face image processing method includes: performing face detection on a to-be-processed image, to obtain at least one face region image included in the to-be-processed image and face key point information in the at least one face region image; and processing, for the at least one face region image, an image corresponding to a first region and/or an image corresponding to a second region in the face region image at least according to the face key point information in the face region image, wherein the image corresponding to the second region and the image corresponding to the first region are at least partially non-overlapping.","['G06V40/161', 'G06T11/60', 'G06K9/00241', 'G06K9/00261', 'G06K9/00281', 'G06K9/4609', 'G06T11/001', 'G06T5/002', 'G06T5/70', 'G06V40/164', 'G06V40/165', 'G06V40/167', 'G06V40/171', 'G06V40/193']"
US8831288B1,Digital processing method and system for determination of object occlusion in an image sequence,A method and system for occlusion region detection and measurement between a pair of images are disclosed. A processing device receives a first image and a second image. The processing device estimates a field of motion vectors between the first image and the second image. The processing device motion compensates the first image toward the second image to obtain a motion-compensated image. The processing device compares a plurality of pixel values of the motion-compensated image to a plurality of pixels of the first image to estimate an error field. The processing device inputs the error field to a weighted error cost function to obtain an initial occlusion map. The processing device regularizes the initial occlusion map to obtain a regularized occlusion map.,"['G06K9/00624', 'G06V10/26', 'G06T7/254']"
US20250166385A1,Item identification and tracking system,"A method for acquiring data relating to an object including arranging a multiplicity of cameras to view a scene, at least one reference object within the scene being viewable by at least a plurality of the multiplicity of cameras, each of the plurality of cameras acquiring at least one image of the reference object viewable thereby, finding a point of intersection of light rays illuminating each of the plurality of cameras and correlating a pixel location at which the reference object appears within each the at least one image to the light rays illuminating each of the plurality of cameras and intersecting with the region of intersection, irrespective of a three-dimensional location of the reference object within the scene.","['G06T7/292', 'G06T7/73', 'G06T7/80', 'G06V10/22', 'G06V20/52', 'G06T2207/20081', 'G06T2207/20084']"
AU2007224085B2,Model- based dewarping method and apparatus,"An apparatus and method for processing a captured image and, more particularly, for processing a captured image comprising a document. In one embodiment, an apparatus comprising a camera to capture documents is described. In another embodiment, a method for processing a captured image that includes a document comprises the steps of distinguishing an imaged document from its background, adjusting the captured image to reduce distortions created from use of a camera and properly orienting the document is described.","['G06T5/80', 'G06V10/30', 'G06T3/06', 'G06T5/70', 'G06T7/13', 'G06V30/1444', 'G06V30/1478', 'G06V30/414', 'H04N1/028', 'H04N1/387', 'G06T2207/30176', 'G06V30/10']"
US20210369195A1,Method and system for disease analysis and interpretation,"Optical coherence tomography (OCT) data can be analyzed with neural networks trained on OCT data and known clinical outcomes to make more accurate predictions about the development and progression of retinal diseases, central nervous system disorders, and other conditions. The methods take 2D or 3D OCT data derived from different light source configurations and analyze it with neural networks that are trained on OCT images correlated with known clinical outcomes to identify intensity distributions or patterns indicative of different retina conditions. The methods have greater predictive power than traditional OCT analysis because the invention recognizes that subclinical physical changes affect how light interacts with the tissue matter of the retina, and these intensity changes in the image can be distinguishable by a neural network that has been trained on imaging data of retinas.","['A61B5/4842', 'G06T7/0012', 'A61B3/102', 'A61B3/12', 'A61B5/7275', 'G06N3/08', 'G06T3/40', 'G16H20/40', 'G16H30/40', 'G16H50/20', 'G06T2207/10101', 'G06T2207/20084', 'G06T2207/30041']"
US12067673B2,"Systems, methods, and media for generating visualization of physical environment in artificial reality","In one embodiment, a computing system determines one or more depth measurements associated with a first physical object. The system captures an image including image data associated with the first physical object. The system identifies and associates a plurality of first pixels with a first representative depth value based on the one or more depth measurements. The system determines, for an output pixel of an output image, that (1) a portion of a virtual object is visible from a viewpoint and (2) the output pixel overlaps with a portion of the first physical object. The system determines that the portion of the first physical object is associated with the plurality of first pixels and renders the output image from the viewpoint. Occlusion at the output pixel is determined based on a comparison between the first representative depth value and a depth value associated with the portion of the virtual object.","['G06T19/006', 'G06T15/40', 'G02B27/017', 'G06F3/011', 'G06T15/20', 'G06T5/77', 'G06T7/11', 'G06T2207/10004', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084']"
US9064316B2,Methods of content-based image identification,"A method of identifying one or more image features of an image based on content of the image according to one example embodiment includes receiving an image, correcting a distortion and a tone of the image, segmenting the corrected image and extracting the one or more image features, generating an input image descriptor based on the one or more extracted image features, matching the input image descriptor with a reference image descriptor, and performing an action based on the matched reference image descriptor.","['G06T7/0079', 'G06V30/1478', 'G06K9/3283', 'G06K9/4647', 'G06V30/18095', 'G06V30/10']"
CN113421263B,"Part defect detection method, device, medium and computer program product","The application discloses a method, equipment, a medium and a computer program product for detecting part defects, wherein the method for detecting the part defects comprises the following steps: the method comprises the steps of obtaining an image to be detected corresponding to a part to be detected, predicting defects of the image to be detected based on a full-scale gray scale priori depth segmentation model, and obtaining an image defect detection result, wherein the full-scale gray scale priori depth segmentation model is obtained by performing iterative training optimization on a defect detection model to be trained, which is formed by cascading a preset number of deep neural network modules, based on a training defect image set collected in advance. The defect detection accuracy of the part is low.","['G06T7/0004', 'G06F18/214', 'G06N3/045', 'G06T2207/10116']"
CN107507239B,An image segmentation method and mobile terminal,"The present invention provides a kind of image partition method and mobile terminal, image partition method includes: photographic subjects object, obtains original image corresponding with the target object；The original image is handled, depth image corresponding with the original image is obtained；Based on the depth image, the object edge profile in the depth image is extracted, and obtains the corresponding depth bounds of pixel in the object edge profile；The target pixel points to match with the depth bounds are extracted from the original image, obtain target image.The present invention can be realized simple and convenient image segmentation, improve the convenient degree and accuracy of image segmentation.","['G06T7/50', 'G06T7/11', 'G06T7/13', 'G06T7/194']"
CN110069650B,Searching method and processing equipment,"The application provides a searching method and processing equipment, wherein the method comprises the following steps: extracting an image feature vector of a target image, wherein the image feature vector is used for representing the image content of the target image; and determining the text corresponding to the target image according to the correlation degree between the image feature vector and the text feature vector of the text in the same vector space, wherein the text feature vector is used for representing the semantics of the text. The method solves the problems of low efficiency and high requirement on system processing capacity of the existing recommended text mode, and achieves the technical effect of simply and accurately realizing image marking.","['G06F40/30', 'G06F16/334', 'G06F16/51', 'G06F16/56', 'G06F16/583', 'G06F16/5838', 'G06F16/5846', 'G06F16/5866', 'G06F18/214', 'G06F18/22', 'G06Q30/0625', 'G06V20/20', 'G06V20/35', 'G06V20/70']"
US11790523B2,Autonomous diagnosis of a disorder in a patient from image analysis,"A device receives an input image of a portion of a patient's body, and applies the input image to a feature extraction model, the feature extraction model comprising a trained machine learning model that is configured to generate an output that comprises, for each respective location of a plurality of locations in the input image, an indication that the input image contains an object of interest that is indicative of a presence of a disease state at the respective location. The device applies the output of the feature extraction model to a diagnostic model, the diagnostic model comprising a trained machine learning model that is configured to output a diagnosis of a disease condition in the patient based on the output of the feature extraction model. The device outputs the determined diagnosis of a disease condition in the patient obtained from the diagnostic model.","['G06T7/0012', 'G06F18/2148', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/082', 'G06N3/09', 'G06N3/0985', 'G06V10/454', 'G06V10/82', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30041']"
US9251431B2,"Object-of-interest detection and recognition with split, full-resolution image processing pipeline","Differing embodiments of this disclosure may employ one or all of the several techniques described herein to utilize a “split” image processing pipeline, wherein one part of the “split” image processing pipeline runs an object-of-interest recognition algorithm on scaled down (also referred to herein as “low-resolution”) frames received from a camera of a computing device, while the second part of the “split” image processing pipeline concurrently runs an object-of-interest detector in the background on full resolution (also referred to herein as “high-resolution”) image frames received from the camera. If the object-of-interest detector detects an object-of-interest that can be read, it then crops the object-of-interest out of the “high-resolution” camera buffer, optionally performs a perspective correction, and/or scaling on the object-of-interest to make it the desired size needed by the object-of-interest recognition algorithm, and then sends the scaled, high-resolution representation of the object-of-interest to the object-of-interest recognition algorithm for further processing.","['G06K9/46', 'G06V30/412', 'G06K9/4604', 'G06T5/006', 'G06V30/414', 'H04N23/667', 'H04N5/23245', 'G06K2009/4666', 'G06V10/247', 'G06V10/44']"
CN109360396B,Remote meter reading method and system based on image recognition technology and NB-IoT technology,"The invention provides a remote meter reading method based on an image recognition technology and an NB-IoT technology, which comprises the following steps: the camera collects water meter dial image data according to a preset period; the acquired dial plate image data are sent to a data memory; sequentially carrying out image preprocessing, inclination correction, character wheel area accurate positioning and character wheel area character segmentation on the dial plate image data; recognizing the character of the character wheel area after segmentation by adopting a digital recognition algorithm based on a BP neural network to obtain a dial reading; and sending the dial plate reading to a transparent cloud server by utilizing NB-IoT technology. The invention provides a remote meter reading system based on an image recognition technology and an NB-IoT technology, which comprises an image acquisition module, a data storage module, a data processing module, an image recognition module and a wireless transmission module. According to the invention, the digital image recognition device for the water meter disc surface is additionally arranged on the original water meter, the NB-IoT internet of things technology and the image recognition technology are fused to realize remote meter reading, and the meter reading efficiency and reliability are improved.","['G08C17/02', 'G06F18/214', 'G06V30/153', 'H04N7/183', 'G06V2201/02', 'G06V30/10']"
US9904845B2,Body feature detection and human pose estimation using inner distance shape contexts,"A system, method, and computer program product for estimating human body pose are described. According to one aspect, a human figure silhouette is segmented from a depth image of a human actor. Contour points are sampled along the human figure silhouette. Inner Distance Shape Context (IDSC) descriptors of the sample contour points are determined and compared to IDSC descriptors of the feature points in an IDSC gallery for similarity. For each of the feature points, the sample contour point with the IDSC descriptor that is most similar to an IDSC of the feature point is identified as that feature point in the depth image. An estimated pose of a human model is estimated based on the detected feature points and kinematic constraints of the human model.","['G06V40/103', 'G06K9/00369', 'G06K9/4671', 'G06K9/48', 'G06V10/46']"
CN111556278B,"Video processing method, video display device and storage medium","The application discloses a video processing method applied to the field of artificial intelligence, which is particularly applied to the field of computer vision and can transmit video data based on cloud computing. The method provided by the application comprises the following steps: acquiring continuous M video frames; obtaining an object segmentation result corresponding to the first video frame through an object segmentation model; acquiring a first background image corresponding to a first video frame through a background model; if the object segmentation result corresponding to the first video frame meets the target removal condition, removing the pixel points in the first pixel region from the first video frame; and filling the pixel point set in the first background image into the first pixel area to obtain a first target video frame. The embodiment of the application also discloses a method, a device and a storage medium for displaying the video, which not only can achieve the purpose of eliminating the interference object without sensing, but also can keep the information existing in the background picture of the video and improve the integrity of the video.","['H04N7/147', 'H04N7/15', 'H04N21/2187', 'H04N21/234', 'H04N21/44']"
CN107578035B,Human body contour extraction method based on super-pixel-multi-color space,"The invention provides a human body contour extraction method based on a superpixel-multicolor space, which is improved from the visual angle of a superpixel SP and a multicolor space MCS, and for contour information of one image, the most important difference is that color or brightness information is subjected to sharp change or jump in a certain gradient direction, and the attribute is selected as the characteristic of a region separated by a contour. The invention also provides a human body contour extraction method based on the minimum block distance MBD, which can greatly enhance the accuracy and integrity of contour extraction under a more complex background. Experiments prove that the problems in non-contact human body contour extraction are well solved in the invention, and the human body contour extraction scheme based on the super-pixel-multi-color space provided by the invention has great practical value.","['G06V10/26', 'G06V10/40']"
CN103646232B,Aircraft ground moving target infrared image identification device,"The invention discloses a kind of aircraft ground moving target infrared image identification device, described device includes the interconnection module that Infrared Image Non-uniformity Correction module, image rotation module, image registration module, multiple-stage filtering module, connected component labeling module, target detection realize with feature recognition module, process control module and FPGA.The present invention uses image procossing and target recognition special ASIC/SoC chip, general dsp processor and FPGA processor, complete image procossing and the Target Recognition Algorithms of different levels, improve system in parallel degree, real-time, achieve aircraft ground moving target infrared image recognizer in real time, meanwhile, the power consumption of device is significantly reduced.","['G06V20/00', 'G06T3/14', 'G06T3/60', 'G06V10/955', 'H04N23/20', 'G06T2207/10016']"
AU2009295350B2,Detection of vehicles in an image,"The invention concerns a traffic surveillance system that is used to detect and track vehicles in video taken of a road from a low mounted camera. The inventors have discovered that even in heavily occluded scenes, due to traffic density or the angle of low mounted cameras capturing the images, at least one horizontal edge of the windshield is least likely to be occluded for each individual vehicle in the image. Thus, it is an advantage of the invention that the direct detection of a windshield on its own can be used to detect a vehicle in a single image. Multiple models are projected (206) onto an image with reference to different points in the image. The probability of each point forming part of a windshield is determined based on a correlation of the horizontal edges in the image with the horizontal edges of the windshield model referenced at that point (220). This probability of neighbouring points is used to possible detect a vehicle in the image (224). Aspects of the invention include a method, software and traffic surveillance system.","['G06T7/12', 'G06F18/24155', 'G06T7/149', 'G06V10/255', 'G06V10/764', 'G06V20/54', 'G06T2207/30236', 'G06V10/752', 'G06V10/759', 'G06V2201/08']"
US5410611A,Method for identifying word bounding boxes in text,"A method for determining the boundaries of text or character strings represented in an array of image data by shape, without a requirement for individually detecting and/or identifying the character or characters making up the strings. The method relies upon the detection of connected components within words to first determine text line boundaries and to isolate the connected components into text rows. Subsequently, the structural relationships between the components within and defining rows (i.e. overlap, inter-character spacing, and inter-word spacing), are used to further combine adjacent sets of connected components into words or similar units of semantic understanding within text rows.","['G06V30/146', 'G06V30/10']"
US7583823B2,Method for localizing irises in images using gradients and textures,"A method for extracting an iris from an image is presented. Edges of an iris are detected in an image. Texture from the image is acquired. The edges and texture are combined to generate an inner boundary and an outer boundary of the iris. Also, to improve the accuracy of the iris boundary detection, a method to select between ellipse and circle models is provided. Finally, a dome model is used to determine mask images and remove eyelid occlusions in unwrapped images.",['G06V40/193']
US11145059B2,Medical scan viewing system with enhanced training and methods for use therewith,"A multi-label generating system is configured to: store a first plurality of medical scans with corresponding global labels and a second plurality of medical scans with corresponding region labels, wherein the global labels each correspond to one of a set of abnormality classes and wherein each of the region labels correspond to one of the set of abnormality classes; generate a computer vision model by training on the first plurality of medical scans with the corresponding global labels and the second plurality of medical scans with the corresponding region labels; receive a new medical scan; generate global probability data based on the computer vision model, wherein the global probability data indicates a set of global probability values corresponding to the set of abnormality classes, and wherein each of the set of global probability values indicates a probability that a corresponding one of the set of abnormality classes is present in the new medical scan; and transmit the global probability data to a client device for display via a display device.","['G06T7/0012', 'G06F18/214', 'G06K9/3233', 'G06K9/6256', 'G06N3/045', 'G06N3/0464', 'G06N3/088', 'G06N3/0895', 'G06N3/09', 'G06N7/005', 'G06N7/01', 'G06T7/187', 'G06V10/25', 'G06V10/764', 'G06V10/774', 'G06T2200/04', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10104', 'G06T2207/10116', 'G06T2207/10132', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30061', 'G06T2207/30064', 'G06T2207/30096', 'G06V2201/03']"
US9483835B2,Depth value restoration method and system,"Disclosed is a method comprising steps of conducting image preprocessing so as to respectively obtain a candidate object region including a foreground image in a depth map and a color image; determining whether it is necessary to conduct a region growing process with respect to the candidate object region of the depth map; in the former case, conducting the region growing process with respect to the candidate object region of the depth map; and conducting, after the region growing process is conducted with respect to the candidate object region of the depth map, a depth value restoration process with respect to a candidate region.","['H04N13/128', 'G06T7/0097', 'G06T5/70', 'G06F3/017', 'G06T5/002', 'G06T5/50', 'G06T7/0081', 'G06T7/11', 'G06T7/174', 'G06T7/187', 'G06T7/194', 'H04N13/0022', 'G06T2200/04', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20141', 'G06T2207/20144', 'G06T2207/30196']"
US11475246B2,System and method for generating training data for computer vision systems based on image segmentation,"A system and method for training a model using a training dataset. The training dataset can be made up of only real data, only synthetic data, or any combination of synthetic data and real data. The images are segmented to define objects with known labels. The object is pasted onto backgrounds to generated synthetic datasets. The various aspects of the invention include generation of data that is used to supplement or augment real data. Labels or attributes can be automatically added to the data as it is generated. The data can be generated using seed data. The data can be generated using synthetic data. The data can be generated from any source, including the user's thoughts or memory. Using the training dataset, various domain adaptation models can be trained.","['G06K9/6257', 'G06N3/08', 'G06F18/214', 'G06F18/2148', 'G06F18/217', 'G06F18/2185', 'G06K9/6256', 'G06K9/6262', 'G06K9/6264', 'G06N20/00', 'G06N3/044', 'G06N3/0445', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/0895', 'G06N3/09', 'G06N3/091', 'G06N3/094', 'G06N3/096', 'G06N3/0985', 'G06T7/11', 'G06V10/764', 'G06V10/7788', 'G06V10/82']"
CN111626290B,Infrared ship target detection and identification method under complex sea surface environment,"An infrared ship target detection and identification method under a complex sea surface environment comprises the following steps: step 1: small target image screening is carried out on the collected infrared images through a target identification detection method, and the areas of the high-speed targets and the ships are separated and pre-classified; step 2: extracting and establishing an infrared image feature set, screening out an effective feature set, and constructing a classical machine learning model based on a support vector machine to realize target ship identification; step 3: establishing a deep learning model based on a convolutional neural network for identifying ships in infrared images; step 4: and carrying out decision-level fusion on the recognition result of the deep learning and the classification result of the machine learning, so as to realize more accurate target recognition.","['G06V10/25', 'G06F18/254', 'G06V10/267', 'Y02A90/10']"
CN107203973B,Sub-pixel positioning method for center line laser of three-dimensional laser scanning system,"The invention discloses a sub-pixel positioning method of a line laser center in a three-dimensional laser scanning system, which comprises the following steps: firstly, acquiring original image information scanned by laser from a camera, and preprocessing the original image to eliminate noise in the image; carrying out threshold segmentation on the image without the noise, and carrying out coarse extraction on the linear laser; eliminating the burr phenomenon of the line laser after the rough extraction, and further reducing the width of the line laser; and obtaining a sub-pixel level coordinate of the line laser center by using an improved gravity center method, and correcting a pseudo target pixel in the sub-pixel level coordinate by using Hough transformation. The positioning method combines an improved gravity center method and Hough transformation, and can meet the requirement that a three-dimensional laser scanning system accurately acquires the sub-pixel coordinates of the line laser center in real time, so that the sub-pixel accurate positioning of the line laser center is realized.","['G06T5/70', 'G01B11/24', 'G06T2207/20032']"
US9542752B2,Document image compression method and its application in document authentication,"A method for compressing a bi-level document image containing text is disclosed. The document image is segmented into symbol images each representing a letter, numeral, etc. in the document. The symbol images are classified into a plurality of classes, each class being associated with a template image and a class index. Classification is done by comparing each symbol to be classified with template of existing classes, using a number of image features including zoning profiles, side profiles, topology statistics, and low-order image moments. These image features are compared using a tolerance based method to determine whether the symbol matches the template. After classification, certain classes that have few symbols classified into them may be merged with other classes. In addition, the template images of the classes are down-sampled, where the final sizes of the template images are dependent on the likelihood of confusion of the template with other templates.","['G06V30/15', 'G06T7/0081', 'G06F18/28', 'G06K9/00456', 'G06K9/00577', 'G06K9/18', 'G06K9/342', 'G06K9/6202', 'G06K9/6255', 'G06T7/11', 'G06V20/80', 'G06V30/1914', 'G06V30/224', 'G06V30/413', 'G07D7/0026', 'G07D7/004', 'G07D7/0043', 'G07D7/20', 'G07D7/2033', 'G07D7/2041', 'G07D7/2058', 'G07D7/206', 'G07D7/2083', 'H04N1/40062', 'G06F18/22', 'G06K2209/01', 'G06K9/6201', 'G06T2207/10004', 'G06T2207/30176', 'G06V30/10']"
CN103052937B,For adjusting the method and system of displaying contents,"The invention provides the method and system for detecting the grasping event on touch-screen display and the position adjustment displayed content based on described grasping event.Identify the content that grasping event covers and by adjustment displayed content layout, described content shifted out below described grasping event.Can determine that the region of described grasping event is to allow to adjust the layout of displayed content.Alarm can be implemented with the warning user when the content that grasping event covers exists change.Grasping event profile can be estimated, and can be that compact profile configuration is located and display menu icon with profile estimated by described grasping event.Suggestion can be carried out based on the touch custom of user or displaying contents layout and grasp event location.","['G06F3/0488', 'G06F1/1626', 'G06F3/041', 'G06F3/04817', 'G09G5/00', 'G09G2340/14', 'G09G2354/00']"
US7587412B2,Mixed media reality brokerage network and methods of use,"A Mixed Media Reality (MMR) system associated techniques are disclosed. In one particular embodiment, the MMR system includes a MMR brokerage network having a customer, an MMR broker, an MMR service bureau and an MMR clearinghouse. The MMR brokerage network allows these entities to interact to provide a unified point of business access for the customer who wants to add MMR functionality to a document. Furthermore, the MMR system of the present invention facilitates business methods that take advantage of the combination of a portable electronic device, such as a cellular camera phone, and a paper document. The present invention also includes a number of novel methods including: a method for operation of a MMR brokerage network, a method for layout independent MMR recognition, a strip fragment candidate generation process, and a page candidate accumulation process.","['G06Q30/00', 'G06F16/434', 'Y10S707/99934', 'Y10S707/99936', 'Y10S707/99943']"
CN112630222B,Mobile phone cover plate glass defect detection method based on machine vision,"The invention discloses a mobile phone glass cover plate defect detection method based on machine vision, which comprises the following steps: collecting an image of the surface of the cover plate glass of the mobile phone; carrying out geometric distortion correction on the collected image on the surface of the cover plate glass of the mobile phone; denoising the image after the distortion correction; performing edge enhancement on the image subjected to denoising processing; extracting ROI (region of interest) areas and positioning template matching on the images subjected to edge enhancement; extracting defects in the extracted ROI area, and qualitatively judging different defects according to seven different vectors; counting the information and the number of various defects; the invention can quickly and accurately extract the areas to be detected of various types of mobile phones and accurately detect and identify defects such as linear scratch, point scratch, dirt, bright white dots, broken filament foreign matters and the like.","['G01N21/8851', 'G06T5/70', 'G06T5/80', 'G06T5/90', 'G06T7/0004', 'G06T7/11', 'G06T7/13', 'G06T7/136', 'G06T7/344', 'G06T7/62', 'G01N2021/8887', 'G06T2207/30168']"
CN114078108B,"Method and device for processing abnormal region in image, and method and device for dividing image","The disclosure relates to a method and a device for processing an abnormal region in an image, and relates to the technical field of image processing. The method comprises the following steps: dividing a plurality of areas to be processed comprising the areas to be detected aiming at the areas to be detected formed by any one or a plurality of pixels in the image to be processed; according to pixel values in a preset range outside each region to be processed, calculating each predicted pixel value of the region to be detected by using a first machine learning model; calculating prediction error distribution corresponding to each prediction pixel value according to the original pixel value of the region to be detected, and taking the prediction error distribution as first error distribution; and judging whether the region to be detected belongs to an abnormal region in the image to be processed according to the first error distribution.","['G06T7/0012', 'G06T7/0002', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/0475', 'G06N3/088', 'G06N3/0895', 'G06N3/094', 'G06T5/60', 'G06T5/77', 'G06T7/0004', 'G06T7/11', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084']"
US20210370993A1,Computer vision based real-time pixel-level railroad track components detection system,"Systems, methods and devices for a computer vision-based pixel-level rail components detection system using an improved one-stage instance segmentation model and prior knowledge, aiming to inspect railway components in a rapid, accurate, and convenient fashion.","['G06T7/0008', 'B61L23/042', 'B61L23/044', 'B61L23/045', 'G06N3/045', 'G06N3/08', 'G06T7/75', 'G06V10/454', 'G06V10/82', 'G06V20/56', 'B61L23/041', 'G06N3/044', 'G06N3/048', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30108']"
CN110310275B,Chain conveyor belt defect detection method based on image processing,"The invention discloses a defect detection method of a chain conveyor belt based on image processing, which relates to the technical field of cigarette production, and comprises the steps of arranging a camera frame on the side surface of the chain conveyor belt to be detected, acquiring a real-time image of the chain conveyor belt by the camera, carrying out scale scaling on the image by a Gaussian scale pyramid, positioning the image subjected to scale scaling by using a pre-constructed multi-angle rotating model to obtain a chain conveyor belt area, converting the area of the chain conveyor belt into a gray scale image by differential multi-core Gaussian filtering, enhancing the detection area of the chain conveyor belt by using index change, carrying out image segmentation on the enhanced image, selecting and counting the characteristics of the defect-free chain conveyor belt, and finally comparing the characteristics of the defect-free chain conveyor belt with the corresponding characteristics of the chain conveyor belt in the image acquired in real time according to the characteristics of the defect-free chain conveyor belt counted in advance, and judging whether the chain type conveyor belt has defects.","['G06T3/40', 'G06T5/70', 'G06T5/94', 'G06T7/0004', 'G06T7/10', 'G06T7/70']"
US12177598B2,Vehicle undercarriage imaging system,"Devices and methods for capturing vehicle undercarriage images are described. In some instances, a mirror assembly may be used to reflect images of portions of a vehicle undercarriage into a field of view of a camera to be captured, e.g., as a vehicle passes over the mirror assembly. Composite images may be reconstructed from the reflected portions of the vehicle undercarriage, and analysis may be performed on those reconstructed, composite images to identify features in the composite vehicle undercarriage images.","['H04N5/2625', 'H04N23/55', 'H04N23/555']"
CN108875827B,Method and system for classifying fine-grained images,"The invention discloses a method and a system for classifying fine-grained images, wherein the method comprises the following steps: and step S1, performing feature extraction on the input picture to obtain a feature map. Step S2, making statistics on the relevance between the category label and the attribute in the data set to construct a knowledge graph; step S3, performing feature expression on the constructed knowledge graph by using the GGNN, and iteratively updating the knowledge graph to obtain feature expression of the knowledge graph; and step S4, fusing the feature map extracted in the step S1 with the advanced knowledge obtained by the GGNN in the step S3, and guiding network classification by combining the advanced knowledge with the feature map.","['G06F18/241', 'G06F18/217', 'G06N3/02', 'G06V10/40']"
US10453197B1,Object counting and instance segmentation using neural network architectures with image-level supervision,This disclosure relates to improved techniques for performing computer vision functions including common object counting and instance segmentation. The techniques described herein utilize a neural network architecture to perform these functions. The neural network architecture can be trained using image-level supervision techniques that utilize a loss function to jointly train an image classification branch and a density branch of the neural network architecture. The neural network architecture constructs per-category density maps that can be used to generate analysis information comprising global object counts and locations of objects in images.,"['G06T7/11', 'G06N3/045', 'G06N3/0464', 'G06N3/048', 'G06N3/0481', 'G06N3/084', 'G06N3/0895', 'G06N3/09', 'G06T7/10', 'G06T7/70', 'G06V10/255', 'G06V10/267', 'G06V10/454', 'G06V10/82', 'G06V20/00', 'G06V20/10', 'G06V20/20', 'G06V20/70', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20112', 'G06T2207/30242']"
CN111784754B,"Tooth orthodontic method, device, equipment and storage medium based on computer vision","The invention discloses a tooth orthodontic method and device based on computer vision, computer equipment and a storage medium, wherein the method comprises the following steps: receiving an intra-oral image acquired by a mobile terminal, estimating pose information, and optimizing the moving range of the mobile terminal based on the pose information; generating a dense point cloud based on the pose information and the image; inputting the image data into a tooth segmentation model to obtain a tooth mask; based on the tooth mask, changing the pixel depth value outside the mask in the depth map into 0; projecting the tooth mask onto the dense point cloud to obtain a tooth point cloud model; filtering the tooth point cloud model, and generating a refined tooth model through the curved surface relation of the dental arch optimized point cloud; and carrying out point cloud registration on the refined tooth model and the treatment scheme model to obtain rotation and translation values of each tooth in space. The invention ensures the high precision of the overall extracted tooth model, and simultaneously, the patient can finish scanning at home by using the own mobile phone to finish real-time monitoring of the orthodontic effect of the teeth.","['G06T17/00', 'A61C7/002', 'G06N3/045', 'G06T7/33', 'G06T2207/10081']"
US10482603B1,Medical image segmentation using an integrated edge guidance module and object segmentation network,This disclosure relates to improved techniques for performing image segmentation functions using neural network architectures. The neural network architecture integrates an edge guidance module and object segmentation network into a single framework for detecting target objects and performing segmentation functions. The neural network architecture can be trained to generate edge-attention representations that preserve the edge information included in images. The neural network architecture can be trained to generate multi-scale feature information that preserves and enhances object-level feature information included in images. The edge-attention representations and multi-scale feature information can be fused to generate segmentation results that identify target object boundaries with increased accuracy.,"['G06T7/11', 'G06T7/12', 'G06F17/15', 'G06N3/04', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/0895', 'G06N3/09', 'G06T7/0012', 'G06N3/088', 'G06T2207/10072', 'G06T2207/10116', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004', 'G06T2207/30041', 'G06T2207/30061', 'G06T7/143', 'G06V10/44']"
US11302012B2,Systems and methods for transparent object segmentation using polarization cues,"A computer-implemented method for computing a prediction on images of a scene includes: receiving one or more polarization raw frames of a scene, the polarization raw frames being captured with a polarizing filter at a different linear polarization angle; extracting one or more first tensors in one or more polarization representation spaces from the polarization raw frames; and computing a prediction regarding one or more optically challenging objects in the scene based on the one or more first tensors in the one or more polarization representation spaces.","['G06T7/11', 'B25J9/1697', 'G05B13/027', 'G06F18/214', 'G06F18/23', 'G06F18/253', 'G06K9/6256', 'G06K9/629', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/09', 'G06N3/096', 'G06T3/02', 'G06T7/174', 'G06V10/147', 'G06V10/26', 'G06V10/40', 'G06V10/454', 'G06V10/56', 'G06V10/60', 'G06V10/764', 'G06V10/7715', 'G06V10/774', 'G06V10/82', 'G06V20/50', 'G06V20/60', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084']"
US10430946B1,Medical image segmentation and severity grading using neural network architectures with semi-supervised learning techniques,"This disclosure relates to improved techniques for performing computer vision functions on medical images, including object segmentation functions for identifying medical objects in the medical images and grading functions for determining severity labels for medical conditions exhibited in the medical images. The techniques described herein utilize a neural network architecture to perform these and other functions. The neural network architecture can be trained, at least in part, using semi-supervised learning techniques that enable the neural network architecture to accurately perform the object segmentation and grading functions despite limited availability of pixel-level annotation information.","['A61B5/02007', 'A61B5/7267', 'A61B5/7282', 'G06F18/2148', 'G06F18/2155', 'G06F18/2178', 'G06F18/2431', 'G06F18/2453', 'G06F18/41', 'G06K9/6257', 'G06K9/6259', 'G06K9/6263', 'G06K9/628', 'G06N20/00', 'G06N3/02', 'G06N3/04', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/08', 'G06N3/088', 'G06N3/0895', 'G06N3/09', 'G06N3/094', 'G06T7/0012', 'G06T7/11', 'G06V10/26', 'G06V10/454', 'G06V10/462', 'G06V10/764', 'G06V10/7753', 'G06V10/82', 'G06V20/695', 'G16H30/40', 'G16H50/20', 'A61B2576/00', 'A61B5/02028', 'A61B5/026', 'A61B5/055', 'A61B5/4064', 'A61B5/4504', 'A61B5/7275', 'G06K2209/05', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004', 'G06T2207/30008', 'G06T2207/30041', 'G06T2207/30048', 'G06T2207/30096', 'G06V2201/03']"
US9197789B2,"Method and system for removal of fog, mist, or haze from images and videos","A method of removing fog from the images/videos independent of the density or amount of the fog and free of user intervention and a system for carrying out such method of fog removal from images/videos are disclosed. The removal of fog from images and video involve airlight estimation and airlight map refinement based restoration of foggy images and videos. Advantageously, removal of fog from images and videos of this invention would require less execution time and yet achieve high perceptual image quality with reduced noise and enhanced contrast. The proposed method is adapted for RGB Color model and advantageously also for HSI color model involving reduced computational requirements and be user friendly and supposed to have wide application and use.","['H04N1/6027', 'G06T5/002', 'G06T5/009', 'G06T5/40', 'G06T5/70', 'G06T5/92', 'G06K9/00', 'G06T2207/10004', 'G06T2207/10016', 'G06T2207/20012', 'G06T2207/20028', 'G06T2207/20192']"
US8417060B2,Methods for multi-point descriptors for image registrations,"Methods for generating an image mosaic are provided. In one respect, pixels saliency of a first image and a second image are determined. One salient pixel may be selected from the determined pixels saliency group of the first image and one salient pixel may be selected from the determined pixels saliency group of the second image. A mosaicking technique of the first and second image may be performed if the one salient pixel of the first image and the one salient pixel of the second image are registered successfully.","['G06T3/4038', 'G06T7/33', 'G06V10/462', 'G06V10/16', 'G06V2201/133']"
AU2024219514A1,Computer vision systems and methods for detecting and modeling features of structures in images,"DCC -28 Abstract A computer vision system for detecting and modeling features of a building in a plurality of images, comprising: 5 at least one computer system in communication with a database of aerial imagery; and computer vision system code executed by said at least one computer system, said computer vision system code causing said computer system to: receive a plurality of images, each of said images including a view of a 10 structure; process the plurality of images to detect a contour of the structure; process the contour of the structure to infer interior roof lines from the structure; and generate a model of the roof of the structure using the detected contour and 15 the interior roof lines, wherein step of processing the plurality of images comprises processing the plurality of images to identify a plurality of two-dimensional (2D) line segments in each image, and wherein the computer vision system code causes said computer system to group the 20 2D lines into groups sharing a level-line angle.","['G06T7/564', 'G06F16/532', 'G06T7/12', 'G06T7/536', 'G06T2207/10032', 'G06T2207/20044']"
CN112184705B,"Human body acupuncture point identification, positioning and application system based on computer vision technology","The invention discloses a human body acupuncture point identification, positioning and application system based on a computer vision technology, which greatly improves the efficiency and the accuracy of acupuncture point identification by constructing an acupuncture point and disease information database, an acupuncture point identification and positioning algorithm module and an interactive processing program, helps a user to easily find acupuncture points, applies a depth learning technology and a binocular stereo imaging technology to identify acupuncture point names and corresponding positions in two-dimensional image characteristics on the surface of a human body, creatively designs an automatic labeling model based on depth learning by using a mode of carrying out original labeling on images, and provides a large amount of accurate data bases for the establishment and the training of an acupuncture point identification model based on depth learning. Meanwhile, the binocular stereo imaging technology is used for providing image three-dimensional space information for the acupuncture point recognition model based on deep learning, the accuracy of positioning of the acupuncture point recognition model is improved, a user can easily search acupuncture points, and the traditional Chinese medicine acupuncture point theory is favorable for popularization and application.","['G06T7/0012', 'G06N3/045', 'G06N3/084', 'G06T7/13', 'G06T7/136', 'G06V10/25', 'G06V10/44', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196']"
US20210286998A1,Field segmentation and classification,"Implementations relate to improved crop field segmentation and crop classification in which boundaries between crop fields are more accurately detected. In various implementations, high-elevation image(s) that capture an area containing multiple demarcated fields may be applied as input across one or more machine learning models to generate a boundary enhancement channel. Each pixel of the boundary enhancement channel may be spatially aligned with a corresponding pixel of the one or more high-elevation images. Moreover, each pixel of the boundary enhancement channel may be classified with a unit angle to a reference location of the field of the multiple demarcated fields that contains the pixel. Based on the boundary enhancement channel, pixel-wise field memberships of pixels of the one or more high-elevation images in the multiple demarcated fields may be determined.","['G06K9/00657', 'G06V20/188', 'G06F18/23', 'G06F18/24', 'G06K9/6267', 'G06N20/00', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'G06T17/05', 'G06T7/12', 'G06V10/762', 'G06V10/764', 'G06V10/82', 'G06T2207/10032', 'G06T2207/20084', 'G06T2207/30188', 'G06V20/194']"
US10839510B2,Methods and systems for human tissue analysis using shearlet transforms,"Various arrangements for identifying and grading cancer in tissue samples are presented. A digital image of a stained tissue sample may be acquired. A Shearlet transform may be performed on the digital image of the stained tissue sample. Shearlet coefficients may be calculated based on the performed Shearlet transform of the normalized digital RGB image of the stained tissue sample. A trained neural network may be applied to create a plurality of feature maps using the digital image and Shearlet coefficients, wherein the trained neural network was trained using a plurality of images and Shearlet coefficients of a plurality of digital images. A classifier may be applied to an output of the trained neural network to identify whether cancer is present in the stained tissue sample. A notification may be output that is indicative of a grade of detected cancer in the sample.","['G06T7/0012', 'G06F18/2411', 'G06K9/4604', 'G06K9/6269', 'G06T7/42', 'G06V10/454', 'G06V10/52', 'G06V10/764', 'G06K9/527', 'G06T2207/10024', 'G06T2207/20048', 'G06T2207/20064', 'G06T2207/30024', 'G06T2207/30068', 'G06T2207/30081', 'G06T2207/30096']"
CN111563889B,Liquid crystal screen Mura defect detection method based on computer vision,"The invention discloses a liquid crystal screen Mura defect detection method based on computer vision, which comprises the following steps: image acquisition under inclined view angle, extraction of an interested region based on a fixed threshold method, view angle transformation based on a perspective transformation method, image texture filtering based on a Gabor filter, image color space conversion, mura defect segmentation based on a sliding window scanning mode of dynamic change size and a double mechanism of a self-adaptive double-threshold segmentation algorithm, and secondary screening of a defect segmentation region. The method for detecting the Mura defect of the liquid crystal screen based on computer vision can solve the problems of missed detection and lack of consistency of detection results caused by visual fatigue easily generated when the Mura defect is detected manually for a long time, improves the detection efficiency of the Mura defect, and ensures the consistency of the detection results.","['G06T7/0004', 'G06T7/11', 'G06T7/136', 'G06T2207/10004', 'G06T2207/20024', 'G06T2207/30121', 'Y02P90/30']"
US9886430B2,Entity based content selection,"Methods, systems, apparatuses, and computer program products are provided for selecting content on a displayed page. A selection mode may be initiated with regard to a displayed document that includes content. The initiation of the selection mode may be detected. Multiple entities in the displayed document are determined, with each entity including a portion of the content of the document. The displayed document is annotated to indicate the determined entities. Content selected by a user in the displayed document may be detected. At least one entity is determined to be associated with the selected content. The determined associated entity/entities are indicated in the displayed document as active entities. The user is enabled to refine the set of active entities. An action is enabled to be performed on the active entities.","['G06F17/241', 'G06F40/169', 'G06F16/957', 'G06F17/278', 'G06F17/30899', 'G06F40/295']"
US20210004589A1,Scene and user-input context aided visual search,"Provided is a technique for determining a context of an image and an object depicted by the image based on the context. A trained context classification model may determine a context of an image, and a trained object recognition model may determine an object depicted by the image based on the image and the context. Provided is also a technique for determining an object depicted within an image based on an input location of an input detected by a display screen. An object depicted within an image may be detected based on a distance in feature space between an image feature vector of an image and a feature vector of the object, and a distance in pixel-space between an input location of an input and location of the object within the image.","['G06K9/00624', 'G06V10/82', 'G06F18/254', 'G06K9/66', 'G06T5/00', 'G06V10/809', 'G06V30/19173', 'G06F3/013', 'G06F3/017', 'G06F3/041', 'G06F3/0488']"
US9953421B2,"Device for determining disappearing direction and method thereof, apparatus for video camera calibration and method thereof","A disappearing direction determination device and method, a video camera calibration apparatus and method, a video camera and a computer program product are provided. The device comprises: a moving target detecting unit for detecting in the video image a moving target area where a moving object locates; a feature point extracting unit for extracting at least one feature point on the moving object in the detected moving target area; a moving trajectory obtaining unit for tracking a movement of the feature point in a predetermined number of video image frames to obtain a movement trajectory of the feature point; and a disappearing direction determining unit for determining, according to the movement trajectories of one or more moving objects in the video image, a disappearing direction pointed by a major moving direction of the moving objects. Thus, a disappearing direction and video camera gesture parameters can be determined accurately.","['G06T7/0018', 'G06T3/0031', 'G06T3/06', 'G06T7/246', 'G06T7/70', 'G06T7/80', 'G06T2207/20021', 'G06T2207/20076', 'G06T2207/30232', 'G06T2207/30241']"
CN118072024B,Fusion convolution self-adaptive network skin lesion segmentation method,"The invention discloses a fusion convolution self-adaptive network skin lesion segmentation method, which relates to the technical field of medical image segmentation and comprises the following steps: data preprocessing, using linear interpolation to reshape the skin lesion image size to 256×256; the model design is used for dividing the image, taking a U-net network model as a framework, wherein the U-net network model comprises an encoder and a decoder, the encoder is used for downsampling the image, the downsampling is the process of rolling and maximally pooling the image, and the characteristic images after upsampling at the corresponding stages of the decoder are spliced through jump connection; model training, through step two training, has the integrality of little target segmentation and handles the complex boundary of pathological change region ability, the encoder portion receives the fused convolution module of size 256 x 256 images, predicts the segmentation map, better shows the segmentation map, has effectively caught the characteristic letter of deep semantic and shallow semantic in the medical image, can concentrate on the boundary of complicated pathological change region more.","['G06V10/26', 'G06N3/0455', 'G06N3/0464', 'G06N3/084', 'G06N3/0985', 'G06V10/42', 'G06V10/454', 'G06V10/52', 'G06V10/806', 'G06V10/82', 'G06V2201/03']"
US6691126B1,Method and apparatus for locating multi-region objects in an image or video database,"A method (and system) for specifying the region layout of objects in an affine invariant manner as a set of affine intervals between pairs of regions, includes representing database and query regions using affine intervals along with their region identity, matching query region layout to layout of database image regions using an index structure, and retrieving relevant images of the database by hashing for dominant hit regions in the index structure.","['G06F16/583', 'Y10S707/914', 'Y10S707/915', 'Y10S707/955', 'Y10S707/957', 'Y10S707/99943']"
CN105844649A,"Statistical method, apparatus and system for the quantity of people","The invention discloses a statistical method, apparatus and system for the quantity of people. The method is performed through the following steps: first, subtracting a to-be-analyzed-image from a preset image to obtain a foreground image, which effectively excludes the interference of background objects in the to-be-analyzed image to the statistical efforts on the quantity of people; then, carrying out threshold segmentation to the foreground image; conducting profile extraction to the obtained binary image; removing defective profiles through the analysis on the profile size, the profile shape and/or the length-width ratio of the profiles; and gathering the number of effective profiles, or rather, the quantity of people in a statistical image. Through this method, the population of people in a specific scenario can be automatically counted. Compared to the prior art using the manual way to count, the statistical method, apparatus and system provided by the invention can count people through a simple process while taking a shorter time.","['G06T7/0002', 'G06F18/23213', 'G06T5/10', 'G06T2207/10016', 'G06T2207/20036', 'G06T2207/30242']"
US8194936B2,Optimal registration of multiple deformed images using a physical model of the imaging distortion,"Methods and systems for image registration implementing a feature-based strategy that uses a retinal vessel network to identify features, uses an affine registration model estimated using feature correspondences, and corrects radial distortion to minimize the overall registration error. Also provided are methods and systems for retinal atlas generation. Further provided are methods and systems for testing registration methods.","['G06T7/33', 'G06V10/754', 'G06V40/197', 'G06T2207/30041']"
WO2022141962A1,"Invasion detection method and apparatus, device, storage medium, and program product","An invasion detection method and apparatus, a device, a storage medium, and a program product. The method comprises: obtaining an image to be processed from a video stream to be processed (S101); detecting objects in said, and obtaining at least one object detection box (S102); determining whether a preset invading object is present in the object detection box (S103); when it is determined that the preset invading object is present in the object detection box, performing recognition on said image, and obtaining an invasion detection area (S104); and determining, on the basis of the location of the preset invading object and the invasion detection area, whether an invasion event has occurred (S105).",['G06F18/00']
CN110261436B,Track fault detection method and system based on infrared thermal imaging and computer vision,"The invention discloses a track fault detection method and a system based on infrared thermal imaging and computer vision, wherein the method comprises the following steps: the unmanned aerial vehicle acquires images of the tramcar track; the ground station receives the image data of the high-definition camera and carries out image preprocessing; performing multi-threshold track area segmentation twice on a darker area in the groove and a lighter area outside the groove, segmenting a track area according to adjacent distance characteristics of the lighter and darker area, and extracting to obtain a track image; graying the infrared thermograph, and extracting a high-temperature area on the track by adopting a relative temperature difference method; superposing the preprocessed image with a track detection window, masking to obtain an interested area, performing edge closing judgment and filling on the interested area to obtain a communicated area, and screening the communicated area to obtain suspected track foreign matters; and inputting suspected track foreign matters into a BP neural network for identification to obtain a foreign matter classification result. The invention can identify foreign matters and detect temperature of the rail in real time, reduce accident rate of rail transit and improve the running safety of the electric car.","['G01N25/72', 'G06T7/11', 'G06T7/13', 'G06T7/136', 'G06T2207/10048', 'G06T2207/20081', 'G06T2207/20084']"
US20240161281A1,Neural network for image registration and image segmentation trained using a registration simulator,"Apparatuses, systems, and techniques to perform registration among images. In at least one embodiment, one or more neural networks are trained to indicate registration of features in common among at least two images by generating a first correspondence by simulating a registration process of registering an image and applying the at least two images and the first correspondence to a neural network to derive a second correspondence of the features in common among the at least two images.","['G06T7/32', 'G06T7/0012', 'G06F30/20', 'G06N3/08', 'G06T7/11', 'G06T7/344', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004']"
CN111862194B,Deep learning plant growth model analysis method and system based on computer vision,"The invention relates to the technical field of precision agricultural planting, and particularly discloses a deep learning plant growth model analysis method based on computer vision, which comprises the following steps: acquiring plant growth environment data and plant images in different growth periods in real time, and extracting images of the same individual plant in different growth periods from the acquired plant images; calculating the areas of plant leaves in the images of the same individual plant in different growth periods, wherein the areas of the plant leaves calculated in different growth periods are plant growth states at corresponding moments; establishing a mapping relation model of the plant growth state and the growth environment data thereof at the corresponding moment; and calculating the weight values of different growth environment factors of the plant according to the mapping relation model. The invention also discloses a deep learning plant growth model analysis system based on computer vision. The plant growth model analysis method provided by the invention can monitor the plant growth state all the day, so as to further quantify the influence degree of the plant growth environment on the plant growth.","['G06T7/62', 'G06F18/2321', 'G06N3/045', 'G06N3/08', 'G06T5/30', 'G06T5/73', 'G06T7/12', 'G06T7/13', 'G06T7/90', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30188']"
US11257204B2,Detailed damage determination with image segmentation,"The present invention relates to the determination of damage to portions of a vehicle. More particularly, the present invention relates to determining whether each part of a vehicle should be classified as damaged or undamaged and optionally the severity of the damage to each part of the damaged vehicle including segmenting the input images.","['G06Q10/20', 'G06F16/24578', 'G06F18/214', 'G06F18/2148', 'G06F18/231', 'G06F18/24', 'G06F18/2415', 'G06F18/2431', 'G06F18/24317', 'G06F18/285', 'G06F40/20', 'G06K9/00664', 'G06K9/2054', 'G06K9/3241', 'G06K9/6219', 'G06K9/6227', 'G06K9/6256', 'G06K9/6257', 'G06K9/6267', 'G06K9/6277', 'G06K9/628', 'G06K9/6281', 'G06N20/00', 'G06N20/20', 'G06N3/04', 'G06N3/044', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/049', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06N3/096', 'G06N5/01', 'G06Q10/06313', 'G06Q10/0875', 'G06Q30/0283', 'G06T7/0002', 'G06T7/0004', 'G06T7/11', 'G06V10/22', 'G06V10/225', 'G06V10/25', 'G06V10/255', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/10', 'G06K2209/23', 'G06K2209/27', 'G06N5/046', 'G06Q30/016', 'G06Q40/08', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30156', 'G06T2207/30164', 'G06T2207/30248', 'G06T2207/30252', 'G06V2201/06', 'G06V2201/08', 'G06V2201/10']"
CN113807355B,Image semantic segmentation method based on coding and decoding structure,"The invention provides an image semantic segmentation method, and belongs to the technical field of image processing. The invention is based on the coding and decoding network architecture and adopts a multi-scale feature fusion method, the model is divided into an encoder and a decoder, the encoder comprises a conventional feature extraction branch and an edge branch, the conventional feature extraction branch is a standard segmentation network, the edge branch only pays attention to the edge contour part by introducing a residual error structure, gating convolution and Canny operators, the decoder adopts a multi-path optimization network structure, and the whole semantic segmentation network forms short-distance connection and long-distance connection with the feature extraction network, thereby not only facilitating network training, but also enabling gradients to be effectively transmitted back to the network, and finally obtaining the semantic segmentation image of the image. According to the method and the device, the learning effect and the segmentation precision of the edge contour in the semantic segmentation of the low-visibility image are improved, and the precision of the final semantic segmentation is improved.","['G06F18/214', 'G06N3/045', 'G06N3/048', 'G06N3/084', 'Y02T10/40']"
US10698560B2,Organizing digital notes on a user interface,"At least some aspects of the present disclosure feature a computing device configured to display visual representations of digital notes and one or more group images representing one or more groups on a user interface, where each group may include one or more digital notes. The computing device is further configured to receive one or more user inputs via the user interface and change the compositions of the groups based on the received user inputs.","['G06F3/0481', 'G06F16/53', 'G06F16/93', 'G06F3/04817', 'G06F3/0482', 'G06F3/0483', 'G06F3/0486', 'G06F40/10', 'G06F40/103', 'G06F9/451', 'G06K9/00442', 'G06V30/40', 'H04L12/1827', 'H04L51/08', 'H04L51/10', 'G06F1/1639', 'G06F2203/04806', 'G06F3/04886']"
CN114708585B,A three-dimensional target detection method based on the fusion of millimeter wave radar and vision based on the attention mechanism,"The invention provides a millimeter wave radar and vision fusion three-dimensional target detection method based on an attention mechanism, which comprises the following steps of data acquisition and processing: acquiring millimeter wave radar point cloud and visual image data with approximately synchronous time; and (3) a fusion step: converting millimeter wave Lei Dadian cloud data from a radar coordinate system to a camera coordinate system to realize space synchronization, then performing preprocessing operation, and extracting the speed and depth information of a target point cloud in a point cloud frame to construct radar matrix data so as to finish radar information extraction; extracting an image feature map and a radar feature map from the fusion frame through a neural network, and carrying out feature fusion by combining an attention mechanism to obtain a fusion feature map; the detection step comprises: and (3) up-sampling the fusion feature map and inputting the up-sampling feature map to a branch convolution network, and decoding output information of the branch convolution to obtain the category and three-dimensional information of the target. The invention effectively fuses the visual image data and the millimeter wave radar data, and effectively improves the accuracy and reliability of three-dimensional target detection in complex scenes.","['G01S13/867', 'G06F18/23', 'G06F18/25', 'G06N3/045']"
US20250137782A1,"Methods and apparatus for automatically defining computer-aided design files using machine learning, image analytics, and/or computer vision","A non-transitory processor-readable medium includes code to cause a processor to receive aerial data having a plurality of points arranged in a pattern. An indication associated with each point is provided as an input to a machine learning model to classify each point into a category from a plurality of categories. For each point, a set of points (1) adjacent to that point and (2) having a common category is identified to define a shape from a plurality of shapes. A polyline boundary of each shape is defined by analyzing with respect to a criterion, a position of each point associated with a border of that shape relative to at least one other point. A layer for each category including each shape associated with that category is defined and a computer-aided design file is generated using the polyline boundary of each shape and the layer for each category.","['G01C11/04', 'G06F18/29', 'G06F30/13', 'G06F30/20', 'G06F30/27', 'G06N20/00', 'G06N3/045', 'G06N3/08', 'G06T3/4038', 'G06V20/13', 'G06N20/20', 'G06N5/01', 'G06N7/01', 'G06T2200/32', 'G06T2207/10032']"
US10325376B2,"Machine vision for ego-motion, segmenting, and classifying objects","Systems and methods for machine vision are presented. Such machine vision includes ego-motion, as well as the segmentation and/or classification of image data of one or more targets of interest. The projection and detection of scanning light beams that generate a pattern are employed. Real-time continuous and accurate spatial-temporal 3D sensing is achieved. The relative motion between an observer and a projection surface is determined. A combination of visible and non-visible patterns, as well as a combination of visible and non-visible sensor arrays is employed to sense 3D coordinates of target features, as well as acquire color image data to generate 3D color images of targets. Stereoscopic pairs of cameras are employed to generate 3D image data. Such cameras are dynamically aligned and calibrated. Information may be encoded in the transmitted patterns. The information is decoded upon detection of the pattern and employed to determine features of the reflecting surface.","['G06T7/20', 'G01P3/36', 'H04N23/56', 'H04N23/71', 'H04N5/04', 'H04N5/2256', 'H04N5/2351', 'H04N13/254']"
US10783691B2,"Generating a stylized image or stylized animation by matching semantic features via an appearance guide, a segmentation guide, and/or a temporal guide","Certain embodiments involve generating one or more of appearance guide and a positional guide and using one or more of the guides to synthesize a stylized image or animation. For example, a system obtains data indicating a target image and a style exemplar image. The system generates an appearance guide, a positional guide, or both from the target image and the style exemplar image. The system uses one or more of the guides to transfer a texture or style from the style exemplar image to the target image.","['G06T13/40', 'G06K9/00228', 'G06K9/00281', 'G06K9/00315', 'G06T11/001', 'G06T13/80', 'G06T15/04', 'G06T3/0068', 'G06T3/0093', 'G06T3/14', 'G06T3/18', 'G06T5/002', 'G06T5/50', 'G06T5/70', 'G06T7/11', 'G06T7/194', 'G06T7/73', 'G06T7/90', 'G06V40/161', 'G06V40/171', 'G06V40/176', 'G06T15/02', 'G06T2207/10016', 'G06T2207/20076', 'G06T2207/20221', 'G06T2207/30201', 'G06T7/60']"
CN110321754B,Human motion posture correction method and system based on computer vision,"The invention relates to a human motion gesture correction method and system based on computer vision. The intelligent motor skill guidance can be obtained without wearing additional equipment. The invention uses the convolutional neural network model in the deep learning to detect the position of the key points of the human body captured by the camera in the image, and the precision can be improved by training the model, so that the precision required by action guidance can be achieved. Meanwhile, as the positions of the cameras can be set according to the actual environment, compared with the kinect equipment, the method has better capturing effect on various complex actions of the user.","['G06N3/045', 'G06V20/40', 'G06V40/20']"
CN111727457B,Cotton crop row detection method and device based on computer vision and storage medium,"The invention discloses a cotton crop row detection method and device based on computer vision and a storage medium, which comprise the following steps: step 1, establishing an image training sample set; step 2: constructing a crop area prediction model based on a semantic segmentation depth neural network; step 3: in a working scene of the cotton tectorial membrane recycling machine, inputting a real-time image into a constructed crop area prediction model to obtain a recognition result; step 4: and (3) processing a crop area identification result by using a crop line fitting algorithm to obtain a cotton crop line detection result, wherein the cotton crop line detection result is used as an automatic navigation basis. The invention obtains accurate cotton crown layer classification result, accurately detects a plurality of cotton crop rows, provides basis for automatic navigation of the tractor, and has high accuracy and good robustness of the obtained result.","['G06T7/0002', 'G06F18/214', 'G06F18/24', 'G06T7/11', 'G06T7/136', 'G06T7/187', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/30188']"
US11341722B2,Computer vision method and system,"A computer vision method for processing an omnidirectional image to extract understanding of a scene, the method comprising:receiving an omnidirectional image of a scene;mapping the omnidirectional image to a mesh on a three-dimensional polyhedron;convert the three dimensional polyhedron into a representation of a neighbourhood structure, wherein the representation of a neighbourhood structure represents vertices of said mesh and their neighbouring vertices; andprocessing the representation of the neighbourhood structure with a neural network processing stage to produce an output providing understanding of the scene,wherein the neural network processing stage comprising at least one module configured to perform convolution with a filter, aligned with a reference axis of the three-dimensional polyhedron.","['G06T3/00', 'G06T3/12', 'G06N20/00', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/082', 'G06N3/09', 'G06T17/10', 'G06T17/20', 'G06T5/00', 'G06T7/00', 'G06T7/10', 'G06V10/764', 'G06V10/82', 'G06V20/56', 'G06T2207/20']"
US20220261593A1,"Using neural networks to perform object detection, instance segmentation, and semantic correspondence from bounding box supervision","Apparatuses, systems, and techniques to train one or more neural networks. In at least one embodiment, one or more neural networks are trained to perform segmentation tasks based at least in part on training data comprising bounding box annotations.","['G06K9/6256', 'G06T7/10', 'G06F18/214', 'G06V10/82', 'G06F18/241', 'G06K9/00791', 'G06K9/6268', 'G06N20/00', 'G06N3/02', 'G06N3/045', 'G06N3/0454', 'G06N3/049', 'G06N3/08', 'G06N3/084', 'G06N3/088', 'G06N7/01', 'G06T1/20', 'G06T1/60', 'G06T7/11', 'G06V10/774', 'G06V10/7792', 'G06V20/56', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30252', 'G06T2210/12']"
US10970543B2,Distributed and self-validating computer vision for dense object detection in digital images,"A system for object recognition and segmentation from digital images provides an intelligent object recognition and segmentation using one or more multilayer convolutional neural network (CNN) models trained in multiple-stages and in a parallel and distributed manner to improve training speed and efficiency. The training dataset used in each of the multiple training stages for the CNN models are generated, expanded, self-validated from a preceding stage. The trained final CNN models are augmented with post-model filters to enhance prediction accuracy by removing false positive object recognition and segmentation. The system provides improved accuracy to predict object labels to append to unlabeled image blocks in digital images. In one embodiment, the system may be useful for enhancing a digital landmark registry by appending identifying labels on new infrastructure improvements recognized in aerial or satellite land images.","['G06V20/176', 'G06K9/00637', 'G06F18/214', 'G06K9/00651', 'G06K9/4642', 'G06K9/6256', 'G06N3/04', 'G06N3/0464', 'G06N3/0895', 'G06N3/09', 'G06N3/098', 'G06T7/11', 'G06V10/82', 'G06V20/182']"
CN111598033A,"Cargo positioning method, device and system and computer readable storage medium","The invention discloses a cargo positioning method, a cargo positioning device, a cargo positioning system and a computer readable storage medium, wherein the method comprises the following steps: collecting an image of a cargo to be positioned, and segmenting the collected image through the ordered point cloud to obtain an image to be positioned; inputting the image to be positioned into an image positioning model to obtain a corner segmentation image and a position sensitive segmentation image corresponding to the image to be positioned; and based on the image positioning model, determining the position of the goods to be positioned according to the corner segmentation graph and the position sensitive segmentation graph. The invention realizes the segmentation of the collected image through the ordered point cloud, inputs the obtained image to be positioned into the image positioning model, and determines the position of the goods to be positioned according to the angular point segmentation graph and the position sensitive segmentation graph output by the image positioning model, thereby improving the goods positioning stability and the goods identification rate.","['G06V20/10', 'G06F18/214', 'G06F18/24', 'G06T11/40', 'G06T7/11', 'G06T7/70', 'G06T7/90', 'G06V10/25', 'G06T2207/10028', 'G06T2207/30204', 'Y02P90/30']"
CN105158258B,A method for detecting surface defects of bamboo strips based on computer vision,"The present invention is a kind of bamboo cane detection method of surface flaw based on computer vision, intake bamboo cane image normalization, filtering process and wavelet transform process；Maximum variance between clusters are to carrying out image threshold segmentation, bamboo cane vacancy is filled in closed operation, the negatively influencing of closed operation is removed with etching operation again, difference shadow method is partitioned into worm hole and/or mildew defect in bamboo cane image, if defect connected domain area is less than defect domain area threshold A, no mildew or worm hole defect are then thought, into Crack Detection；Otherwise defect connected domain gradation of image average Gm, Gm and bamboo cane gradation of image average G is calculated0Difference be more than threshold value G then have mildew or worm hole, be judged as defect bamboo cane；Otherwise detection edge and crack, remove the bamboo cane edge of image, non-zero pixels point total number or largest connected length of field are more than correspondence threshold value, and crack gray average is more than G0, rift defect is determined with, it is otherwise qualified.This law detection efficiency is high, and noise immunity is strong, and stability is high, and correct recognition rata is up to more than 95%.",[]
CN116229276B,River entering pollution discharge detection method based on computer vision,"The invention relates to the technical field of image data processing, and provides a river entering pollution discharge detection method based on computer vision, which comprises the following steps: acquiring river aerial images, dividing a river region, and acquiring a flow direction and a river skeleton line; dividing a river region by super-pixel segmentation to obtain a plurality of suspected pollution communication domains, and obtaining the pollution possibility of each suspected pollution communication domain according to the gray gradient of pixel points in each suspected pollution communication domain and river skeleton lines and obtaining the filter window sizes of different regions; acquiring detail retention degree according to the distribution of gray values in each suspected pollution connected domain, river flow direction and pollution possibility, and further obtaining Gaussian function standard deviations of different areas; and acquiring high-quality river aerial images through Gaussian filtering, and finishing river entering pollution discharge detection through a river entering pollution discharge detection system. The invention aims to denoising river images and simultaneously reserve detailed information to improve the accuracy of river-entering pollution discharge detection.","['G06V20/182', 'G06V10/267', 'G06V10/36', 'G06V10/56', 'G06V10/751', 'G06V10/761']"
US12051206B2,Deep neural network for segmentation of road scenes and animate object instances for autonomous driving applications,"A deep neural network(s) (DNN) may be used to perform panoptic segmentation by performing pixel-level class and instance segmentation of a scene using a single pass of the DNN. Generally, one or more images and/or other sensor data may be stitched together, stacked, and/or combined, and fed into a DNN that includes a common trunk and several heads that predict different outputs. The DNN may include a class confidence head that predicts a confidence map representing pixels that belong to particular classes, an instance regression head that predicts object instance data for detected objects, an instance clustering head that predicts a confidence map of pixels that belong to particular instances, and/or a depth head that predicts range values. These outputs may be decoded to identify bounding shapes, class labels, instance labels, and/or range values for detected objects, and used to enable safe path planning and control of an autonomous vehicle.","['G06T7/11', 'G01S13/89', 'G01S13/931', 'G01S17/89', 'G01S17/931', 'G01S7/417', 'G05D1/0088', 'G05D1/81', 'G06F18/22', 'G06F18/23', 'G06T5/50', 'G06T7/10', 'G06V10/82', 'G06V20/56', 'G06V20/58', 'G01S15/931', 'G01S2013/932', 'G06T2207/10012', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30252', 'G06T2207/30261', 'G06V10/454']"
US20210200988A1,Method and equipment for classifying hepatocellular carcinoma images by combining computer vision features and radiomics features,"The present disclosure discloses a method and equipment for classifying hepatocellular carcinoma images by combining computer vision features and radiomics features, wherein the method comprising: 1) collecting eligible clinical images of patients and preprocessing the collected images; 2) extracting computer vision features from a segmented image of a hepatic tumor region; 3) extracting the manual radiomics features from the segmented image of the hepatic tumor region; 4) by combining the computer vision features and the radiomics features, screening by univariate filtering and then by LASSO regression; 5) using the features resulted from screening and clinical features together for modeling by a multivariable logistic regression model, and using the Akaike information criterion (AIC) to search backward and select clinical features suitable for the best model, so as to implement the prediction of hepatocellular carcinoma pathological grading.","['G06T7/0012', 'G06K9/00147', 'G06F18/241', 'G06F18/2135', 'G06F18/22', 'G06K9/0014', 'G06T7/11', 'G06T7/41', 'G06V10/25', 'G06V10/446', 'G06V10/462', 'G06V10/50', 'G06V10/52', 'G06V10/761', 'G06V20/695', 'G06V20/698', 'G06T2207/30056', 'G06T2207/30096', 'G06V10/467', 'G06V2201/03', 'G06V2201/031']"
CN110956185B,Method for detecting image salient object,"The invention provides a method for detecting a salient object of an image, and belongs to the technical field of image saliency detection. In the first stage, a classification network CNet and a character sequence generation network PNet are constructed to respectively obtain a significance map from classification and character sequence generation tasks; in the second stage, CNet and PNet are used for establishing two complementary training data sets, namely a natural image data set with a noise label and a network image data set, and SNet is alternately trained by using the two complementary data sets; in the third stage, the method updates the natural image data set and the network image data set by using the prediction result of the SNet, and recursively optimizes the model. In the testing phase, only SNet is utilized to predict significance maps. Experiments show that the method is superior to unsupervised and weakly supervised methods, and still has good performance compared with some supervised methods.","['G06T7/0002', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G06T7/194', 'G06T2207/20081']"
US11109941B2,Tracking surgical items with prediction of duplicate imaging of items,"A computer-implemented method for tracking surgical textiles includes receiving a first image comprising a first textile-depicting image region, receiving a second image comprising a second textile-depicting image region, measuring a likelihood that the first and second image regions depict at least a portion of the same textile, and incrementing an index counter if the measure of likelihood does not meet a predetermined threshold. The measure of likelihood may be based on at least one classification feature at least partially based on aspects or other features of the first and second images.","['G06T7/33', 'A61B90/92', 'A61B90/96', 'G06K9/4671', 'G06K9/6211', 'G06T7/0014', 'G06T7/0016', 'G06T7/337', 'G06T7/62', 'G06V10/462', 'G06V10/54', 'G06V10/757', 'A61B34/20', 'A61B50/37', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20021', 'G06T2207/20061', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/20164', 'G06T2207/30124', 'G06T2207/30204']"
US12072443B2,Segmentation of lidar range images,"A deep neural network(s) (DNN) may be used to detect objects from sensor data of a three dimensional (3D) environment. For example, a multi-view perception DNN may include multiple constituent DNNs or stages chained together that sequentially process different views of the 3D environment. An example DNN may include a first stage that performs class segmentation in a first view (e.g., perspective view) and a second stage that performs class segmentation and/or regresses instance geometry in a second view (e.g., top-down). The DNN outputs may be processed to generate 2D and/or 3D bounding boxes and class labels for detected objects in the 3D environment. As such, the techniques described herein may be used to detect and classify animate objects and/or parts of an environment, and these detections and classifications may be provided to an autonomous vehicle drive stack to enable safe planning and control of the autonomous vehicle.","['G01S7/4802', 'B60W60/0011', 'B60W60/0016', 'B60W60/0027', 'G01S17/89', 'G01S17/931', 'G05D1/0088', 'G05D1/81', 'G06F18/2413', 'G06F18/251', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/09', 'G06T19/006', 'G06V10/25', 'G06V10/26', 'G06V10/454', 'G06V10/764', 'G06V10/774', 'G06V10/803', 'G06V10/82', 'G06V20/56', 'G06V20/58', 'G06V20/584', 'B60W2420/403', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30261', 'G06V10/16']"
CN111368846B,Road ponding identification method based on boundary semantic segmentation,"The invention discloses a road ponding identification method based on boundary semantic segmentation, and aims to solve the problems of low identification accuracy and low real-time performance in the existing road ponding identification method. The technical scheme is that a semantic segmentation data set is manufactured and expanded to obtain a training data set; constructing a road ponding recognition system based on boundary semantic segmentation, which consists of a road ponding recognition network based on boundary semantic segmentation and an output decider, wherein the road ponding recognition network based on boundary semantic segmentation consists of an asymmetric encoder network and a decoder network; training a road ponding recognition network based on boundary semantic segmentation to obtain a set of network weight parameters; and carrying out accumulated water identification on the picture I by the trained road accumulated water identification network based on boundary semantic segmentation, and judging the network output by an output judger to obtain an accumulated water identification result. By adopting the method and the device, the road ponding area can be accurately extracted, and the speed and the accuracy of road ponding identification are improved.","['G06V10/26', 'G06F18/213', 'G06F18/2148', 'G06F18/241', 'G06F18/253', 'G06N3/045', 'Y02A90/30']"
US11449993B2,Automated bone segmentation in images,"A method for automated segmentation of a bone in an image dataset of a region of a human body, wherein said region comprises at least two adjacent bones, and wherein one of the bones comprises a shaft region and a head region, the method comprising identifying in the image dataset voxels belonging to a contiguous volume comprising both bones based on a density threshold; extracting a subset comprising the head region of the first bone, based on a boundary image slice; and applying a trained machine learning classifier to said subset, to generate a classification of each of voxel as belonging to one of the bones or a none-bone tissue.","['G06T7/174', 'G06F18/24', 'G06K9/6267', 'G06T11/008', 'G06T7/0012', 'G06T7/11', 'G06T7/136', 'G06T2200/04', 'G06T2207/10072', 'G06T2207/20081', 'G06T2207/30008']"
JP2016110196A,"Barcode detection method, barcode detection system, and program therefor","PROBLEM TO BE SOLVED: To provide a barcode detection method, a barcode detection system, and a program therefor.SOLUTION: In a method for detecting a barcode, an information processing apparatus executes the steps of: detecting a plurality of blobs from an image (S302); and, for each detected blob, obtaining imaginary lines bisecting the detected blob perpendicularly at the middle, determining at least one cluster of the imaginary lines in a feature space according to slopes and positions of the imaginary lines, and grouping the blobs in one of the clusters to a rectangular shape corresponding to the barcode (S304).SELECTED DRAWING: Figure 3","['G06K7/1443', 'G06V10/242', 'G06V10/48']"
US7760956B2,System and method for producing a page using frames of a video stream,"A method and system for automatically producing a page using frames of a video stream. A plurality of video frames from a video stream or clip are read, and multiple frames are extracted from the video stream, based on the content of each frame. Frames which neighbor the extracted frames in the video stream are also extracted, with the number of neighboring frames to be extracted being controlled by parameters provided to the page production system. The resolution of the images in the extracted video frames is enhanced, based on the information in each extracted frame and each respective extracted neighboring frame. The enhanced images are automatically cropped to enhance the important features in each image, and the cropped images are composed into one or more pages that are representative of the content of the video stream.",['G06V20/40']
CN110935644B,A machine vision-based bearing needle roller size detection system and method,"The invention discloses a bearing needle roller size detection system and method based on machine vision.A single low-cost 2D camera is adopted to collect images, an industrial camera collects the images, the collected images are stored on an industrial personal computer, the industrial personal computer processes the images, the processed images remove background interference by using a horizontal and vertical projection method, angle correction is carried out on the extracted ROI area, the ROI images are converted into binary images by adopting a threshold segmentation method and are filled, and the internal color of the needle roller outline is ensured to be white; extracting the outline of the processed image, calculating the detection size of the needle roller according to the proportional relation between the visual field range of the camera and the image pixels, judging that the qualified needle roller reaches the containing box along with the transmission belt and that the unqualified needle roller is judged, sending a rejection signal to the PLC by the industrial personal computer, and rejecting the needle roller by the PLC control rejection mechanism; and detecting and removing the rolling needles moving on the production line in real time.","['B07C5/10', 'B07C5/12']"
CN107766855B,"Chessman positioning method and system based on machine vision, storage medium and robot","The invention discloses a chess piece positioning method based on machine vision, which is applied to positioning chess pieces on a chessboard provided with plane markers, and comprises the following steps: acquiring a video stream through a camera, and acquiring video image frames from the video stream; carrying out image processing on the video image frame, and identifying a plane marker; and calculating the positions of the chess pieces on the chessboard relative to the markers according to the plane markers and the pre-acquired internal reference matrix of the camera, thereby positioning the positions of the chess pieces. In addition, based on the same technical concept, the invention also discloses a chess piece positioning system based on machine vision, a storage medium and a robot. The invention does not need a chessboard to realize a special circuit, and data communication is not needed between the chessboard and the robot, thereby being more convenient to arrange. In addition, the establishment of the plane marker enables the robot to accurately identify the plane marker in a complex scene without being interfered by the complex scene.","['G06T7/11', 'G06T7/85', 'G06V10/22', 'G06V10/245', 'G06V10/44', 'G06V20/62']"
CN103778635B,For the method and apparatus processing data,"A kind of computer implemented method for processing data and a kind of device for processing data.The method includes: receive the depth map of the scene of the health comprising humanoid subject, this depth map includes picture element matrix, each pixel is corresponding with the relevant position in this scene and has respective pixel values, and this respective pixel values represents the distance from reference position to this relevant position；Identify the background object being not connected to health in this scene, and from this scene, remove background object；After removing background object, split this depth map to find out the profile of health；Process this profile to identify the trunk of object and one or more limbs；And by analyzing the layout of at least one limbs in the identified limbs in this depth map, produce the input of the application program run on computers for control.","['G06V40/10', 'G06T7/12', 'G06T7/246', 'G06T2207/10028', 'G06T2207/30196']"
US20220012890A1,Model-Based Deep Learning for Globally Optimal Surface Segmentation,An automated method for segmentation includes steps of receiving at a computing device an input image representing at least one surface and performing by the computing device image segmentation on the input image based on a graph surface segmentation model with deep learning. The deep learning may be used to parameterize the graph surface segmentation model.,"['G06T7/11', 'G06T7/12', 'G06T7/0012', 'G06T7/162', 'G06T2207/10072', 'G06T2207/10101', 'G06T2207/10132', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30041', 'G06T2207/30101']"
US20210027098A1,Weakly Supervised Image Segmentation Via Curriculum Learning,"Weakly supervised instance segmentation refers to the task of training a system to detect object locations and segment instances of the detected objects, where the training data includes only images and image-level labels. This disclosure includes an enhanced pipeline and enhanced training methods that progressively mine pixel-wise labels, when trained via image-level labels. Four cascaded modules are employed, including: a multi-label classification module, an object detection module, an instance refinement module, and instance segmentation module. The modules share a common backbone. The cascaded pipeline is trained alternatively with a curriculum learning strategy which generalizes image level supervision to pixel level supervision, and a post validation training stage, which runs in the inverse order. In the curriculum learning stage, a proposal refinement sub-module is employed to locate object parts and finding key pixels during classification.","['G06T7/11', 'G06K9/6256', 'G06F18/214', 'G06F18/217', 'G06F18/24', 'G06K9/6262', 'G06K9/6267', 'G06T11/20', 'G06T7/10', 'G06T7/194', 'G06T7/75', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06T2207/20081', 'G06T2207/20084', 'G06T2210/12']"
CN116758061B,Casting surface defect detection method based on computer vision,"The application relates to the technical field of image processing, in particular to a casting surface defect detection method based on computer vision, which comprises the following steps: acquiring a gray image of the surface of a casting, dividing the image to obtain a plurality of connected domains which are marked as suspected defect areas; acquiring a set number of edge pixel points on the edge of the suspected defect area as a starting point according to a fixed interval, acquiring a characteristic chain with gradually reduced gray values from the starting point, and acquiring a first probability index according to the number of the pixel points and pixel coordinates on the characteristic chain; obtaining a second probability index according to the distance from the starting point to the central area of the suspected defect area and the number of pixel points on the characteristic chain of the starting point; and calculating a third probability index according to gray values of pixel points on the characteristic chains of the starting points in the two categories, further obtaining a global probability index, and judging the defect damage degree of the casting surface according to the global probability index. The application can rapidly and effectively finish the defect detection of the casting surface.","['G06T7/0004', 'G06T7/11', 'G06T7/13', 'G06T7/187', 'G06T7/62', 'G06T2207/30164', 'Y02P90/30']"
US9965865B1,Image data segmentation using depth data,"Devices and techniques are generally described for segmentation of image data using depth data. In various examples, color image data may be received from a digital camera. In some examples, depth image data may be received from a depth sensor. In various examples, the depth image data may be separated into a plurality of clusters of depth image data, wherein each cluster is associated with a respective range of depth values. In some further examples, a determination may be made that a first cluster of image data corresponds to an object of interest, such as a human subject, in the image data. In various examples, pixels of the first cluster may be encoded with foreground indicator data. In some further examples, segmented image data may be generated. The segmented image data may comprise pixels encoded with the foreground indicator data.","['G06T7/194', 'G06F18/2148', 'G06F18/23', 'G06F18/24', 'G06K9/6218', 'G06K9/6267', 'G06T7/11', 'G06T7/174', 'G06V10/141', 'G06V10/17', 'G06V10/56', 'G06V10/762', 'G06V10/764', 'G06V10/7747', 'G06V20/64', 'G06V40/103', 'G06V40/162', 'H04N13/0203', 'H04N13/204', 'H04N13/271', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/30196', 'H04N2213/003']"
US10685446B2,Method and system of recurrent semantic segmentation for image processing,"A system, article, and method of recurrent semantic segmentation for image processing by factoring historical semantic segmentation.","['G06V10/82', 'G06F18/2413', 'G06F18/29', 'G06K9/00711', 'G06K9/4628', 'G06K9/627', 'G06K9/6296', 'G06N3/04', 'G06N3/0464', 'G06N3/09', 'G06T17/00', 'G06T7/174', 'G06T7/55', 'G06T7/579', 'G06T7/75', 'G06V10/454', 'G06V10/764', 'G06V20/40', 'H04N13/261', 'H04N23/90', 'H04N5/247', 'G06N3/044', 'G06N3/045', 'G06N3/08', 'G06T2207/10016', 'G06T2207/10021', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20076', 'G06T2207/20084', 'G06T2207/30244']"
CN111325764B,A method of fruit image contour recognition,"The invention provides a fruit image contour identification method, which comprises the following steps: training based on a Mask R-CNN deep convolution neural network, inputting a fruit image training set into the Mask R-CNN deep convolution neural network, and training to obtain a target detection model; extracting an interested region of the fruit image verification set through the target detection model, and generating a target regression box according to the interested region; performing multi-feature fusion analysis on the fruit image in the target regression frame to determine the edge contour position of the fruit; and carrying out contour fitting optimization processing on the fruit edge contour position to obtain an optimized fruit edge contour. The method can effectively reduce and reduce the influence of the complex background interference phenomenon of uneven illumination, partial shielding and similar background characteristics on fruit identification and contour fitting, and improve the robustness.","['G06T7/13', 'G06F18/253', 'G06N3/045', 'G06N3/08', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30188']"
CN112184617B,Spine MRI image key point detection method based on deep learning,"The invention is called a spine MRI image key point detection method based on deep learning. The invention discloses a spine MRI image key point detection method based on deep learning, which comprises the steps of firstly detecting and positioning vertebrae in a spine MRI image by utilizing a deep target detection network, identifying S1 (sacrum 1) as positioned vertebrae, and then filtering false positive detection results and judging fine grain labels of all the vertebrae by combining with the structural information of the vertebrae. And then, respectively detecting six key points of the upper and lower boundaries UA, UM, UP, LA, LM and LP of each vertebra by using a key point detection network, determining and correcting the key point positions of each vertebra by combining edge information, and finally, developing interactive visual MRI spine image key point automatic labeling software. The method can automatically extract the key points of the spine MRI image, and has great application value in the aspects of medical image analysis, auxiliary medical treatment and the like.","['G06T7/0012', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T7/13', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30012']"
US10235771B2,Methods and systems of performing object pose estimation,"Techniques are provided for estimating a three-dimensional pose of an object. An image including the object can be obtained, and a plurality of two-dimensional (2D) projections of a three-dimensional bounding (3D) box of the object in the image can be determined. The plurality of 2D projections of the 3D bounding box can be determined by applying a trained regressor to the image. The trained regressor is trained to predict two-dimensional projections of the 3D bounding box of the object in a plurality of poses, based on a plurality of training images. The three-dimensional pose of the object is estimated using the plurality of 2D projections of the 3D bounding box.","['G06T7/73', 'G06K9/00671', 'G06K9/3208', 'G06K9/3241', 'G06K9/4628', 'G06V10/242', 'G06V10/255', 'G06V10/454', 'G06V20/20', 'G06V20/647', 'G06T2207/20084']"
US9978177B2,Reconstructing a 3D modeled object,"The invention notably relates to a computer-implemented method for reconstructing a 3D modeled object that represents a real object, from a 3D mesh and measured data representative of the real object, the method comprising providing a set of deformation modes; determining a composition of the deformation modes which optimizes a program that rewards fit between the 3D mesh as deformed by the composition and the measured data, and that further rewards sparsity of the deformation modes involved in the determined composition; and applying the composition to the 3D mesh. The method improves reconstructing a 3D modeled object that represents a real object.","['G06T17/205', 'G06T17/30', 'G06F17/10', 'G06F17/15', 'G06T19/20', 'G06T5/00', 'G06T7/12', 'G06T7/149', 'G06T7/194', 'G06T2200/24', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20']"
CN107832672B,Pedestrian re-identification method for designing multi-loss function by utilizing attitude information,"The invention discloses a pedestrian re-identification method for designing a multi-loss function by utilizing attitude information, which can effectively solve the difficulties caused by frequent pedestrian shielding, large video illumination difference and variable non-rigid pedestrian attitude in a monitoring video and is widely applied to the fields of security monitoring and the like. The method is mainly divided into two stages, namely an off-line stage and an on-line stage. The off-line stage is responsible for training and learning a deep learning network model with high accuracy, and comprises preprocessing, joint point information extraction, local feature extraction and feature fusion with global features extracted by a main network framework, and finally training is completed by utilizing a quintuple loss function for the fused features. And in the online stage, the trained deep learning network model is used for feature extraction, so that the pedestrian re-identification between the target to be analyzed and the stored target picture library is realized through similarity calculation.","['G06V40/103', 'G06F18/214', 'G06F18/22', 'G06F18/2431', 'G06F18/253']"
US10977530B2,ThunderNet: a turbo unified network for real-time semantic segmentation,"System and method for semantic segmentation. The system includes a computing device. The computing device has a processor and a non-volatile memory storing computer executable code. The computer executable code, when executed at the processor, is configured to: receive an image of a scene; process the image by an encoder to form an encoder feature map; process the encoder feature map by a pyramid pooling module (PPM) to form an PPM feature map; and process the PPM feature map by a decoder to form a segmentation feature map.","['G06K9/726', 'G06V20/10', 'G06V10/267', 'G06K9/6232', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06T9/002', 'G06V10/7715', 'G06V10/82', 'G06V20/70', 'G06V30/19173', 'G06V30/274', 'G06F18/241', 'G06K2009/366', 'G06K9/6268', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30236']"
US9710716B2,Computer vision pipeline and methods for detection of specified moving objects,"A computer vision pipeline detects tracks and classifies people or other specified class of objects in a steam of video. The ability to not only detect motion, but to distinguish people or other specified objects, can improve the systems usefulness in applications like security monitoring. A motion detection module provides a motion bitmap and a background subtraction module provides a foreground bitmap, and an object tracking module uses these bitmaps identify and track the specified classes of objects. From these objects and tracks, categorized object data can then be generated.","['G06K9/00805', 'G06V20/52', 'G06V20/58', 'G06K9/00221', 'G06K9/00261', 'G06K9/00342', 'G06K9/00771', 'G06K9/00778', 'G06K9/00791', 'G06T7/194', 'G06T7/215', 'G06T7/254', 'G06V20/53', 'G06V20/56', 'G06V40/16', 'G06V40/167', 'G06V40/23', 'G06T2207/10024', 'G06T2207/30196']"
CN111582241B,"Video subtitle recognition method, device, equipment and storage medium","The disclosure provides a video subtitle recognition method, a video subtitle recognition device, video subtitle recognition equipment and a storage medium, and relates to the technical field of computer vision. The method comprises the following steps: acquiring a multi-frame image from a video to be identified containing subtitles; recognizing the subtitles in the multi-frame images to obtain an initial subtitle recognition result of each frame image; acquiring the editing distance between the initial subtitle recognition results of two adjacent frames of images in the multi-frame image; obtaining a plurality of continuous similar images based on the editing distance between the initial caption identification results of the two adjacent frames of images; obtaining the semantic credibility of the initial caption recognition result of the multi-frame continuous similar images; and determining the final caption identification result of the multi-frame continuous similar images according to the semantic credibility. The method improves the accuracy of the identification result of the video subtitles to a certain extent.","['G06V20/41', 'G06F18/22', 'G06F18/2415', 'G06N3/045', 'G06N3/047', 'G06N3/049', 'G06V20/635', 'G06V30/10']"
CN111860140B,"Target event detection method, device, computer equipment and storage medium","A target event detection method, apparatus, computer device and storage medium, the method comprising: acquiring a current video frame, and identifying a preset warning area from the current video frame; performing target detection in the current video frame, and obtaining a target detection frame when a target is detected in the current video frame; if the target detection frame and the preset warning area meet the preset position relation condition, sequentially identifying continuous interaction actions of the target and the preset warning area in each subsequent continuous video frame; and determining whether a target event occurs or not based on an initial action and an interaction action sequence formed by each continuous interaction action, wherein the initial action is an action corresponding to the target in the target detection frame. By the method, the position relation between the target and the warning area is detected, and whether the target event occurs is determined by combining the action state change of the target within a certain time, so that the accuracy of detecting the target event can be improved, and misjudgment is reduced.","['G06V20/41', 'G06V40/20', 'G06V2201/07']"
US20130061132A1,System and method for web page segmentation using adaptive threshold computation,"A system and method for an adaptive threshold Web Page segmenting is disclosed. In one embodiment, a method performed by a physical computing system having one or more processors for segmenting a Web page including a plurality of nodes includes parsing content in the Web page into the plurality of nodes using the physical computing system, obtaining feature values between each pair of nodes using the physical computing system, estimating an adaptive threshold value using the obtained feature values using the physical computing system, and segmenting the Web page by comparing the feature values associated with each pair of nodes with the estimated adaptive threshold value.","['G06F40/137', 'G06V20/62', 'G06V30/412']"
CN108334892B,"Vehicle type identification method, device and equipment based on convolutional neural network","The application discloses a vehicle type identification method, device and equipment based on a convolutional neural network. The vehicle type identification method specifically comprises the steps of extracting local features of a vehicle image to be identified by utilizing a first number of convolutional layer units; extracting global features of the vehicle image to be identified by utilizing a second number of convolutional layer units based on the local features; and identifying the vehicle type of the vehicle in the vehicle image by utilizing the classification layer according to the local feature and the global feature. According to the vehicle type identification method and device, the local features and the global features extracted by the convolutional layer unit in the convolutional neural network are input into the classification layer, the local features and the global features of the vehicle image to be identified can be considered simultaneously in vehicle type identification, the problem that due to the fact that the image features are too single, feature detail information is lost and accuracy is reduced is avoided, and accuracy of identification results is improved.","['G06F18/24', 'G06N3/045', 'G06V2201/08']"
US20190095807A1,Image-based popularity prediction,"A machine may be configured to access an image of an item described by a description of the item. The machine may determine an image quality score of the image based on an analysis of the image. A request for search results that pertain to the description may be received by the machine, and the machine may present a search result that references the item's image, based on its image quality score. Also, the machine may access images of items and descriptions of items and generate a set of most frequent text tokens included in the item descriptions. The machine may identify an image feature exhibited by an item's image and determine that a text token from the corresponding item description matches one of the most frequent text tokens. A data structure may be generated by the machine to correlate the identified image feature with the text token.","['G06F16/583', 'G06F16/00', 'G06F16/24', 'G06F16/242', 'G06F16/24578', 'G06F16/2462', 'G06F16/2465', 'G06F16/2471', 'G06F16/248', 'G06F16/332', 'G06F16/338', 'G06F16/345', 'G06F16/5838', 'G06F16/5866', 'G06F16/587', 'G06F16/951', 'G06F17/00', 'G06F17/30', 'G06F17/30247', 'G06F17/3025', 'G06F17/30268', 'G06F17/3053', 'G06F17/30536', 'G06F17/30539', 'G06F17/30545', 'G06F17/30864', 'G06F18/2115', 'G06F18/22', 'G06K9/00', 'G06K9/0014', 'G06K9/20', 'G06K9/46', 'G06K9/4647', 'G06K9/6201', 'G06K9/6231', 'G06K9/68', 'G06N20/00', 'G06N5/048', 'G06Q30/02', 'G06V10/10', 'G06V10/40', 'G06V10/507', 'G06V10/75', 'G06V10/771', 'G06V20/695']"
WO2021031815A1,"Medical image segmentation method and device, computer device and readable storage medium","A medical image segmentation method performed by a computer device. The method comprises: acquiring a first medical image and a second medical image with a labeled target object region; performing feature extraction on the first medical image and the second medical image respectively, to obtain first feature information of the first medical image and second feature information of the second medical image; acquiring optical flow motion information of the second medical image to the first medical image according to the first feature information and the second feature information; and segmenting a target object in the first medical image according to the optical flow motion information and the labeled target object region to obtain a segmentation result of the first medical image.","['G06T7/269', 'G06T7/215', 'G06N3/088', 'G06T7/0012', 'G06T7/11', 'G06T7/246', 'G06T2207/10016', 'G06T2207/10056', 'G06T2207/10068', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30048', 'G06T2207/30056', 'G06T2207/30084', 'G06T2207/30096', 'G06T2207/30101']"
US12176085B2,Complex image data analysis using artificial intelligence and machine learning algorithms,"Disclosed herein are systems, methods, and software for providing a platform for complex image data analysis using artificial intelligence and/or machine learning algorithms. One or more subsystems allow for the capturing of user input such as eye gaze and dictation for automated generation of findings. Additional features include quality metric tracking and feedback, and worklist management system and communications queueing.","['G16H15/00', 'G06F18/2148', 'G06F18/22', 'G06F18/41', 'G06F3/013', 'G06F3/167', 'G06N3/04', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/091', 'G06N3/092', 'G06N3/0985', 'G06T7/0012', 'G06T7/11', 'G06V10/95', 'G10L15/22', 'G16H30/40', 'G06T2200/24', 'G06T2207/10056', 'G06T2207/10068', 'G06T2207/10072', 'G06T2207/10116', 'G06T2207/10132', 'G06T2207/30004', 'G06T2207/30041', 'G06V2201/03']"
US9801601B2,Method and system for performing multi-bone segmentation in imaging data,"A computer implemented method for performing bone segmentation in imaging data of a section of a body structure is provided. The method includes: Obtaining the imaging data including a plurality of 2D images of the section of the body structure; and performing a multiphase local-based hybrid level set segmentation on at least a subset of the plurality of 2D images by minimizing an energy functional including a local-based edge term and a local-based region term computed locally inside a local neighborhood centered at each pixel of each one of the 2D images on which the multiphase local-based hybrid level set segmentation is performed, the local neighborhood being defined by a Gaussian kernel whose size is determined by a scale parameter (σ).","['A61B6/505', 'G06T7/11', 'G06T7/12', 'G06T7/136', 'G06T7/174', 'A61B5/055', 'A61B5/4504', 'A61B5/4528', 'A61B6/032', 'A61B6/5211', 'G06T2207/10072', 'G06T2207/20036', 'G06T2207/20161', 'G06T2207/30008']"
US12260530B2,Generating a modified digital image utilizing a human inpainting model,"The present disclosure relates to systems, methods, and non-transitory computer-readable media that modify digital images via scene-based editing using image understanding facilitated by artificial intelligence. For example, in one or more embodiments the disclosed systems utilize generative machine learning models to create modified digital images portraying human subjects. In particular, the disclosed systems generate modified digital images by performing infill modifications to complete a digital image or human inpainting for portions of a digital image that portrays a human. Moreover, in some embodiments, the disclosed systems perform reposing of subjects portrayed within a digital image to generate modified digital images. In addition, the disclosed systems in some embodiments perform facial expression transfer and facial expression animations to generate modified digital images or animations.","['G06F3/0482', 'G06F3/04842', 'G06F3/04845', 'G06F3/04847', 'G06F3/0486', 'G06F3/04883', 'G06N3/02', 'G06T11/60', 'G06T5/60', 'G06T5/77', 'G06V10/25', 'G06V10/44', 'G06V10/82', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196']"
EP1436689B1,Portable virtual reality,Portable virtual reality is disclosed. A digital camera captures an input video stream comprising video frames that carry information about a surrounding environment limited by a field of view of the camera. The input video stream is processed by a handheld computer coupled to the camera. Parameters that define a rendered scene of a virtual environment are adjusted based upon motion parameters extracted from the input video stream by the handheld computer during the processing.,"['G06F3/012', 'G06F3/011', 'G06T15/20', 'G06T2215/16']"
US11416772B2,Integrated bottom-up segmentation for semi-supervised image segmentation,"Embodiments of the present disclosure include a computer-implemented method, a system, and a computer program product for integrating bottom-up segmentation techniques into a semi-supervised image segmentation machine learning model. The computer implemented method includes training a machine learning model with a labeled dataset. The labeled dataset includes ground truth segmentation labels for each sample in the labeled dataset. The computer implemented method also includes generating a pseudo labeled dataset by applying an unlabeled dataset to the machine learning model using a top-down segmentation grouping rule. The computer implemented method further includes evaluating the pseudo labeled dataset using a bottom-up segmentation grouping rule to produce evaluation results, combining the pseudo labeled dataset with the second pseudo labeled dataset into a training dataset, and then retraining the machine learning model with the training dataset.","['G06N3/08', 'G06F18/214', 'G06F18/231', 'G06K9/6256', 'G06N20/00', 'G06N3/045', 'G06N3/0464', 'G06N3/0895', 'G06N3/09', 'G06T7/10', 'G06V10/40', 'G06V10/7625', 'G06V10/774', 'G06V20/70', 'G06V30/274', 'G06T2207/20081', 'G06T2207/20084', 'G06V2201/03']"
CN112220562B,Method and system for enhancing surgical tool control during surgery using computer vision,"The present disclosure relates to systems and methods for improving patient safety during surgical procedures using a computer vision processing system. The computer vision processing system may use machine learning techniques to train the machine learning model. The machine learning techniques may be performed to train the machine learning model to identify, classify, and interpret objects within a live video feed. Certain embodiments of the present disclosure may use a trained machine learning model to control (or facilitate control of) surgical tools during a surgical procedure.","['A61F5/0076', 'A61B1/000094', 'A61B1/000096', 'A61B1/00055', 'A61B1/313', 'A61B17/068', 'A61B17/07207', 'A61B17/320016', 'A61B17/320092', 'A61B34/10', 'G05B19/4155', 'G06F18/21', 'G06F18/217', 'G06N20/00', 'G06N3/0464', 'G06N3/0475', 'G06N3/08', 'G06N3/0895', 'G06N3/09', 'G06N3/094', 'G06V10/70', 'G06V20/41', 'G16H30/40', 'G16H40/60', 'A61B2017/00017', 'A61B2017/00061', 'A61B2017/00119', 'A61B2017/00128', 'A61B2017/00199', 'A61B2017/00207', 'A61B2017/00212', 'A61B2017/320082', 'A61B2017/320095', 'A61B2034/101', 'A61B2034/107', 'A61B2034/2065', 'A61B2090/064', 'A61B2090/371', 'A61B2090/502', 'A61B90/361', 'A61B90/37', 'G05B2219/36414', 'G06N3/045', 'G06N3/088', 'G06V2201/03', 'G06V2201/034', 'G06V2201/07', 'G06V2201/10']"
CN111710412B,"Calibration method, device and electronic equipment for diagnosis result","The application provides a method and a device for verifying a diagnosis result and electronic equipment, and belongs to the technical field of artificial intelligent medical treatment and knowledge graph. Wherein the method comprises the following steps: obtaining a diagnosis result to be checked and associated target medical record data sent by a first client; determining a first label set corresponding to a diagnosis result to be checked according to a preset mapping relation between labels and diseases; processing the target medical record data by using a preset label classification model to determine a second label set corresponding to the target medical record data; and determining the credibility of the diagnosis result to be checked according to the coincidence degree of the first label set and the second label set. Therefore, by the checking method of the diagnosis result, misdiagnosis checking of the diagnosis result of a doctor is realized according to the coincidence degree of the disease major class to which the diagnosis result to be checked belongs and the disease major class to which the target medical record data belongs, so that the diagnosis accuracy of basic medical institutions is improved, and the misdiagnosis rate is reduced.","['G16H50/20', 'G16H10/60', 'Y02A90/10']"
US8995740B2,System and method for multiplexed biomarker quantitation using single cell segmentation on sequentially stained tissue,"Improved systems and methods for the analysis of digital images are provided. More particularly, the present disclosure provides for improved systems and methods for the analysis of digital images of biological tissue samples. Exemplary embodiments provide for: i) segmenting, ii) grouping, and iii) quantifying molecular protein profiles of individual cells in terms of sub cellular compartments (nuclei, membrane, and cytoplasm). The systems and methods of the present disclosure advantageously perform tissue segmentation at the sub-cellular level to facilitate analyzing, grouping and quantifying protein expression profiles of tissue in tissue sections globally and/or locally. Performing local-global tissue analysis and protein quantification advantageously enables correlation of spatial and molecular configuration of cells with molecular information of different types of cancer.","['G06K9/0014', 'G06T7/0012', 'G06V20/695', 'G06T2207/10064', 'G06T2207/30024']"
CN115004249A,Root cause analysis of process cycle images based on machine learning,A technique is disclosed that involves classifying process cycle images to predict success or failure of a process cycle. The disclosed techniques include capturing and processing images of segments disposed on an image generation chip during genotyping. Image descriptive features of the production cycle image are created and provided as input to the classifier. The trained classifier separates successful production images from unsuccessful or failed production images. These failed production images are further classified into various categories of failures by a trained root cause classifier.,"['G06V10/765', 'G06T7/0012', 'G01N21/6428', 'G06F18/2148', 'G06F18/2163', 'G06F18/24323', 'G06F18/2433', 'G06N20/20', 'G06N5/01', 'G06N5/04', 'G06T5/20', 'G06V10/28', 'G06V10/507', 'G06V10/56', 'G06V10/764', 'G06V10/7715', 'G06V10/774', 'G06V10/7747', 'G06V10/993', 'G06V20/69', 'G06V20/698', 'G16B25/00', 'G16B40/10', 'G16B40/20', 'G01N2021/6439', 'G06N20/10', 'G06N3/045', 'G06N7/01', 'G06T2207/10064', 'G06T2207/20028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20192', 'G06T2207/30072', 'G06T2207/30168']"
US20220185625A1,Camera-based sensing devices for performing offline machine learning inference and computer vision,"A sensor module includes at least a camera module and one or more machine learning (ML) inference application-specific integrated circuits (ASICs), which are configured to detect the presence of people in an elevator. The sensor module includes at least one processor, which executes instructions that enable the sensor module to detect, count, and anonymously track one or more persons in an elevator. The sensor module may also sensors, such as an accelerometer and an altimeter, which are used to estimate the kinematic state of the elevator. The camera, ML ASIC(s), sensors, and embedded application enable the sensor device to anonymously monitor the movement of people through a building via the elevator. The ML ASIC(s) allow the sensor module to count occupants in the elevator in near-real time, enabling the sensor to transmit signals for controlling aspects of the elevator system.","['B66B5/0012', 'B66B1/28', 'B66B1/34', 'B66B1/3461', 'B66B1/3476', 'G06V10/26', 'G06V10/454', 'G06V10/82', 'G06V20/53', 'G06V40/103', 'B66B2201/222', 'G06F17/15', 'G06N3/045', 'G06N3/08']"
US8379950B2,Medical image processing,"A computer-implemented method of detecting an object in a three-dimensional medical image comprises determining the values of a plurality of features at each voxel in at least a portion of the medical image. Each feature characterises a respective property of the medical image at a particular voxel. The likelihood probability distribution of each feature is calculated based on the values of the features and prior medical knowledge. A probability map is generated by using Bayes' law to combine the likelihood probability distributions, and the probability map is analysed to detect an object.","['G06T7/0012', 'G06F18/00', 'G06T7/143', 'G06T2207/10072', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/30032']"
CN113538480B,"Image segmentation processing method, device, computer equipment and storage medium","The application relates to an image segmentation processing method, an image segmentation processing device, computer equipment and a storage medium, which comprise the steps of obtaining a sample image, respectively carrying out image segmentation processing on a target object in the sample image through at least two trained guide models to obtain a first segmentation result corresponding to each guide model, carrying out image segmentation processing on the target object in the sample image through an image segmentation model to be trained to obtain a second segmentation result, determining a distillation error according to the difference between the second segmentation result and the first segmentation result, determining an edge error according to the difference between an edge prediction result in the second segmentation result and an edge prediction result in the first segmentation result, and adjusting model parameters of the image segmentation model and continuing distillation training according to the distillation error and the edge error until a training stop condition is met to obtain a trained image segmentation model. By adopting the method, the accuracy can be improved while the processing efficiency of image segmentation can be effectively ensured.","['G06T7/13', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T7/194', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196', 'G06T2207/30201', 'Y02T10/40']"
CN112052787B,Target detection method and device based on artificial intelligence and electronic equipment,"The application provides a target detection method, a target detection device, electronic equipment and a computer-readable storage medium based on artificial intelligence; the method comprises the following steps: carrying out feature extraction processing on an image to be detected to obtain a feature map; performing region generation processing on the feature map to obtain a plurality of detection frames; performing prediction processing on each detection frame to obtain the confidence degree of each detection frame including the target to be recognized; according to the coincidence degree of the detection frames, the confidence degree of each detection frame including the target is restrained to obtain a new confidence degree; and according to the new confidence that each detection frame comprises the target, screening the detection frames, and taking the detection frames obtained by screening as target detection frames comprising the target. By the method and the device, the precision of target detection can be improved, and the method and the device are suitable for scenes with dense target distribution.","['G06V20/53', 'G06N3/045', 'G06N3/084', 'G06V10/22', 'G06V2201/07']"
CN109035293B,Method suitable for segmenting remarkable human body example in video image,"In order to solve the defects of the prior art, the invention provides a method suitable for segmenting a remarkable human body example in a video image, which comprises the following steps: the motion persistence and the space-time structure consistency of the moving object in the video sequence are introduced, and the human body example segmentation method combining optical flow clustering, significance detection and multi-feature voting is realized on the basis of the constraints of the motion persistence and the space-time structure consistency. For motion continuity, a foreground object probability calculation strategy based on optical flow region clustering is adopted, namely, regions are clustered based on optical flow characteristics, and foreground probability is calculated by taking the region area size as weight.","['G06T7/215', 'G06F18/23', 'G06N3/04', 'G06T7/11', 'G06T7/246', 'G06T2207/10016']"
US11886815B2,Self-supervised document representation learning,"One example method involves operations for a processing device that include receiving, by a machine learning model trained to generate a search result, a search query for a text input. The machine learning model is trained by receiving pre-training data that includes multiple documents. Pre-training the machine learning model by generating, using an encoder, feature embeddings for each of the documents included in the pre-training data. The feature embeddings are generated by applying a masking function to visual and textual features in the documents. Training the machine learning model also includes generating, using the feature embeddings, output features for the documents by concatenating the feature embeddings and applying a non-linear mapping to the feature embeddings. Training the machine learning model further includes applying a linear classifier to the output features. Additionally, operations include generating, for display, a search result using the machine learning model based on the input.","['G06F40/279', 'G06F16/334', 'G06F16/93', 'G06F40/103', 'G06F40/131', 'G06F40/205', 'G06F40/216', 'G06F40/289', 'G06F40/30', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/088', 'G06N3/0895']"
US12340907B2,"Systems, methods, and apparatuses for implementing advancements towards annotation efficient deep learning in computer-aided diagnosis",Embodiments described herein include systems for implementing annotation-efficient deep learning in computer-aided diagnosis. Exemplary embodiments include systems having a processor and a memory specially configured with instructions for learning annotation-efficient deep learning from non-labeled medical images to generate a trained deep-learning model by applying a multi-phase model training process via specially configured instructions for pre-training a model by executing a one-time learning procedure using an initial annotated image dataset; iteratively re-training the model by executing a fine-tuning learning procedure using newly available annotated images without re-using any images from the initial annotated image dataset; selecting a plurality of most representative samples related to images of the initial annotated image dataset and the newly available annotated images by executing an active selection procedure based on the which of a collection of un-annotated images exhibit either a greatest uncertainty or a greatest entropy; extracting generic image features; updating the model using the generic image features extracted; and outputting the model as the trained deep-learning model for use in analyzing a patient medical image. Other related embodiments are disclosed.,"['G16H30/40', 'G06F18/217', 'G06V10/26', 'G06V10/454', 'G06V10/7753', 'G06V10/82', 'G16H40/67', 'G16H50/20', 'G16H50/70', 'G06V2201/03']"
CN111797187B,"Map data update method, device, electronic device and storage medium","The application discloses a map data updating method, a map data updating device, electronic equipment and a storage medium, and relates to the technical fields of automatic driving and computer vision. The implementation scheme is as follows: the method comprises the steps of projecting a point cloud data set acquired by a vehicle in a driving process into an acquired image to acquire first position information of a plurality of elements in the image; acquiring elements to be updated from a plurality of elements; acquiring current semantic information of an element to be updated from an image; acquiring historical semantic information of the element to be updated from historical map data according to first position information of the element to be updated in the image; and if the historical semantic information of the element to be updated is not matched with the current semantic information, updating the map data according to the current semantic information of the element to be updated. And the high-precision semantic information is acquired through merging the point cloud data and the image information, so that the high-precision map data is updated based on the high-precision semantic information, and the accuracy of the map data is improved.","['G06F16/29', 'G06F16/23', 'G06V10/267', 'G06V20/00']"
CN115239735B,Communication cabinet surface defect detection method based on computer vision,"The invention relates to the field of image processing, in particular to a communication cabinet surface defect detection method based on computer vision. The method comprises the following steps: collecting a surface image of the communication cabinet, drawing a gradient amplitude histogram based on gradient amplitudes of pixel points in the image to obtain a first threshold, screening suspected edge points and obtaining all suspected edges; segmenting the suspected edge; the method comprises the steps that smoothness and weight of each suspected edge are obtained based on the tangential direction of all pixel points in each suspected edge, further the edge rate of the suspected edge is obtained, an accurate edge is screened out, interference of noise points and a false edge is eliminated, and the defect that the surface of a communication cabinet is not obvious is obtained more accurately; the invention improves the reliability of detecting the surface defects of the communication cabinet.","['G06T7/0004', 'G06N3/04', 'G06N3/08', 'G06T5/20', 'G06T5/40', 'G06T5/70', 'G06T5/90', 'G06T7/13', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30164']"
US11875510B2,Generating refined segmentations masks via meticulous object segmentation,"The present disclosure relates to systems, methods, and non-transitory computer-readable media that utilizes a neural network having a hierarchy of hierarchical point-wise refining blocks to generate refined segmentation masks for high-resolution digital visual media items. For example, in one or more embodiments, the disclosed systems utilize a segmentation refinement neural network having an encoder and a recursive decoder to generate the refined segmentation masks. The recursive decoder includes a deconvolution branch for generating feature maps and a refinement branch for generating and refining segmentation masks. In particular, in some cases, the refinement branch includes a hierarchy of hierarchical point-wise refining blocks that recursively refine a segmentation mask generated for a digital visual media item. In some cases, the disclosed systems utilize a segmentation refinement neural network that includes a low-resolution network and a high-resolution network, each including an encoder and a recursive decoder, to generate the refined segmentation masks.","['G06V10/82', 'G06T7/11', 'G06F18/213', 'G06N3/044', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06T3/4046', 'G06V10/26', 'G06V10/75', 'G06T2207/20016', 'G06T2207/20084']"
US10671892B1,"Apparatuses, methods, and systems for 3-channel dynamic contextual script recognition using neural network image analytics and 4-tuple machine learning with enhanced templates and context data","In some embodiments, a method includes training a first machine learning model based on multiple documents and multiple templates associated with the multiple documents. The method further includes executing the first machine learning model to generate multiple relevancy masks, the multiple relevancy masks to remove a visual structure of the multiple templates from a visual structure of the multiple documents. The method further includes generating multiple multichannel field images to include the multiple relevancy masks and at least one of the multiple documents or the multiple templates. The method further includes training a second machine learning model based on the multiple multichannel field images and multiple non-native texts associated with the multiple documents. The method further includes executing the second machine learning model to generate multiple non-native texts from the multiple multichannel field images.","['G06K9/6262', 'G06F18/214', 'G06F18/217', 'G06F18/2413', 'G06F18/295', 'G06K9/46', 'G06K9/6256', 'G06N3/044', 'G06N3/0442', 'G06N3/0445', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'G06N3/09', 'G06N7/01', 'G06V10/764', 'G06V10/82', 'G06V10/85', 'G06V30/18', 'G06V30/412', 'G06K2209/01', 'G06V10/30', 'G06V2201/01', 'G06V30/1478', 'G06V30/2276']"
US5889886A,Method and apparatus for detecting running text in an image,"The present invention is a method and apparatus for analyzing image data, and more particularly for analyzing image data representing images containing text to partition the image into running and non-running text regions therein. The present invention utilizes characteristics of running text regions to identify such regions and to subsequently group all non-running text regions into related groups.",['G06V30/413']
US10860836B1,Generation of synthetic image data for computer vision models,Techniques are generally described for object detection in image data. First image data comprising a first plurality of pixel values representing an object and a second plurality of pixel values representing a background may be received. First foreground image data and first background image data may be generated from the first image data. A first feature vector representing the first plurality of pixel values may be generated. A second feature vector representing a first plurality of pixel values of second background image data may be generated. A first machine learning model may determine a first operation to perform on the first foreground image data. A transformed representation of the first foreground image data may be generated by performing the first operation on the first foreground image data. Composite image data may be generated by compositing the transformed representation of the first foreground image data with the second background image data.,"['G06T7/194', 'G06K9/00221', 'G06F18/2413', 'G06N20/00', 'G06N3/045', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06N3/094', 'G06T7/11', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V40/16', 'G06T2207/20081', 'G06T2207/20084']"
US11676279B2,Utilizing a segmentation neural network to process initial object segmentations and object user indicators within a digital image to generate improved object segmentations,"The present disclosure relates to systems, non-transitory computer-readable media, and methods that utilize a deep neural network to process object user indicators and an initial object segmentation from a digital image to efficiently and flexibly generate accurate object segmentations. In particular, the disclosed systems can determine an initial object segmentation for the digital image (e.g., utilizing an object segmentation model or interactive selection processes). In addition, the disclosed systems can identify an object user indicator for correcting the initial object segmentation and generate a distance map reflecting distances between pixels of the digital image and the object user indicator. The disclosed systems can generate an image-interaction-segmentation triplet by combining the digital image, the initial object segmentation, and the distance map. By processing the image-interaction-segmentation triplet utilizing the segmentation neural network, the disclosed systems can provide an updated object segmentation for display to a client device.","['G06T7/11', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/088', 'G06N3/09', 'G06T7/136', 'G06T7/143', 'G06T7/162', 'G06T7/90', 'G06T2200/24', 'G06T2207/20072', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20092']"
CN109961444B,Image processing method and device and electronic equipment,"The embodiment of the invention provides an image processing method and device and electronic equipment, and belongs to the technical field of computers. The image processing method comprises the following steps: acquiring an image to be processed; generating super pixels of the image to be processed, and randomly replacing pixels in the super pixels of the image to be processed to generate a segmented shielding image of the image to be processed; inputting the segmented shielding image of the image to be processed into a depth neural network to generate a rough saliency map of the image to be processed; inputting the rough saliency map and the image to be processed into a conditional random field model to generate a refined saliency map of the image to be processed for predicting salient objects in the image to be processed. The technical scheme of the embodiment of the invention can enhance the robustness of the detection of the salient object and improve the accuracy of the identification of the salient object.","['G06N3/044', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06V10/462', 'G06T2207/20024']"
CN111178251B,"Pedestrian attribute identification method and system, storage medium and terminal","The invention provides a pedestrian attribute identification method and system, a storage medium and a terminal, wherein the method comprises a self-supervision learning stage and an reasoning stage; the self-supervision learning stage comprises the steps of extracting human skeleton key points of training pictures; extracting candidate frames of human body blocks according to key points of human bones; acquiring binary attribute masks corresponding to the natural semantics of each candidate frame; extracting a three-dimensional feature map of the training picture; training a thermodynamic diagram for extracting human body blocks from the three-dimensional feature map based on a self-supervision learning algorithm; generating a block feature map of the human body block according to the three-dimensional feature map and the thermodynamic diagram; generating a corresponding tag probability set according to each block feature map; generating a pedestrian attribute identification result according to the tag probability set and the binary attribute mask; and the reasoning stage carries out pedestrian attribute identification according to the model acquired in the self-supervision learning stage. The pedestrian attribute identification method and system, the storage medium and the terminal can accurately identify the pedestrian attribute and effectively reduce the network calculation complexity.","['G06V40/103', 'G06F18/241', 'G06N3/045', 'G06N3/048', 'G06N3/084', 'G06V10/44', 'G06V20/64', 'Y02T10/40']"
US11526668B2,"Method and apparatus for obtaining word vectors based on language model, device and storage medium","A method and apparatus for obtaining word vectors based on a language model, a device and a storage medium are disclosed, which relates to the field of natural language processing technologies in artificial intelligence. An implementation includes inputting each of at least two first sample text language materials into the language model, and outputting a context vector of a first word mask in each first sample text language material via the language model; determining the word vector corresponding to each first word mask based on a first word vector parameter matrix, a second word vector parameter matrix and a fully connected matrix respectively; and training the language model and the fully connected matrix based on the word vectors corresponding to the first word masks in the at least two first sample text language materials, so as to obtain the word vectors.","['G06F40/289', 'G06F40/30', 'G06F16/90332', 'G06F18/214', 'G06F18/2415', 'G06F40/205', 'G06F40/279', 'G06K9/6256', 'G06K9/6277', 'G06N20/00', 'G06N3/0442', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/096', 'G06F40/284']"
US12276731B2,Method of individual tree crown segmentation from airborne LiDAR data using novel gaussian filter and energy function minimization,"Provided are a method of individual tree crown segmentation from airborne LiDAR data using a novel Gaussian filter and energy function minimization. First, a dual Gaussian filter was designed with automated adaptive parameter assignment and a screening strategy for false treetops. This preserved the geometric characteristics of sub-canopy trees while eliminating false treetops. Second, anisotropic water expansion controlled by the energy function was applied to accurate crown segmentation. This utilized gradient information from the digital surface model and explored the morphological structures of tree crown boundaries as analogous to the maximal valley height difference from surrounding treetops. We demonstrate the generality of our approach using seven diverse plots in the subtropical Gaofeng Forest, China, coupled with ground verification. Our approach enhanced the detection rate of treetops and ITC segmentation relative to the marked-control watershed method, especially in complicated intersections of multiple crowns.","['G06V20/17', 'G01S17/89', 'G06V10/30', 'G06V20/182', 'G06V20/188']"
US9993182B2,Computer vision system for ambient long-term gait assessment,"A method for gait analysis of a subject performed periodically over time to detect changes in one or more gait characteristics. The method includes detecting and identifying a subject and analyzing the gait of the subject on a plurality of occasions. Analyzing the gait of the subject includes, a detecting body parts, generating a joint model depicting the location of the at least one joint in each of the at least two frames, using the joint model to segment a gait cycle for the at least one joint, and comparing the gait cycle to a threshold value to detect abnormal gait.","['A61B5/112', 'A61B5/7235', 'A61B5/1128', 'A61B5/117', 'A61B5/7275', 'G06K9/00348', 'G06K9/00362', 'G06K9/4604', 'G06K9/6202', 'G06T7/0081', 'G06T7/11', 'G06T7/20', 'G06T7/246', 'G06T7/285', 'G06V40/10', 'G06V40/23', 'G06V40/25', 'G06T2207/10004', 'G06T2207/20164', 'G06T2207/30196', 'G06V10/62']"
US11615519B2,Method and apparatus for identifying concrete crack based on video semantic segmentation technology,"A method and apparatus for identifying a concrete crack includes: obtaining a crack video, and manually annotating a video image frame by using a label; predicting a future frame and label for the annotated frame by using a spatial displacement convolutional block, propagating the future frame and label, to obtain a synthetic sample, and preprocessing the synthetic sample, to form a crack database; modifying input and output ports of data of a deep learning model for video semantic image segmentation and a parameter, to enable the deep learning model to accept video input, and establishing a concrete crack detection model based on the video output; using a convolutional layer in a trained deep learning model as an initial weight of the concrete crack detection model for migration; inputting the crack database into a migrated concrete crack detection model, and training the concrete crack detection model for crack data.","['G06T7/0004', 'G06T7/0008', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/096', 'G06T7/10', 'G06T7/11', 'G06V10/82', 'G06V20/41', 'G06V20/49', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30132']"
US10642364B2,Processing tracking and recognition data in gestural recognition systems,"Systems and methods are described for detecting an event of a source device, and generating at least one data sequence comprising device event data specifying the event and state information of the event. The device event data and state information are type-specific data having a type corresponding to an application of the source device. A data capsule is formed to include the at least one data sequence. The data capsule has a data structure comprising an application-independent representation of the at least one data sequence. The systems and methods detect poses and motion of an object, translate the poses and motion into a control signal using a gesture notation, and control a computer application using the control signal. The systems and methods automatically detect a gesture of a body, translate the gesture to a gesture signal, and control a component coupled to a computer in response to the gesture signal.","['G06F3/017', 'G06F3/005', 'G06F3/011', 'G06F3/0325', 'G06K9/00201', 'G06K9/00335', 'G06K9/00362', 'G06K9/00375', 'G06V20/64', 'G06V40/10', 'G06V40/107', 'G06V40/20', 'H04N7/15']"
US12205024B2,Computing device and method of classifying category of data,A device and method for classifying a category of data are provided. The device includes: a memory storing one or more instructions; and at least one processor configured to execute the one or more instructions stored in the memory to cause the processor to: identify a classification system of a category of data comprising a classification criterion of the category of the data and a plurality of keywords; obtain data comprising at least one sentence; and determine at least one category with respect to the at least one sentence of the data based on the classification system of the category of the data using a neural network that performs classification by unsupervised learning.,"['G06F40/205', 'G06F16/35', 'G06F40/295', 'G06F40/30', 'G06N3/044', 'G06N3/045', 'G06N3/0464', 'G06N3/0499', 'G06N3/08', 'G06N3/088']"
US9811721B2,Three-dimensional hand tracking using depth sequences,"In the field of Human-computer interaction (HCI), i.e., the study of the interfaces between people (i.e., users) and computers, understanding the intentions and desires of how the user wishes to interact with the computer is a very important problem. The ability to understand human gestures, and, in particular, hand gestures, as they relate to HCI, is a very important aspect in understanding the intentions and desires of the user in a wide variety of applications. In this disclosure, a novel system and method for three-dimensional hand tracking using depth sequences is described. Some of the major contributions of the hand tracking system described herein include: 1.) a robust hand detector that is invariant to scene background changes; 2.) a bi-directional tracking algorithm that prevents detected hands from always drifting closer to the front of the scene (i.e., forward along the z-axis of the scene); and 3.) various hand verification heuristics.","['G06F3/017', 'G06K9/00355', 'G06F3/0304', 'G06F3/0425', 'G06K9/4609', 'G06T7/246', 'G06T7/254', 'G06V40/28', 'H04N13/0207', 'H04N13/0271', 'H04N13/207', 'H04N13/271', 'G06T2200/04', 'G06T2207/30196', 'H04N2013/0085']"
US10915793B2,Method and system for converting point cloud data for use with 2D convolutional neural networks,"Methods and systems for encoding 3D data for use with 2D convolutional neural networks (CNNs) are described. A set of 3D data is encoded into a set of one or more arrays. A 2D index of the arrays is calculated by projecting 3D coordinates of the 3D point onto a 2D image plane that is defined by a set of defined virtual camera parameters. The virtual camera parameters include a camera projection matrix defining the 2D image plane. Each 3D coordinate of the point is stored in the arrays at the calculated 2D index. The set of encoded arrays is provided for input to a 2D CNN, for training or inference.","['G06V20/64', 'G06K9/6262', 'G06F18/217', 'G06F18/2413', 'G06F18/251', 'G06K9/627', 'G06V10/25', 'G06V10/255', 'G06V10/454', 'G06V10/764', 'G06V10/803', 'G06V10/82', 'G06V20/58']"
US11954886B2,Systems and methods for six-degree of freedom pose estimation of deformable objects,"A method for estimating a pose of a deformable object includes: receiving, by a processor, a plurality of images depicting the deformable object from multiple viewpoints; computing, by the processor, one or more object-level correspondences and a class of the deformable object depicted in the images; loading, by the processor, a 3-D model corresponding to the class of the deformable object; aligning, by the processor, the 3-D model to the deformable object depicted in the plurality of images to compute a six-degree of freedom (6-DoF) pose of the object; and outputting, by the processor, the 3-D model and the 6-DoF pose of the object.","['G06T7/75', 'G06F18/214', 'G06V10/143', 'G06V10/147', 'G06V10/16', 'G06V10/40', 'G06V10/74', 'G06V10/774', 'G06V10/82', 'G06V20/647', 'G06T2207/20081', 'G06T2207/20084', 'G06V2201/06']"
CN113128405B,Plant identification and model construction method combining semantic segmentation and point cloud processing,"The invention provides a plant identification and model construction method combining semantic segmentation and point cloud processing, which comprises the following steps: 1. generating an orthoimage according to a landscape image obtained by oblique photography; 2. training a deep learning network, and performing semantic segmentation on the orthoimage by using a neural network to identify a plant region; 3. generating point cloud corresponding to the image, and realizing the coordinate correspondence between the point cloud data and the orthoimage through coordinate system conversion; 4. dividing the point cloud data to obtain a plant area point cloud; 5. combining the oblique photographic image and the point cloud data, and further identifying the plant species through methods such as k-means point cloud clustering and target detection; 6. establishing a plant model library; 7. processing the point cloud of the plant area, determining parameters such as plant species, position, size and the like contained in the point cloud, and introducing a plant model to replace the point cloud; 8. the plant model is converted into the desired format. The method can realize efficient and accurate identification of plant species and three-dimensional plant scene construction with sense of reality.","['G06F18/24', 'G06F18/214', 'G06F18/23213', 'G06N3/045', 'G06N3/08', 'G06V10/267', 'G06V20/13', 'G06V20/188']"
US11024093B2,Live augmented reality guides,"Various embodiments of the present invention relate generally to systems and methods for analyzing and manipulating images and video. According to particular embodiments, the spatial relationship between multiple images and video is analyzed together with location information data, for purposes of creating a representation referred to herein as a surround view for presentation on a device. A visual guide can provided for capturing the multiple images used in the surround view. The visual guide can be a synthetic object that is rendered in real-time into the images output to a display of an image capture device. The visual guide can help user keep the image capture device moving along a desired trajectory.","['G06T19/006', 'G06K9/00671', 'G06K9/22', 'G06T7/33', 'G06V10/17', 'G06V20/20', 'G06K2009/366', 'G06K9/00228', 'G06K9/3233', 'G06T2200/32', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/30241', 'G06T2207/30244', 'G06V10/248', 'G06V10/25', 'G06V40/161']"
US7231063B2,Fiducial detection system,"A new fiducial design allows having thousands of different codes. These fiducials are printed on a standard black-and-white (or color) printer and easily could be mounted on a walls, ceiling and objects in a room. The design includes “solid” outside mono-color ring and 2-D dense inside coding scheme. Image processing algorithms are implemented in Smart Camera with a built-in DSP to run all required image-processing tasks. A tracking system implementation includes an inertial measurement unit and one outward-looking wide-angle Smart Camera with possible extensions to number of stationary inward-looking cameras. The system operates in various real-world lighting conditions without any user intervention due to homomorphic image processing processes for extracting fiducials in the presence of very non-uniform lighting.","['G06K19/06131', 'G06K19/06168', 'G06V10/443', 'G06K2019/06243', 'G06V10/245']"
CN109409272B,Cable acceptance system and method based on machine vision,"The invention relates to a cable acceptance system and a method based on machine vision, wherein the system comprises: the image shooting module is used for collecting various appearance images of the cable and the connector through the external control shooting equipment; the image processing module is used for filtering impurity information from the acquired image and extracting identification characteristic information through character positioning, character segmentation and character identification; the data comparison module is used for comparing the characteristic information extracted from the image with the prefabricated knowledge information; the self-learning module is used for learning images of each standard lead and each binding wire through the image input interface, learning the code number of the electric connector, the model number of the electric connector, the character lettering mode of the code number of the cable and the color information of the binding wire, extracting characteristic information from the information and curing the characteristic information into knowledge; the human-computer interface module is used for providing interface control display and sound reminding for the system and an operator; and the information data interaction module is used for storing the acceptance data and supporting the import of the information to be verified and the export of the acceptance data.","['G06V20/10', 'G06V10/22', 'G06V20/63', 'G07C3/00', 'G06V30/10']"
US10261574B2,Real-time detection system for parked vehicles,"The present invention discloses a real-time detection system based on hybrid background modeling for detecting parked vehicles along the side of a road. The hybrid background model consists of three components: 1) a scene background model, 2) a computed restricted area map, and 3) a dynamic threshold curve for vehicles. By exploiting the motion information of normal activity in the scene, we propose a hybrid background model that determines the location of the road, estimates the roadside and generates the adaptive threshold of the vehicle size. The system triggers a notification when a large vehicle-like foreground object has been stationary for more than a pre-set number of video frames (or time). The present invention is tested on the AVSS 2007 PV dataset. The results are satisfactory compared to other state-of-the-art methods.","['G06F3/00', 'G06K9/00785', 'G06T7/00', 'G06T7/254', 'G06V20/54', 'G08G1/0116', 'G08G1/0133', 'G08G1/0175', 'G08G1/04', 'G06K9/00825', 'G06T2207/20036', 'G06T2207/20081', 'G06T2207/30236', 'G06V20/584']"
CA2636858C,Methods and systems for digitally re-mastering of 2d and 3d motion pictures for exhibition with enhanced visual quality,"The present invention relates to methods and systems for the exhibition of a motion picture with enhanced perceived resolution and visual quality. The enhancement of perceived resolution is achieved both spatially and temporally.
Spatial resolution enhancement creates image details using both temporal-based methods and learning-based methods. Temporal resolution enhancement creates synthesized new image frames that enable a motion picture to be displayed at a higher frame rate. The digitally enhanced motion picture is to be exhibited using a projection system or a display device that supports a higher frame rate and/or a higher display resolution than what is required for the original motion picture.","['H04N13/139', 'G06T3/4007', 'G06T7/207', 'G06T7/215', 'G06T7/269', 'H04N19/52', 'H04N19/527', 'H04N19/537', 'H04N19/577', 'H04N19/59', 'H04N5/253', 'H04N7/0112', 'H04N7/0127']"
CN115457035B,Machine vision-based construction hanging basket welding quality detection method,"The invention discloses a machine vision-based construction hanging basket welding quality detection method, which relates to the field of image processing and comprises the following steps: acquiring a gray level image of a welding seam image; dividing the gray level image into a plurality of image blocks by using the central columns of adjacent wave troughs in the fitted curve; obtaining the diameter of the suspected pore defect in the image block by utilizing the gray value of the suspected defect point in the trough column and the gray average value of the trough column; obtaining the probability of suspected defects of each corresponding pixel point by utilizing the gray value of the target pixel point; acquiring the abnormal probability of each gray value; obtaining a plurality of suspected segmentation threshold values by using the difference of the abnormal probability of the gray values; obtaining an optimal segmentation threshold by using the segmentation weight and the inter-class variance of each suspected segmentation threshold; the method and the device utilize the gray difference value of the target area and the background area to determine whether the target area is the air hole defect area or not so as to determine whether the welding quality is qualified or not, and improve the accuracy of welding quality detection.","['G06T7/0002', 'G06T7/136', 'G06T7/62', 'G06T2207/20021', 'Y02P90/30']"
US9300947B2,Producing 3D images from captured 2D video,"A method of producing a stereo image from a temporal sequence of digital images, comprising: receiving a temporal sequence of digital images; analyzing pairs of digital images to produce corresponding stereo suitability scores, wherein the stereo suitability score for a particular pair of images is determined responsive to the relative positions of corresponding features in the particular pair of digital image; selecting a pair of digital images including a first image and a second image based on the stereo suitability scores; using a processor to analyze the selected pair of digital images to produce a motion consistency map indicating regions of consistent motion, the motion consistency map having an array of pixels; producing a stereo image pair including a left view image and a right view image by combining the first image and the second image responsive to the motion consistency map; and storing the stereo image pair in a processor-accessible memory.","['H04N13/0264', 'H04N13/264', 'G06K9/32', 'G06T7/0075', 'G06T7/593', 'H04N13/211', 'H04N13/221', 'H04N13/0221']"
US20240177836A1,Systems and methods for artificial intelligence-assisted image analysis,"Disclosed herein are systems, methods, and software for providing a platform for artificial intelligence-assisted image analysis.","['G16H30/40', 'G06T7/0012', 'A61B5/4566', 'A61B5/742', 'G06F40/134', 'G06F40/20', 'G06N20/00', 'G06N3/08', 'G06T7/11', 'G06V10/25', 'G06V10/82', 'G16H10/60', 'G16H15/00', 'G16H30/20', 'G16H50/20', 'G16H50/70', 'G16H70/00', 'G06T2207/10056', 'G06T2207/10068', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10104', 'G06T2207/10108', 'G06T2207/10116', 'G06T2207/10132', 'G06T2207/30004', 'G06V2201/03']"
CN112329725B,"Method, device and equipment for identifying elements of road scene and storage medium","The application discloses a method, a device, equipment and a storage medium for identifying elements of a road scene, which are applied to the computer vision technology of artificial intelligence. By acquiring a target image; then inputting the target image into a first recognition layer in a target recognition model to obtain the position information and the first recognition information of the target element; determining a mapping area image of the target feature in the feature extraction layer based on the position information; and inputting the mapping area image into a second identification layer corresponding to the first identification information to obtain second identification information. Therefore, the stage-type element recognition process is realized, and the target features in the first recognition layer are shared with the second recognition layer, so that the target features in the first recognition layer guide and optimize the recognition process in the second recognition layer, and the efficiency of element recognition in a road scene is further improved.","['G06V20/588', 'G06F18/213', 'G06N3/045', 'G06N3/08', 'G06V10/25', 'G06V10/44', 'G06V20/582']"
US8565525B2,Edge comparison in segmentation of video sequences,"A method of image processing, includes: receiving at least one video frame of a video sequence, the at least one video frame including at least one foreground subject and a background; and processing the at least one video frame so as to separate the at least one foreground subject from the background. The processing includes: obtaining a reference image including the background; comparing the at least one video frame to the reference image; and generating a pixel mask as a result of the comparison, the pixel mask indicating whether a pixel of the at least one video frame belongs to the foreground subject or to the background. The method further comprises at least partially determining edges of the at least one foreground subject in the at least one video frame, and modifying the pixel mask based on the determined foreground subject edges.","['G06T7/12', 'G06T7/155', 'G06T7/194', 'G06V10/28', 'G06T2207/10016', 'G06T2207/20081']"
US12333715B2,Method and system for selecting embryos,"An Artificial Intelligence (AI) computational system for generating an embryo viability score from a single image of an embryo to aid selection of an embryo for implantation in an In-Vitro Fertilisation (IVF) procedure is described. The AI model uses a deep learning method applied to images in which the Zona Pellucida region in the image is identified using segmentation, and ground truth labels such as detection of a heartbeat at a six week ultrasound scan.","['G06N20/20', 'G06F18/24133', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/096', 'G06T7/0012', 'G06T7/11', 'G06T7/12', 'G06T7/136', 'G06T7/149', 'G06V10/50', 'G06V10/54', 'G06V10/82', 'G06V20/695', 'A61B17/435', 'G06F17/145', 'G06T2200/28', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/20061', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30044']"
CN107945267B,Method and equipment for fusing textures of three-dimensional model of human face,"The invention discloses a method and equipment for fusing textures of a three-dimensional model of a human face, which can enable the result after the texture fusion to be more natural and avoid the phenomenon of uneven and unnatural transition easily occurring on two sides of a nose wing during texture mapping. The method comprises the following steps: inputting three-dimensional model data of a human face; inputting a plurality of face texture images at different viewing angles; carrying out color correction on the face texture image; calculating the visibility of each triangular patch in each texture camera; calculating texture weight values of each triangular face relative to each texture camera; correcting texture weight values of the triangular face in a preset area on the human face three-dimensional model relative to the front face texture camera; smoothing and normalizing the texture weight value relative to each texture camera; and performing texture fusion according to the texture weight of each triangular face relative to each texture camera, and acquiring the three-dimensional texture of the face.","['G06T17/00', 'G06T15/04', 'G06T3/4038', 'G06T5/50', 'G06T7/90']"
CN109492643B,"Certificate identification method and device based on OCR, computer equipment and storage medium","The invention discloses an OCR-based certificate recognition method, a device, a computer device and a storage medium, wherein the OCR-based certificate recognition method comprises the following steps: acquiring an original certificate image and a certificate type; preprocessing an original certificate image to obtain a certificate image to be identified; performing text positioning on the certificate image to be recognized by adopting a text positioning model to obtain the text position to be recognized; cutting the certificate image to be identified based on the position of the text to be identified to obtain a text area to be identified; classifying the text region to be identified by adopting a preset region template corresponding to the certificate type, and obtaining the text category corresponding to the text region to be identified; based on the character category corresponding to the character area to be identified, the character area to be identified is input into a target identification model corresponding to the character category for identification, and certificate text information corresponding to the character area to be identified is obtained.",['G06V30/153']
US7035461B2,Method for detecting objects in digital images,"A method for detecting objects in a digital image includes the steps of generating a first segmentation map of the digital image according to a non-object specific criterion, generating a second segmentation map of the digital image according to an object specific criterion, and detecting objects in the digital image using both the first and second segmentation maps. In a preferred embodiment of the invention, the non-object specific criterion is a color homogeneity criterion and the object specific criterion is an object specific color similarity, wherein the object specific color is skin color and the method further comprises the step of detecting red-eye in the detected skin color regions.","['G06T7/11', 'G06V40/161', 'G06V40/193', 'G06T2207/30216']"
US8958600B2,Monocular 3D pose estimation and tracking by detection,"Methods and apparatus are described for monocular 3D human pose estimation and tracking, which are able to recover poses of people in realistic street conditions captured using a monocular, potentially moving camera. Embodiments of the present invention provide a three-stage process involving estimating (10, 60, 110) a 3D pose of each of the multiple objects using an output of 2D tracking-by detection (50) and 2D viewpoint estimation (46). The present invention provides a sound Bayesian formulation to address the above problems. The present invention can provide articulated 3D tracking in realistic street conditions.","['G06V20/64', 'G06V40/103', 'G06K9/00201', 'G06K9/00362', 'G06K9/00369', 'G06T7/2046', 'G06T7/251', 'G06V40/10', 'G06T2207/10016', 'G06T2207/30196', 'G06T2207/30232', 'G06T2207/30244']"
CN116309510B,Numerical control machining surface defect positioning method and device,"The application relates to the technical field of intelligent manufacturing, in particular to a method and a device for positioning defects on a numerical control machining surface, wherein the method comprises the following steps: the method comprises the steps of obtaining an image to be detected of a numerical control machining surface by adopting a linear array camera, filtering and denoising the image to be detected by utilizing a Gaussian convolution kernel to obtain a smooth image, calculating the global gray gradient of the smooth image, calculating the initial position of a defect edge, carrying out refinement treatment on the defect edge image based on a skeleton extraction algorithm, carrying out fitting calculation on the relative position relationship between a sub-pixel level edge point and a pixel level edge point based on an improved sub-pixel edge fitting algorithm and the processed defect edge image, and obtaining the accurate position of the defect edge of the numerical control machining surface. According to the method and the device for detecting the surface defects, the positions of the edges of the corresponding defects can be obtained through calculation and fitting based on the images to be detected of the numerical control machining surface, so that accurate measurement of the actual positions of the surface defects is achieved, and defect positioning results are accurate and reliable.","['G06T7/0004', 'G06T5/70', 'G06T7/13', 'G06T7/136', 'G06T7/73', 'G06T7/90', 'G06T2207/10004', 'G06T2207/30164']"
CN109550712B,Chemical fiber filament tail fiber appearance defect detection system and method,"The invention discloses a chemical fiber filament tail appearance defect detection system and method. The system comprises a tray for loading the silk ingots and a conveyor belt for conveying the tray, wherein the silk ingots are provided with labels, a camera bellows is arranged on the conveyor belt, a sorting unit is arranged on the conveyor belt behind the camera bellows, an image acquisition unit for acquiring label images and silk ingot images is arranged in the camera bellows, and the image acquisition unit sends acquisition information to a processing unit for tail silk defect detection. The system identifies and reads label information from the label graph, screens an image to be detected with a silk spindle paper tube from the silk spindle image, carries out edge detection on the paper tube area of the image to be detected, extracts a main characteristic description paper tube area, brings the processed image to be detected into a tail fiber classifier for classification, obtains silk spindle tail fiber detection information and sends the information to a sorting unit, and the sorting unit sorts good and defective silk spindles according to the classification of the silk spindle tail fibers. The invention saves cost, has higher working efficiency and more accurate detection.","['B07C5/3422', 'G01N21/892']"
US10963993B2,"Image noise intensity estimation method, image noise intensity estimation device, and image recognition device","An image noise intensity estimation method, an image noise intensity estimation device, and an image recognition device are disclosed. The method includes: obtaining a first image to be estimated; filtering the first image to obtain a second image; dividing the first and second images to obtain a plurality of first image sub-blocks and a plurality of second image sub-blocks respectively; calculating error values between the first image sub-blocks and the second image sub-blocks in corresponding positions; and estimating the noise intensity of the first image according to a plurality of error values obtained by calculation. The method can improve the accuracy of noise estimation.","['G06T5/70', 'G06T5/002', 'G06T5/40', 'G06T5/50', 'G06T7/0002', 'G06T7/11', 'G06T7/13', 'G06T2207/20021', 'G06T2207/20024', 'G06T2207/20182', 'G06T2207/30168']"
US11631193B1,System for estimating a pose of one or more persons in a scene,A system for estimating a pose of one or more persons in a scene includes a camera configured to capture one or more images of the scene; and a data processor configured to execute computer executable instructions for: (i) receiving the one or more images of the scene from the camera; (ii) extracting features from the one or more images of the scene for providing inputs to a keypoint subnet and a person detection subnet; (iii) generating one or more keypoints using the keypoint subnet; (iv) generating one or more person instances using the person detection subnet; (v) assigning the one or more keypoints to the one or more person instances by learning pose structures from image data; and (vi) determining one or more poses of the one or more persons in the scene using the assignment of the one or more keypoints to the one or more person instances.,"['G06T7/73', 'G06V10/34', 'G06F18/21', 'G06K9/6217', 'G06N3/045', 'G06N3/0454', 'G06V10/22', 'G06V10/40', 'G06V10/82', 'G06V40/10', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30196', 'G06V2201/07', 'H04N23/90', 'H04N5/247']"
US11651499B2,Reducing structural redundancy in automatic image segmentation,"A method for automatically training and applying automatic segmentation in digital image processing is provided. The method may include, in response to receiving a plurality of digital images wherein each digital image associated with the plurality of digital images comprises only one annotated structure out of a plurality of structures included in each digital image, applying a predictive algorithm to each digital image that determines a predicted probability of each annotation in each digital image, determines a predicted background for each digital image, and merges the predicted probability of each annotation with the predicted background in each digital image. The method may further include, in response to applying the predictive algorithm, using the received plurality of digital images to train and apply an application for automatically segmenting unlabeled digital images.","['G06T7/143', 'G06T7/0012', 'G06T7/11', 'G16H30/40', 'G16H40/67', 'G06T2200/24', 'G06T2207/10116', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20128']"
CN104809689B,A kind of building point cloud model base map method for registering based on profile,"The present invention relates to a kind of building point cloud model base map method for registering based on profile, belong to technical field of computer vision.The inventive method carries out structural analysis by the building point cloud model obtained to three-dimensional reconstruction, the posture of automatic detection ""current"" model, finally realizes the normalization of point cloud model coordinate, and the point cloud for obtaining current building on this basis overlooks lines of outline figure；Edge analysis is carried out to the remote sensing images for rebuilding building region, automatic detection obtains the lines of outline of base map；The structure flex point that lines of outline estimation according to building is rebuild currently is built, then by the spatial alignment between outline information realization point cloud model and satellite base map, obtain, from point cloud model to the mapping matrix satellite base map, realizing placement of the building point cloud model on satellite base map automatically.Prior art is contrasted, the inventive method can accurately carry out automation registration to point cloud model and base map, labor workload effectively needed for reduction registration, reduce registering cost.",['G06T3/147']
US10223838B2,Method and system of mobile-device control with a plurality of fixed-gradient focused digital cameras,"In one aspect, a method of a mobile-device control with a plurality of rear-facing fixed-focus image sensors including the step of providing a mobile device. The mobile device comprises an array comprising a plurality of rear-facing fixed-focus image sensors. Each fixed-focus image sensors comprises a different focus range value. The method includes the step of associating each rear-facing fixed-focus image sensors with a command input of the mobile device. The method includes the step of detecting a specified object in a depth of field of a specified rear-facing fixed-focus image sensor of the rear-facing fixed-focus image sensors. The method includes the step of implementing the command input of the mobile device associated with the specified rear-facing fixed-focus image sensor.","['G06T19/20', 'G06F3/005', 'G06F3/013', 'G06F3/017', 'G06F3/0304', 'G06T19/006', 'G06F3/033', 'G06F3/048', 'H04N23/45', 'H04N5/2258']"
US8515138B2,Image processing method and apparatus,An image processing technique includes acquiring a main image of a scene and determining one or more facial regions in the main image. The facial regions are analysed to determine if any of the facial regions includes a defect. A sequence of relatively low resolution images nominally of the same scene is also acquired. One or more sets of low resolution facial regions in the sequence of low resolution images are determined and analysed for defects. Defect free facial regions of a set are combined to provide a high quality defect free facial region. At least a portion of any defective facial regions of the main image are corrected with image information from a corresponding high quality defect free facial region.,"['G06V40/168', 'G03B19/02', 'G06F18/28', 'G06V10/772', 'G06V40/172', 'G06V40/176', 'H04N23/667']"
CN111091576B,"Image segmentation method, device, equipment and storage medium","The application discloses an image segmentation method, an image segmentation device, image segmentation equipment and a storage medium, and belongs to the technical field of image processing. According to the method, a target image and a first mask are respectively processed through neural networks with different scales in an image segmentation model, a plurality of feature maps with different scales and a middle mask are obtained, and high-resolution image information is fully reserved; applying attention weight to each feature map by a plurality of attention units to carry out weighted operation to obtain a plurality of intermediate feature maps; performing feature fusion on each intermediate feature map to obtain a second mask; and if the second mask meets the condition, performing image segmentation based on the second mask, otherwise, adjusting the first mask based on the second mask, outputting a new second mask based on the adjusted first mask and the target image, and judging whether the new second mask meets the condition. In the scheme, three ways of attention mechanism, multi-scale feature fusion and iterative adjustment of the segmentation region are applied to improve the accuracy of the image segmentation result.","['G06T7/11', 'G06F18/253', 'G06N3/045', 'G06T2207/20081', 'G06T2207/20084']"
CN110458249B,Focus classification system based on deep learning and probabilistic imaging omics,"The invention relates to a focus classification system based on deep learning and probabilistic imaging omics, and belongs to the technical field of medical image classification. Aiming at the problems of ambiguity and low classification precision caused by classification ambiguity of the existing focus classification system, the invention provides a non-local shape analysis module to extract feature cloud of a focus on a medical image by taking a deep convolutional neural network as a main trunk, removes the interference of peripheral pixels of the focus on classification judgment and obtains the essential representation of the focus; meanwhile, in order to capture the ambiguity of the label, a fuzzy prior network is provided to simulate the ambiguity distribution of different expert labels, the ambiguity of the modeled expert labels is displayed, the classification result of model training has better robustness, a fuzzy prior sample is combined with lesion characterization, a new lesion classification system is constructed, and the system has controllability and probability.","['G06F18/214', 'G06F18/2415', 'G06N3/045', 'G06N3/084', 'G06T7/0012', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/30096']"
US7693349B2,Systems and methods for interactive image registration,"System and method for incorporating user input on the fly during an otherwise automatic registration process. During rigid registration, user input adjusts the current computed pose or transformation that relates the two images being aligned. During warping, user input adjusts the flow field locally, and is gradually smoothed into the surrounding flow field. During multi-scale registration where images are first aligned at a course resolution, and subsequently at progressively finer resolutions, user input is applied at the current scale. User input is detected during the automated process either by interrupts or polling. Between user inputs the registration results are re-rendered.","['G06V10/7515', 'A61B8/565']"
US8380558B1,Method and system for analyzing shopping behavior in a store by associating RFID data with video-based behavior and segmentation data,"The present invention is a method and system for analyzing shopping behavior by associating RFID data, such as tracking data by the RFID tag identifications, with video-based behavior and segmentation data, such as behavior analysis and demographic composition analysis of the customers, utilizing a plurality of means for sensing and using RFID tags, a plurality of means for capturing images, and a plurality of computer vision technologies. In the present invention, the association can further comprise the association of the RFID with the transaction data or any time-based measurement in the retail space. The analyzed shopping behavior in the present invention helps people to better understand business elements in a retail space. It is one of the objectives of the present invention to provide an automatic video-based segmentation of customers in the association with the RFID based tracking of the customers, based on a novel usage of a plurality of means for capturing images and a plurality of computer vision technologies on the captured visual information of the people in the retail space. The plurality of computer vision technologies can comprise face detection, person tracking, body parts detection, and demographic classification of the people, on the captured visual information of the people in the retail space.",['G06Q30/02']
US11928822B2,Intersection region detection and classification for autonomous machine applications,"In various examples, live perception from sensors of a vehicle may be leveraged to detect and classify intersection contention areas in an environment of a vehicle in real-time or near real-time. For example, a deep neural network (DNN) may be trained to compute outputs—such as signed distance functions—that may correspond to locations of boundaries delineating intersection contention areas. The signed distance functions may be decoded and/or post-processed to determine instance segmentation masks representing locations and classifications of intersection areas or regions. The locations of the intersections areas or regions may be generated in image-space and converted to world-space coordinates to aid an autonomous or semi-autonomous vehicle in navigating intersections according to rules of the road, traffic priority considerations, and/or the like.","['G06T7/11', 'G05B13/027', 'G06F18/21', 'G06F18/24', 'G06F18/2414', 'G06N20/00', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T11/20', 'G06T11/60', 'G06T3/4046', 'G06T5/002', 'G06T5/70', 'G06V10/267', 'G06V10/34', 'G06V10/454', 'G06V10/82', 'G06V20/56', 'G06V30/19173', 'G06V30/274', 'G06N3/063', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20152', 'G06T2207/30236', 'G06T2207/30252', 'G06T2210/12']"
CN114066811B,"Industrial product abnormality detection method, system, device and storage medium","The invention discloses an industrial product abnormality detection method, a system, a device and a storage medium, wherein the method comprises the following steps: obtaining an image to be detected, inputting the image to be detected into an encoder to obtain a first coding feature, and performing cosine similarity calculation according to the first coding feature and a preset global template feature to obtain a similarity score graph; comparing the similarity score graph with a preset threshold score graph to obtain abnormal features, and replacing the abnormal features with template features at corresponding positions; inputting the coding features with the corrected features into a decoder to obtain a reconstructed image; and calculating similarity scores of the image to be detected and the reconstructed image, and performing threshold segmentation according to the similarity scores to obtain an image region with an abnormality in the image to be detected. The invention establishes template characteristics according to the characteristics of the actual detection image, overcomes the defect that the reconstruction model cannot reconstruct serious abnormality, has self-adaptability and anti-interference capability, and can be widely applied to the technical field of industrial product abnormality detection.","['G06T7/0004', 'G06F18/22', 'G06F18/23213', 'G06N3/045', 'G06N3/08', 'G06T5/20', 'G06T7/136']"
CN111906784B,Pharyngeal swab double-arm sampling robot based on machine vision guidance and sampling method,"The invention discloses a pharynx swab double-arm sampling robot and a sampling method based on machine vision guidance, and provides a pharynx swab double-arm sampling robot and a sampling method based on machine vision guidance. The invention can avoid the direct contact between medical care personnel and infectious matters in the body of a patient, monitor the double-arm robot to finish the complicated throat swab sampling process, ensure higher execution precision and success rate, and avoid the accidental risks of injury and the like of the patient in the sampling process.","['B25J9/1602', 'A61B10/0045', 'B25J9/1682', 'B25J9/1697', 'G06T17/00']"
CN106682633B,The classifying identification method of stool examination image visible component based on machine vision,"The present invention relates to technical field of image processing, disclose a kind of classifying identification method of stool examination image visible component based on machine vision.It first uses Level Set Method to carry out image segmentation to stool examination image, then the shape contour of the visible component obtained for segmentation, successively carry out the preliminary classification based on morphological feature and the verification identification based on HOG+VSM training classifier, high-precision visible component classification recognition result is quickly finally obtained, so that patient is gone to a doctor in time.In addition, this method also has many advantages, such as that image segmentation accuracy is high, processing speed is fast and user experience is good, it is convenient for practical application and popularization.","['G06V20/695', 'G06F18/2411', 'G06V10/507', 'G06V20/698']"
WO2020228525A1,"Place recognition method and apparatus, model training method and apparatus for place recognition, and electronic device","Provided are a place recognition method and apparatus, a model training method and apparatus for place recognition, a computer readable storage medium and an electronic device. The model training method comprises: extracting local features of a sample image on the basis of a first portion of a CNN model (310); aggregating the local features into feature vectors having a first number of dimensions on the basis of a second portion of the CNN model (330); obtaining, on the basis of a third portion of the CNN model, compressed representation vectors of the feature vectors, wherein the compressed representation vectors have a second number of dimensions that is less than the first number of dimensions (350); and adjusting model parameters of the first to third portions with the aim of minimizing the distance between the compressed representation vectors corresponding to a plurality of images photographed at the same place (370). According to the method, a compression process with trainable parameters is introduced into a CNN model, so that an end-to-end training place recognition model can be fully realized, and the obtained CNN model can directly obtain low-dimensional image features, thereby improving the place recognition performance on the basis of artificial intelligence.","['G06V10/454', 'G06F16/583', 'G06F16/5866', 'G06F16/587', 'G06F18/21343', 'G06F18/2135', 'G06F18/2148', 'G06N20/10', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G06V10/7715', 'G06V10/7747', 'G06V10/82']"
US11906660B2,Object detection and classification using LiDAR range images for autonomous machine applications,"In various examples, a deep neural network (DNN) may be used to detect and classify animate objects and/or parts of an environment. The DNN may be trained using camera-to-LiDAR cross injection to generate reliable ground truth data for LiDAR range images. For example, annotations generated in the image domain may be propagated to the LiDAR domain to increase the accuracy of the ground truth data in the LiDAR domain—e.g., without requiring manual annotation in the LiDAR domain. Once trained, the DNN may output instance segmentation masks, class segmentation masks, and/or bounding shape proposals corresponding to two-dimensional (2D) LiDAR range images, and the outputs may be fused together to project the outputs into three-dimensional (3D) LiDAR point clouds. This 2D and/or 3D information output by the DNN may be provided to an autonomous vehicle drive stack to enable safe planning and control of the autonomous vehicle.","['G01S7/4802', 'G01S13/862', 'G01S13/865', 'G01S13/867', 'G01S13/931', 'G01S17/86', 'G01S17/894', 'G01S17/931', 'G01S7/481', 'G06F18/25', 'G06V10/764', 'G06V10/80', 'G06V10/82', 'G06V20/58', 'G01S2013/9316', 'G01S7/28']"
EP2345999A1,Method for automatic detection and tracking of multiple objects,"A method for automatically detecting and tracking objects in a scene. The method acquires video frames from a video camera; extracts discriminative features from the video frames; detects changes in the extracted features using background subtraction to produce a change map; uses the change map to use a hypothesis to estimate of an approximate number of people along with uncertainty in user specified locations; and using the estimate, track people and update the hypotheses for a refinement of the estimation of people count and location.","['G08B13/19608', 'G06T7/251', 'G06V20/52', 'G06T2207/10016', 'G06T2207/30196', 'G06T2207/30232', 'G06V10/62']"
US12236616B2,Method and system for automatic extraction of virtual on-body inertial measurement units,"An exemplary virtual IMU extraction system and method are disclosed for human activity recognition (HAR) or classifier system that can estimate inertial measurement units (IMU) of a person in video data extracted from public repositories of video data having weakly labeled video content. The exemplary virtual IMU extraction system and method of the human activity recognition (HAR) or classifier system employ an automated processing pipeline (also referred to herein as “IMUTube”) that integrates computer vision and signal processing operations to convert video data of human activity into virtual streams of IMU data that represents accelerometer, gyroscope, or other inertial measurement unit estimation that can measure acceleration, inertia, motion, orientation, force, velocity, etc. at a different location on the body. In other embodiments, the automated processing pipeline can be used to generate high-quality virtual accelerometer data from a camera sensor.","['G06T7/251', 'G06T7/20', 'G06V10/80', 'G06V20/41', 'G06V30/194', 'G06V40/23', 'G06T2207/30196']"
CN110853022B,"Pathological section image processing method, device and system and storage medium","The application provides a pathological section image processing method, a pathological section image processing device, a pathological section image processing system and a storage medium, and relates to the technical field of artificial intelligence intelligent medical treatment. The method comprises the following steps: acquiring staining images of the pathological section stained by the cell membrane under n visual fields of a microscope; for the staining image under the ith vision field in the n vision fields, determining the nuclear position of the cancer cells in the staining image under the ith vision field; generating a cell membrane description of the staining image in the ith field, the cell membrane description indicating the integrity and intensity of the cell membrane staining; determining the number of various types of cells in the staining image under the ith visual field according to the description result of the nucleus positions and the cell membranes; and determining the analysis result of the pathological section according to the number of various types of cells in the stained image under the n visual fields. The application provides a technical scheme for carrying out systematic cell membrane staining analysis on pathological sections, and the accuracy of detection results is promoted.","['G06T7/0012', 'G06T7/10', 'G06T7/11', 'G06T7/136', 'G06T7/187', 'G06T7/70', 'G06V10/34', 'G06V20/695', 'G06V20/698', 'G16H10/40', 'G16H30/00', 'G16H30/40', 'G16H50/20', 'G16H70/60', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/10061', 'G06T2207/20036', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30024', 'G06T2207/30068', 'G06T2207/30096', 'G06T2207/30242', 'G06V2201/03']"
US11900558B2,Reconstructing three-dimensional models of objects from real images based on depth information,"The present disclosure relates to systems, non-transitory computer-readable media, and methods that tune a 3D-object-reconstruction-machine-learning model to reconstruct 3D models of objects from real images using real images as training data. For instance, the disclosed systems can determine a depth map for a real two-dimensional (2D) image and then reconstruct a 3D model of a digital object in the real 2D image based on the depth map. By using a depth map for a real 2D image, the disclosed systems can generate reconstructed 3D models that better conform to the shape of digital objects in real images than existing systems and use such reconstructed 3D models to generate more realistic looking visual effects (e.g., shadows, relighting).","['G06T19/20', 'G06F18/214', 'G06N3/04', 'G06N3/0442', 'G06N3/0464', 'G06N3/09', 'G06T7/50', 'G06V10/82', 'G06V20/647', 'G06T17/00', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2219/2004']"
US8660303B2,Detection of body and props,"A system and method for detecting and tracking targets including body parts and props is described. In one aspect, the disclosed technology acquires one or more depth images, generates one or more classification maps associated with one or more body parts and one or more props, tracks the one or more body parts using a skeletal tracking system, tracks the one or more props using a prop tracking system, and reports metrics regarding the one or more body parts and the one or more props. In some embodiments, feedback may occur between the skeletal tracking system and the prop tracking system.",['G06V40/103']
CN111178253B,"Visual perception method and device for automatic driving, computer equipment and storage medium","The application relates to a visual perception method, a visual perception device, computer equipment and a storage medium for automatic driving. The method comprises the following steps: acquiring an acquired visual perception image; inputting the visual perception image into a trunk network of a trained multi-task neural network, and extracting sharing characteristics of the visual perception image through the trunk network to obtain a sharing characteristic diagram; inputting the shared feature map into each branch network in the multi-task neural network respectively, classifying corresponding tasks by each branch network based on the shared feature map, and outputting classification results of the corresponding tasks; and extracting classification results of the corresponding tasks according to preset visual perception targets, and fusing to obtain visual perception results, wherein the visual perception results comprise at least one of lane line information, road sign information, traffic area road condition information and road obstacle information. By adopting the method, the accuracy of visual perception can be improved.","['G06V20/588', 'G06F18/214', 'G06V10/25', 'G06V10/267', 'G06V20/58', 'G06V20/582', 'G06V40/10']"
US20170131760A1,"Systems, methods and techniques for inputting text into mobile devices using a camera-based keyboard","Systems, methods and techniques are provided for interacting with mobile devices using a camera-based keyboard. The system comprises a processor system including at last one processor. The processor system is configured to at least capture a plurality of images in connection with the keyboard and at least one hand typing on the keyboard via the camera. Based on the plurality of captured images, the processor system is further configured to locate the keyboard, extract at least a portion of the keys on the keyboard, extract a hand, and detect a fingertip of the extracted hand. After that, a keystroke may be detected and localized through tracking the detected fingertip in at least one of the plurality of captured images, and a character corresponding to the localized keystroke may be determined.","['G06F3/0426', 'G06F3/005', 'G06F3/02', 'G06K9/46', 'G06T5/30', 'G06T7/004', 'G06T7/0081', 'G06T7/0085', 'G06T7/136', 'G06T7/194', 'G06T7/246', 'G06T7/408', 'G06T7/90', 'G06V10/28', 'G06V10/56', 'G06V40/28', 'H04N23/57', 'H04N5/2257', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/30196']"
CN114998096B,Method and system for detecting proportion of cracks in pavement image,"The invention discloses a method and a system for detecting the proportion of cracks in a pavement image, comprising the following steps: collecting pavement crack images, and performing closed operation on the binarized pavement crack images; setting three point light sources in a simulator based on a three-point light distribution principle, and polishing a pavement crack image; conducting guided filtering by using the closed operation result and the image obtained by the polishing simulation; based on dark channel prior and image fusion, processing the filtered image to obtain a pavement crack gray level image; sequentially performing binarization, image refinement, connected domain analysis and top hat operation, removing crack root nodes, constructing ROI (region of interest) by taking the end points of all connected domains as the center, connecting broken cracks to obtain a complete crack image, performing expansion operation, and calculating the proportion of crack pixels to the image. The invention can improve the crack extraction precision and the crack proportion calculation precision.","['G06T3/04', 'G06T5/10', 'G06T5/30', 'G06T5/50', 'G06T5/70', 'G06T7/187', 'G06T7/60', 'G06V10/26', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/20216', 'G06T2207/20221']"
US12353840B2,Computer vision based sign language interpreter,"A system and method for translating sign language utterances into a target language, including: receiving motion capture data; producing phonemes/sign fragments from the received motion capture data; producing a plurality of sign sequences from the phonemes/sign fragments; parsing these sign sequences to produce grammatically parsed sign utterances; translating the grammatically parsed sign utterances into grammatical representations in the target language; and generating output utterances in the target language based upon the grammatical representations.","['G06F40/58', 'G06F3/014', 'G06F3/017', 'G06F3/0304', 'G06F40/205', 'G06V40/28', 'G09B21/009']"
US20230360208A1,Training end-to-end weakly supervised networks at the specimen (supra-image) level,"Techniques for determining a presence of a pathology property in a supra-image are presented. The techniques can include receiving an electronic evaluation supra-image; providing the electronic evaluation supra-image to an electronic neural network that has been trained, using a training corpus of training supra-images and on an electronic computer, to determine the presence of the pathology property in a supra-image, each training supra-image including at least one image, each image corresponding to a plurality of components, wherein each training supra-image of the training corpus is associated with a respective electronic label indicating whether the pathology property is present, where the training corpus is sufficient to train the electronic neural network to determine a presence of the pathology property; receiving from the electronic neural network an output indicative of whether the pathology property is present in the evaluation supra-image; and providing the output.","['G16H50/20', 'G06N3/045', 'G06N3/0895', 'G06T7/0012', 'G06V10/774', 'G06V10/82', 'G06V20/70', 'G16H30/40', 'G16H50/30', 'G16H50/70', 'A61B5/055', 'A61B5/7267', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30088', 'G06T2207/30096', 'G06V2201/03', 'G16H70/60']"
US8160322B2,Joint detection and localization of multiple anatomical landmarks through learning,"A method for detecting and localizing multiple anatomical landmarks in medical images, including: receiving an input requesting identification of a plurality of anatomical landmarks in a medical image; applying a multi-landmark detector to the medical image to identify a plurality of candidate locations for each of the anatomical landmarks; for each of the anatomical landmarks, applying a landmark-specific detector to each of its candidate locations, wherein the landmark-specific detector assigns a score to each of the candidate locations, and wherein candidate locations having a score below a predetermined threshold are removed; applying spatial statistics to groups of the remaining candidate locations to determine, for each of the anatomical landmarks, the candidate location that most accurately identifies the anatomical landmark; and for each of the anatomical landmarks, outputting the candidate location that most accurately identifies the anatomical landmark.","['G06T7/74', 'G06T2207/10072', 'G06T2207/30004']"
US11763542B2,"Apparatus and method for image classification and segmentation based on feature-guided network, device, and medium","The present invention provides an apparatus and method for image classification and segmentation based on a feature-guided network, a device, and a medium, and belongs to the technical field of deep learning. A feature-guided classification network and feature-guided segmentation network of the present invention include basic unit blocks. A local feature is enhanced and a global feature is extracted among the basic unit blocks. This resolves a problem that features are not fully utilized in existing image classification and image segmentation network models. In this way, a trained feature-guided classification network and feature-guided segmentation network have better effects and are more robust. The present invention selects the feature-guided classification network or the feature-guided segmentation network based on a requirement of an input image and outputs a corresponding category or segmented image, to resolve a problem that the existing classification or segmentation network model has an unsatisfactory classification or segmentation effect.","['G06F18/2414', 'G06V10/267', 'G06V10/82', 'G06F16/55', 'G06F18/253', 'G06N3/045', 'G06N3/084', 'G06T7/11', 'G06V10/247', 'G06V10/25', 'G06V10/26', 'G06V10/32', 'G06V10/40', 'G06V10/42', 'G06V10/764', 'G06V10/7715', 'G06V10/806']"
CN110443813B,"Segmentation method, device and equipment for blood vessel and fundus image and readable storage medium","The embodiment of the application discloses a segmentation method, a segmentation device, segmentation equipment and a readable storage medium of blood vessels and fundus images, which relate to the computer vision technology of artificial intelligence; specifically, a blood vessel image to be segmented such as a fundus image may be acquired; extracting features of blood vessel images such as fundus images to obtain high-level feature information; performing dictionary learning on the high-level characteristic information based on a preset dictionary to obtain dictionary representation corresponding to the high-level characteristic information; selecting a plurality of channels of the high-level characteristic information according to the dictionary representation to obtain target characteristic information; fusing the target characteristic information with the high-level characteristic information to obtain channel attention characteristic information; and dividing blood vessels in the blood vessel image such as fundus image according to the channel attention characteristic information to obtain a blood vessel division result. According to the scheme, the global information loss of the characteristic blood vessel image such as the fundus image can be avoided, and the segmentation accuracy of the blood vessel image such as the fundus image is greatly improved.","['G06F18/253', 'G06F18/28', 'G06T3/4038', 'G06T7/11', 'G06T9/00', 'G06V10/462', 'G06T2207/20021', 'G06T2207/30041', 'G06T2207/30101', 'G06V2201/03']"
US5208869A,Character and pattern recognition machine and method,"Banking apparatus for high speed processing of bank checks, drafts and like documents having numeric characters in courtesy amount fields. The documents are conveyed along a path and the courtesy amount field with the numeric character are located, scanned, digitized and the numeric characters are supplied to a plurality of parallel connected artificial fovea, each artificial fovea including temporary storage and image section measurement circuits. Recognition scoring circuits identify the numeric characters from the respective artificial fovea.",['G06V30/40']
CN111310645B,"Method, device, equipment and storage medium for warning overflow bin of goods accumulation","The invention relates to the technical field of image recognition, and discloses a cargo accumulation amount-based bin overflow early warning method, device, equipment and storage medium, which are used for recognizing a target characteristic region and calculating the corresponding outline area of the target characteristic region and the target cargo accumulation amount, so that the cargo recognition efficiency and accuracy are improved. The overflow bin early warning method of the cargo accumulation amount comprises the following steps: acquiring a scene image to be detected through a monitoring platform, wherein the scene image to be detected comprises a plurality of cargos; dividing the model and the scene image to be detected according to the trained examples to obtain a target scene image to be detected and a target characteristic region; acquiring a contour area corresponding to a target feature area according to a target scene image to be detected and the target feature area based on an open source computer vision library; acquiring a target cargo accumulation rate according to the contour area corresponding to the target characteristic area, and judging whether the target cargo accumulation rate is greater than an accumulation rate alarm threshold value or not; and if the stacking rate of the target cargoes is greater than the stacking rate alarm threshold value, alarm processing is carried out.","['G06V20/35', 'G06F18/214', 'G06V10/267', 'G06V2201/07']"
US8971584B2,Methods and apparatus for chatter reduction in video object segmentation using a variable bandwidth search region,"Systems, methods, and computer-readable storage media for chatter reduction in video object segmentation using a variable bandwidth search region. A variable bandwidth search region generation method may be applied to a uniform search region to generate a variable bandwidth search region that reduces the search range for segmentation methods such as a graph cut method. The method may identify parts of the contour that are moving slowly, and reduce the search region bandwidth in those places to stabilize the segmentation. This method may determine a bandwidth for each of a plurality of local windows of an image according to an estimate of how much an object in the image has moved from a previous image. The method may blend the bandwidths for the plurality of local windows to generate a blended map. The method may then generate a variable bandwidth search region for an object according to the blended map.","['G06V10/764', 'G06F18/2415', 'G06K9/6277', 'G06T7/0083', 'G06T7/0087', 'G06T7/0089', 'G06T7/12', 'G06T7/143', 'G06T7/149', 'G06T7/20', 'G06T2207/10024', 'G06T2207/20121', 'G06T2207/20124', 'G06T2207/30196']"
US11640681B2,Retinal encoder for machine vision,"A method is disclosed including: receiving raw image data corresponding to a series of raw images; processing the raw image data with an encoder to generate encoded data, where the encoder is characterized by an input/output transformation that substantially mimics the input/output transformation of one or more retinal cells of a vertebrate retina; and applying a first machine vision algorithm to data generated based at least in part on the encoded data.","['G06T9/00', 'G06N3/049', 'G06T7/0012', 'G06T9/002', 'G06T9/007', 'G06V10/20', 'G06V10/449', 'G06V10/454', 'H04N19/60', 'H04N19/62', 'H04N19/85', 'G06T2207/20024', 'G06T2207/20048', 'G06T2207/20084', 'G06T2207/30041', 'G06V2201/03']"
US11613201B2,Automatic high beam control for autonomous machine applications,"In various examples, high beam control for vehicles may be automated using a deep neural network (DNN) that processes sensor data received from vehicle sensors. The DNN may process the sensor data to output pixel-level semantic segmentation masks in order to differentiate actionable objects (e.g., vehicles with front or back lights lit, bicyclists, or pedestrians) from other objects (e.g., parked vehicles). Resulting segmentation masks output by the DNN(s), when combined with one or more post processing steps, may be used to generate masks for automated high beam on/off activation and/or dimming or shading—thereby providing additional illumination of an environment for the driver while controlling downstream effects of high beam glare for active vehicles.","['B60Q1/143', 'B60Q1/076', 'G06F18/24155', 'G06K9/6278', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N7/01', 'G06T7/11', 'G06T7/174', 'G06V10/141', 'G06V10/30', 'G06V10/764', 'G06V10/82', 'G06V20/56', 'G06V20/58', 'G06V20/584', 'G06V20/695', 'H05B47/125', 'B60Q2300/056', 'B60Q2300/41', 'B60Q2300/42', 'B60Q2300/45', 'G06N20/10', 'G06N20/20', 'G06N3/044', 'G06N3/047', 'G06N5/01', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20182', 'G06T2207/30252', 'H04N19/70', 'H05B45/10', 'Y02B20/40']"
CN113516664B,Visual SLAM method based on semantic segmentation dynamic points,"A visual SLAM method based on semantic segmentation dynamic points relates to the technical field of computer vision. The invention collects environmental image information through an RGB-D camera, performs feature extraction and semantic segmentation on the obtained image, and obtains an ORB feature point and semantic segmentation result; detecting the residual dynamic object and eliminating dynamic feature points by using a dynamic object detection algorithm based on multi-view geometric constraint; the tracking, the local mapping and the loop detection threads are sequentially executed, so that an accurate static scene octree three-dimensional semantic map is constructed under a dynamic scene, and finally, the visual SLAM method based on semantic segmentation dynamic points facing the dynamic scene is realized.","['G06T7/10', 'G06N3/045', 'G06N3/08', 'G06T17/00', 'G06T2207/10004', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084']"
CN113516124B,Recognition Algorithm of Energy Meter Electricity Information Based on Computer Vision Technology,"The electric energy meter electricity consumption identification algorithm based on the computer vision technology integrates detection and identification, and achieves end-to-end text positioning and prediction. Firstly, a detection end combines the ideas of a characteristic pyramid network and a residual error network, performs characteristic extraction on an input image, and generates a Bezier curve through four control points to better fit a text box; then, the recognition end adopts a text recognition algorithm based on a convolutional cyclic neural network, a gating cyclic unit is introduced to replace a long-period memory unit, and then a target area text is recognized by combining an attention mechanism; finally, five groups of ablation experiments are performed, and performance comparison and evaluation analysis are performed through experimental data. Experimental results show that the algorithm has high recognition accuracy up to 99.08%, and high reasoning speed, and can be applied to practical application of power consumption information detection and recognition.","['G06F18/241', 'G06F18/2415', 'G06N3/044', 'G06N3/045', 'G06N3/047', 'G06N3/048', 'G06N3/08']"
US8391645B2,Detecting orientation of digital images using face detection information,"A method of automatically establishing the correct orientation of an image using facial information. This method is based on the exploitation of the inherent property of image recognition algorithms in general and face detection in particular, where the recognition is based on criteria that is highly orientation sensitive. By applying a detection algorithm to images in various orientations, or alternatively by rotating the classifiers, and comparing the number of successful faces that are detected in each orientation, one may conclude as to the most likely correct orientation. Such method can be implemented as an automated method or a semi automatic method to guide users in viewing, capturing or printing of images.","['G06V40/161', 'G06V10/242', 'G06V40/16', 'G06V40/162', 'G06V40/171']"
CN111095291B,Real-time lane and boundary detection by autonomous vehicles,"In various examples, sensor data representing an image of a field of view of a vehicle sensor may be received and the sensor data may be applied to a machine learning model. The machine learning model may calculate a segmentation mask that represents a portion of the image corresponding to a lane marking of a driving surface of the vehicle. Analysis of the segmentation masks may be performed to determine lane marking types, and lane boundaries may be generated by performing curve fitting on lane markings corresponding to each lane marking type. Data representing lane boundaries may then be transmitted to components of the vehicle for use in navigating the vehicle through the driving surface.","['G06V10/44', 'G05D1/00', 'G05D1/0088', 'G05D1/0221', 'G06F18/24143', 'G06N3/084', 'G06T7/10', 'G06V10/457', 'G06V10/46', 'G06V10/764', 'G06V10/82', 'G06V20/41', 'G06V20/588', 'G05D2101/15', 'G06V10/471']"
CN114299430A,"Training method and device of video analysis model, and video analysis method and device","The present disclosure relates to a training method and apparatus for a video analysis model, and a video analysis method and apparatus, where the video analysis model includes a video segmentation network and a video classification network, and the training method includes: acquiring image characteristics of a video sample and label information corresponding to the video sample; the video sample comprises at least one standard video clip, and the label information comprises a standard video clip type corresponding to each standard video clip; according to the image characteristics of the video sample, performing video segment segmentation processing by using a video segmentation network to obtain at least one first video segment; according to the image characteristics of at least one first video clip, carrying out video clip classification processing by using a video classification network to obtain the estimated video clip type of each first video clip; determining the value of a first loss function according to the type of the pre-estimated video clip and the type of the standard video clip; and adjusting parameters of the video segmentation network and the video classification network according to the value of the first loss function until the end condition is met.",[]
CN107643295B,A method and system for online detection of cloth defects based on machine vision,"The present invention provides a kind of method of cloth defect on-line checking based on machine vision, comprising the following steps: the image of image acquisition units acquisition cloth；Image processing unit obtains above-mentioned image, and gray processing processing, image mosaic and filtering and noise reduction processing are successively carried out to above-mentioned image, distinguishes the clear zone and dark space in image finally by binary conversion treatment；Threshold division processing is carried out to the clear zone in image, pixel is then traversed and judges in image with the presence or absence of defect；Defect if it exists, the defects of then use morphological analysis and handle image, make the regular edges, smooth of defect, then using the pixel coordinate of the defects of Blob analysis image, and convert the pixel coordinate of defect to the world coordinates of marking machine, and mark to the fault location mark of cloth by controlling marking machine from single-chip microcontroller；Using BP neural network analysis by morphological analysis and treated the defects of image, the type of defect and storage are judged.The utility model has the advantages that precision is good, high-efficient.",[]
US20220237414A1,Confidence generation using a neural network,"Apparatuses, systems, and techniques to generate one or more confidence values associated with one or more objects identified by one or more neural networks. In at least one embodiment, one or more confidence values associated with one or more objects identified by one or more neural networks are generated based on, for example, one or more neural network outputs.","['G06V10/82', 'G06K9/6262', 'G06F18/217', 'A01B69/001', 'A01C21/005', 'A01G25/09', 'A01G25/16', 'A01M21/00', 'B60W10/04', 'B60W10/18', 'B60W30/09', 'B60W30/0956', 'B60W50/14', 'B60W60/0015', 'G06F18/2163', 'G06F18/24', 'G06K9/0063', 'G06K9/00791', 'G06K9/6261', 'G06K9/6267', 'G06N3/04', 'G06N3/08', 'G06V10/26', 'G06V20/13', 'G06V20/188', 'G06V20/56', 'G06V20/58', 'G16H40/67', 'B60W2420/40', 'B60W2556/45', 'G06K2209/05', 'G06V10/95', 'G06V10/955', 'G06V2201/03', 'G06V2201/031']"
US11295458B2,Object tracking by an unmanned aerial vehicle using visual sensors,"Systems and methods are disclosed for tracking objects in a physical environment using visual sensors onboard an autonomous unmanned aerial vehicle (UAV). In certain embodiments, images of the physical environment captured by the onboard visual sensors are processed to extract semantic information about detected objects. Processing of the captured images may involve applying machine learning techniques such as a deep convolutional neural network to extract semantic cues regarding objects detected in the images. The object tracking can be utilized, for example, to facilitate autonomous navigation by the UAV or to generate and display augmentative information regarding tracked objects to users.","['G06V20/13', 'B64C39/024', 'B64U10/14', 'G05D1/0011', 'G05D1/0094', 'G05D1/221', 'G05D1/689', 'G06F18/2431', 'G06K9/0063', 'G06K9/628', 'G06K9/66', 'G06K9/726', 'G06N3/0464', 'G06T3/60', 'G06T7/10', 'G06T7/11', 'G06T7/20', 'G06T7/292', 'G06T7/579', 'G06T7/75', 'G06V10/82', 'G06V20/17', 'G06V30/274', 'H04N13/239', 'H04N13/243', 'H04N13/282', 'B64C2201/123', 'B64C2201/126', 'B64C2201/127', 'B64C2201/141', 'B64U2101/00', 'B64U2101/30', 'B64U2101/31', 'B64U2201/10', 'G06N3/045', 'G06N3/0454', 'G06T2207/10012', 'G06T2207/10028', 'G06T2207/10032', 'G06T2207/20084', 'G06T2207/20088', 'G06T2207/30196', 'G06T2207/30241', 'H04N2013/0081', 'H04N2013/0085', 'H04N2013/0092']"
CN111199550B,"Training method, segmentation method, device and storage medium of image segmentation network","The embodiment of the application discloses a training method, a segmentation method, a device and a storage medium of an image segmentation network; the method and the device can acquire the target domain image and the source domain image with marked information; respectively segmenting the source domain image and the target domain image by adopting a generating network in a generating countermeasure network to obtain a source domain segmentation result and a target domain segmentation result; judging the target domain segmentation result and the source domain segmentation result by adopting a countermeasure network in the generated countermeasure network to obtain a judgment result; acquiring the information entropy of the target domain image according to the target domain segmentation result, and constructing the resistance loss according to the information entropy, the target domain segmentation result and the source domain segmentation result; constructing a segmentation loss according to the target domain segmentation result, the source domain segmentation result and the label information; and performing iterative training on the generation countermeasure network according to the countermeasure loss, the segmentation loss and the judgment result to obtain the trained generation countermeasure network. The scheme can effectively improve the accuracy of image segmentation.","['G06T7/11', 'G06N20/00', 'G06T2207/20081']"
US5842194A,Method of recognizing images of faces or general images using fuzzy combination of multiple resolutions,"A system comprising a neural network, or computer, implementing a feature detection and a statistical procedure, together with fuzzy logic for solving the problem of recognition of faces or other objects) at multiple resolutions is described. A plurality of previously described systems for recognizing faces (or other objects) which use local autocorrelation coefficients and linear discriminant analysis are trained on a data set to recognize facial images each at a particular resolution. In a second training stage, each of the previously described systems is tested on a second training set in which the images presented to the previously described recognition systems have a matching resolution to those of the first training set, the statistical performance of this second training stage being used to train a fuzzy combination technique, that of fuzzy integrals. Finally, in a test stage, the results from the classifiers at the multiple resolutions are combined using fuzzy combination to produce an aggregated system whose performance is higher than that of any of the individual systems and shows very good performance relative to all known face recognitior systems which operate on similar types of training and testing data, this aggregated system, however, not being limited to the recognition of faces and being able to be applied to the recognition of other objects.","['G06V40/172', 'G06F18/254', 'G06V10/431', 'G06V30/2504']"
US8605946B2,Moving object detection apparatus and moving object detection method,"A moving object detection apparatus includes: a trajectory calculating unit which calculates a plurality of trajectories for each image subset; a subclass classification unit which classifies the trajectories into subclasses for each of the image subsets, an inter-subclass similarity calculating unit which calculates a trajectory share ratio which indicates the degree of sharing of the same trajectory between arbitrary two of the subclasses and calculates the similarity between the subclasses based on the share ratio; a moving object detection unit which detects the moving object in video by classifying the subclasses into classes such that the subclasses between which a higher similarity is present are more likely to be classified into the same class.","['G06T7/246', 'G06V10/255', 'G06V20/56', 'G06V20/58', 'G06T2207/10016', 'G06T2207/30196', 'G06T2207/30232', 'G06T2207/30241', 'G06T2207/30252']"
US12093871B2,Ocular system to optimize learning,"A method to measure a cognitive load based upon ocular information of a subject includes the steps of: providing a video camera configured to record a close-up view of at least one eye of the subject; providing a computing device electronically connected to the video camera and the electronic display; recording, via the video camera, the ocular information of the at least one eye of the subject; processing, via the computing device, the ocular information to identify changes in ocular signals of the subject through the use of convolutional neural networks; evaluating, via the computing device, the changes in ocular signals from the convolutional neural networks by a machine learning algorithm; determining, via the machine learning algorithm, the cognitive load for the subject; and displaying, to the subject and/or to a supervisor, the cognitive load for the subject.","['G06Q10/0635', 'A61B3/0025', 'A61B3/0041', 'A61B3/0091', 'A61B3/112', 'A61B3/113', 'A61B3/14', 'A61B3/145', 'A61B5/0077', 'A61B5/1103', 'A61B5/161', 'A61B5/163', 'A61B5/164', 'A61B5/165', 'A61B5/4845', 'A61B5/4863', 'A61B5/6898', 'A61B5/7246', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/08', 'G06N3/09', 'G06N3/094', 'G06Q10/06398', 'G06Q10/10', 'G06T7/73', 'G06V10/143', 'G06V10/454', 'G06V10/764', 'G06V20/46', 'G06V40/18', 'G06V40/19', 'G06V40/193', 'G16H10/20', 'G16H15/00', 'G16H20/70', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'G16H50/50', 'G16H50/70', 'A61B2503/20', 'A61B5/7267', 'G06N3/088', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30041', 'G16H50/30']"
US20220153262A1,Object detection and collision avoidance using a neural network,"Apparatuses, systems, and techniques to identify objects in view of a camera associated with a vehicle. In at least one embodiment, objects with which a vehicle may collide are identified, based on, for example, a difference between a size of an image of the objects detected at a first point in time and a size of an image of the objects detected at a subsequent point in time.","['G06T7/0002', 'G06V20/58', 'G06T7/70', 'B60W30/09', 'B60W30/0956', 'G01S13/931', 'G01S7/417', 'G06K9/00805', 'G06K9/4671', 'G06N3/045', 'G06N3/084', 'G06T7/11', 'G06T7/20', 'G06T7/60', 'G06V10/462', 'G08G1/165', 'G08G1/166', 'G08G1/167', 'B60W2420/403', 'B60W2420/42', 'B60W2554/40', 'G01S17/931', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30241', 'G06T2207/30252', 'G06T2207/30261', 'G06T7/10', 'G06V10/82']"
US7949190B2,Real-time nighttime vehicle detection and recognition system based on computer vision,"The present invention relates to a real-time nighttime vehicle detection and identification system comprises an illuminant object image segmentation device 1, an illuminant object classifying device 2, a vehicle lighting object identification device 3, a vehicle position determining device 4 and a vehicle tracking device 5. Under various circumstances of road lighting during nighttime, the system can efficiently and accurately demarcate and identify the lamps of oncoming and preceding vehicles and accurately provides the driver with auxiliary information needed to analyze the traffic conditions in front of the vehicle during the conditions met on the road at that time.","['G06V20/58', 'G06V20/584']"
US10531825B2,Thresholding methods for lesion segmentation in dermoscopy images,"Systems and methods facilitate segmenting a dermoscopy image of a lesion to facilitate classification of the lesion. A dermoscopy image is received from an image source; pre-processed; and segmented. Segmenting the pre-processed dermoscopy image includes applying a thresholding algorithm to the dermoscopy image. The thresholding algorithm includes at least one of a Huang auto-thresholding algorithm, a Li auto-thresholding algorithm, a Shanbhag auto-thresholding algorithm, an Otsu auto-thresholding algorithm, or an Isodata thresholding algorithm. A weighted border error metric is provided to allow prediction of whether segmentation is likely to avoid excluding portions of the lesion. A test is provided for determination of whether image inversion is needed for the methods to provide optimal borders. An iterative expansion algorithm is provided to for determine whether expansion is needed for the methods to provide optimal borders and to determine the degree of expansion needed for more inclusive segmentation.","['A61B5/444', 'A61B5/0077', 'A61B5/7264', 'G06T7/0012', 'G06T7/11', 'G06T7/136', 'G06T2207/10024', 'G06T2207/20036', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/30088', 'G06T2207/30096', 'G16H50/20']"
US9025825B2,System and method for visual motion based object segmentation and tracking,"The PMP Growth algorithm described herein provides for image tracking, segmentation and processing in environments where the camera system moves around a great deal, i.e., causing image jumps from one image frame to the next. It also is operative in systems where the objects themselves are making quick movements that alter their path. Attributes of the PMP Growth algorithm allow tracking systems using the PMP Growth algorithm to follow objects a long distance in a scene. This detection and tracking method is designed to track objects within a sequence of video image frames, and includes detecting keypoints in a current image frame of the video image frames, assigning local appearance features to the detected keypoints, establishing Point-Motion-Pairs between two successive image frames of the video image frames, and accumulating additional matches between image locations to form complete coherent motion object models of the objects being tracked. The segmentation aspect permits for the discovery of different coherently moving regions in the images.","['G06T7/20', 'G06T7/215', 'G06T7/246', 'B64U2101/30', 'G06T2207/10032', 'G06T2207/30236', 'G06T2207/30252', 'G06T2207/30256']"
US11461998B2,System and method for boundary aware semantic segmentation,"Some aspects of embodiments of the present disclosure relate to using a boundary aware loss function to train a machine learning model for computing semantic segmentation maps from input images. Some aspects of embodiments of the present disclosure relate to deep convolutional neural networks (DCNNs) for computing semantic segmentation maps from input images, where the DCNNs include a box filtering layer configured to box filter input feature maps computed from the input images before supplying box filtered feature maps to an atrous spatial pyramidal pooling (ASPP) layer. Some aspects of embodiments of the present disclosure relate to a selective ASPP layer configured to weight the outputs of an ASPP layer in accordance with attention feature maps.","['G06V10/48', 'G06N3/084', 'G06F18/214', 'G06F18/2413', 'G06K9/6256', 'G06N20/00', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06V10/758', 'G06V10/764', 'G06V10/774', 'G06V10/776', 'G06V10/82', 'G06V20/10', 'G06V20/35', 'G06V20/70']"
US6879709B2,System and method for automatically detecting neutral expressionless faces in digital images,"A system and method for automatic detecting neutral expressionless faces in digital images and video is described. First a face detector is used to detect the pose and position of a face and find the facial components. Second, the detected face is normalized to a standard size face. Then a set of geometrical facial features and three histograms in zones of mouth are extracted. Finally, by feeding these features to a classifier, the system detects if there is the neutral expressionless face or not.",['G06V40/175']
US12172667B2,3D surface reconstruction with point cloud densification using deep neural networks for autonomous systems and applications,"In various examples, a 3D surface structure such as the 3D surface structure of a road (3D road surface) may be observed and estimated to generate a 3D point cloud or other representation of the 3D surface structure. Since the estimated representation may be sparse, a deep neural network (DNN) may be used to predict values for a dense representation of the 3D surface structure from the sparse representation. For example, a sparse 3D point cloud may be projected to form a sparse projection image (e.g., a sparse 2D height map), which may be fed into the DNN to predict a dense projection image (e.g., a dense 2D height map). The predicted dense representation of the 3D surface structure may be provided to an autonomous vehicle drive stack to enable safe and comfortable planning and control of the autonomous vehicle.","['G06N3/0464', 'G06T17/20', 'B60W40/00', 'B60W40/09', 'B60W50/00', 'B60W60/001', 'G06N20/00', 'G06N3/02', 'G06N3/063', 'G06N3/08', 'G06N3/09', 'G06N7/01', 'B60W2050/0005', 'B60W2050/0028', 'B60W2420/403', 'B60W2420/408', 'B60W2552/00', 'B60W2552/20', 'B60W2552/35', 'G06N3/0442', 'G06N3/0455', 'G06N3/084']"
US11801861B2,Using image augmentation with simulated objects for training machine learning models in autonomous driving applications,"In various examples, systems and methods are disclosed that preserve rich, detail-centric information from a real-world image by augmenting the real-world image with simulated objects to train a machine learning model to detect objects in an input image. The machine learning model may be trained, in deployment, to detect objects and determine bounding shapes to encapsulate detected objects. The machine learning model may further be trained to determine the type of road object encountered, calculate hazard ratings, and calculate confidence percentages. In deployment, detection of a road object, determination of a corresponding bounding shape, identification of road object type, and/or calculation of a hazard rating by the machine learning model may be used as an aid for determining next steps regarding the surrounding environment—e.g., navigating around the road debris, driving over the road debris, or coming to a complete stop—in a variety of autonomous machine applications.","['G06N20/00', 'B60W60/001', 'B60W50/06', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'B60W2420/403', 'B60W2420/42', 'B60W2554/4029', 'B60W2554/80']"
US12100230B2,Using neural networks for 3D surface structure estimation based on real-world data for autonomous systems and applications,"In various examples, to support training a deep neural network (DNN) to predict a dense representation of a 3D surface structure of interest, a training dataset is generated from real-world data. For example, one or more vehicles may collect image data and LiDAR data while navigating through a real-world environment. To generate input training data, 3D surface structure estimation may be performed on captured image data to generate a sparse representation of a 3D surface structure of interest (e.g., a 3D road surface). To generate corresponding ground truth training data, captured LiDAR data may be smoothed, subject to outlier removal, subject to triangulation to filling missing values, accumulated from multiple LiDAR sensors, aligned with corresponding frames of image data, and/or annotated to identify 3D points on the 3D surface of interest, and the identified 3D points may be projected to generate a dense representation of the 3D surface structure.","['G06V20/64', 'G06V20/647', 'G05D1/0236', 'G01S17/86', 'G01S17/89', 'G01S17/931', 'G01S7/4808', 'G05D1/024', 'G05D1/0242', 'G05D1/0251', 'G05D1/0255', 'G06F18/214', 'G06V10/26', 'G06V10/774', 'G06V10/82', 'G06V20/56', 'G06V20/58', 'G06V20/70', 'B60G17/0165', 'B60K31/00', 'B60W2420/408', 'B60W60/001']"
US11514579B2,Deformable capsules for object detection,"An improved method of performing object segmentation and classification that reduces the memory required to perform these tasks, while increasing predictive accuracy. The improved method utilizes a capsule network with dynamic routing. Capsule networks allow for the preservation of information about the input by replacing max-pooling layers with convolutional strides and dynamic routing, allowing for the reconstruction of an input image from output capsule vectors. The present invention expands the use of capsule networks to the task of object segmentation and medical image-based cancer diagnosis for the first time in the literature; extends the idea of convolutional capsules with locally-connected routing and propose the concept of deconvolutional capsules; extends the masked reconstruction to reconstruct the positive input class; and proposes a capsule-based pooling operation for diagnosis. The convolutional-deconvolutional capsule network shows strong results for the tasks of object segmentation and classification with substantial decrease in parameter space.","['G06N3/084', 'G06F18/217', 'G06F18/241', 'G06K9/6232', 'G06K9/6262', 'G06K9/6268', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/0475', 'G06N3/0495', 'G06N3/09', 'G06T7/11', 'G06V10/26', 'G06V10/764', 'G06V10/82', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30064', 'G06V2201/03']"
US9922238B2,"Apparatuses, systems, and methods for confirming identity","A system, apparatus, and method for confirming the identity of a person for access using an image of that person. The system includes a camera, a data storage device, a subject identification device, and a processor to compare a plurality of current images of the subject to a plurality of stored images of that subject and confirm that the subject live and an approved subject.","['G06V40/171', 'G06V40/172', 'G06K9/00288', 'G06K9/00221', 'G06K9/00228', 'G06K9/00281', 'G06K9/228', 'G06T5/50', 'G06V30/142', 'G06V40/16', 'G06V40/161', 'G06V40/45', 'G06V40/70', 'G06K9/00926', 'G06T2207/20221', 'G06V40/50']"
US12380651B2,Structure annotation,"A computer-implemented method of creating one or more annotated perception inputs, the method comprising, in an annotation computer system: receiving a plurality of captured frames, each frame comprising a set of 3D structure points, in which at least a portion of a common structure component is captured; computing a reference position within at least one reference frame of the plurality of frames; generating a 3D model for the common structure component by selectively extracting 3D structure points of the at least one reference frame based on the reference position within that frame; determining an aligned model position for the 3D model within a target frame of the plurality of frames based on one or more manual alignment inputs received in respect of the target frame at a user interface whilst rendering the 3D model for manually aligning the 3D model with the common structure component in the target frame; and storing annotation data of the aligned model position in computer storage, in association with at least one perception input of the target frame for annotating the common structure component therein.","['G06T19/00', 'G06F18/214', 'G06N20/00', 'G06N3/08', 'G06N5/00', 'G06T1/60', 'G06T15/005', 'G06T15/50', 'G06T17/00', 'G06T19/20', 'G06T3/08', 'G06T5/70', 'G06T7/10', 'G06T7/11', 'G06T7/60', 'G06T7/70', 'G06T7/73', 'G06V10/774', 'G06V10/945', 'G06V20/56', 'G06V20/647', 'G06F18/40', 'G06T2200/08', 'G06T2200/24', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20092', 'G06T2207/30252', 'G06T2210/12', 'G06T2219/004', 'G06T2219/2004']"
CN111429460B,"Image segmentation method, image segmentation model training method, device and storage medium","The present application relates to the field of artificial intelligence technologies, and in particular, to an image segmentation method, an image segmentation model training device, and a storage medium. The image segmentation method comprises the following steps: acquiring an image to be segmented; extracting voxel characteristics from the image to be segmented; determining a first similarity between the voxel characteristics and voxel reference values corresponding to different classes; the voxel reference value is a model parameter obtained by performing model training by using an image sample to be segmented; and when the first similarity between the target voxel characteristic in the voxel characteristics and the corresponding voxel reference value corresponding to the category meets the segmentation condition, segmenting a picture block corresponding to the target voxel characteristic in the image to be segmented. By adopting the method, the image segmentation efficiency can be improved.","['G06T7/10', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/30016']"
US11182896B2,Automated segmentation of organ chambers using deep learning methods from medical imaging,"A method of detecting whether or not a body chamber has an abnormal structure or function including: (a) providing a stack of images as input to a system comprising one or more hardware processors configured to obtain a stack of medical images comprising at least a representation of the body chamber inside the patient; to obtain a region of interest using a convolutional network trained to locate the body chamber, wherein the region of interest corresponds to the body chamber from each of the medical images; and to infer a shape of the body chamber using a stacked auto-encoder (AE) network trained to delineate the body chamber, wherein the AE network segments the body chamber; (b) operating the system to detect the body chamber in the images using deep convolutional networks trained to locate the body chamber, to infer a shape of the body chamber using a stacked auto-encoder trained to delineate the body chamber, and to incorporate the inferred shape into a deformable model for segmentation; and (c) detecting whether or not the body chamber has an abnormal structure, wherein an abnormal structure is indicated by a body chamber clinical indicia that is different from a corresponding known standard clinical indicia for the body chamber.","['G06V10/82', 'G06T3/0006', 'G06T3/02', 'G06T7/0012', 'G06T7/11', 'G06T7/149', 'G06T7/162', 'G06T7/38', 'G06V10/454', 'G06V10/764', 'G06V20/64', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20124', 'G06T2207/30048', 'G06V2201/031']"
US11651581B2,System and method for correspondence map determination,A system and method for determining a correspondence map between a first and second image by determining a set of correspondence vectors for each pixel in the first image and selecting a correspondence vector from the set of correspondence vectors based on a cost value.,"['G06V10/751', 'G06F18/213', 'G06N3/0464', 'G06T5/60', 'G06T5/77', 'G06T7/246', 'G06T7/579', 'G06T7/593', 'G06V10/7715', 'G06N3/045', 'G06N3/08', 'G06T2207/10028', 'G06T2207/20084', 'G06V10/82']"
CN114724120B,Vehicle target detection method and system based on radar vision semantic segmentation adaptive fusion,"The invention provides a vehicle target detection method and system based on radar vision semantic segmentation self-adaptive fusion, which are characterized in that a semantic segmentation method is adopted to segment a camera depth map and radar point cloud data, a self-adaptive radar vision information fusion method PC-ARVF based on a reflection point and confidence coefficient is provided, the depth segmentation map and a point cloud segmentation result are fused based on a classification set and confidence coefficient of each point, a fusion point cloud is reconstructed, and supplement and fusion of multi-source data are guaranteed. The single-stage target detection model CDA-SSD based on the central point, the distance and the angle is provided, a vehicle surrounding frame is drawn by means of a cylindrical area, a loss function of target position regression is designed, and complexity of the vehicle detection model is reduced. Compared with the prior vehicle target detection method, the method has higher accuracy and lower complexity, and has important significance for the fusion of radar and video images and the detection of the vehicle target.","['G06F18/214', 'G06F18/23', 'G06F18/24', 'G06F18/253', 'Y02T10/40']"
US10410350B2,Skip architecture neural network machine and method for improved semantic segmentation,"A method of using a computer to semantically segment an image using a convolutional neural network system where a processor configured to convolve an input image with a plurality of filters and outputting a first output volume, pool the first output volume and creating a first activation map, determine the level of influence of the first activation map on the semantic segmentation, up-pool the first activation map to form an output image having a same number of pixels as the input image, output a probabilistic segmentation result, labeling each pixel's probability that it is a particular label, and the determination of the level of influence of the first activation map on the semantic segmentation is done using a gate layer that is positioned between a pooling layer and an up-pooling layer.","['G06V10/82', 'G06K9/66', 'G06N3/045', 'G06N3/0464', 'G06N3/048', 'G06N3/082', 'G06N3/084', 'G06N3/09', 'G06N3/0985', 'G06N7/005', 'G06N7/01', 'G06T7/11', 'G06V30/422', 'G06T2207/20081']"
CN114279357B,Die casting burr size measurement method and system based on machine vision,"The invention discloses a die casting burr size measurement method and system based on machine vision. The invention is a technology for extracting a target from the background by respectively preprocessing and dividing a workpiece image containing burrs and a standard image (workpiece image without burrs) into a plurality of mutually non-overlapping areas, wherein the inside of each area has the same or similar characteristics, and the characteristics of adjacent areas are different. And then, carrying out image difference on the workpiece image containing the burrs and the standard image to obtain a burr image, and calculating the plane size and the position of the burrs. The invention has the advantages of wide application range, high detection speed, high precision, low fault tolerance, long-time work and the like, can obtain the plane size and position information of burrs of workpieces with various shapes, and realizes the non-contact measurement of the sizes of burrs of die castings with various patterns.",[]
US11132578B2,System and method for creating navigable views,"A method for creating navigable views includes receiving digital images, computing a set of feature points for each of the digital images, selecting one of the digital images as a reference image, identifying a salient region of interest in the reference image, identifying other digital images containing a region of interest similar to the salient region of interest in the reference image using the set of feature points computed for each of other digital images, designating a reference location for the salient region of interest in the reference image, aligning the other digital images to the image that contains the designated reference location, ordering the image that contains the designated reference location and the other digital images, and generating a navigable view.","['G06K9/46', 'G06V20/30', 'G06F16/5854', 'G06K9/00677', 'G06K9/4671', 'G06T5/00', 'G06V10/462', 'G06K2009/4666']"
WO2022033076A1,"Target detection method and apparatus, device, storage medium, and program product","Embodiments of the present disclosure provide a target detection method and apparatus, a device, a storage medium, and a program product. The target detection method comprises: obtaining an image acquired by an image acquisition component, and internal parameters of the image acquisition component; determining, on the basis of the acquired image and the internal parameters, three-dimensional coordinate information of respective pixels of the acquired image in a world coordinate system; generating, according to the acquired image and the three-dimensional coordinate information of the respective pixels of the acquired image in the world coordinate system, a three-dimensional information image corresponding to the acquired image, wherein pixels of the three-dimensional information image and the pixels of the acquired image are arranged in the same order; and determining, on the basis of the three-dimensional information image, three-dimensional detection information of a target object in the world coordinate system, the target object being included in the acquired image.","['G06V20/56', 'G06F18/241', 'G06N3/08', 'G06T7/73', 'G06V10/764', 'G06N3/045', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30252']"
US8949252B2,Product category optimization for image similarity searching of image-based listings in a network-based publication system,"A weighted combination of attributes including but not limited to color, shape, pattern, brand, style, size, and material may be used to rank items when searching for similar items of a given category on an ecommerce system website. The weights on each attribute may be optimized by using feedback from users, the feedback indicating the importance of each attribute to a purchaser when searching for similar items of various categories. Alternatively, click-through patterns from the ecommerce system website can be mined and used to optimize the importance of each attribute to users by item category. The optimum attribute weights for that items category are used to rank the results in a search for similar items in that category.","['G06F17/30259', 'G06F16/5854', 'G06F16/5838', 'G06F17/3025', 'G06Q30/00']"
US11321868B1,System for estimating a pose of one or more persons in a scene,A system for estimating a pose of one or more persons in a scene includes a camera configured to capture an image of the scene; and a data processor configured to execute computer executable instructions for: (i) receiving the image of the scene from the camera; (ii) extracting features from the image of the scene for providing inputs to a keypoint subnet and a person detection subnet; (iii) generating one or more keypoints using the keypoint subnet; (iv) generating one or more person instances using the person detection subnet; (v) assigning the one or more keypoints to the one or more person instances by learning pose structures from the image data; and (vi) determining one or more poses of the one or more persons in the scene using the assignment of the one or more keypoints to the one or more person instances.,"['G06T7/73', 'G06F18/21', 'G06K9/00362', 'G06K9/46', 'G06K9/6217', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V40/10', 'G06V40/20', 'H04N23/611', 'H04N23/90', 'G06K2209/21', 'G06T2207/20016', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30196', 'G06V2201/07', 'H04N5/247']"
US20240057502A1,Performing image based actions on a moving vehicle,"A method implemented by a treatment system disposed on a vehicle, the treatment system having one or more processors, a storage, and a treatment mechanism, includes capturing a first image of a region of an agricultural environment, detecting, by implementing a first machine learning (ML) algorithm on a first portion of the first image, a presence of at least a portion of a first object in the first image, determining whether the first object detected is a treatment candidate, determining, upon determining that the first object is a treatment candidate, a first three dimensional (3D) location of at least a portion of the first object in the agricultural environment, and applying, a treatment to at least the portion of the first object by activating the treatment mechanism to interact with the first object.","['A01B69/001', 'A01M7/0089', 'A01B79/005', 'G05D1/0246', 'G05D1/249', 'G05D1/43', 'G06N3/08', 'G06Q50/02', 'G06T7/194', 'G06T7/74', 'G06V10/44', 'G06V10/56', 'G06V10/764', 'G06V20/188', 'G06V20/56', 'G06V20/70', 'A01M7/006', 'G05D2107/21', 'G05D2111/60', 'G05D2201/0201', 'G06N20/00', 'G06N3/04', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30188']"
CN118865426A,A method for extracting key information from airport luggage tags,"The invention discloses a key information extraction method of airport luggage labels, and belongs to the technical field of image processing and aviation luggage management systems. The method comprises the steps of preprocessing an acquired original image of an airport luggage tag, adopting a convolutional neural network model to primarily identify the edge of the luggage tag, determining a text region image of a target tag, calculating inclination and distortion in the image through an automatic perspective correction algorithm, applying a perspective matrix to carry out geometric correction, secondarily identifying through the convolutional circular neural network, and extracting key information such as passenger names, flight numbers, luggage serial numbers and the like. The invention provides an efficient and accurate baggage tag image recognition and key information extraction method for an airport baggage handling system, thoroughly changes the traditional baggage handling flow and greatly improves the handling speed and accuracy.","['G06V30/414', 'G06N3/044', 'G06N3/045', 'G06N3/0464', 'G06V10/82', 'G06V30/1463', 'G06V30/147', 'G06V30/16', 'G06V30/1607', 'G06V30/162', 'G06V30/164', 'G06V30/166', 'G06V30/18057', 'G06V30/19007', 'G06V30/19173', 'G06V30/416', 'G06V30/42']"
US11957629B2,Systems and methods for assisting surgery,"Disclosed are systems and methods for assisting with procedures involving a subject's joint, such as a joint. Robotic devices move and position the subject to manipulate the joint and sensing devices sense the joint gap. The robotic devices are controlled based on the joint gap. Novel techniques are disclosed for joint gap segmentation, approximating an uncertainty in determination of the varying dimension of the joint gap, and real-time motion analysis of the joint gap size. In some examples, a kinematic model of the patient's anatomy is utilized to provide robotically assisted manipulation of the same using the techniques described herein.","['A61G13/1295', 'A61B34/10', 'A61B34/20', 'A61B34/30', 'A61B34/70', 'A61G13/0063', 'A61G13/0081', 'A61G13/1245', 'G06T7/11', 'G06T7/60', 'A61B2034/102', 'A61B2034/105', 'A61B2034/2048', 'A61B2034/2051', 'A61B2034/2055', 'A61B2034/2059', 'A61B2034/2065', 'A61B2034/2072', 'A61B2034/301', 'A61B2090/3614', 'A61B2090/3983', 'A61B34/25', 'A61G2203/20', 'G06T2207/10068', 'G06T2207/30008', 'G06T2207/30196']"
US9626769B2,"Digital video encoder system, method, and non-transitory computer-readable medium for tracking object regions","A video compression framework based on parametric object and background compression is proposed. At the encoder, an object is detected and frames are segmented into regions corresponding to the foreground object and the background. The encoder generates object motion and appearance parameters. The motion or warping parameters may include at least two parameters for object translation; two parameters for object scaling in two primary axes and one object orientation parameter indicating a rotation of the object. Particle filtering may be employed to generate the object motion parameters. The proposed methodology is the formalization of the concept and usability for perceptual quality scalability layer for Region(s) of Interest. A coded video sequence format is proposed which aims at “network friendly” video representation supporting appearance and generalized motion of object(s).","['G06T7/215', 'G06T7/2006', 'G06T7/0079', 'G06T7/10', 'G06T7/20', 'H04N19/12', 'H04N19/132', 'H04N19/139', 'H04N19/14', 'H04N19/154', 'H04N19/159', 'H04N19/17', 'H04N19/20', 'H04N19/36', 'H04N19/436', 'H04N19/527', 'H04N19/543', 'H04N19/61', 'H04N19/70', 'G06T2207/10016']"
US10546387B2,Pose determination with semantic segmentation,"A method determines a pose of an image capture device. The method includes accessing an image of a scene captured by the image capture device. A semantic segmentation of the image is performed, to generate a segmented image. An initial pose of the image capture device is generated using a three-dimensional (3D) tracker. A plurality of 3D renderings of the scene are generated, each of the plurality of 3D renderings corresponding to one of a plurality of poses chosen based on the initial pose. A pose is selected from the plurality of poses, such that the 3D rendering corresponding to the selected pose aligns with the segmented image.","['G06T7/73', 'G06T7/11', 'G06T7/74', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30244']"
US10713794B1,Method and system for using machine-learning for object instance segmentation,"In one embodiment, a method includes a computing system accessing a training image. The system may generate a feature map for the training image using a first neural network. The system may identify a region of interest in the feature map and generate a regional feature map for the region of interest based on sampling locations defined by a sampling region. The sampling region and the region of interest may correspond to the same region in the feature map. The system may generate an instance segmentation mask associated with the region of interest by processing the regional feature map using a second neural network. The second neural network may be trained using the instance segmentation mask. Once trained, the second neural network is configured to generate instance segmentation masks for object instances depicted in images.","['G06T7/11', 'G06K9/3233', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06V10/25', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/00', 'G06T2207/20081', 'G06T2207/20084', 'G06T2210/12']"
US20160162723A1,"Tenrprint card input device, tenrprint card input method and storage medium","A fingerprint image processing device includes a memory, and a processor coupled to the memory. The processor performs operations, including reading a tenprint card image which includes a plurality of fingerprint patterns and at least one ruled line to separate one fingerprint imprint area from another fingerprint imprint area, and extracting from the tenprint card image a fingerprint image which includes a part of a fingerprint imprint area and apart of a next fingerprint imprint area.","['G06K9/00067', 'G06T7/74', 'G06K9/00013', 'G06K9/346', 'G06V10/273', 'G06V40/13', 'G06V40/1347', 'G06V40/1359', 'A61B5/1172', 'G06T2207/10004']"
US20210397943A1,Techniques for classification with neural networks,"Apparatuses, systems, and techniques to train neural networks to perform classification. In at least one embodiment, one or more neural networks are trained to perform classification based on, for example, using one or more compressed representations of one or more class labels, where the one or more compressed representations have fewer bits than a representation of the one or more class labels.","['G06F18/214', 'G06F18/241', 'G06N3/063', 'G06F18/24137', 'G06N3/045', 'G06N3/0454', 'G06N3/08', 'G06N3/084', 'G06N5/04', 'G06V10/764', 'G06V10/774', 'G06V10/82']"
CN111652827B,Front face synthesis method and system based on generation countermeasure network,"The invention provides a front face synthesis method and a system based on a generation countermeasure network, which are used for detecting and segmenting a face part from an input image, and aligning the face to acquire a face image to be synthesized; estimating the head pose of the human face according to the key points of the human face, and dividing the human face data set into a front human face set and a non-front human face set according to the rotational freedom of the head; extracting the identity characteristics of an input face image by using a pre-training model of a face recognition deep neural network to train a supervision network; and synthesizing corresponding front face images based on the generation countermeasure network according to the input side face images. The face symmetry constraint and the identity feature constraint make the synthesized face more natural and better maintain the identity feature.","['G06T7/11', 'G06N3/045', 'G06N3/084', 'G06T3/60', 'G06T5/50', 'G06T7/194', 'G06T7/50', 'G06V40/171', 'G06T2207/20221', 'Y02T10/40']"
US10733727B2,Application of deep learning for medical imaging evaluation,This disclosure generally pertains to methods and systems for processing electronic data obtained from imaging or other diagnostic and evaluative medical procedures. Certain embodiments relate to methods for the development of deep learning algorithms that perform machine recognition of specific features and conditions in imaging and other medical data. Another embodiment provides systems configured to detect and localize medical abnormalities on medical imaging scans by a deep learning algorithm.,"['G16H30/40', 'G06T7/0012', 'A61B6/503', 'A61B6/5217', 'G06F40/30', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/096', 'G06T7/73', 'G16H50/20', 'G16H50/30', 'G06T2207/10116', 'G06T2207/20081']"
US10248744B2,"Methods, systems, and computer readable media for acoustic classification and optimization for multi-modal rendering of real-world scenes","Methods, systems, and computer readable media for acoustic classification and optimization for multi-modal rendering of real-world scenes are disclosed. According to one method for determining acoustic material properties associated with a real-world scene, the method comprises obtaining an acoustic response in a real-world scene. The method also includes generating a three-dimensional (3D) virtual model of the real-world scene. The method further includes determining acoustic material properties of surfaces in the 3D virtual model using a visual material classification algorithm to identify materials in the real-world scene that make up the surfaces and known acoustic material properties of the materials. The method also includes using the acoustic response in the real-world scene to adjust the acoustic material properties.","['G06T17/20', 'G06F17/5009', 'G06F18/2413', 'G06F30/13', 'G06F30/20', 'G06K9/00201', 'G06K9/00671', 'G06K9/4628', 'G06K9/627', 'G06T7/11', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/20', 'G06V20/64', 'H04S7/305', 'A63F13/54', 'G06F17/5004', 'G06F2119/10', 'G06F2217/82', 'G06F3/165', 'G06T2200/04', 'G06T2207/20081', 'H04R1/20', 'H04R27/00', 'H04R29/00', 'H04S2400/11', 'H04S2420/01', 'H04S7/306']"
US11837354B2,Contrast-agent-free medical diagnostic imaging,Described herein is medical imaging technology for concurrent and simultaneous synthesis of a medical CA-free-AI-enhanced image and medical diagnostic image analysis comprising: receiving a medical image acquired by a medical scanner in absence of contrast agent enhancement; providing the medical image to a computer-implemented machine learning model; concurrently performing a medical CA-free-AI-enhanced image synthesis task and a medical diagnostic image analysis task with the machine learning model; reciprocally communicating between the image synthesis task and the image analysis task for mutually dependent training of both tasks. Methods and systems and non-transitory computer readable media are described for execution of concurrent and simultaneous synthesis of a medical CA-free-AI-enhanced image and medical diagnostic image analysis.,"['G16H30/40', 'G06T7/0012', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/084', 'G06N3/088', 'G06N3/0895', 'G06N3/09', 'G06N3/092', 'G06N3/094', 'G06T7/0016', 'G06T7/11', 'G06T7/174', 'G16H30/20', 'G16H50/20', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30048', 'G06T2207/30096']"
US20210248751A1,"Brain image segmentation method and apparatus, network device, and storage medium","Embodiments of this application disclose a brain image segmentation method and apparatus, a network device, and a storage medium. The method includes obtaining, by a device, a to-be-segmented image group comprising a plurality of modal images of a brain. The device includes a memory storing instructions and a processor in communication with the memory. The method further includes performing, by the device, skull stripping according to the plurality of modal images to obtain a skull-stripped mask; separately performing, by the device, feature extraction on the plurality of modal images to obtain extracted features, and fusing the extracted features to obtain a fused feature; segmenting, by the device, encephalic tissues according to the fused feature to obtain an initial segmentation result; and fusing, by the device, the skull-stripped mask and the initial segmentation result to obtain a segmentation result corresponding to the to-be-segmented image group.","['G06T7/11', 'G06F18/25', 'G06K9/6232', 'G06K9/6288', 'G06N3/04', 'G06N3/08', 'G06T7/0012', 'G06V10/26', 'G06V10/454', 'G06V10/764', 'G06V10/80', 'G06V10/82', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30016', 'G06T2207/30096', 'G06V2201/03', 'Y02A90/10']"
US20190371134A1,"Self-checkout system, method thereof and device therefor","A self-checkout system capable of product identification and customer abnormal behavior detection, a method thereof and a device therefor are provided herein. The self-checkout system includes a product identification device and a customer abnormal behavior detection device. The product identification device is configured to perform a product identification, in which whether products are correctly placed on a platform and whether the identification can be completed are determined. The customer abnormal behavior detection device is configured to detect whether a customer has an abnormal checkout behavior.","['G06Q20/3278', 'G06F18/22', 'G06K9/00342', 'G06K9/00369', 'G06K9/00771', 'G06K9/46', 'G06K9/6215', 'G06Q20/18', 'G06Q20/206', 'G06Q20/208', 'G06Q20/405', 'G06V10/26', 'G06V20/10', 'G06V20/52', 'G06V40/103', 'G06V40/23', 'G07G1/0036', 'G06V20/68', 'G06V40/20']"
US11263736B2,Mapping and encoding gemological features,"Embodiments of the present invention disclose a method, computer program product, and system for mapping one or more inclusions in a mineral crystal. A set of image data associated with the mineral crystal is receiving. The received set of image data is analyzed. One or more inclusions associated with the mineral crystal is identified based on the analyzed image data. The identified one or more inclusions of the mineral crystal are mapped to a tree structure representing the surface of the mineral crystal. The mapped one or more inclusions are encoded as a chain-code associated with the mineral crystal. A radial distance between a center of mass value of the mineral crystal and a center of mass value of the identified one or more inclusions is calculated and a mineral crystal fingerprint is generated.","['G06T7/0004', 'G06T7/0002', 'G01N21/87', 'G01N33/39', 'G06T15/205', 'G06T7/11', 'G06T7/13', 'G06T7/55', 'G06T7/60', 'G06T2200/08', 'G06T2207/20081', 'G06T2207/30108', 'G06T3/60']"
US11100643B2,Training strategy search using reinforcement learning,"In at least one embodiment, a reinforcement-learning-based searching approach is used to produce a training configuration for a machine-learning model. In at least one embodiment, 3D medical image segmentation is performed using learned image preprocessing parameters.","['G06N3/006', 'G06N3/08', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/048', 'G06N3/0481', 'G06N3/09', 'G06N3/092', 'G06N3/0985', 'G06T5/002', 'G06T5/70', 'G06T7/0012', 'G06T7/10']"
US20190251372A1,System and method for lane detection,"Systems and methods for lane detection that are capable of providing a warning to a driver of a vehicle in case the vehicle deviates from the detected lane. The method includes receiving a plurality of images captured by an image capturing device, pre-processing the received plurality of images to obtain a Region of Interest (RoI) in the plurality of images, and obtain one or more edge features over the RoI, extracting one or more ridge features based on processing of the RoI, detecting an indication of a footpoint of one or more probable lane lines based on the extracted ridge features, detecting a potential lane based on the footpoint of the probable lane lines, applying a mask on the one or more edge features to obtain relevant edges, and detecting a final lane based on the extracted relevant edges and based on the footpoint of the probable lane lines.","['G06T7/73', 'G06K9/00798', 'G06K9/4604', 'G06T7/13', 'G06V10/25', 'G06V10/255', 'G06V10/443', 'G06V10/457', 'G06V20/588', 'G06K2009/4666', 'G06T2207/20076', 'G06T2207/30256', 'G06T9/20']"
EP3675008A1,Systems and methods for identifying processes for robotic automation and building models therefor,"According to one aspect, a computer-implemented method of discovering processes for robotic process automation (RPA) includes: recording a plurality of event streams, each event stream corresponding to a human user interacting with a computing device to perform one or more tasks; concatenating the event streams; segmenting some or all of the concatenated event streams to generate one or more individual traces performed by the user interacting with the computing device, each trace corresponding to a particular task; clustering the traces according to a task type; identifying, from among some or all of the clustered traces, one or more candidate processes for robotic automation; prioritizing the candidate processes; and selecting at least one of the prioritized candidate processes for robotic automation. Further aspects building upon the above include generating RPA models to perform tasks determined to be processes for RPA. Corresponding systems and computer program products are also described.","['G06Q10/0633', 'G06F11/3438', 'G06F11/3476', 'G06F18/23213', 'G06Q10/06398', 'G06F2201/81', 'G06F2201/86']"
US10579153B2,"One-handed gesture sequences in virtual, augmented, and mixed reality (xR) applications","Systems and methods for enabling one-handed gesture sequences in virtual, augmented, and mixed reality (xR) applications are described. In an illustrative, non-limiting embodiment, an Information Handling System (IHS) may include a processor and a memory coupled to the processor, the memory having program instructions stored thereon that, upon execution, cause the IHS to: receive a gesture sequence from a user wearing a Head-Mounted Device (HMD) coupled to the IHS, where the HMD is configured to display an xR application; identify the gesture sequence as a one-handed sequence; and execute a command in response to the one-handed sequence.","['G06F3/017', 'G06F3/013', 'G06F3/0346', 'G06F3/0482', 'G06F3/167', 'G06K9/00355', 'G06K9/00382', 'G06K9/00389', 'G06K9/00744', 'G06V20/20', 'G06V20/46', 'G06V40/11', 'G06V40/113', 'G06V40/28']"
CN114219910B,Automatic driving vehicle livestock cleaning method integrating laser radar and machine vision,"A laser radar and machine vision integrated automatic driving vehicle animal husbandry cleaning method includes the steps of firstly, constructing a 3D animal husbandry model through the laser radar, and marking a passable path capable of completely cleaning the animal husbandry according to the model by adopting A-calculation rules. And secondly, framing a target three-dimensional detection frame by using a laser radar and a machine vision. Accurate position information is obtained through matching the three-dimensional detection frames with the corresponding 3D model, and local path planning is performed by adopting an artificial potential field method according to information of obstacle targets. The processor sends a running instruction to the micro controller through the CAN bus to realize the functions of forward, backward, braking, steering and speed regulation of the vehicle, and the processor determines the cleaning range and mode through cleaning target information. The PLC controls the swing of the cleaning spray head and the rotation of the cleaning rod, so that the size of the cleaning object of the self-adaptive livestock house is realized, and the omnibearing cleaning of fences, baffles and the like of the livestock house is completed. The invention is applicable to various livestock scenes, has high universality and is integrated with an automatic driving technology to achieve the effects of high efficiency, low cost and labor saving.","['G06T17/20', 'G06N3/045', 'G06N3/08', 'G06T5/70', 'G06T7/337', 'G06T7/85', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30244', 'Y02A40/70']"
US11526704B2,Method and system of neural network object recognition for image processing,"A system, article, and method of neural network object recognition for image processing includes customizing a training database and adapting an instance segmentation neural network used to perform the customization.","['G06K9/6267', 'G06V10/25', 'G06F18/214', 'G06F18/217', 'G06F18/24', 'G06K9/6256', 'G06K9/6262', 'G06V10/454', 'G06V10/50', 'G06V10/56', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06V20/40']"
US20190042826A1,Automatic nuclei segmentation in histopathology images,"Provided herein are systems and computer-implemented methods for quantitative analyses of tissue sections (including, histopathology samples, such as immunohistochemically labeled or H&E stained tissue sections), involving automatic unsupervised segmentation of image(s) of the tissue section(s), measurement of multiple features for individual nuclei within the image(s), clustering of nuclei based on extracted features, and/or analysis of the spatial arrangement and organization of features in the image based on spatial statistics. Also provided are computer-readable media containing instructions to perform operations to carry out such methods. A quantitative image analysis pipeline for tumor purity estimation is also described","['G06K9/0014', 'G06T7/155', 'G06F18/23213', 'G06K9/4638', 'G06T7/0012', 'G06T7/11', 'G06T7/44', 'G06V10/443', 'G06V10/763', 'G06V20/695', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/30024']"
US11157726B2,Person identification systems and methods,"Techniques disclosed herein relate to identifying individuals in digital images. In some embodiments, a digital image(s) that captures a scene containing one or more people may be acquired. The single digital image may be applied as input across a single machine learning model. In some implementations, the single machine learning model may be trained to perform a non-facial feature recognition task and a face-related recognition task. Output may be generated over the single machine learning model based on the input. The output may include first data indicative of non-facial features of a given person of the one or more people and second data indicative of at least a location of a face of the given person in the digital image relative to the non-facial features. In various embodiments, the given person may be identified based at least in part on the output.","['G06V10/82', 'G06K9/00362', 'G06F18/24133', 'G06K9/00221', 'G06K9/00288', 'G06K9/00771', 'G06K9/00892', 'G06K9/3233', 'G06K9/4628', 'G06K9/6271', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06V10/25', 'G06V10/454', 'G06V10/764', 'G06V20/52', 'G06V40/10', 'G06V40/16', 'G06V40/172', 'G06V40/70', 'G06N7/005', 'G06N7/01']"
US12367585B2,Utilizing machine learning models to generate refined depth maps with segmentation mask guidance,"The present disclosure relates to systems, non-transitory computer-readable media, and methods for utilizing machine learning models to generate refined depth maps of digital images utilizing digital segmentation masks. In particular, in one or more embodiments, the disclosed systems generate a depth map for a digital image utilizing a depth estimation machine learning model, determine a digital segmentation mask for the digital image, and generate a refined depth map from the depth map and the digital segmentation mask utilizing a depth refinement machine learning model. In some embodiments, the disclosed systems generate first and second intermediate depth maps using the digital segmentation mask and an inverse digital segmentation mask and merger the first and second intermediate depth maps to generate the refined depth map.","['G06T7/11', 'G06T7/215', 'G06T7/50', 'G06T2207/20081', 'G06T2207/20084']"
US20180176318A1,Method and system for dynamic application management,"A system (and method) for device-independent point to multipoint communication is disclosed. The system is configured to receive a message addressed to one or more destination users, the message type being, for example, Short Message Service (SMS), Instant Messaging (IM), E-mail, web form input, or Application Program Interface (API) function call. The system also is configured to determine information about the destination users, the information comprising preferred devices and interfaces for receiving messages, the information further comprising message receiving preferences. The system applies rules to the message based on destination user information to determine the message endpoints, the message endpoints being, for example, Short Message Service (SMS), Instant Messaging (IM), E-mail, web page output, or Application Program Interface (API) function call. The system translates the message based on the destination user information and message endpoints and transmits the message to each endpoint of the message. A system (and method) also enables user to perform a keyword search, after performing the keyword search, a result display on the first computing device to the first user, in which are displayed individual messages located in the keyword search, wherein the result display provides a graphical subscribe indicator that, when selected after the search is performed and after the individual messages are displayed, subscribes the first user to a second user who provided the selected individual message from the search results, so as to enable the first user to be a follower of the second user, wherein the first user becomes one of several followers of the second user and the second user has a second computing device and storing, by the one or more computer processors, the followers of the second user, including the first user, in a first storage. User is also enabling to post, a new message for distribution to one or more unspecified recipients wherein server identifies the followers of the posting user as recipients of the new message and sending the new message to the followers of the second user, including the first user.","['H04L67/26', 'H04L67/55', 'G06Q20/384', 'G06F16/1837', 'G06F16/2428', 'G06F16/275', 'G06F16/285', 'G06F16/3331', 'G06F16/40', 'G06F16/48', 'G06F16/951', 'G06F16/9535', 'G06F16/9536', 'G06F16/9538', 'G06F17/30017', 'G06F17/30209', 'G06F17/30398', 'G06F17/30581', 'G06F17/30598', 'G06F17/30657', 'G06F17/30864', 'G06F17/30867', 'G06F3/0482', 'G06F8/61', 'G06F8/65', 'G06Q10/00', 'G06Q10/10', 'G06Q20/10', 'G06Q20/1235', 'G06Q20/145', 'G06Q20/322', 'G06Q20/386', 'G06Q30/02', 'G06Q30/0243', 'G06Q30/0249', 'G06Q30/0251', 'G06Q30/0255', 'G06Q30/0269', 'G06Q30/0282', 'G06Q50/184', 'H04L12/18', 'H04L51/046', 'H04L51/066', 'H04L51/14', 'H04L51/214', 'H04L51/28', 'H04L51/32', 'H04L51/48', 'H04L51/52', 'H04L63/061', 'H04L67/02', 'H04L67/10', 'H04L67/22', 'H04L67/306', 'H04L67/34', 'H04L67/535', 'H04W4/60', 'G06Q50/01', 'H04L51/16', 'H04L51/216', 'H04L63/0227', 'H04L67/104', 'H04L67/1074']"
US20230237649A1,Systems and Methods for Quantification of Liver Fibrosis with MRI and Deep Learning,"Embodiments provide a deep learning framework to accurately segment liver and spleen using a convolutional neural network with both short and long residual connections to extract their radiomic and deep features from multiparametric MRI. Embodiments will provide an “ensemble” deep learning model to quantify biopsy derived liver fibrosis stage and percentage using the integration of multiparametric MRI radiomic and deep features, MRE data, as well as routinely available clinical data. Embodiments will provide a deep learning model to quantify MRE-derived liver stiffness using multiparametric MRI, radiomic and deep features and routinely-available clinical data.","['A61B5/055', 'G06T7/0012', 'A61B5/4244', 'A61B5/7264', 'G06F18/24133', 'G06V10/82', 'G16H30/40', 'G16H50/20', 'G16H50/30', 'A61B2576/02', 'G06T2207/10088', 'G06T2207/20084', 'G06T2207/30056', 'G06V2201/031']"
US20240055101A1,"Food and nutrient estimation, dietary assessment, evaluation, prediction and management","The disclosure generally relates to the artificial intelligence (AI) automatic methods, computer program product, and systems and methodology for dietary and medical treatment planning, food waste estimation, analyzing three-dimensional food image construction, measurement, nutrient estimation, nutritional assessment, evaluation, prediction and management. More particularly, the embodiments described herein relate to utilizing an AI-based algorithm that can automatically, detect food items from images acquired by cameras for dietary assessment, dietary planning, and for estimating food waste. In one aspect, the method may include food calorie estimation techniques using machine learning and computer vision techniques for dietary assessment. In another aspect, the tools may apply to personalized nutrition. The method may also include the automation of nutrition planning. In yet another aspect, the tools may apply to medical treatment planning, wherein meals and treatment plans are individualized explicitly for each user according to several unique characteristics associated with that user.","['G16H20/60', 'G06N20/10', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06Q10/06', 'G06Q10/30', 'G06Q50/22', 'G16H50/20', 'G16H50/30', 'G06N3/048']"
US20230118737A1,Soil treatment using image processing,"A method performed by a treatment system disposed on a moving platform, the treatment system having one or more processors, a storage and a treatment mechanism, comprising: receiving one or more images of an environment in which the moving platform is operating; identifying, in real-time, a pose of the moving platform using sensor inputs; identifying one or more target objects by processing the one or more images using a machine learning (ML) algorithm; and controlling the treatment mechanism to treat the one or more target objects by orienting the treatment mechanism towards the one or more target objects at least partially based on the pose.","['A01M7/0089', 'A01B79/005', 'A01D91/00', 'A01G7/06', 'A01M21/00', 'A01M9/0092', 'G05B15/02', 'A01C21/007']"
US11010902B2,Capsules for image analysis,"An improved method of performing object segmentation and classification that reduces the memory required to perform these tasks, while increasing predictive accuracy. The improved method utilizes a capsule network with dynamic routing. Capsule networks allow for the preservation of information about the input by replacing max-pooling layers with convolutional strides and dynamic routing, allowing for the reconstruction of an input image from output capsule vectors. The present invention expands the use of capsule networks to the task of object segmentation and medical image-based cancer diagnosis for the first time in the literature; extends the idea of convolutional capsules with locally-connected routing and propose the concept of deconvolutional capsules; extends the masked reconstruction to reconstruct the positive input class; and proposes a capsule-based pooling operation for diagnosis. The convolutional-deconvolutional capsule network shows strong results for the tasks of object segmentation and classification with substantial decrease in parameter space.","['G06T7/11', 'G06N20/10', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/0495', 'G06N3/084', 'G06N3/09', 'G06T7/0012', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30064']"
US12327345B2,Vision-based safety monitoring and/or activity analysis,"Presented herein are embodiments of a vision-based object perception system for activity analysis, safety monitoring, or both. Embodiments of the perception subsystem detect multi-class objects (e.g., construction machines and humans) in real-time while estimating the poses and actions of the detected objects. Safety monitoring embodiments and object activity analysis embodiments may be based on the perception result. To evaluate the performance of embodiments, a dataset was collected including multi-class of objects in different lighting conditions with human annotations. Experimental results show that the proposed action recognition approach outperforms the state-of-the-art approaches on top-1 accuracy by about 5.18%.","['G06T7/0004', 'G06T7/11', 'G06T7/73', 'G06V10/225', 'G06V10/764', 'G06V10/82', 'G06V20/52', 'G08B21/02', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30108', 'G06T2207/30196', 'G06T2207/30248', 'G08B21/0476']"
US8255793B2,Automatic visual segmentation of webpages,"To provide valuable information regarding a webpage, the webpage must be divided into distinct semantically coherent segments for analysis. A set of heuristics allow a segmentation algorithm to identify an optimal number of segments for a given webpage or any portion thereof more accurately. A first heuristic estimates the optimal number of segments for any given webpage or portion thereof. A second heuristic coalesces segments where the number of segments identified far exceeds the optimal number recommended. A third heuristic coalesces segments corresponding to a portion of a webpage with much unused whitespace and little content. A fourth heuristic coalesces segments of nodes that have a recommended number of segments below a certain threshold into segments of other nodes. A fifth heuristic recursively analyzes and splits segments that correspond to webpage portions surpassing a certain threshold portion size.","['G06F16/353', 'G06F16/951', 'G06F40/131', 'G06F40/143']"
CN114585297B,Includes machine learning and uses its results for artificial intelligence registration and landmark detection,"Provided herein are one or more devices, systems, methods, and storage media that use artificial intelligence applications using devices or systems that use and/or control one or more imaging modalities, such as, but not limited to, angiography, optical Coherence Tomography (OCT), multi-modality OCT, near infrared fluorescence (NIRAF), OCT-NIRAF, and the like. Examples of AI applications discussed herein include, but are not limited to, use of one or more of AI registration, AI label detection, depth or machine learning, computer vision or image recognition tasks, keypoint detection, feature extraction, model training, input data preparation techniques, input mapping to models, post-processing and/or interpretation of output data, one or more types of machine learning models (including, but not limited to segmentation, regression, combining or repeating regression and/or segmentation), label detection success rate, and/or registration success rate to improve or optimize label detection and/or registration.","['A61B34/20', 'A61B5/7267', 'A61B6/12', 'A61B6/463', 'A61B6/504', 'A61B6/5247', 'G06T7/0012', 'A61B2034/2051', 'A61B2090/3966', 'G06T2207/10016', 'G06T2207/10121', 'G06T2207/10132', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004', 'G06T2207/30021', 'G06T2207/30104', 'G06T2207/30204']"
US20240232292A1,Pattern change discovery between high dimensional data sets,"The general problem of pattern change discovery between high-dimensional data sets is addressed by considering the notion of the principal angles between the subspaces is introduced to measure the subspace difference between two high-dimensional data sets. Current methods either mainly focus on magnitude change detection of low-dimensional data sets or are under supervised frameworks. Principal angles bear a property to isolate subspace change from the magnitude change. To address the challenge of directly computing the principal angles, matrix factorization is used to serve as a statistical framework and develop the principle of the dominant subspace mapping to transfer the principal angle based detection to a matrix factorization problem. Matrix factorization can be naturally embedded into the likelihood ratio test based on the linear models. The method may be unsupervised and addresses the statistical significance of the pattern changes between high-dimensional data sets.","['G06F17/18', 'G06F18/213']"
US11238650B2,Self-supervised single-view 3D reconstruction via semantic consistency,"Apparatuses, systems, and techniques to identify a shape or camera pose of a three-dimensional object from a two-dimensional image of the object. In at least one embodiment, objects are identified in an image using one or more neural networks that have been trained on objects of a similar category and a three-dimensional mesh template.","['G06T17/20', 'G06F18/217', 'G06K9/6262', 'G06T17/00', 'G06T7/40', 'G06T7/50', 'G06T7/74', 'G06V10/26', 'G06V10/776', 'G06V10/82', 'G06V20/64', 'G06T2200/08', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30244']"
US9155675B2,Portable robotic device,"A portable robotic device (PRD) as well as related devices and methods are described herein. The PRD includes a 3-D imaging sensor configured to acquire corresponding intensity data frames and range data frames of the environment. An imaging processing module configured to identify a matched feature in the intensity data frames, obtain sets of 3-D coordinates representing the matched feature in the range data frames, and determine a pose change of the PRD based on the 3-D coordinates; and perform 3-D data segmentation of the range data frames to extract planar surfaces.","['A61H3/061', 'A61H3/068', 'A61H2003/063', 'A61H2201/5007', 'A61H2201/5048', 'A61H2201/5058', 'A61H2201/5084', 'A61H2201/5092', 'A61H2201/5097', 'G01S17/89', 'G01S17/894', 'G06T2207/10016', 'G06T2207/10028', 'G06T2207/20076']"
US10038896B2,Interactive imaging systems and methods for motion control by users,"In various embodiments, the present invention provides a system and associated methods of calibration and use for an interactive imaging environment based on the optimization of parameters used in various segmentation algorithm techniques. These methods address the challenge of automatically calibrating an interactive imaging system, so that it is capable of aligning human body motion, or the like, to a visual display. As such the present invention provides a system and method of automatically and rapidly aligning the motion of an object to a visual display.","['H04N17/002', 'G06F3/0304', 'G06F3/038', 'G06K9/00335', 'G06K9/00362', 'G06T7/11', 'G06T7/194', 'G06V40/10', 'G06V40/20', 'H04N23/11', 'H04N5/332', 'G06T2207/10012', 'G06T2207/10048', 'G06T2207/10152', 'G06T2207/30196']"
US20210264195A1,Technologies for enabling analytics of computing events based on augmented canonicalization of classified images,"This disclosure discloses various computing technologies that enable a user to operate a browser to browse a web page that hosts a set of images and an operator of the web page to granularly track how the user is operating the browser with respect to the set of images based on various contextual information depicted in the set of images. Note that this disclosure is not limited to browsers and can be applied to other types of software applications, such as domain dedicated applications, such as e-commerce applications, photo gallery applications, encyclopedia applications, inventory applications, videogame applications, educational applications, social media applications, video streaming applications, or others, or others.","['G06F16/55', 'G06K9/628', 'G06T19/20', 'G06F16/51', 'G06F16/53', 'G06F16/535', 'G06F16/56', 'G06F16/58', 'G06F16/583', 'G06F16/5866', 'G06F16/957', 'G06F18/2431', 'G06K9/00248', 'G06K9/00288', 'G06T19/006', 'G06V10/82', 'G06V40/165', 'G06V40/172', 'G06K2009/00328', 'G06V40/179']"
US11397421B2,"Systems, devices and methods for bearing analysis in an industrial environment","Systems, devices and methods for bearing analysis in an industrial environment are disclosed. A data acquisition circuit structured to interpret a plurality of detection values corresponding to a plurality of input sensors coupled to the data acquisition circuit, a data storage for storing specifications and anticipated state information for a plurality of bearing types and buffering the plurality of detection values for a predetermined length of time, and a bearing analysis circuit to analyze buffered detection values relative to specifications and anticipated state information resulting in at least one bearing parameter are described. Analysis may include filtering the detection values through a high pass filter, identifying rapid changes in detection values, identifying frequencies at which spikes occur and comparing frequencies and spikes in amplitude relative to an anticipated state information and specification.","['H04B17/29', 'B62D15/0215', 'G01M13/028', 'G01M13/04', 'G01M13/045', 'G05B13/028', 'G05B19/4183', 'G05B19/4184', 'G05B19/41845', 'G05B19/4185', 'G05B19/41865', 'G05B19/41875', 'G05B23/0221', 'G05B23/0229', 'G05B23/024', 'G05B23/0264', 'G05B23/0283', 'G05B23/0286', 'G05B23/0289', 'G05B23/0291', 'G05B23/0294', 'G05B23/0297', 'G06F16/2477', 'G06F18/2178', 'G06F3/0608', 'G06F3/0619', 'G06F3/0635', 'G06F3/067', 'G06K9/6263', 'G06N20/00', 'G06N3/006', 'G06N3/02', 'G06N3/044', 'G06N3/0445', 'G06N3/045', 'G06N3/0454', 'G06N3/047', 'G06N3/0472', 'G06N3/0499', 'G06N3/084', 'G06N3/088', 'G06N5/046', 'G06N7/005', 'G06N7/01', 'G06Q10/04', 'G06Q10/0639', 'G06Q30/02', 'G06Q30/0278', 'G06Q30/06', 'G06Q50/00', 'G06V10/7784', 'G06V10/82', 'G16Z99/00', 'H02M1/12', 'H03M1/12', 'H04B17/26', 'H04B17/309', 'H04B17/318', 'H04L1/0002', 'H04L1/0041', 'H04L1/18', 'H04L1/1874', 'H04L67/1097', 'H04L67/12', 'H04W4/38', 'H04W4/70', 'B62D5/0463', 'F01D21/003', 'F01D21/12', 'F01D21/14', 'G05B19/042', 'G05B2219/32287', 'G05B2219/35001', 'G05B2219/37337', 'G05B2219/37351', 'G05B2219/37434', 'G05B2219/37537', 'G05B2219/40115', 'G05B2219/45004', 'G05B2219/45129', 'G05B23/02', 'G05B23/0208', 'G06F17/18', 'G06F18/21', 'G06F18/217', 'G06F18/25', 'G06K9/6288', 'G06N3/126', 'H04B17/23', 'H04B17/345', 'H04B17/40', 'H04L1/0009', 'H04L5/0064', 'H04L67/306', 'Y02P80/10', 'Y02P90/02', 'Y02P90/80', 'Y04S50/00', 'Y04S50/12', 'Y10S707/99939']"
CN110243372B,Intelligent agricultural machinery navigation system and method based on machine vision,"The invention relates to an intelligent agricultural machinery navigation system based on machine vision, which comprises: the image sensing module is used for acquiring a farmland image on the navigation path; the image processing module is used for preprocessing the farmland image, extracting a seedling line and a navigation line from the farmland image, calculating a transverse deviation value and a course angle deviation value of an agricultural machine and the navigation line, and judging the reliability of a calculation result; the agricultural machinery path planning module is used for planning a field operation path according to the navigation line; and the agricultural machine steering system is used for calculating the correct steering wheel angle of the agricultural machine according to the field operation path plan and carrying out steering control on the steering wheel of the agricultural machine. The method can quickly and accurately detect the leading line in a complex farmland environment, and can check and process the detection result.","['G01C11/02', 'G01C11/04', 'G01C21/20']"
US11868892B2,Partially-frozen neural networks for efficient computer vision systems,"An apparatus to facilitate partially-frozen neural networks for efficient computer vision systems is disclosed. The apparatus includes a frozen core to store fixed weights of a machine learning model, one or more trainable cores coupled to the frozen core, the one or more trainable cores comprising multipliers for trainable weights of the machine learning model, and wherein the alpha blending layer includes a trainable alpha blending parameter, and wherein the trainable alpha blending parameter is a function of a trainable parameter, a sigmoid function, and outputs of frozen and trainable blocks in a preceding layer of the machine learning model.","['G06N3/08', 'G06F18/214', 'G06N3/045', 'G06N3/0464', 'G06N3/048', 'G06N3/0495', 'G06N3/063', 'G06N3/082', 'G06N3/09', 'G06N3/096', 'G06V10/70', 'G06V10/764', 'G06V10/774', 'G06V10/776', 'G06V10/82']"
US20230306719A1,"Model structure, method for training model, image enhancement method, and device","Embodiments of this application disclose a model structure, a method for training a model, an image enhancement method, and a device, and may be applied to the computer vision field in the artificial intelligence field. The model structure includes: a selection module, a plurality of first neural network layers, a segmentation module, a transformer module, a recombination module, and a plurality of second neural network layers. The model overcomes a limitation that the transformer module can only be used to process a natural language task, and may be applied to a low-level vision task. The model includes the plurality of first/second neural network layers, and different first/second neural network layers correspond to different image enhancement tasks. Therefore, after being trained, the model can be used to process different image enhancement tasks.","['G06N3/045', 'G06V10/82', 'G06V10/771', 'G06F18/241', 'G06F18/2415', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/08', 'G06N3/088', 'G06N3/09', 'G06T5/50', 'G06T7/11', 'G06V10/44', 'G06V10/454', 'G06V10/7715', 'G06V10/774', 'G06V10/778', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221']"
US10192099B2,Systems and methods for automated screening and prognosis of cancer from whole-slide biopsy images,"Systems and methods for detection, grading, scoring and tele-screening of cancerous lesions are described. A complete scheme for automated quantitative analysis and assessment of human and animal tissue images of several types of cancers is presented. Various aspects of the invention are directed to the detection, grading, prediction and staging of prostate cancer on serial sections/slides of prostate core images, or biopsy images. Accordingly, the invention includes a variety of sub-systems, which could be used separately or in conjunction to automatically grade cancerous regions. Each system utilizes a different approach with a different feature set. For instance, in the quantitative analysis, textural-based and morphology-based features may be extracted at image- and (or) object-levels from regions of interest.","['G06K9/0014', 'A61B5/4381', 'A61B5/7203', 'A61B5/725', 'A61B5/7257', 'A61B5/726', 'A61B5/7267', 'G01N15/1433', 'G01N15/1475', 'G06F18/211', 'G06F19/00', 'G06F19/321', 'G06K9/00127', 'G06K9/00147', 'G06K9/46', 'G06K9/6228', 'G06T5/002', 'G06T5/20', 'G06T5/70', 'G06T7/0012', 'G06T7/13', 'G06T7/181', 'G06T7/90', 'G06V20/69', 'G06V20/695', 'G06V20/698', 'G16H30/20', 'G16H50/20', 'G16H50/30', 'G16H50/70', 'A61B10/0241', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/20036', 'G06T2207/20081', 'G06T2207/30024', 'G06T2207/30081', 'G06T2207/30096']"
US20240410981A1,Top-down object detection from lidar point clouds,"A deep neural network(s) (DNN) may be used to detect objects from sensor data of a three dimensional (3D) environment. For example, a multi-view perception DNN may include multiple constituent DNNs or stages chained together that sequentially process different views of the 3D environment. An example DNN may include a first stage that performs class segmentation in a first view (e.g., perspective view) and a second stage that performs class segmentation and/or regresses instance geometry in a second view (e.g., top-down). The DNN outputs may be processed to generate 2D and/or 3D bounding boxes and class labels for detected objects in the 3D environment. As such, the techniques described herein may be used to detect and classify animate objects and/or parts of an environment, and these detections and classifications may be provided to an autonomous vehicle drive stack to enable safe planning and control of the autonomous vehicle.","['G06V20/584', 'B60W60/0011', 'B60W60/0016', 'B60W60/0027', 'G01S17/89', 'G01S17/931', 'G01S7/4802', 'G05D1/0088', 'G05D1/81', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/09', 'G06T19/006', 'G06V10/25', 'G06V10/26', 'G06V10/454', 'G06V10/764', 'G06V10/774', 'G06V10/803', 'G06V10/82', 'G06V20/56', 'G06V20/58', 'B60W2420/403', 'B60W2420/408', 'G06N20/00', 'G06N3/084', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30261', 'G06V10/16']"
CN107545302B,A gaze direction calculation method for joint left and right eye images,"The invention provides a method for calculating the combined sight direction of left and right eye images of human eyes, which comprises the following steps: the extraction model of the binocular information inputs a human eye image, and automatically extracts the information characteristics of the left eye and the right eye contained in the image through a dual-channel model respectively; the model for extracting the human eye joint information features inputs the binocular images of the user, and extracts the human eye joint information features by combining the binocular information; the invention discloses a joint algorithm, and the three-dimensional sight line direction is calculated by inputting characteristic information. One of the applications of the invention is virtual reality and man-machine interaction, and the principle is to calculate the sight direction of a user by shooting an eye image of the user so as to interact with an intelligent system interface or a virtual reality object. The invention can also be widely used in the fields of training and training, game and entertainment, video monitoring, medical monitoring and the like.",['A61B3/113']
CN110678875B,System and method for guiding a user to take a self-photograph,The invention discloses a system and a method for improving the quality of a self-photographed image. The system and method include analyzing the self-photograph using an image processing module and a self-photograph quality index module. The image processing module processes the image using face and logo detection techniques and face normalization techniques. The self-photographing quality index module analyzes the self-photographing for important image quality metrics and provides feature vectors for generating self-photographing quality index scores. The self-photograph quality index score may be used to determine whether the self-photograph meets a minimum threshold quality requirement for further analysis of skin condition by the system.,"['G06V40/165', 'G06V10/993', 'G06T7/194', 'G06T7/536', 'G06V10/235', 'G06V10/32', 'G06V10/60', 'G06V10/82', 'G06V40/166', 'G06V40/171', 'G06V40/174', 'H04N23/63', 'H04N23/632', 'H04N23/64', 'H04N5/445', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30168', 'G06T2207/30201', 'G06T2207/30242']"
US12354264B2,Three dimensional object segmentation of medical images localized with object detection,"The present disclosure relates to techniques for segmenting objects within medical images using a deep learning network that is localized with object detection based on a derived contrast mechanism. Particularly, aspects are directed to localizing an object of interest within a first medical image having a first characteristic, projecting a bounding box or segmentation mask of the object of interest onto a second medical image having a second characteristic to define a portion of the second medical image, and inputting the portion of the second medical image into a deep learning model that is constructed as a detector using a weighted loss function capable of segmenting the portion of the second medical image and generating a segmentation boundary around the object of interest. The segmentation boundary may be used to calculate a volume of the object of interest for determining a diagnosis and/or a prognosis of a subject.","['G06T7/0012', 'G06T7/12', 'G06F18/241', 'G06F18/2415', 'G06N3/045', 'G06N3/047', 'G06N3/08', 'G06T7/10', 'G06T7/174', 'G06T7/62', 'G06T7/70', 'G06T7/73', 'G06V10/758', 'G06V10/764', 'G06V20/64', 'G06T2200/04', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10101', 'G06T2207/10104', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30004', 'G06V2201/031']"
US11797863B2,Systems and methods for synthesizing data for training statistical models on different imaging modalities including polarized images,"A method of generating synthetic images of virtual scenes includes: placing, by a synthetic data generator implemented by a processor and memory, three-dimensional (3-D) models of objects in a 3-D virtual scene; adding, by the synthetic data generator, lighting to the 3-D virtual scene, the lighting including one or more illumination sources; applying, by the synthetic data generator, imaging modality-specific materials to the 3-D models of objects in the 3-D virtual scene in accordance with a selected multimodal imaging modality, each of the imaging modality-specific materials including an empirical model; setting a scene background in accordance with the selected multimodal imaging modality; and rendering, by the synthetic data generator, a two-dimensional image of the 3-D virtual scene based on the selected multimodal imaging modality to generate a synthetic image in accordance with the selected multimodal imaging modality.","['G06T17/00', 'G06N3/088', 'G06T15/00', 'G06F18/214', 'G06N3/045', 'G06N3/0464', 'G06N3/0475', 'G06N3/09', 'G06N3/094', 'G06N3/096', 'G06T15/04', 'G06T15/20', 'G06T17/20', 'G06T19/20', 'G06T2207/20081', 'G06T2219/2021', 'G06T2219/2024']"
CN116071387B,Sleeper rail production quality detection method based on machine vision,"The invention relates to the technical field of image data processing, in particular to a sleeper production quality detection method based on machine vision, which comprises the following steps: acquiring a gray level image of the surface of the sleeper rail and performing preliminary segmentation to obtain a suspected crack region; marking the center line of each suspected crack region, and obtaining the shortest distance between each pixel point in the suspected crack region and the center line, so as to obtain the crack width of the suspected crack region and the influence degree of each pixel point in the suspected crack region; obtaining a self-adaptive local window size of a corresponding pixel point based on the crack width of the suspected crack area and gray information corresponding to the pixel point in the suspected crack area and the influence degree; and dividing the suspected crack region by using a Niblack algorithm based on the self-adaptive local window size to obtain a crack region, and obtaining the sleeper production quality according to the crack region. The invention can improve the accuracy of crack area detection, thereby ensuring the accuracy of sleeper rail production quality judgment.","['G06T7/136', 'G06T7/0004', 'G06T7/11', 'G06T2207/30108', 'Y02P90/30']"
CN110569838B,An autonomous landing method of quadrotor UAV based on visual positioning,"The invention discloses a vision positioning-based autonomous landing method of a quad-rotor unmanned aerial vehicle, which is characterized in that in order to enable the quad-rotor unmanned aerial vehicle to land on a ground landing platform independently and accurately so as to carry out fixed protection, charging endurance and mobile carrying on the quad-rotor unmanned aerial vehicle, a vision identification mark consisting of a regular triangle and concentric rings is designed, and a novel vision-based autonomous landing method of the quad-rotor unmanned aerial vehicle is provided based on the vision identification mark. In the first stage of the method, the position parameters of the unmanned aerial vehicle and the center of the identification mark are calculated through the regular triangle of the identification mark; and in the second stage, the position deviation and the yaw angle of the unmanned aerial vehicle and the center of the identification mark are determined by using the concentric rings and the heading reference lines inside the concentric rings, and finally the posture adjustment and the autonomous landing of the unmanned aerial vehicle are completed. The landing experiment of the unmanned aerial vehicle is verified, the method can accurately extract the mark characteristics, and the position information and the angle information of the unmanned aerial vehicle relative to the identification mark are calculated in real time, so that accurate landing is realized.","['G06V10/255', 'G06V10/48', 'G06V20/10']"
CN117494905B,Construction method and system for digital counterweight balance arch bridge,"The invention relates to the technical field of construction flow management, in particular to a method and a system for constructing a digital counterweight balance arch bridge, which comprise the following steps: based on the topography and bridge design parameters, intelligent analysis of the bridge structure is performed by adopting a data analysis algorithm and a computer vision technology, and a bridge deck system segmentation scheme is generated. In the invention, the data analysis algorithm and the computer vision technology are adopted to intelligently analyze the topography and design parameters of the bridge structure, an economic and reasonable bridge deck system segmentation scheme is generated, the cost is reduced, the material use is optimized, the construction environment and structure data are collected in real time through the internet of things technology and time sequence analysis, the safety and efficiency of the construction process are improved, the automatic balance weight of the vector machine is supported, the human error is reduced, the construction reliability is enhanced, the measurement accuracy is improved through the laser scanning and unmanned aerial vehicle mapping technology, the decision tree and the neural network algorithm are used for predictive maintenance and risk assessment, and the digital twin technology optimizes the construction flow through virtual simulation, so that the engineering efficiency and quality are improved.","['G01D21/02', 'G06F18/23213', 'G06F18/2411', 'G06F18/24323', 'G06F18/27', 'G06F30/13', 'G06F30/23', 'G06F30/27', 'G06N3/0499', 'G06N3/084', 'G06N5/01', 'G06Q10/04', 'G06Q10/047', 'G06Q10/06312', 'G06Q10/06314', 'G06Q10/0633', 'G06Q10/0635', 'G06Q50/08', 'G06F2111/08', 'G06F2111/10', 'G06F2119/14']"
US9495228B2,Multi-process interactive systems and methods,"A multi-process interactive system is described. The system includes numerous processes running on a processing device. The processes include separable program execution contexts of application programs, such that each application program comprises at least one process. The system translates events of each process into data capsules. A data capsule includes an application-independent representation of event data of an event and state information of the process originating the content of the data capsule. The system transfers the data messages into pools or repositories. Each process operates as a recognizing process, where the recognizing process recognizes in the pools data capsules comprising content that corresponds to an interactive function of the recognizing process and/or an identification of the recognizing process. The recognizing process retrieves recognized data capsules from the pools and executes processing appropriate to contents of the recognized data capsules.","['G06F9/542', 'G06F16/00', 'G06F3/017', 'G06F3/0325', 'G06F3/04883', 'G06F9/544']"
CN115082492B,"A semantic segmentation method, device and storage medium based on multispectral image","The invention discloses a semantic segmentation method, a semantic segmentation device and a storage medium based on a multispectral image, wherein the method comprises the steps of collecting and labeling multispectral data sets for semantic segmentation, constructing a semantic segmentation model, training the semantic segmentation model by adopting the multispectral data sets, acquiring an image to be processed, inputting the image to be processed into the trained semantic segmentation model, and outputting a semantic segmentation result, wherein the semantic segmentation model comprises a category-spectrum association module, and the category-spectrum association module is used for reducing the difference between categories by improving the similarity between pixels of the same category so as to obtain a continuous and accurate segmentation result. The invention can better extract complementary information from the multispectral image by improving the similarity between pixels of the same category and reducing the difference between the categories, thereby solving the problem of large difference in the categories. The invention can be widely applied to the technical field of computer vision.","['G06T7/10', 'G06V10/40', 'G06V10/761', 'G06V10/774', 'G06T2207/10024', 'G06T2207/30181']"
CN112070818B,Robot disordered grabbing method and system based on machine vision and storage medium,"The invention discloses a robot disordered grabbing method and system based on machine vision. The method comprises the steps that a kinect camera and a robot grabbing system are built, the kinect camera is used for collecting image data of the surface of a target object, and the image data are preprocessed to obtain three-dimensional point cloud data; carrying out target detection, target segmentation, target clustering, key point feature extraction and feature registration on the acquired three-dimensional point cloud data, identifying a target object and obtaining pose information of the target object; and according to the calibration result of the hand-eye system, performing coordinate conversion on the pose information, and sending a control instruction to the robot to realize the grabbing of the target object. By analyzing the algorithm deficiency in the disordered grabbing process of the robot and improving the existing algorithm, the method adapts to the complex environment where the robot is located, and therefore the disordered grabbing accuracy of the robot and the flexibility of the robot are improved.","['G06T7/50', 'G01C11/02', 'G06F18/23', 'G06N3/045', 'G06T5/70', 'G06T7/11', 'G06T7/33', 'G06T7/85', 'G06V10/757']"
US12205041B2,Training a generative adversarial network for performing semantic segmentation of images,"A method of training a generative adversarial network for performing semantic segmentation of images. The generative adversarial network includes a generator neural network and a critic neural network. The method includes using the generator neural network to generate predicted image segmentation maps from input images, wherein each predicted image segmentation map includes a classification prediction for each of a plurality of pixels of a respective input image, providing predicted image segmentation maps generated by the generator neural network to the critic neural network, training the critic neural network to determine weights for respective pixels of a predicted image segmentation map generated by the generator neural network, wherein the weight for each pixel is used to weight a pixel-wise cross entropy term in an objective function for the critic neural network, and using the weights determined by the critic neural network to train the generator neural network.","['G06F18/2413', 'G06F18/2415', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/084', 'G06N3/088', 'G06N3/09', 'G06N3/094', 'G06T7/11', 'G06V10/26', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06V20/56', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30252']"
US10997779B2,Method of generating an image file of a 3D body model of a user wearing a garment,"A computer-implemented method of generating an image file of a 3D body model of a user wearing a garment, comprising: (i) receiving one or more two dimensional images of a model wearing a garment, which images provide a view of an outer surface of the garment; (ii) for each two dimensional image, segmenting an image of the garment to produce a set of segmented garment images; (iii) using the set of segmented garment images to generate a complete 3D garment model; (iv) receiving a 3D body model of a user; (v) simulating the complete 3D garment model worn on the 3D body model of the user and, (vi) generating an image file of the 3D body model of the user wearing the complete 3D garment model, using the simulated complete 3D garment model worn on the 3D body model of the user.","['G06T19/00', 'G06T15/205', 'G06T15/04', 'G06T17/00', 'G06T19/20', 'G06T7/11', 'G06T7/70', 'G06T2200/24', 'G06T2207/20084', 'G06T2207/30196', 'G06T2210/16', 'G06T2219/2004']"
US11657230B2,Referring image segmentation,"A method, apparatus, and non-transitory computer readable medium for referring image segmentation are described. Embodiments of the method, apparatus, and non-transitory computer readable medium may extract an image feature vector from an input image, extract a plurality of language feature vectors for a referral expression, wherein each of the plurality of language feature vectors comprises a different number of dimensions, combine each of the language feature vectors with the image feature vector using a fusion module to produce a plurality of self-attention vectors, combine the plurality of self-attention vectors to produce a multi-modal feature vector, and decode the multi-modal feature vector to produce an image mask indicating a portion of the input image corresponding to the referral expression.","['G06F40/30', 'G06F16/90332', 'G06F17/16', 'G06F18/25', 'G06F40/169', 'G06F40/20', 'G06T7/10', 'G06V20/70', 'G06T2207/20081', 'G06T2207/20084']"
CN111932529B,"Image classification and segmentation method, device and system","The application provides an image segmentation method, device and system, belongs to the technical field of computers, and relates to artificial intelligence and computer vision technologies. The image segmentation method comprises the following steps: acquiring a target detection object from a target image sequence, and determining a target characteristic vector of the target detection object; respectively aiming at each classification category, determining the probability that the target detection object belongs to each classification category according to the distance between the target feature vector and each category feature vector in the classification category and the weight of each category feature vector; wherein, one classification category comprises a plurality of fine classification categories, and the category feature vector is determined by learning the training feature vector of the training sample under the corresponding fine classification category; taking the classification category with the probability meeting the set condition as the classification category of the target detection object; and carrying out image segmentation on the target image sequence according to the classification type of each target detection object in the target image sequence.","['G06T7/0012', 'G06F18/22', 'G06F18/2415', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30016']"
US20130170749A1,Method and apparatus for document image indexing and retrieval using multi-level document image structure and local features,"An image based document index and retrieval method is described. During document indexing, each source document is analyzed to generate index information at document, page, region and unit levels. Region and unit level index information is generated by segmenting each text region into units, constructing unit length or unit density histograms, and analyzing the units in a few most frequent bins of the histogram. The index information and the source document images are stored in a database. During document retrieval, a target document is analyzed to generate target index information in the same way as during document indexing. The target index information is compared to stored index information in a progressive manner (from higher to lower levels) to identify source documents with index information that matches the target index information. Fuzzy logic is used in the comparison steps to increase the robustness of the document retrieval.","['G06F16/93', 'G06V30/18086', 'G06V30/413', 'G06V30/10']"
CN113065578B,Image visual semantic segmentation method based on double-path region attention coding and decoding,"The invention discloses an image visual semantic segmentation method based on double-path region attention coding and decoding, which comprises the specific steps of obtaining an image sample of a specific scene in advance; normalizing RBG channels of the sample images to prepare a training depth model; coding the image through a double-channel coder to obtain multi-scale and refined image depth characteristics; performing adaptive channel characteristic enhancement on targets with different distributions through region information by using a decoder based on region attention; the shallow features of the encoder and the deep features of the decoder in different extraction stages are fused through skip-connection, and the depth features are multiplexed to the maximum extent; and finally, mapping the final convolution layer of the deep neural network to the original image, and classifying each pixel point to obtain a final image visual segmentation map. The invention can be embedded into equipment such as a monitoring probe and the like and guides the images with complex distribution through the regional information, thereby realizing the accurate visual semantic segmentation of the images.","['G06T7/11', 'G06F18/214', 'G06F18/241', 'G06F18/253', 'G06N3/04', 'G06N3/08', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221']"
US11257272B2,Generating synthetic image data for machine learning,"Described are systems and methods for generating synthetic image data including synthetic images, depth information, and optical flow data. Embodiments of the invention assemble image scenes from virtual objects and capture realistic perspectives of images scenes as synthetic images. Realistic perspectives captured in synthetic images are defined by camera views created from camera settings files. To simulate capture performance of smartphone cameras, stereo cameras, and other actual camera devices, capture, calibration, and camera intrinsic parameters included in camera settings files are identical to parameters included in actual cameras. Synthetic image datasets generated by the systems and methods described herein are used to train machine learning systems generating machine learning models for performing computer vision tasks.","['G06T15/20', 'G06F18/214', 'G06K9/6256', 'G06T1/0014', 'G06T15/005', 'G06T15/04', 'G06T17/20', 'G06T7/50', 'G06T7/75', 'G06T7/85', 'G06V10/764', 'G06V10/82']"
US5531227A,Imaging device and method,"A method and apparatus for obtaining and displaying in real time an image of an object obtained by one modality such that the image corresponds to a line of view established by another modality. In a preferred embodiment, the method comprises the following steps: obtaining a follow image library of the object via a first imaging modality; providing a lead image library obtained via the second imaging modality; referencing the lead image library to the follow image library; obtaining a lead image of the object in real time via the second imaging modality along a lead view; comparing the real time lead image to lead images in the lead image library via digital image analysis to identify a follow image line of view corresponding to the real time lead view; transforming the identified follow image to correspond to the scale, rotation and position of the lead image; and displaying the transformed follow image, the comparing, transforming and displaying steps being performed substantially simultaneously with the step of obtaining the lead image in real time.","['G16H50/50', 'G16H20/40', 'G16H30/20', 'G16H30/40', 'A61B2090/374', 'A61B2090/378', 'A61B90/10', 'A61B90/36']"
US10032287B2,System and method for assessing wound,"The wound assessing method and system of the present teachings provide a convenient, quantitative mechanism for diabetic foot ulcer assessment.","['G06T7/408', 'G06T7/90', 'A61B5/445', 'G06T7/0012', 'G06T7/11', 'G06T7/194', 'G06T2207/10024', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/30088']"
US10983217B2,Method and system for semantic label generation using sparse 3D data,"Methods and apparatuses for generating a frame of semantically labeled 2D data are described. A frame of sparse 3D data is generated from a frame of sparse 3D data. Semantic labels are assigned to the frame of dense 3D data, based on a set of 3D bounding boxes determined for the frame of sparse 3D data. Semantic labels are assigned to a corresponding frame of 2D data based on a mapping between the frame of sparse 3D data and the frame of 2D data. The mapping is used to map a 3D data point in the frame of dense 3D data to a mapped 2D data point in the frame of 2D data. The semantic label assigned to the 3D data point is assigned to the mapped 2D data point. The frame of semantically labeled 2D data, including the assigned semantic labels, is outputted.","['G06T19/00', 'G06N20/00', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/0895', 'G06N3/09', 'G06T11/00', 'G06T7/521', 'G01S17/89', 'G01S17/931', 'G06T2210/12', 'G06T2219/004']"
US10885384B2,Local tone mapping to reduce bit depth of input images to high-level computer vision tasks,Techniques related to computer vision tasks are discussed. Such techniques include applying a pretrained non-linear transform and pretrained details boosting factor to generate an enhanced image from an input image and reducing the bit depth of the enhanced image prior to applying a pretrained computer vision network to perform the computer vision task.,"['G06V10/764', 'G06K9/6256', 'G06F18/214', 'G06F18/24143', 'G06K9/00624', 'G06K9/4628', 'G06K9/54', 'G06K9/6274', 'G06T5/20', 'G06V10/454', 'G06V20/00', 'G06T2207/20048', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20224']"
US10217120B1,Method and system for in-store shopper behavior analysis with multi-modal sensor fusion,"The present invention provides a comprehensive method for automatically and unobtrusively analyzing the in-store behavior of people visiting a physical space using a multi-modal fusion based on multiple types of sensors. The types of sensors employed may include cameras for capturing a plurality of images and mobile signal sensors for capturing a plurality of Wi-Fi signals. The present invention integrates the plurality of input sensor measurements to reliably and persistently track the people's physical attributes and detect the people's interactions with retail elements. The physical and contextual attributes collected from the processed shopper tracks includes the motion dynamics changes triggered by an implicit and explicit interaction to a retail element, comprising the behavior information for the trip of the people. The present invention integrates point-of-sale transaction data with the shopper behavior by finding and associating the transaction data that corresponds to a shopper trajectory and fusing them to generate a complete an intermediate representation of a shopper trip data, called a TripVector. The shopper behavior analyses are carried out based on the extracted TripVector. The analyzed behavior information for the shopper trips yields exemplary behavior analysis comprising map generation as visualization of the behavior, quantitative shopper metric derivation in multiple scales (e.g., store-wide and category-level) including path-to-purchase shopper metrics (e.g., traffic distribution, shopping action distribution, buying action distribution, conversion funnel), category dynamics (e.g., dominant path, category correlation, category sequence). The present invention includes a set of derived methods for different sensor configurations.","['G06Q30/0201', 'G01S5/0252', 'G01S5/0264', 'G01S5/0294', 'G01S5/16', 'G06Q10/06393', 'H04N17/002', 'H04N23/90', 'H04N5/232', 'H04N5/247', 'H04W4/02', 'H04W64/006', 'H04N7/181']"
US11430124B2,Visual object instance segmentation using foreground-specialized model imitation,"A method includes training, using at least one processor, a specialized teacher model to perform visual object instance segmentation in order to segment and classify objects in first training images. The first training images contain foreground objects without backgrounds. The method also includes training, using the at least one processor, a student model to perform visual object instance segmentation in order to segment and classify objects in second training images. The second training images contain the foreground objects and the backgrounds. Training the student model includes using selected outputs of the specialized teacher model. The method further includes deploying the trained student model to perform visual object instance segmentation in an external device.","['G06V10/25', 'G06F18/24', 'G06K9/6267', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/0495', 'G06N3/08', 'G06N3/082', 'G06N3/09', 'G06T7/10', 'G06T7/11', 'G06V10/774', 'G06N3/048', 'G06T2207/20081', 'G06T2207/20084']"
EP3477549A1,Computer vision architecture with machine learned image recognition models,"In an example, a first machine learning algorithm is used to train a smart contour model to identify contours of product shapes in input images and to identify backgrounds in the input images. A second machine learning algorithm is used to train a plurality of shape-specific classification models to output identifications of products in input images. A candidate image of one or more products is obtained. The candidate image is passed to the smart contour model, obtaining output of one or more tags identifying product contours in the candidate image. The candidate image and the one or more tags are passed to an ultra-large scale multi-hierarchy classification system to identify one or more classification models for one or more individual product shapes in the candidate image. The one or more classification models are used to distinguish between one or more products and one or more unknown products in the image.","['G06V10/82', 'G06F18/214', 'G06F18/23', 'G06F18/241', 'G06F18/2414', 'G06F18/2433', 'G06N20/00', 'G06N3/02', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/082', 'G06N3/084', 'G06N3/09', 'G06N3/092', 'G06N3/096', 'G06N3/0985', 'G06N5/047', 'G06T7/60', 'G06T7/75', 'G06V10/255', 'G06V10/26', 'G06V10/454', 'G06V10/752', 'G06V10/764', 'G06V10/94', 'G06V20/52', 'G06V20/647', 'G06Q10/087', 'G06T2200/28', 'G06T2207/30242']"
US8897596B1,System and method for rapid image sequence depth enhancement with translucent elements,"Motion picture scenes to be colorized/depth enhanced (2D→3D) are broken into separate elements, backgrounds/sets or motion/onscreen-action. Background and motion elements are combined into composite frame which becomes a visual reference database that includes data for all frame offsets used later for the computer controlled application of masks within a sequence of frames. Masks are applied to subsequent frames of motion objects based on various differentiating image processing methods, including automated mask fitting/reshaping. Colors and/or depths are automatically applied to masks throughout a scene from the composite background, translucent, motion objects. Areas never exposed by motion or foreground objects in a series of images may be partially or fully realistically drawn or rendered and applied to the occluded areas of the background and then automatically applied throughout the images to generate of minimal artifact or artifact-free secondary viewpoints when translating foreground objects horizontally during 2D→3D conversion.","['H04N13/266', 'H04N13/257']"
CN113487664B,"Three-dimensional scene perception method, three-dimensional scene perception device, electronic equipment, robot and medium","The application discloses a three-dimensional scene perception method, a three-dimensional scene perception device, a robot, electronic equipment and a readable storage medium. The method comprises the steps of respectively carrying out two-dimensional semantic segmentation and monocular depth estimation on two-dimensional image data of RGB-D multi-mode data of a three-dimensional scene to be perceived to obtain two-dimensional semantic features and two-dimensional structural features; respectively carrying out three-dimensional semantic segmentation and three-dimensional scene completion on three-dimensional depth data of the RGB-D multi-mode data to obtain three-dimensional semantic features and three-dimensional structural features; feature fusion is carried out on the two-dimensional semantic features and the three-dimensional semantic features to obtain fusion semantic features, and feature fusion is carried out on the two-dimensional structural features and the three-dimensional structural features to obtain fusion structural features; based on the fusion semantic features and the fusion structural features, three-dimensional semantic scene complement is carried out on the three-dimensional scene to be perceived through a semantic structure parallel interaction iteration fusion mode, so that semantic category information and three-dimensional scene structure information of the three-dimensional scene to be perceived are obtained, and efficient and accurate three-dimensional scene perception is realized.","['G06T7/593', 'G06N3/045', 'G06N3/08', 'G06T2207/10012', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20221', 'Y02T10/40']"
US12293579B2,Systems and methods for analyzing remote sensing imagery,"Disclosed systems and methods relate to remote sensing, deep learning, and object detection. Some embodiments relate to machine learning for object detection, which includes, for example, identifying a class of pixel in a target image and generating a label image based on a parameter set. Other embodiments relate to machine learning for geometry extraction, which includes, for example, determining heights of one or more regions in a target image and determining a geometric object property in a target image. Yet other embodiments relate to machine learning for alignment, which includes, for example, aligning images via direct or indirect estimation of transformation parameters.","['G06V20/176', 'G06F18/2413', 'G06V10/451', 'G06V10/764', 'G06V10/82', 'H04N23/10', 'H04N23/11']"
CN117571721B,Method and device for detecting surface defects of circuit board bonding pad and storage medium,"The invention relates to a method, a device and a storage medium for detecting surface defects of a bonding pad of a circuit board, wherein the method comprises the steps of constructing a bonding pad defect identification model based on a computer vision identification method; acquiring a front image of a circuit board to be tested to obtain an original image to be tested; identifying whether the pad characteristics exist in the original image to be detected or not by using the pad defect identification model; when the pad characteristics are not in the original image to be tested, judging that the circuit board to be tested is unqualified; when the pad characteristics exist, extracting the pad characteristics by using the pad defect identification model to obtain pad characteristic information; calculating the coverage area of each bonding pad by using the bonding pad defect identification model, and correspondingly obtaining coverage area information; and judging whether the circuit board to be tested is qualified or not according to the quantity information, the bonding pad position information and the bonding pad coverage area information. The invention has the advantages of high detection speed and high efficiency, and greatly improves the productivity.","['G01N21/8851', 'G01B11/00', 'G01B11/28', 'G01N21/956', 'G06T7/0004', 'G06T7/0008', 'G06T7/11', 'G06T7/62', 'G06T7/73', 'G06T7/90', 'G01N2021/8887', 'G01N2021/95646', 'Y02P90/30']"
CN103706568B,Based on the robot method for sorting of machine vision,"The invention discloses a kind of robot sorting system based on machine vision and method.This system comprises CCD digital camera, camera lens, light source, six axle articulated robot bodies, electrical control cabinet and vacuum cup; CCD digital camera is connected with industrial computer by switch, and six axle articulated robot bodies are connected with electrical control cabinet, electrical control cabinet access switch, and vacuum cup is rigidly fixed in the end of six axle articulated robot bodies; Camera unit carries out the picture shooting, the data acquisition that are sorted article, and transfers to industrial computer by switch; Industrial computer is carried out image procossing to the item pictures that is sorted collected and after accurately locating, is transmitted control signal by switch to electrical control cabinet; Electrical control cabinet is handled six axle articulated robot bodies according to the control signal received and is performed corresponding sorting operation.The present invention improves operating efficiency, decreases operator in sorting operation, reduces production cost.",[]
US11087125B2,Document authenticity determination,"A computer-implemented method for assessing if characters in a sample image are formed from a predefined font. The method comprises forming a first embedded space representation for the predefined font, extracting sample characters from the sample image, forming a second embedded space presentation of the sample characters, and comparing the first and second embedded space representation to assess if the sample characters are of the predefined font.","['G06V30/413', 'G06K9/00456', 'G06F40/109', 'G06K9/00463', 'G06K9/3233', 'G06K9/6828', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/0895', 'G06N3/09', 'G06V30/147', 'G06V30/245', 'G06V30/414', 'G06K2209/01', 'G06V30/10']"
US12400753B2,Collaborative feature ensembling adaptation for domain adaptation in unsupervised optic disc and cup segmentation,Embodiments of the present disclosure are directed to training a neural network for ocular cup (OC) or ocular disc (OD) detection. One such method comprises initiating training of a first network to learn detection of OC/OD regions within a labeled source sample from a source domain; sharing training weights of the first network with a second network; initiating training of the second network to learn detection of OC/OD regions within an unlabeled sample from a target domain; transferring average training weights of the second network to a third network; initiating training of the third network to learn detection of OC/OD regions within an unlabeled sample from the target domain; computing a mean square error loss between the third network and the second network for a same target sample; and adjusting training weights of the second network based on the mean square error loss computation.,"['G16H30/40', 'A61B3/0025', 'A61B3/12', 'G06F18/2414', 'G06V10/82', 'G06V40/197', 'G16H50/20']"
CN111582021B,Text detection method and device in scene image and computer equipment,"The invention relates to the technical field of image processing, in particular to a text detection method and device of a scene image and computer equipment, wherein the method comprises the following steps: detecting and determining a plurality of text prediction boxes in the scene image through the trained full convolution network model; screening high confidence pixel points in the text prediction box; calculating a minimum circumscribed rectangle corresponding to the text prediction box according to the high-confidence pixel points; when the overlapping degree is larger than a preset overlapping degree threshold value, the width of the text prediction frame is adjusted through the minimum circumscribed rectangle; and cutting in the scene image to obtain a text image to be identified and identifying text information therein. The method provided by the embodiment of the invention can correct and adjust the width of the text prediction frame through the region with high confidence on the basis of realizing text detection by using the EAST method, so that the width of the text prediction frame is reliably reduced, and more accurate text recognition is realized.","['G06V30/414', 'G06N3/045', 'G06N3/08', 'G06V10/225', 'G06V10/267']"
US12292737B2,Applications and skills for an autonomous unmanned aerial vehicle,"Techniques are described for developing and using applications and skills with autonomous vehicles. In some embodiments, a development platform is provided that enables access to a developer console for developing software modules for use with autonomous vehicles. For example, a developer can specify instructions for causing an autonomous vehicle to perform one or more operations. To control the behavior of an autonomous vehicle, the instructions can cause an executing computer system at the autonomous vehicle to generate calls to an application programming interface (API) associated with an autonomous navigation system of autonomous vehicle. Such calls to the API can be configured to adjust parameters of a behavioral objective associated with a trajectory generation process performed by the autonomous navigation system that controls the behavior of the autonomous vehicle. The instructions specified by the developer can be packaged as a software module that can be deployed for use at autonomous vehicle.","['G05D1/0094', 'G05D1/0088', 'B64C39/024', 'B64D47/08', 'B64U20/87', 'G05D1/0016', 'G05D1/223', 'G05D1/46', 'G06F8/20', 'G06F8/30', 'G06F9/547', 'G06N20/00', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/105', 'B64U10/13', 'B64U10/14', 'B64U2101/30', 'B64U2201/00', 'B64U2201/10', 'B64U2201/104', 'B64U2201/20', 'G06Q10/00']"
US20220076133A1,Global federated training for neural networks,"Apparatuses, systems, and techniques to facilitate global semi-supervised training of neural networks to perform image segmentation related to diagnosis and management of emerging diseases, such as COVID-19. In at least one embodiment, distributed client training frameworks train one or more client neural networks to perform image segmentation according to a local training data set as well as global neural network data aggregated, by one or more central servers, from each of one or more globally distributed client neural networks.","['G06N3/088', 'G06N3/08', 'G06N3/084', 'G06N3/044', 'G06N3/0445', 'G06N3/045', 'G06N3/063', 'G16H50/20']"
US20200293828A1,Techniques to train a neural network using transformations,"Apparatuses, systems, and techniques to perform training of neural networks using stacked transformed images. In at least one embodiment, a neural network is trained on stacked transformed images and trained neural network is provided to be used for processing images from an unseen domain distinct from a source domain, wherein stacked transformed images are transformed according to transformation aspects related to domain variations.","['G06T7/11', 'G06K9/6257', 'G06F18/2148', 'G06F18/217', 'G06K9/00208', 'G06K9/00979', 'G06K9/6262', 'G06N20/00', 'G06N3/04', 'G06N3/063', 'G06N3/08', 'G06V10/7715', 'G06V10/774', 'G06V10/95', 'G06V20/647', 'G06V30/19127', 'G06V30/19147', 'G06K2209/05', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30048', 'G06T2207/30081', 'G06V2201/03']"
US20220343178A1,Method and system for performing non-invasive genetic testing using an artificial intelligence (ai) model,"An Artificial Intelligence (AI) based computational system is used to non-invasively estimate the presence of a range of aneuploidies and mosaicism in an image of embryo prior to implantation. Aneuploidies and mosaicism with similar risks of adverse outcomes are grouped and training images are labelled with their group. Separate AI models are trained for each group using the same training dataset and the separate models are then combined, such as by using an Ensemble or Distillation approach to develop a model that can identify a wide range of aneuploidy and mosaicism risks. The AI model for a group is generated by training multiple models including binary models, hierarchical layered models and a multi-class model. In particular the hierarchical layered models are generated by assigning quality labels to images. At each layer the training set is partitioned in the best quality images and other images. The model at that layer is trained on the best quality images, and the other images are passed down to the next layer and the process repeated (so the remaining images are separated into next best quality images and other images). The final model can then be used to non-invasively identify aneuploidy and mosaicism and associated risk of adverse outcomes from an image of an embryo prior to implantation.","['G06T7/0012', 'G06T7/0014', 'G06F18/214', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/126', 'G06N7/01', 'G06V20/69', 'G16H30/40', 'G06T2207/10056', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30044']"
US20220374013A1,Fitness And Sports Applications For An Autonomous Unmanned Aerial Vehicle,"Sports and fitness applications for an autonomous unmanned aerial vehicle (UAV) are described. In an example embodiment, a UAV can be configured to track a human subject using perception inputs from one or more onboard sensors. The perception inputs can be utilized to generate values for various performance metrics associated with the activity of the human subject. In some embodiments, the perception inputs can be utilized to autonomously maneuver the UAV to lead the human subject to satisfy a performance goal. The UAV can also be configured to autonomously capture images of a sporting event and/or make rule determinations while officiating a sporting event.","['G05D1/0094', 'B64C39/024', 'B64U20/87', 'G05D1/0088', 'G06T7/20', 'G06V20/13', 'G06V20/17', 'G06V40/23', 'B64C2201/127', 'B64C2201/141', 'B64U10/14', 'B64U10/25', 'B64U2101/30', 'B64U2201/10', 'G06T2207/30224', 'G06T2207/30228']"
US12387096B2,Image-to-image mapping by iterative de-noising,"A method includes receiving training data comprising a plurality of pairs of images. Each pair comprises a noisy image and a denoised version of the noisy image. The method also includes training a multi-task diffusion model to perform a plurality of image-to-image translation tasks, wherein the training comprises iteratively generating a forward diffusion process by predicting, at each iteration in a sequence of iterations and based on a current noisy estimate of the denoised version of the noisy image, noise data for a next noisy estimate of the denoised version of the noisy image, updating, at each iteration, the current noisy estimate to the next noisy estimate by combining the current noisy estimate with the predicted noise data, and determining a reverse diffusion process by inverting the forward diffusion process to predict the denoised version of the noisy image. The method additionally includes providing the trained diffusion model.","['G06V10/30', 'G06N3/045', 'G06N3/08', 'G06N3/084', 'G06V10/454', 'G06V10/80', 'G06V10/82']"
US9165194B2,Heuristic-based approach for automatic payment gesture classification and detection,"A system and method for automatic classification and detection of a payment gesture are disclosed. The method includes obtaining a video stream from a camera placed above at least one region of interest, the region of interest classifying the payment gesture. A background image is generated from the obtained video stream. Motion is estimated in at least two consecutive frames from the video stream. A representation is created from the background image and the estimated motion occurring within the at least one region of interest. The payment gesture is detected based on the representation.","['G06K9/00771', 'G06K9/00335', 'G06Q20/30', 'G06T7/2006', 'G06T7/215', 'G06V20/52', 'G06V40/20', 'H04N7/18', 'H04N7/181', 'G06T2207/20112', 'G06T2207/30232']"
US12231767B2,Method and system for tuning a camera image signal processor for computer vision tasks,"Image Signal Processing (ISP) optimization framework for computer vision applications is disclosed. The tuning of the ISP is performed automatically and presented as a nonlinear multi-objective optimization problem, followed by solving the problem using an evolutionary stochastic solver. An improved ISP of the embodiments of the invention includes at least features of search space reduction for reducing a number of ISP configurations, remapping the generated population to the reduced search space via mirroring, and global optimization function processing, which allow tuning all the blocks of the ISP at the same time instead of the prior art tuning of each ISP block separately. Also shown that an ISP tuned for image quality performs inferior compared with an ISP trained for a specific downstream image recognition task.","['G06T1/00', 'H04N23/64', 'H04N23/80']"
US7823055B2,"System and method for indexing, searching, identifying, and editing multimedia files","A method and system are provided for tagging, indexing, searching, retrieving, manipulating, and editing video images on a wide area network such as the Internet. A first set of methods is provided for enabling users to add bookmarks to multimedia files, such as movies, and audio files, such as music. The multimedia bookmark facilitates the searching of portions or segments of multimedia files, particularly when used in conjunction with a search engine. Additional methods are provided that reformat a video image for use on a variety of devices that have a wide range of resolutions by selecting some material (in the case of smaller resolutions) or more material (in the case of larger resolutions) from the same multimedia file. Still more methods are provided for interrogating images that contain textual information (in graphical form) so that the text may be copied to a tag or bookmark that can itself be indexed and searched to facilitate later retrieval via a search engine.","['G06Q50/10', 'G11B27/28', 'G06F16/71', 'G06F16/7844', 'G06F16/7847', 'G06F16/7867', 'G06T3/4092', 'G11B27/034', 'G11B27/105', 'G11B2220/20', 'G11B2220/41', 'G11B27/34']"
CN111582054B,Point cloud data processing method and device and obstacle detection method and device,"The embodiment of the invention provides a point cloud data processing method and device and an obstacle detection method and device, and belongs to the technical field of computers. The point cloud data processing method comprises the following steps: three-dimensional reconstruction is carried out on the depth image of the current environment to generate a point cloud data set; dividing the point cloud data set into a plurality of sub point cloud data sets; performing point cloud filtering processing on each of the plurality of sub point cloud data sets in a radius outlier removal manner based on a preset standard outlier removal radius and a standard depth interval length; and executing segmentation processing on each sub-point cloud data set after the point cloud filtering processing is executed to obtain a plurality of clustered point cloud clusters. The point cloud data processing method can improve the point cloud filtering effect based on the removal of the radius outlier, so that the separation processing is more convenient and faster.","['G06V20/58', 'G06T5/20', 'G06T5/40', 'G06T2207/10028', 'G06T2207/30252']"
CN111402248B,Power transmission line wire defect detection method based on machine vision,"The invention discloses a machine vision-based transmission line wire defect detection method, which comprises the following steps: acquiring field image data and manufacturing a training data set; constructing and training an instance segmentation network to obtain a prediction model; the model derives an input picture to obtain a rectangular area image and a binarization mask image of the lead; extracting a wire skeleton by adopting a skeleton algorithm, calculating the average width of the wire, and reconstructing a binarization mask image; adopting homomorphic filtering algorithm to eliminate illumination uneven effect of rectangular area image, and combining reconstructed binarization mask image to extract segmented wire area image; generating a large number of rectangular frames on the wire area for screening; manufacturing a classification training data set, constructing and training a shallow classification network, and obtaining a classification prediction model; and inputting the lead section area picture into a classification prediction model, and counting the defect type and defect proportion of the lead section state. The invention can accurately divide the wire and sectionally detect the state of the wire, and judge the defect type and defect degree of the wire.","['G06T7/0004', 'G01N21/8851', 'G06F18/214', 'G06F18/241', 'G06N3/045', 'G06N3/08', 'G06T7/11', 'G06V10/28', 'G01N2021/8854', 'G01N2021/8874', 'G01N2021/8887', 'G06T2207/20081', 'G06T2207/20084', 'Y04S10/50']"
US8363959B2,Detecting irregularities,"Method and apparatus for inferring irregularities in query data relative to referential data includes attempting to compose the query data, like a puzzle, from large chunks of the referential data, and inferring irregularities in the query data based on at least the size of the matching chunks. The larger the size of a matching chunk, the more likely it is that its corresponding region in the query data is valid and not irregular. Regions in the query data which cannot be composed from the referential data, or can only be composed using small fragmented pieces and not large chunks of the referential data, are considered irregular. The method and apparatus is applicable to all types of signals, including images, video data, medical data, one-dimensional signals and multi-dimensional signals, and can be used to identify, inter alia, suspicious behaviors, suspicious objects, irregular patterns and defects in goods.","['G06T7/0004', 'G06F18/2433', 'G06F18/29', 'G06T7/97', 'G06V10/764', 'G06V10/84', 'G06V40/23', 'G06T2207/20016']"
CA2734143C,Method and apparatus for estimating body shape,"A system and method of estimating the body shape of an individual from input data such as images or range maps. The body may appear in one or more poses captured at different times and a consistent body shape is computed for all poses. The body may appear in minimal tight-fitting clothing or in normal clothing wherein the described method produces an estimate of the body shape under the clothing. Clothed or bare regions of the body are detected via image classification and the fitting method is adapted to treat each region differently. Body shapes are represented parametrically and are matched to other bodies based on shape similarity and other features. Standard measurements are extracted using parametric or non-parametric functions of body shape. The system components support many applications in body scanning, advertising, social networking, collaborative filtering and Internet clothing shopping.","['G06T17/00', 'G06F18/2321', 'G06Q30/0601', 'G06T7/75', 'G06T7/77', 'G06V10/763', 'G06V40/103', 'G06T2207/10028', 'G06T2207/30196']"
CN110276344B,"Image segmentation method, image recognition method and related device","The application discloses an image segmentation method, which comprises the following steps: acquiring an image to be segmented, wherein the image to be segmented comprises a plurality of extreme points; generating image characteristic information according to an image to be segmented, wherein the image characteristic information comprises N image matrixes and a heat map, and the heat map is generated according to a plurality of extreme points; the image segmentation method comprises the steps that an image segmentation area corresponding to image characteristic information is obtained through an image segmentation model, the image segmentation model comprises N matrix input channels and a heat map input channel, the N matrix input channels and the N image matrixes have a one-to-one correspondence, and one heat map input channel and the heat map have a correspondence; and generating an image recognition result of the image to be segmented according to the image segmentation area. The application also discloses an image recognition method and device. The heat map generated by the extreme points is used as a part of the image characteristic information, so that the characteristics of the image are enriched, and a more accurate image segmentation area is generated, and therefore the universality and the applicability of image segmentation are improved.","['G06T7/11', 'G06T7/149', 'G06V10/25', 'G06V10/26', 'G06V10/40', 'G06V10/44', 'G06V10/462', 'G06V10/48', 'G06V10/82', 'G06T2207/20084']"
CA2953394C,System and method for visual event description and event analysis,"A system and method are provided for analyzing a video. The method comprises: sampling the video to generate a plurality of spatio-temporal video volumes; clustering similar ones of the plurality of spatio-temporal video volumes to generate a low-level codebook of video volumes; analyzing the low-level codebook of video volumes to generate a plurality of ensembles of volumes surrounding pixels in the video; and clustering the plurality of ensembles of volumes by determining similarities between the ensembles of volumes, to generate at least one high-level codebook. Multiple high-level codebooks can be generated by repeating steps of the method. The method can further include performing visual event retrieval by using the at least one high- level codebook to make an inference from the video, for example comparing the video to a dataset and retrieving at least one similar video, activity and event labeling, and performing abnormal and normal event detection.","['G06F16/7847', 'G06F16/7328', 'G06F18/22', 'G06F18/231', 'G06F18/2323', 'G06F18/2433', 'G06V10/464', 'G06V10/764', 'G06V20/41', 'G06V20/48', 'G06V20/49']"
US10719939B2,Real-time mobile device capture and generation of AR/VR content,"Various embodiments describe systems and processes for generating AR/VR content. In one aspect, a method for generating a three-dimensional (3D) projection of an object is provided. A sequence of images along a camera translation may be obtained using a single lens camera. Each image contains at least a portion of overlapping subject matter, which includes the object. The object is semantically segmented from the sequence of images using a trained neural network to form a sequence of segmented object images, which are then refined using fine-grained segmentation. On-the-fly interpolation parameters are computed and stereoscopic pairs are generated for points along the camera translation from the refined sequence of segmented object images for displaying the object as a 3D projection in a virtual reality or augmented reality environment. Segmented image indices are then mapped to a rotation range for display in the virtual reality or augmented reality environment.","['G06V20/70', 'G06T7/174', 'G06F16/532', 'G06F16/5838', 'G06F16/738', 'G06F16/783', 'G06F3/011', 'G06K9/00664', 'G06T3/4038', 'G06V20/10', 'H04N13/243', 'H04N13/279', 'H04N13/282', 'H04N23/698', 'H04N5/23238', 'H04N5/265']"
CN107341804B,"Method and device for determining plane in point cloud data, and method and equipment for image superposition","The invention discloses a method and a device for determining a plane in point cloud data, and an image superposition method and equipment, wherein the method for determining the plane in the point cloud data comprises the following steps: dividing the point cloud data into a plurality of subdata sets according to coordinates; determining a local plane for each of the plurality of sub-data sets; and fusing local planes corresponding to all the sub data sets, and determining a global plane in the point cloud data, wherein the global plane comprises at least one plane. The global plane in the point cloud data can be determined without performing multi-plane detection on the point cloud data, so that the time spent in the step of performing multi-plane detection on the point cloud data is saved, and the processing speed for determining the plane from the point cloud data is improved.","['G06T2207/10028', 'G06T2207/20221']"
US12020437B2,Computer-implemented method of analyzing an image to segment article of interest therein,"A computer-implemented method of analyzing an image to segment an article of interest in the image comprises (i) receiving the image having a width of n1 pixels, a height of n2 pixels and a depth of d channels; (ii) processing the image using a machine learning algorithm configured to segment the article of interest, the machine learning algorithm comprising a convolutional neural network including: at least one convolution layer; after said at least one convolution layer, at least one separable convolution module comprising a series of separable convolutions, each separable convolution comprising a depthwise convolution and a pointwise convolution; after said at least one separable convolution module, a pooling module; and a decoder module after the pooling module; and (iii) displaying the image with location of the article of interest being indicated if determined to be present by the machine learning algorithm.","['G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/048', 'G06N3/082', 'G06N3/084', 'G06N3/09', 'G06N3/096', 'G06T7/0004', 'G06T7/11', 'G06T7/50', 'G06T7/62', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30132']"
CN108960266B,Image target detection method and device,"The embodiment of the application discloses an image target detection method and device, wherein the method comprises the following steps: determining image features of a plurality of candidate detection frames in a target image; predicting object classifications to which the images of the candidate detection frames belong based on the determined image features, wherein the object classifications to which the images of the candidate detection frames belong comprise main classifications and corresponding sub classifications; and repeating the filtering processing of the candidate detection frame on the candidate detection frame based on the object classification to which the image of the candidate detection frame belongs and the filtering rules of the main classification and the sub classification to obtain the filtered detection frame. According to the scheme of the embodiment of the application, missing detection or error detection in the target detection process can be reduced or avoided, and the accuracy of image target detection is improved.","['G06F18/2415', 'G06F18/2431']"
CN113470029B,"Training method and device, image processing method, electronic device and storage medium","A training method and device for an image processing network, an image processing method, an electronic device and a storage medium are provided. The training method of the image processing network comprises the following steps: acquiring a first training image, wherein the first training image comprises a target area to be segmented; processing the first training image by using a classification network to obtain a peak response image corresponding to the first training image, wherein the peak response image comprises position information corresponding to the target area; and performing region segmentation training on the segmentation network by using the position information as auxiliary information and using the first training image. The image processing method uses a peak response map with position guidance information as auxiliary information, performs target region segmentation on an input training image, solves the problem of data loss of a medical image (such as a medical image obtained through endoscopy), and improves the generalization property and clinical usability of a network.","['G06T7/0012', 'G06F18/241', 'G06F18/2415', 'G06N3/045', 'G06N3/047', 'G06N3/084', 'G06T7/11', 'G06T2207/10068', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30028', 'G06T2207/30092', 'G06T2207/30096']"
CN110232696B,"Image region segmentation method, model training method and device","The application discloses a method for image region segmentation, which comprises the following steps: acquiring an image to be segmented, wherein the image to be segmented comprises a plurality of extreme points; generating first image feature information according to an image to be segmented, wherein the first image feature information comprises N image matrixes and a first heat map, and the first heat map is generated according to a plurality of extreme points; acquiring a first image segmentation area corresponding to the first image characteristic information through a first image segmentation model; acquiring a second heat map according to the first image segmentation area, wherein the second heat map is generated according to the marking points; and acquiring a second image segmentation area corresponding to the image to be segmented through a second image segmentation model. The application also discloses a model training method and device. According to the method and the device, the region with poor effect in the image segmentation of the first stage is further segmented through the auxiliary segmentation of the second stage, so that a more accurate image segmentation result is obtained, and the image segmentation performance is improved.","['G06T7/11', 'G06T7/12', 'G06T7/149', 'G06T7/187', 'G06V10/26', 'G06V10/454', 'G06V10/469', 'G06V10/774', 'G06V10/82', 'G06V20/20', 'G06T2207/10004', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20101']"
US20220375187A1,"Method of performing object segmentation on video using semantic segmentation model, device and storage medium","A method of performing an object segmentation on a video using a semantic segmentation model, a device, and a storage medium, which relate to a field of artificial intelligence, in particular to computer vision and deep learning technologies. The method includes: sequentially inputting a current video frame and a previous video frame into a first feature extraction network to obtain a feature map sequence; sequentially inputting object segmentation information of the previous video frame into a second feature extraction network to obtain a segmentation feature sequence; sequentially inputting the current video frame and the previous video frame into a temporal encoding network to obtain a temporal feature sequence; generating a fused feature sequence based on the feature map sequence, the segmentation feature sequence and the temporal feature sequence; and inputting the fused feature sequence into a segmentation network to obtain an object segmentation information of the current video frame.","['G06V10/26', 'G06T7/10', 'G06F18/253', 'G06V10/454', 'G06V10/62', 'G06V10/7715', 'G06V10/806', 'G06V10/82', 'G06V10/95', 'G06V20/41', 'G06V20/49', 'G06T2207/10016', 'G06V20/54']"
US10937169B2,Motion-assisted image segmentation and object detection,"Techniques and systems are provided for segmenting one or more frames. For example, image segmentation can be performed on a first frame of a plurality of frames. The image segmentation results in generation of a segmentation mask. Pixels of the first frame can be modified using the segmentation mask. An amount of movement of one or more pixels of a second frame can be determined. The amount of movement can be determined based on one or more motion characteristics of the second frame. It can be determined whether to perform image segmentation using the second frame or a third frame of the plurality of frames based on the amount of movement of the one or more pixels of the second frame.","['G06N3/084', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/0472', 'G06N3/0495', 'G06N3/09', 'G06T7/11', 'G06T7/143', 'G06T7/254', 'G06T2207/20081', 'G06T2207/20084', 'G06T2210/12']"
US10817740B2,Instance segmentation inferred from machine learning model output,"Techniques for using instance segmentation with machine learning (ML) models are discussed herein. An image can be provided as input to a ML model, which can generate, as an output from the ML model, a feature map comprising a plurality of features. Each feature of the plurality of features can comprise a confidence score, classification information, and a region of interest (ROI) determined in accordance with a non-maximal suppression (NMS) technique. Individual ROIs that are similar can be associated together for segmentation purposes. That is, instead of requiring a second ML model and/or a second operation to segment the image (e.g., identify which pixels correspond with the detected object, for example, by outputting a mask or set of lines and/or curves), the techniques discussed herein substantially simultaneously detect an object (e.g., determine an ROI) and segment the image.","['G06K9/3233', 'G06V20/56', 'G05D1/0088', 'G05D1/0221', 'G06F18/2413', 'G06K9/00369', 'G06K9/00805', 'G06T7/10', 'G06V10/764', 'G06V20/58', 'G06V40/103', 'G06T2207/20084', 'G06T2207/30261']"
US9972092B2,Utilizing deep learning for boundary-aware image segmentation,"Systems and methods are disclosed for segmenting a digital image to identify an object portrayed in the digital image from background pixels in the digital image. In particular, in one or more embodiments, the disclosed systems and methods use a first neural network and a second neural network to generate image information used to generate a segmentation mask that corresponds to the object portrayed in the digital image. Specifically, in one or more embodiments, the disclosed systems and methods optimize a fit between a mask boundary of the segmentation mask to edges of the object portrayed in the digital image to accurately segment the object within the digital image.","['G06T7/11', 'G06T7/0081', 'G06K9/66', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N7/01', 'G06T7/12', 'G06T7/13', 'G06V10/28', 'G06V10/454', 'G06V40/103', 'G06T2207/20076', 'G06T2207/20081']"
US12282696B2,Method and system for semantic appearance transfer using splicing ViT features,"Using a pre-trained and fixed Vision Transformer (ViT) model as an external semantic prior, a generator is trained given only a single structure/appearance image pair as input. Given two input images, a source structure image and a target appearance image, a new image is generated by the generator in which the structure of the source image is preserved, while the visual appearance of the target image is transferred in a semantically aware manner, so that objects in the structure image are “painted” with the visual appearance of semantically related objects in the appearance image. A self-supervised, pre-trained ViT model, such as a DINO-VIT model, is leveraged as an external semantic prior, allowing for training of the generator only on a single input image pair, without any additional information (e.g., segmentation/correspondences), and without adversarial training. The method may generate high quality results in high resolution (e.g., HD).","['G06V10/82', 'G06F3/14', 'G06F18/2415', 'G06T7/11', 'G06T7/143', 'G06V10/422', 'G06V10/54', 'G06V10/56', 'G06V10/761', 'G06V20/70', 'G06T2207/20084']"
US11380232B2,"Display screen quality detection method, apparatus, electronic device and storage medium","A display screen quality detection method, an apparatus, an electronic device and a storage medium. The method includes receiving a quality detection request sent by a console deployed on a display screen production line, where the quality detection request includes a display screen image captured by an image capturing device on the display screen production line, performing image preprocessing on the display screen image, and inputting the preprocessed display screen image into a defect detection model to obtain a defect detection result, where the defect detection model is obtained by training with a historical defect display screen image using a deep convolutional neural network structure and an instance segmentation algorithm, determining, according to the defect detection result, quality of a display screen corresponding to the display screen image. The technical solution has high defect detection accuracy, good system performance, and high business expansion capability.","['G01N21/8851', 'G06T7/0008', 'G09G3/006', 'G01N21/956', 'G06N3/045', 'G06Q10/06395', 'G06T7/0002', 'G01N2021/8887', 'G01N2021/9513', 'G09G2320/103', 'G09G2330/10', 'G09G2340/0407', 'G09G2340/045']"
US12107907B2,System and method enabling interactions in virtual environments with virtual presence,"A system enabling interactions in virtual environments comprises one or more cloud server computers comprising at least one processor and memory storing data and instructions implementing a virtual environment platform comprising at least one virtual environment; at least one camera obtaining live data feed from a user of a client device; and a client device communicatively connected to the one or more cloud server computers and at least one camera. The system generates a user graphical representation from the live data feed that is inserted into a selected virtual environment and is therein updated, enabling real-time multi-user collaboration and interactions in the virtual environment. Suitable system architectures and methods thereof are also herein disclosed.","['H04L65/4053', 'G06T19/00', 'G06F3/011', 'G06F9/45545', 'G06F9/5072', 'G06T1/20', 'G06T19/003', 'G06T19/006', 'G06T7/194', 'H04L63/16', 'H04L65/4015', 'H04L67/02', 'H04L67/104', 'H04N7/147', 'H04N7/155', 'H04N7/157', 'G06F2009/45587', 'G06F2009/45595', 'G06F2203/012', 'G09B5/12']"
US12138805B2,Machine learning of grasp poses in a cluttered environment,"Apparatuses, systems, and techniques to grasp objects with a robot. In at least one embodiment, a neural network is trained to determine a grasp pose of an object within a cluttered scene using a point cloud generated by a depth camera.","['B25J9/1671', 'B25J9/161', 'B25J9/1612', 'B25J9/1669', 'B25J9/1676', 'B25J9/1697', 'G06N3/0464', 'G06N3/0475', 'G06N3/08', 'G06N3/09', 'G06T7/55', 'G05B2219/39271', 'G05B2219/39536', 'G05B2219/40317', 'G05B2219/40323', 'G06N3/045', 'G06N3/049', 'G06N3/063', 'G06T2207/10028']"
CN108537292B,"Semantic segmentation network training method, image semantic segmentation method and device","The embodiment of the invention relates to the technical field of computer vision, and provides a semantic segmentation network training method, an image semantic segmentation method and an image semantic segmentation device, wherein the semantic segmentation network training method comprises the following steps: acquiring an image to be trained; inputting an image to be trained into a pre-established semantic segmentation network, and extracting features of the image to be trained by utilizing a front network layer of the semantic segmentation network to obtain a feature map containing block, integral and edge features of the image to be trained; inputting a feature map containing block, whole and edge features of an image to be trained into a rear network layer of a semantic segmentation network to perform image pixel classification, so as to obtain a semantic segmentation map containing segmentation pixel types; and updating parameters of the semantic segmentation network according to the semantic segmentation graph. Compared with the prior art, the method and the device have the advantages that the edge features of the image to be trained are extracted and restored independently, and the training recognition effect of the edge of the segmentation area is improved.","['G06V30/274', 'G06N3/045']"
US8818082B2,Classifying blur state of digital image pixels,"A blur classification module may compute the probability that a given pixel in a digital image was blurred using a given two-dimensional blur kernel, and may store the computed probability in a blur classification probability matrix that stores probability values for all combinations of image pixels and the blur kernels in a set of likely blur kernels. Computing these probabilities may include computing a frequency power spectrum for windows into the digital image and/or for the likely blur kernels. The blur classification module may generate a coherent mapping between pixels of the digital image and respective blur states, and/or may perform a segmentation of the image into blurry and sharp regions, dependent on values stored in the matrix. Input image data may be pre-processed. Blur classification results may be employed in image editing operations to automatically target image subjects or background regions, or to estimate the depth of image elements.","['G06V10/147', 'G06F18/24155', 'G06K9/209', 'G06K9/40', 'G06K9/4647', 'G06K9/6278', 'G06T11/60', 'G06V10/30', 'G06V10/507', 'G06V10/764']"
US10217195B1,Generation of semantic depth of field effect,"Devices and techniques are generally described for image editing. In various examples, color image data and segmentation image data may be identified. In some examples, the segmentation image data may identify a first portion of the color image data as a foreground region and a second portion of the color image data as a background region. A segmentation boundary may be identified between the foreground region and the background region. A first area of the second portion may be identified, where the first area extends from the segmentation boundary to a first area outer boundary. A second area of the second portion may be identified, where the second area extends from the first area outer boundary to a second area outer boundary. A blended and blurred representation of the first area and the second area may be generated.","['G06T5/003', 'G06T5/70', 'G06T7/11', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20012', 'G06T7/194']"
US9412040B2,Method for extracting planes from 3D point cloud sensor data,"A method extracts planes from three-dimensional (3D) points by first partitioning the 3D points into disjoint regions. A graph of nodes and edges is then constructed, wherein the nodes represent the regions and the edges represent neighborhood relationships of the regions. Finally, agglomerative hierarchical clustering is applied to the graph to merge regions belonging to the same plane.","['G06T7/12', 'G06K9/48', 'G06K9/00664', 'G06K9/4638', 'G06K9/469', 'G06T7/0083', 'G06T7/187', 'G06V10/426', 'G06V10/457', 'G06V10/46', 'G06V20/10', 'G06T2207/10028', 'G06T2207/20141']"
CN105956942B,Quality detection method and detection device for steel mesh of power pipe gallery based on machine vision,"The quality detection method and the detection device of the electric power pipe gallery reinforcing mesh based on machine vision comprise the steps of detecting the number of reinforcing steel bars, detecting the spacing between the reinforcing steel bars and detecting the diameter of the reinforcing steel bars; the number of the steel bars is detected by adopting the technologies of image splicing, Hough transformation linear detection and the like to count the number of the steel bars in a meter standard and judge whether the steel bars are qualified or not; the steel bar spacing detection adopts an edge extraction technology, each steel bar is fitted by using a least square method, the distance between adjacent steel bars in a rectangular detection area is calculated, and whether the steel bar spacing of the section is qualified or not is judged. The steel bar diameter detection is similar to the steel bar interval detection, and the difference lies in that the double boundaries of each steel bar are fitted out by using a least square method in a rectangular detection area, the diameter of each steel bar is calculated, and whether the diameter of the steel bar at the section is qualified or not is judged. The detection device comprises a telescopic triangular support, a double-tube guide rail, a stepping motor, a level gauge and a holder for loading the ultrahigh-definition camera. The invention can collect the reinforcing bar net information in the construction project in advance before the reinforced concrete is not poured.",['G06Q50/08']
CN111310624B,"Occlusion recognition method, occlusion recognition device, computer equipment and storage medium","The application relates to a shielding identification method, a shielding identification device, computer equipment and a storage medium. The method comprises the following steps: acquiring an image to be processed including a target part; performing target position positioning processing on the image to be processed to obtain a key point image comprising key points of the target position; carrying out semantic segmentation on the target part of the image to be processed to obtain a segmented image of the target part; superposing the key point image and the segmentation image to obtain a superposed image; positioning the region of the target part in the superimposed image according to the key points in the superimposed image, and determining the pixel ratio of the target part in the region; and carrying out shielding recognition on the target part according to the pixel duty ratio. The method and the device can improve the accuracy of shielding identification.","['G06V40/171', 'G06N3/045', 'G06N3/08', 'G06V10/267']"
CN110490851B,"Mammary gland image segmentation method, device and system based on artificial intelligence","The invention provides a mammary gland image segmentation method, a device and a system based on artificial intelligence. The method comprises the following steps: determining a plurality of cross sections corresponding to mammary tissue in a preset direction, and acquiring to-be-processed mammary images of the cross sections at different time points; acquiring four-dimensional data according to the different time points and image data corresponding to the breast image to be processed, and acquiring a time dynamic image according to the four-dimensional data; and extracting the target region characteristics in the time dynamic image, and acquiring a focus region in the breast tissue according to the target region characteristics. According to the technical scheme, on one hand, the accuracy of the mammary gland image segmentation can be improved; on the other hand, the number of the film reading of doctors can be reduced, the focus information in the mammary gland image can be extracted quickly, and the diagnosis efficiency is improved.","['G06T7/11', 'G06F18/214', 'G06T7/0012', 'G06T7/149', 'G06V10/26', 'G06V10/774', 'G06V20/64', 'G06T2207/10028', 'G06T2207/10088', 'G06T2207/10096', 'G06T2207/10116', 'G06T2207/20081', 'G06T2207/30004', 'G06T2207/30068', 'G06T2207/30096', 'G06V20/49', 'G06V20/695', 'G06V2201/03']"
US11048277B1,Objective-based control of an autonomous unmanned aerial vehicle,"A technique is described for controlling an autonomous vehicle such as an unmanned aerial vehicle (UAV) using objective-based inputs. In an embodiment, the underlying functionality of an autonomous navigation system is via an application programming interface (API). In such an embodiment, the UAV can be controlled trough specifying a behavioral objective, for example, using a call to the API to set parameters for the behavioral objective. The autonomous navigation system can then incorporate perception inputs such as sensor data from sensors mounted to the UAV and the set parameters using a multi-objective motion planning process to generate a proposed trajectory that most closely satisfies the behavioral objective in view of certain constraints. In some embodiments, developers can utilize the API to build customized applications for utilizing the UAV to capture images. Such applications, also referred to as “skills,” can be developed, shared, and executed to control the behavior of an autonomous UAV and to aid in overall system improvement.","['G05D1/12', 'H04N23/90', 'B64C39/024', 'G01C21/20', 'G05D1/0094', 'G05D1/101', 'G06T7/20', 'G06T7/251', 'G06T7/285', 'H04N13/239', 'H04N23/695', 'H04N5/23299', 'H04N5/272', 'B64C2201/127', 'B64C2201/141', 'B64U2201/10', 'B64U2201/104', 'G06T2207/20084', 'G06T2207/30196', 'G06T2207/30241', 'G06T2207/30252', 'G06T2207/30261', 'H04N2013/0081']"
US11950961B2,Automated cardiac function assessment by echocardiography,"A computer vision pipeline is provided for fully automated interpretation of cardiac function, using a combination of machine learning strategies to enable building a scalable analysis pipeline for echocardiogram interpretation. Videos from patients with heart failure can be analyzed and processed as follows: 1) preprocessing of echo studies; 2) convolutional neural networks (CNN) processing for view identification; 3) segmentation of chambers and delineation of cardiac boundaries using CNNs; 4); particle tracking to compute longitudinal strain; and 5) targeted disease detection.","['A61B8/5223', 'A61B5/7267', 'A61B8/065', 'A61B8/461', 'A61B8/485', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/0014', 'G16H30/40', 'G16H50/20', 'G16H50/70', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30048']"
US10002416B2,"Inventory, growth, and risk prediction using image processing","According to examples, inventory, growth, and risk prediction using image processing may include receiving a plurality of images captured by a vehicle during movement of the vehicle along a vehicle path. The images may include a plurality of objects. The images may be pre-processed for feature extraction. A plurality of features of the objects may be extracted from the pre-processed images by using a combination of computer vision techniques. A parameter related to the objects may be determined from the extracted features. A spatial density model may be generated, based on the determined parameter and the extracted features, to provide a visual indication of density of distribution of the objects related to a portion of the images, and/or to provide an alert corresponding to the objects related to the portion of the images.","['G06V20/188', 'G06V20/80', 'G06F18/24', 'G06K9/00577', 'G06K9/0061', 'G06K9/0063', 'G06K9/00791', 'G06K9/4642', 'G06K9/4652', 'G06K9/52', 'G06K9/6267', 'G06T11/206', 'G06T5/00', 'G06T5/70', 'G06T7/0002', 'G06T7/60', 'G06T7/73', 'G06T7/90', 'G06V10/25', 'G06V10/462', 'G06V10/507', 'G06V10/56', 'G06V10/751', 'G06V20/13', 'G06V20/56', 'G06V40/193', 'G06T2207/20024', 'G06T2207/20192', 'G06T2207/30188']"
CN111768425B,"Image processing method, device and equipment","The embodiment of the application discloses an image processing method, device and equipment, and belongs to the technical field of computer vision correlation in artificial intelligence. The method comprises the following steps: performing candidate segmentation identification on an original image by adopting a first segmentation model to obtain a candidate foreground image area and a candidate background image area of the original image; recombining the candidate foreground image area, the candidate background image area and the original image to obtain a recombined image; the pixel points in the recombined image and the pixel points in the original image have one-to-one correspondence; and carrying out region segmentation identification on the recombined image by adopting a second segmentation model to obtain a target foreground image region and a target background image region of the original image. The image segmentation method and device can improve the accuracy of image segmentation.","['G06T7/194', 'G06T5/70', 'G06T7/11', 'G06V10/22', 'G06V10/267', 'G06V10/40', 'G06V10/776', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221']"
CN110866526B,"Image segmentation method, electronic device and computer readable storage medium","The application provides an image segmentation method, electronic equipment and a computer readable storage medium. Relates to the technical field of computer vision and artificial intelligence. The method comprises the following steps: extracting the characteristics of the image through a characteristic extraction network to obtain a characteristic diagram; and carrying out feature map processing on the feature map to complete semantic segmentation of the image. The application can effectively improve the performance of semantic segmentation.","['G06F18/214', 'G06F18/24', 'G06F18/253', 'G06T7/11', 'G06T7/136', 'G06V10/26', 'G06V10/267', 'G06V10/82', 'G06T2207/20084']"
US20220104884A1,Image-Guided Surgery System,"An image-guided surgical system includes a processor, a display communicatively coupled to the processor, and an imaging system communicatively coupled to the processor. A memory device, communicatively coupled to the processor, stores instructions, executable by the processor, to cause the processor to receive, from the imaging system, real-time image data of an ophthalmological surgical field during an ophthalmological surgical procedure, and analyze the image data in real-time to identify an ocular tissue boundary present in the image data of the ophthalmological surgical field. The instructions cause the processor to provide real-time visual, auditory, and/or haptic feedback in response to the identified ocular tissue boundary.","['A61F9/00736', 'A61B1/000094', 'A61B1/000095', 'A61B1/000096', 'A61B34/20', 'A61B34/25', 'A61B34/72', 'A61B34/76', 'A61B90/20', 'G06T7/11', 'G06T7/12', 'A61B2017/00115', 'A61B2017/00119', 'A61B2034/2065', 'A61B2090/061', 'A61B2090/365', 'A61B2090/3735', 'A61B2090/502', 'A61B90/03', 'A61F9/007', 'G06T2207/10021', 'G06T2207/10101', 'G06T2207/20132', 'G06T2207/30041', 'G06T2210/41']"
US12315238B2,Computer-implemented arrangements for processing image having article of interest,"A computer-implemented method for analyzing an image to detect an article of interest (AOI) comprises processing the image using a machine learning algorithm configured to detect the AOI and comprising a convolutional neural network (CNN); and displaying the image with location of the AOI being indicated if determined to be present. The CNN comprises an input module configured to receive the image and comprising at least one convolutional layer, batch normalization and a nonlinear activation function; an encoder thereafter and configured to extract features indicative of a present AOI to form a feature map; a decoder thereafter and configured to discard features from the feature map that are not associated with the present AOI and to revert the feature map to a size matching an initial image size; and a concatenation module configured to link outputs of the input module, the encoder and the decoder for subsequent segmentation.","['G06V10/82', 'G06T7/0002', 'G06T7/001', 'G06V10/255', 'G06V10/26', 'G06V10/771', 'G06V10/7715', 'G06V10/80', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30132']"
WO2023116635A1,Mutual learning-based semi-supervised medical image segmentation method and system,"The present invention relates to the technical field of image segmentation and recognition, and in particular, to a mutual learning-based semi-supervised medical image segmentation method and system. According to the present invention, the method is based on the technical thought of a mutual learning algorithm, and at least two semi-supervised learning models are subjected to dual combination to construct a mutual learning-based deep semi-supervised learning network. By means of alternating mutual supervision of different sub-networks (a student network and a teacher network) in a network in the training process, the category prediction probability output by the different sub-networks is forced to be kept consistent, the accuracy of determining an unlabeled sample image is effectively improved, and the robustness of feature extraction of a labeled sample and an unlabeled sample is improved.","['G06T7/0012', 'G06N3/08', 'G06T7/11', 'G06T2207/20076', 'G06T2207/20081', 'Y02T10/40']"
US9535563B2,Internet appliance system and method,"An Internet appliance, comprising, within a single housing, packet data network interfaces, adapted for communicating with the Internet and a local area network, at least one data interface selected from the group consisting of a universal serial bus, an IEEE-1394 interface, a voice telephony interface, an audio program interface, a video program interface, an audiovisual program interface, a camera interface, a physical security system interface, a wireless networking interface; a device control interface, smart home interface, an environmental sensing interface, and an environmental control interface, and a processor, for controlling a data transfer between the local area network and the Internet, and defining a markup language interface communicated through a packet data network interface, to control a data transfer or control a remote device.","['G06F3/048', 'G05B15/02', 'G06N5/025', 'G06Q30/02', 'G06Q30/0248', 'G06Q30/0255', 'G06Q30/0267', 'G06Q30/0269', 'G06Q30/0273', 'H04N21/4131', 'H04N21/42201', 'H04N21/44222', 'H04N21/44224', 'H04N21/4532', 'H04N21/47', 'H04N5/782', 'H04N5/913', 'G06N20/00', 'H04N2005/91328', 'H04N2005/91364']"
CN106770332B,A kind of electronic die blank defects detection implementation method based on machine vision,"The electronic die blank defects detection implementation method based on machine vision that the invention discloses a kind of, comprising the following steps: (a) detection template is loaded into；(b) detection target transmission；(c) sensor monitors；(d) image-capture；(e) target identification；(f) image procossing；(g) binary conversion treatment；(h) data statistics and detection and analysis.This method uses contactless optical sensor system, human eye is replaced using machine to measure and judge, system captures target measurement subject image using CCD camera, and the information such as the pixel distribution of target measurement subject image and brightness, color are converted into data image signal；Industrial control system carries out operation to these signals, to extract measurement clarification of objective；The cores demand such as on-line checking product defects and dimensional measurement is finally realized, it can be ensured that the reliability under measurement accuracy and speed and industrial environment as a result, to control the device action at scene according to permission and other conditions output.","['G01N21/8851', 'G01N2021/8887']"
US11749124B2,User interaction with an autonomous unmanned aerial vehicle,"A technique for user interaction with an autonomous unmanned aerial vehicle (UAV) is described. In an example embodiment, perception inputs from one or more sensor devices are processed to build a shared virtual environment that is representative of a physical environment. The sensor devices used to generate perception inputs can include image capture devices onboard an autonomous aerial vehicle that is in flight through the physical environment. The shared virtual environment can provide a continually updated representation of the physical environment which is accessible to multiple network-connected devices, including multiple UAVs and multiple mobile computing devices. The shared virtual environment can be used, for example, to display visual augmentations at network-connected user devices and guide autonomous navigation by the UAV.","['G08G5/0069', 'B64C39/024', 'B64U20/87', 'G05D1/0044', 'G05D1/0088', 'G05D1/0094', 'G05D1/106', 'G05D1/227', 'G05D1/606', 'G06T19/003', 'G06T19/006', 'G06T7/70', 'G08G5/0039', 'G08G5/34', 'G08G5/57', 'H04L67/12', 'H04L67/131', 'H04L67/52', 'B64U10/14', 'B64U10/25', 'B64U2101/30', 'B64U2201/10', 'G06T2207/10032', 'G06T2219/024', 'G08G5/32', 'G08G5/53', 'G08G5/55']"
US9390329B2,Method and system for automatically locating static occlusions,"This disclosure provides a method and system to locate/detect static occlusions associated with an image captured scene including a tracked object. According to an exemplary method, static occlusions are automatically located by monitoring the motion of single or multiple objects in a scene over time and with the use of an associated accumulator array.","['G06K9/00624', 'G06T7/77', 'G06K9/46', 'G06K9/52', 'G06T7/20', 'G06K2009/4666', 'G06T2207/10016', 'G06T2207/30236']"
CN111712853B,Processing method and processing device using same,"An input unit (132) inputs an image to be processed. A processing unit (114) executes processing of a convolutional neural network other than the fully connected layer with respect to the image input to the input unit (132). The convolutional neural network in the processing unit (114) includes a convolutional layer and a pooling layer. An output unit (136) outputs the processing result of the processing unit (114). Here, in the convolutional neural network in the processing unit (114), the filter of the convolutional layer is learned for the processing result having the spatial dimension of 1×1.","['G06T7/0004', 'G06T7/0008', 'G06F17/15', 'G06N3/04', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T2207/10048', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30148']"
US11704802B2,Multi-dimensional model merge for style transfer,"Implementations are directed to receiving a target object model representative of a target object, receiving a source object model representative of a source object, defining a set of target segments and a set of source segments using a segmentation machine learning (ML) model, for each target segment and source segment pair in a set of target segment and source segment pairs, generating a compatibility score representing a degree of similarity between a target segment and a source segment, the compatibility score calculated based on global feature representations of each of the target segment and the source segment, each global feature representation determined from a ML model, selecting a source segment for style transfer based on compatibility scores, and merging the source segment into the target object model to replace a respective target segment within the target object model and providing a stylized target object model.","['G06T19/20', 'G06T7/10', 'G06N20/00', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G06N3/08', 'G06T2207/20081', 'G06T2207/20084', 'G06T2219/2021', 'G06T2219/2024']"
CN111639566B,Method and device for extracting form information,"The application provides a method and a device for extracting form information. The method comprises the following steps: acquiring a form image, wherein the form image comprises a form to be processed; determining a text region of a form in the form image and an inclination angle of the text region; performing angle correction on the image in the text region according to the inclination angle to obtain an angle-corrected text image of the form to be identified; performing character grid recognition on the form text image to be recognized, clustering the character grids obtained by recognition and performing curve fitting to obtain a text line area in the form text image to be recognized; and carrying out character recognition on the image in the text line area in the text image of the form to be recognized to obtain the text in the text line area, thereby improving the accuracy of form information extraction.","['G06V30/412', 'G06F40/232', 'G06V10/243', 'G06V20/62', 'G06V30/153', 'G06V30/10', 'G06V30/43']"
US20190250601A1,Aircraft flight user interface,"A graphical user interface (GUI) for controlling the flight of an aircraft such as an unmanned aerial vehicle (UAV) is described. In example embodiments, the GUI includes a view of a physical environment from the perspective of the aircraft as well as various interactive elements through which a user can interact. The interactive elements presented in the GUI and the way in which user interaction with such elements is interpreted can depend on user selection from multiple available control modes. In some embodiments, user interaction with the GUI is interpreted based on a selected control mode and translated into behavioral objectives that are processed, along with other behavioral objectives, by a motion planner to maneuver the aircraft.","['G05D1/0016', 'B64C39/024', 'G05D1/0033', 'G05D1/0038', 'G05D1/101', 'G06F3/0482', 'B64C2201/146', 'B64U2101/30', 'B64U2201/10', 'B64U2201/20', 'G06F3/0488']"
US7121946B2,Real-time head tracking system for computer games and other applications,"A real-time computer vision system tracks the head of a computer user to implement real-time control of games or other applications. The imaging hardware includes a color camera, frame grabber, and processor. The software consists of the low-level image grabbing software and a tracking algorithm. The system tracks objects based on the color, motion and/or shape of the object in the image. A color matching function is used to compute three measures of the target's probable location based on the target color, shape and motion. The method then computes the most probable location of the target using a weighting technique. Once the system is running, a graphical user interface displays the live image from the color camera on the computer screen. The operator can then use the mouse to select a target for tracking. The system will then keep track of the moving target in the scene in real-time.","['A63F13/42', 'A63F13/213', 'G06F18/22', 'G06F3/012', 'G06T11/20', 'G06T7/20', 'G06T7/251', 'G06T7/73', 'G06T7/90', 'G06V40/20', 'A63F2300/1012', 'A63F2300/1087', 'A63F2300/1093', 'A63F2300/69', 'G06T2207/10016', 'G06T2207/30196', 'G06T2207/30201']"
CN113642390B,Street view image semantic segmentation method based on local attention network,"The invention discloses a street view image semantic segmentation method based on a local attention network, which comprises the following specific implementation steps: step 1, firstly, randomly selecting partial image data from a public data set Cityscapes, and dividing the selected partial image data into a training set, a verification set and a test set; step 2, constructing a MobileNet V2 network model by using an inverted residual error module and hole convolution; step 3, designing a local attention module and a residual block, and constructing a coding network; step 4, constructing a decoding network, gradually recovering the image resolution, and finally outputting the semantic segmentation result; and 5, training the model by using the training set and the verification set, and verifying the segmentation effect of the model on the test set. The method solves the problem that the ubiquitous local information in the prior art cannot be completely reserved in the feature extraction process, so that the inconsistent segmentation result in the category is solved.","['G06F18/214', 'G06F18/24', 'G06N3/04', 'G06N3/08']"
CN111915627B,"Semantic segmentation method, network, device and computer storage medium","The invention discloses a semantic segmentation method, a network, a device and a computer storage medium, comprising the following steps: extracting deep features of the input image, and performing coarse segmentation based on the deep features to obtain a coarse segmentation result; based on the deep features, obtaining a region existence prediction result by utilizing a multitask loss function; extracting shallow layer features of an input image, taking a region existence prediction result as input, obtaining a region existence prediction probability mapping result, and extracting local features guided by region existence by combining the shallow layer features and the region existence prediction probability mapping result; carrying out segmentation correction by combining the rough segmentation result and the local characteristics guided by the existence of the region to obtain a segmentation correction result; and calculating to obtain a pixel-level semantic segmentation result based on the segmentation correction result. The problem of accurate semantic segmentation is solved. The method achieves efficient multi-layer feature fusion, reduces calculation cost and dependence on an original rough segmentation result and bilinear interpolation, and achieves efficient and accurate pixel-level semantic segmentation.","['G06T7/11', 'G06F18/24', 'G06N3/045', 'G06V10/44', 'G06T2207/20081', 'G06T2207/20084']"
TWI829116B,"Robotic singulation system, dynamic singulation method and computer program product embodied in a non-transitory computer readable medium","A robotic system comprising an end effector-mounted sensor is disclosed. In various embodiments, a robotic arm is manipulated to move a sensor to a position such that an object of interest is within a read range of the sensor. A sensor data read by the sensor is received via a communication interface. The sensor data is used to determine an attribute of the object; and the determined attribute of the object is used to determine a plan to grasp and move the object.","['B25J19/023', 'B65G47/917', 'B07C1/04', 'B07C5/36', 'B25J19/027', 'B25J9/0093', 'B25J9/1653', 'B25J9/1664', 'G05B19/4183', 'G05B19/41865', 'B07C2501/0063', 'B25J9/1697', 'B65G2203/0216', 'B65G2203/0283', 'B65G2203/041', 'B65G2203/046', 'G05B19/128', 'G05B2219/39102', 'G05B2219/49302']"
US12148123B2,Multi-stage multi-reference bootstrapping for video super-resolution,"An embodiment method includes performing first convolutional filtering on a first tensor constructed using a current frame and reference frames (or digital world reference images) of the current frame in a video, to generate a first estimated image of the current frame having a higher resolution than an image of the current frame. The method also includes performing second convolutional filtering on a second tensor constructed using the first estimated image and estimated reference images of the reference frames, to generate a second estimated image of the current having a higher resolution than the image of the current frame. The estimated reference images of the reference frames are reconstructed high resolution images of the reference images.","['G06T3/4053', 'G06T5/20', 'G06T5/50', 'G06T7/248', 'G06T7/74', 'G06T2207/10016', 'G06T2207/20084']"
US12271208B2,Aerial vehicle touchdown detection,"A technique is introduced for touchdown detection during autonomous landing by an aerial vehicle. In some embodiments, the introduced technique includes processing perception inputs with a dynamics model of the aerial vehicle to estimate the external forces and/or torques acting on the aerial vehicle. The estimated external forces and/or torques are continually monitored while the aerial vehicle is landing to determine when the aerial vehicle is sufficiently supported by a landing surface. In some embodiments, semantic information associated with objects in the environment is utilized to configure parameters associated with the touchdown detection process.","['G05D1/102', 'G05D1/042', 'B64U10/14', 'B64U70/00', 'G05D1/654', 'G06N20/00', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'B64U2101/30', 'B64U2201/10', 'G06N20/10']"
CN114201037B,User authentication system and method using graphical representation-based,"A user authentication system, comprising: one or more cloud server computers comprising at least one processor and memory storing data and instructions, a user database comprising user data associated with a user account and one or more corresponding user graphical representations, and a face scanning and authentication module connected to the database; wherein the one or more cloud server computers are configured to perform the steps of: authenticating a user by performing a facial scan of the user by a facial scan and authentication module, wherein the facial scan includes extracting facial feature data from camera data received from a client device, and checking for a match of the extracted facial feature data with a graphical representation of the user associated with a user account in a user database; providing the user with access to the corresponding user account if a matching user graphical representation is found; and if no matching user graphical representation is found, generating a new user graphical representation from the camera data and a new user account stored in the user database and access to the user account. User authentication methods are also disclosed herein.","['H04L65/4053', 'H04L63/0861', 'G06F21/36', 'G06F21/32', 'G06F21/46', 'G06F3/011', 'G06Q50/01', 'G06T19/003', 'G06T19/006', 'G06V20/20', 'G06V20/64', 'H04L63/083', 'H04L65/4015', 'G06F2203/012', 'G06V40/168', 'G06V40/171', 'G06V40/172', 'G06V40/174', 'G09B5/12', 'H04L2463/082']"
EP4446941A2,Methods and apparatus for discriminative semantic transfer and physics-inspired optimization of features in deep learning,"Methods and apparatus for discrimitive semantic transfer and physics-inspired optimization in deep learning are disclosed. A computation training method for a convolutional neural network (CNN) includes receiving a sequence of training images in the CNN of a first stage to describe objects of a cluttered scene as a semantic segmentation mask. The semantic segmentation mask is received in a semantic segmentation network of a second stage to produce semantic features. Using weights from the first stage as feature extractors and weights from the second stage as classifiers, edges of the cluttered scene are identified using the semantic features.","['G06N3/063', 'G06V10/26', 'G06F18/214', 'G06F18/24143', 'G06N3/04', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06N3/096', 'G06T1/20', 'G06T7/12', 'G06V10/454', 'G06V10/764', 'G06V10/774', 'G06V10/82', 'G06V10/955', 'G06V20/10', 'G06V20/41', 'G06V20/70', 'G06N3/044', 'G06N3/047', 'G06T2207/20081', 'G06T2207/20084']"
WO2020107687A1,"Vision-based working area boundary detection system and method, and machine equipment","Provided are a vision-based working area boundary detection system and method, and a machine equipment. The method, in implementation, comprises: firstly, a constructed neural network model performing autonomous training and learning based on a training data set, and extracting and learning corresponding working area features; and then the neural network model completing training and learning, and performing real-time image semantic segmentation on the acquired video image based on the working area features extracted by the training and learning, thereby perceiving an environment and identifying a boundary of a working area. The method is based on a neural network machine vision technology, the boundary of the working area can be efficiently recognized by extracting and learning the working area features in the earlier stage, and the robustness to a change in environments such as illumination is relatively high.","['G06T7/12', 'G06T7/11', 'G05D1/02', 'G06T7/90', 'G06V10/82', 'G06V20/41', 'G06V20/56', 'G06V20/70', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084']"
CN111814711B,Image feature quick matching method and system applied to mine machine vision,"The invention relates to a rapid image feature matching method and system applied to mine machine vision, belongs to the technical field of mine safety, and solves the problems of low detection efficiency, poor real-time performance and low accuracy of the mine abnormal condition in the prior art. Denoising an image to be analyzed; performing super-pixel segmentation to obtain a plurality of image blocks; calculating the information entropy of each image block to obtain the image blocks with the information entropy larger than a first preset threshold value; extracting characteristic points of the image blocks by using a SURF algorithm, thereby obtaining a characteristic point set of the image to be analyzed; describing the characteristic points in the characteristic point set by adopting a Harr wavelet method to obtain a characteristic point descriptor set of the image to be analyzed; and matching the characteristic points in the characteristic point set of the image to be analyzed with the characteristic points of the target image based on the characteristic point descriptor set of the image to be analyzed so as to confirm whether the mine is abnormal or not. The method can rapidly and accurately detect whether the mine is abnormal, and is favorable for timely processing the mine abnormality.","['G06V20/46', 'G06T7/001', 'G06V10/26', 'G06F18/22', 'G06F18/2321', 'G06V10/30', 'G06V10/443', 'G06V10/56', 'G06V10/751', 'G06V10/761', 'G06V20/52', 'G06T2207/10016', 'G06T2207/20076', 'G06T2207/30108', 'G06T2207/30181']"
US10636206B2,Method and system for generating an image file of a 3D garment model on a 3D body model,"A method of generating an image file of a high resolution 3D garment model on a 3D body model, comprising the steps of: (i) acquiring at least three 2D images of a garment on a mannequin, wherein the at least three 2D images capture a 360° azimuthal angle range of view of the garment, including a texture of the garment; (ii) creating a simplified 3D garment model using the 2D images of the garment; (iii) simulating the smooth and simplified 3D garment model on the 3D body model; (iv) deforming and rendering the high-resolution garment texture on the simulated smooth and simplified 3D garment model on the 3D body model, and (v) generating an image file of the 3D body model, the image file including the deformed and rendered high-resolution garment texture, on the simulated smooth and simplified 3D garment model, on the 3D body model.","['G06T17/20', 'G06F17/5009', 'G06F30/20', 'G06T15/04', 'G06T19/20', 'G06F2113/12', 'G06F2217/32', 'G06T2215/16']"
CN116703883A,Method and device for determining pinhole defects after welding of power battery and electronic equipment,"The application provides a method and a device for determining a welding defect of a power battery after welding and electronic equipment, wherein the method for determining the welding defect of the power battery after welding comprises the following steps: determining an initial weld image of the power battery welding position based on the initial welding image of the power battery welding position, the target welding image of the power battery welding position and a first preset dynamic threshold value; determining whether welding explosion point defects exist at the welding position of the power battery or not based on the initial welding line image, the minimum external rectangular image corresponding to the initial welding line image and a second preset dynamic threshold value; if so, determining a target weld image of the welding part of the power battery based on the explosion point area image corresponding to the welding explosion point defect, the initial weld image and a preset global threshold; and determining whether pinhole defects exist in the target weld image based on the trained defect principal component analysis-classification model. The method can accurately identify all target weld images, and further improve the accuracy of defect identification after welding.","['G06T7/0004', 'G06T7/62', 'G06V10/764', 'G06V10/77', 'Y02E60/10']"
US10726264B2,Object-based localization,"Techniques for localizing a device based on images captured by the device. The techniques include receiving, from a device via a data communication network, image frame data for frames captured by an imaging camera included in the device, the frames including a first frame, automatically detecting real-world objects captured in the image frame data, automatically classifying the detected real-world objects as being associated with respective object classes, automatically identifying object class and instance dependent keypoints for the real-world objects based on the object classes associated with the real-world objects, and estimating a pose of the device for the first frame based on at least the identified object class and instance dependent keypoints.","['G06K9/00664', 'G06V20/20', 'G06F18/2431', 'G06K9/00201', 'G06K9/628', 'G06N20/00', 'G06T7/20', 'G06T7/70', 'G06V20/10', 'G06V20/64', 'G06T2210/12', 'G06V10/757']"
US8126268B2,Edge-guided morphological closing in segmentation of video sequences,"A method of image processing, includes receiving at least one video frame of a video sequence, the at least one video frame including at least one foreground subject and a background, and processing the at least one video frame so as to separate the at least one foreground subject from the background. The processing includes: generating a pixel mask indicating whether a pixel of the at least one video frame belongs to the foreground subject or to the background, applying morphological closing to the pixel mask, wherein the applying morphological closing includes, for each pixel of the pixel mask, conditioning a pixel value in the mask to values of neighboring pixels. The conditioning includes: determining at least edges of the at least one foreground subject in the at least one video frame; and, for the generic pixel under processing, determining the neighboring pixels based on the determined edges.","['G06T7/12', 'G06T7/155', 'G06V10/28', 'G06T2207/10016', 'G06T2207/20081']"
US11928834B2,Systems and methods for generating three-dimensional measurements using endoscopic video data,"Presented herein are systems and methods for performing three-dimensional measurements of a surgical space using two-dimensional endoscopic images. According to an aspect, video data taken from an endoscopic imaging device can be used to generate a three-dimensional model of the surgical space represented by the video data. In one or more examples, two-dimensional images from the video data can be used to generate a three-dimensional model of the surgical space. In one or more examples, the one or more two-dimensional images of the surgical space can include a fiducial marker as part of the image. Using both the depth information and a size reference provided by the fiducial marker, the systems and methods herein can generate a three-dimensional model of the surgical space. The generated three-dimensional model can then be used to perform a variety of three-dimensional measurements in a surgical cavity in an accurate and efficient manner.","['G06T7/60', 'G06T7/62', 'A61B1/000094', 'A61B34/10', 'G01S17/894', 'G06T17/00', 'G06T7/11', 'G06T7/579', 'G06T7/593', 'A61B2034/105', 'G06T2200/24', 'G06T2207/10012', 'G06T2207/10016', 'G06T2207/10068', 'G06T2207/20081', 'G06T2207/30204']"
US11468538B2,Segmentation and prediction of low-level temporal plume patterns,"Computer vision based systems, and methods are provided for generating plume analysis data from a plurality of input image frames as captured by a camera. Image data associated with a plume of gas is received and provided as inputs to a predictive model used to determine plume analysis data including a plume prediction segmentation mask associated with the plume of gas. Other attributes of the plume are also estimated from the prediction segmentation mask such as cross-sectional area, cross-sectional velocity, leak source pixel and volumetric leak rates. The plume analysis data can be provided as an overlay atop the image data. The plume analysis data overlaid atop the image data can be transmitted by and/or to one or more computing devices.","['G06T7/0004', 'G06T1/0014', 'B25J19/023', 'G01M3/38', 'G06T7/12', 'G06T2207/10016', 'G06T2207/10048', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30108']"
US11004202B2,Systems and methods for semantic segmentation of 3D point clouds,"Systems and methods for obtaining 3D point-level segmentation of 3D point clouds in accordance with various embodiments of the invention are disclosed. One embodiment includes: at least one processor, and a memory containing a segmentation pipeline application. In addition, the segmentation pipeline application configures the at least one processor to: pre-process a 3D point cloud to group 3D points; provide the groups of 3D points to a 3D neural network to generate initial label predictions for the groups of 3D points; interpolate label predictions for individual 3D points based upon initial label predictions for at least two neighboring groups of 3D points including the group of 3D points to which a given individual 3D point belongs; refine the label predictions using a graph neural network; and output a segmented 3D point cloud.","['G06V10/764', 'G06K9/00', 'G06N3/044', 'G06N3/045', 'G06N3/0464', 'G06N3/048', 'G06N3/084', 'G06N3/09', 'G06N5/04', 'G06N7/01', 'G06T1/20', 'G06T3/4007', 'G06T7/10', 'G06T7/11', 'G06T7/143', 'G06V10/82', 'G06V20/10', 'G06V20/64', 'G06V20/70', 'G06T2207/10028', 'G06T2207/20081', 'G06T2207/20084', 'G06V2201/12']"
US8723789B1,Two-dimensional method and system enabling three-dimensional user interaction with a device,"User interaction with a display is detected using at least two cameras whose intersecting FOVs define a three-dimensional hover zone within which user interactions can be imaged. Each camera substantially simultaneously acquires from its vantage point two-dimensional images of the user within the hover zone. Separately and collectively the image data is analyzed to identify therein a relatively few landmarks definable on the user. A substantially unambiguous correspondence is established between the same landmark on each acquired image, and as to those landmarks a three-dimensional reconstruction is made in a common coordinate system. This landmark identification and position information can be converted into a command causing the display to respond appropriately to a gesture made by the user. Advantageously size of the hover zone can far exceed size of the display, making the invention usable with smart phones as well as large size entertainment TVs.","['G06F3/017', 'G06F3/011']"
WO2021012757A1,Real-time target detection and tracking method based on panoramic multichannel 4k video images,"A real-time target detection and tracking method based on panoramic multichannel 4k video images, mainly used for solving the problems in the prior art that the processing speed of panoramic multichannel 4k images is slow, a target crossing multichannel cameras is erroneously detected or missed, and the stability of detecting and tracking the target is low. The method comprises: first, performing long-time target probability counting on a panoramic video image, realizing region importance division, and setting a background modeling parameter threshold; next, performing adaptive background modeling on the panoramic video image and obtaining a foreground target candidate region of a scene; then, performing fusing and processing on the foreground target candidate region to form a candidate target plot; finally, performing dynamic flight track management to realize the multi-target stable tracking of a panoramic video. The present method can be used in the fields of the remote monitoring of an airport tower, panoramic video enhancement, road traffic vehicle detection and the like, and is excellent in target detection and tracking performance.","['G06T5/30', 'G06T5/70', 'G06T7/246', 'G06T2207/10016', 'G06T2207/20032', 'G06T2207/30241']"
CN103292701B,The online dimension measurement method of accurate device based on machine vision,"The online dimension measurement method of accurate device based on machine vision, relates to a kind of dimension measurement method based on machine vision, solves the problem that the accuracy requirement of modern industry to workpiece calibration is high.Comprise step: adjustment camera obtains the picture rich in detail of normal component, gathers theastencil region in picture rich in detail simultaneously; The segmentation threshold that iterative threshold segmentation obtains image is carried out to described theastencil region, carries out by theastencil region the bianry image that binaryzation obtains theastencil region; Obtain the bianry image of image to be detected, obtained the target area of image to be detected by stencil matching; Image procossing is carried out in the target area for the treatment of detected image, obtains the edge of device to be detected for dimensional measurement, and the pixel distance between edge calculation; The relation of computed image pixel distance and actual range; Calculate the physical size of device to be detected.The present invention can be widely used in the machine vision dimensional measurement to accurate device.",[]
CN109376611B,Video significance detection method based on 3D convolutional neural network,"The invention relates to a method for detecting the significance of a video image, which is characterized by comprising the following steps: firstly, a 2D deep convolution neural network is established by utilizing 2D convolution, a frame of video frame is input to obtain semantic features of a moving object, then a 3D deep convolution neural network is established by utilizing 3D convolution, three continuous frames of video frames are input to obtain space-time significance features, the semantic features of the moving object and the space-time significance information are connected and then input into a 3D deconvolution network to learn and mix the space-time significance features, and finally a significance map is obtained through the 3D deconvolution network. Thus, we obtain a saliency map of the whole image, and the larger the saliency value is, the more salient the pixel is, i.e. the more attractive the human eye is. The experimental result shows that the video image significance detection model established by the method has excellent detection performance.",['G06V20/46']
RU2426172C1,Method and system for isolating foreground object image proceeding from colour and depth data,FIELD: physics.,"['H04N5/272', 'G06T7/11', 'G06T7/194', 'G06V10/28', 'G06T2207/10028', 'H04N13/239', 'H04N2013/0074']"
US11403495B2,Using synthetic data sets to train a neural network for three-dimensional seismic fault segmentation,"A machine learning system efficiently detects faults from three-dimensional (“3D”) seismic images, in which the fault detection is considered as a binary segmentation problem. Because the distribution of fault and nonfault samples is heavily biased, embodiments of the present disclosure use a balanced loss function to optimize model parameters. Embodiments of the present disclosure train a machine learning system by using a selected number of pairs of 3D synthetic seismic and fault volumes, which may be automatically generated by randomly adding folding, faulting, and noise in the volumes. Although trained by using only synthetic data sets, the machine learning system can accurately detect faults from 3D field seismic volumes that are acquired at totally different surveys.","['G06K9/6262', 'G06N3/08', 'G06F18/214', 'G06F18/217', 'G06K9/6256', 'G06N3/04', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/09', 'G06V10/764', 'G06V10/82']"
US7801896B2,Database access system,"An improved human user computer interface system, wherein a user characteristic or set of characteristics, such as demographic profile or societal “role”, is employed to define a scope or domain of operation. The operation itself may be a database search, to interactively define a taxonomic context for the operation, a business negotiation, or other activity. After retrieval of results, a scoring or ranking may be applied according to user define criteria, which are, for example, commensurate with the relevance to the context, but may be, for example, by date, source, or other secondary criteria. A user profile is preferably stored in a computer accessible form, and may be used to provide a history of use, persistent customization, collaborative filtering and demographic information for the user. Advantageously, user privacy and anonymity is maintained by physical and algorithmic controls over access to the personal profiles, and releasing only aggregate data without personally identifying information or of small groups.","['G06Q30/0275', 'G06F16/2457', 'G06F16/24578', 'G06F16/248', 'G06F16/3323', 'G06F21/6245', 'G06Q30/0212', 'G06Q30/0226', 'G06Q30/0242', 'G06Q30/0247', 'G06Q30/0269', 'G06Q30/0277', 'Y10S707/99932', 'Y10S707/99933']"
CN106599897B,Readings of pointer type meters recognition methods and device based on machine vision,"The present invention provides a kind of versatile readings of pointer type meters recognition methods and device based on machine vision.This method and device acquire the complete dial plate image of pointer instrument to be identified, the pointer linear equation of straight line where establishing pointer；The n subgraph that segmentation obtains is matched with property data base, determines m Matching sub-image picture, and determine the center pixel coordinate of m Matching sub-image picture；When m is not 0, designation number corresponding to the corresponding subgraph of center pixel coordinate closest to the corresponding straight line of pointer linear equation is determined as by the designation number closest to pointer position according to m center pixel coordinate；It determines the pixel coordinate of fixed point to be checked, and the pixel coordinate of fixed point to be checked is converted into real space coordinate；Control image capture device is moved at real space coordinate, acquires the final dial plate image of pointer instrument to be identified；According to final dial plate image, the reading of current pointer instrument to be identified is obtained using Furthest Neighbor.","['G06V10/267', 'G06F18/2411', 'G06V2201/02']"
US11030485B2,"Systems and methods for feature transformation, correction and regeneration for robust sensing, transmission, computer vision, recognition and classification",Embodiments of a deep learning enabled generative sensing and feature regeneration framework which integrates low-end sensors/low quality data with computational intelligence to attain a high recognition accuracy on par with that attained with high-end sensors/high quality data or to optimize a performance measure for a desired task are disclosed.,"['G06K9/6257', 'G06N3/084', 'G06F18/2132', 'G06F18/2134', 'G06F18/2148', 'G06F18/217', 'G06F18/22', 'G06F18/2413', 'G06F18/2431', 'G06F18/251', 'G06K9/6215', 'G06K9/6234', 'G06K9/6262', 'G06K9/628', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T5/002', 'G06T5/70', 'G06V10/454', 'G06V10/764', 'G06V10/7715', 'G06V10/774', 'G06V10/803', 'G06V10/993', 'G06V20/00', 'G06N20/10', 'G06V10/143']"
US8295597B1,Method and system for segmenting people in a physical space based on automatic behavior analysis,"The present invention is a method and system for segmenting a plurality of persons in a physical space based on automatic behavior analysis of the persons in a preferred embodiment. The behavior analysis can comprise a path analysis as one of the characterization methods. The present invention applies segmentation criteria to the output of the video-based behavior analysis and assigns segmentation label to each of the persons during a predefined window of time. In addition to the behavioral characteristics, the present invention can also utilize other types of visual characterization, such as demographic analysis, or additional input sources, such as sales data, to segment the plurality of persons in another exemplary embodiment. The present invention captures a plurality of input images of the persons in the physical space by a plurality of means for capturing images. The present invention processes the plurality of input images in order to understand the behavioral characteristics, such as shopping behavior, of the persons for the segmentation purpose. The processes are based on a novel usage of a plurality of computer vision technologies to analyze the visual characterization of the persons from the plurality of input images. The physical space may be a retail space, and the persons may be customers in the retail space.","['G06T7/20', 'G06V20/41', 'G06V20/52', 'G06V40/23', 'G06T2207/30196', 'G06T2207/30232', 'G06T2207/30241', 'G06V2201/10']"
US12374107B2,Object-aware temperature anomalies monitoring and early warning by combining visual and thermal sensing sensing,"An apparatus including an interface and a processor. The interface may be configured to receive pixel data generated by a capture device and a temperature measurement generated by a thermal sensor. The processor may be configured to receive the pixel data and the temperature measurement from the interface, generate video frames in response to the pixel data, perform computer vision operations on the video frames to detect objects, perform a classification of the objects detected based on characteristics of the objects, detect a temperature anomaly in response to the temperature measurement and the classification, and generate a control signal in response to the temperature anomaly. The control signal may provide a warning based on the temperature anomaly. The classification may provide a normal temperature range for the objects detected.","['G06V20/52', 'G06V20/41', 'G08B21/18', 'G01J5/00', 'G01K13/00', 'G06F1/206', 'G06F18/24', 'G06N3/045', 'G06V10/143', 'G06V10/751', 'G06V10/764', 'G06V20/20', 'G06V20/64', 'G08B17/00', 'G08B17/125', 'G01J2005/0077']"
US7583841B2,Table detection in ink notes,"Computer-readable media having computer-executable instructions and apparatuses detect a table in a handwritten document. Line segments are derived from drawing strokes so that a bounding frame of a candidate table is obtained. An associated table structure is consequently recognized from the bounding frame, lines segments within the bounding frame, and their intersection points. A classifier that reflects at least one table characteristic is determined, and the candidate table is consequently validated or rejected.","['G06V30/412', 'G06V30/347']"
US6975755B1,Image processing method and apparatus,"An apparatus (2) for matching features in images of objects taken from different viewpoints is provided comprising: an image buffer (60) for receiving image data; and output buffer (62) for outputting pairs of matched features and processing means (64–78) for processing received image data to determine matched pairs of features in images. The processing means (64–78) includes a detection module (72) for detecting features at a number of different scales to account for the possibility that a feature in one image may correspond to a larger or smaller feature in another image; a characterization module (74) for generating characterization data for selected features where the characterization data is substantially independent of changes of scale, and the effects of stretch and skew resulting from viewing objects from different viewpoints; and a matching module (76) for outputting as pairs of matched features, features which most closely correspond to each other which are unambiguously better matches than any alternative match between features in different images.","['G06T7/97', 'G06V10/462', 'G06V10/50']"
US11798169B2,Sensor data segmentation,"A system may include one or more processors configured to receive a plurality of images representing an environment. The images may include image data generated by an image capture device. The processors may also be configured to transmit the image data to an image segmentation network configured to segment the images. The processors may also be configured to receive sensor data associated with the environment including sensor data generated by a sensor of a type different than an image capture device. The processors may be configured to associate the sensor data with segmented images to create a training dataset. The processors may be configured to transmit the training dataset to a machine learning network configured to run a sensor data segmentation model, and train the sensor data segmentation model using the training dataset, such that the sensor data segmentation model is configured to segment sensor data.","['G06T7/11', 'G06V10/26', 'G05D1/0253', 'G06N20/00', 'G06V10/774', 'G06V20/56', 'G01S17/02', 'G01S17/931', 'G05D1/0088', 'G05D1/0221', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30252']"
US10853449B1,Report formatting for automated or assisted analysis of medical imaging data and medical diagnosis,"Methods and systems for medical diagnosis by machine learning are disclosed. Imaging data obtained from different medical techniques can be used as a training set for a machine learning method, to allow diagnosis of medical conditions in a faster a more efficient manner. A three-dimensional convolutional neural network can be employed to interpret volumetric data available from multiple scans of a patient.","['G06F19/321', 'G16H50/20', 'G06N20/00', 'G06N3/045', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G16H15/00', 'G16H30/20', 'G16H30/40', 'G06N5/02', 'G06N7/01']"
US6847728B2,Dynamic depth recovery from multiple synchronized video streams,"A method of generating a dynamic depth map for a sequence of images from multiple cameras models a scene as a collection of 3D piecewise planar surface patches induced by color based image segmentation. This representation is continuously estimated using an incremental formulation in which the 3D geometric, motion, and global visibility constraints are enforced over space and time. The proposed algorithm optimizes a cost function that incorporates the spatial color consistency constraint and a smooth scene motion model.",['G06T7/579']
CN110533048B,Realization method and system of combined semantic hierarchical connection model based on panoramic area scene perception,"A realization method and system of a combined semantic hierarchical connection model based on panoramic area scene perception comprises the following steps: the system comprises an ROI extraction module, a panoramic area segmentation module, a spatial information acquisition module and a multi-level modeling module, wherein: the ROI extraction module is connected with the object instance segmentation and transmits object salient region information, the panoramic region segmentation module is connected with the 3D reconstruction of the interest point and transmits region boundary information, the spatial information acquisition module is connected with the semantic subspace model and transmits region position correlation information, and the multi-level modeling module outputs spatial semantics and correlation information of each region. On the basis of panoramic segmentation by acquiring the ROI by utilizing the region saliency, on the premise of extracting interest points to carry out geometric reconstruction and element space semantic information association, the invention realizes multi-level modeling of scene perception according to analysis of scene constituent element probability symbiosis.","['G06N3/045', 'G06V10/255', 'G06V10/507', 'Y02T10/40']"
US10857036B2,Active confocal imaging systems and methods for visual prostheses,"The disclosure features systems and methods for providing information to a user about the user's environment. The systems feature a detection apparatus configured to obtain image information about the environment, where the image information corresponds to information at multiple distances relative to a position of the user within the environment, and an electronic processor configured to obtain focal plane distance information defining a set of one or more distance values relative to the position of the user within the environment, construct one or more confocal images of the environment from the image information and the set of one or more distance values, wherein each of the one or more confocal images corresponds to a different distance value and includes a set of pixels, and transform the one or more confocal images to form one or more representative images having fewer pixels and a lower dynamic range.","['A61F9/08', 'A61H3/061', 'A61N1/0543', 'A61N1/36046', 'A61H2003/063', 'A61H2201/165', 'A61H2201/5038', 'A61H2201/5048', 'A61H2201/5084', 'A61H2201/5092', 'A61H2201/5097']"
CN112945972B,A machine vision-based wire rope condition detection device and method,"A wire rope state detection device and method based on machine vision, the circumferential measuring unit includes the inner gear orbit, driven gear circle, pinion motor, camera cloud terrace, image acquisition camera and light source A; the pinion motor drives the driven gear ring to rotate relative to the inner gear track, and the image acquisition camera and the light source A are fixedly connected to the driven gear ring; the dynamic acquisition mechanism comprises a supporting frame B, a pair of X-axis guide rails arranged in parallel, an X-axis lead screw positioned between the pair of X-axis guide rails, an X-axis motor fixedly installed at the left end part of the supporting frame B, a supporting frame C, a pair of Y-axis guide rails arranged in parallel, a Y-axis lead screw positioned between the pair of Y-axis guide rails, a Y-axis motor fixedly connected to the rear end part of the supporting frame C and an acquisition mounting frame arranged at the front end of the supporting frame C; the method comprises the following steps: collecting an image; pre-treating; splicing; performing semantic segmentation and recognition; identifying a result; uploading and storing. The device and the method can realize comprehensive detection of the state of the steel wire rope.","['G01N21/8806', 'G01B11/08', 'G01N21/8851', 'G01N21/952', 'G01N2021/8835', 'G01N2021/8874', 'G01N2021/8887']"
US11715241B2,Privacy protection in vision systems,A system and method for privacy protection in vision systems that can include collecting image data from an environment; collecting spatial information corresponding to the image data; selecting a sanitization image region in the image data based at least in part on the spatial information; and applying image sanitization to the sanitization image region thereby generating sanitized image data.,"['G06T11/00', 'G06T11/60', 'G06T7/11', 'G06V10/25', 'G06V20/52', 'G06V40/20']"
US9734583B2,Systems and methods for controlling vehicle position and orientation,"Systems and related methods for controlling vehicle position and movement are disclosed. A system includes one or more computers configured to receive data corresponding to video image frames of at least a portion of a course, and analyze the video image frame to estimate at least one of position and orientation of the vehicle within the course. A method includes identifying groups of pixels from the video image frames that are determined to correspond to non-background objects in the course. The method also includes correlating at least a portion of the identified groups of pixels with known objects in the course, and analyzing positions of the groups of pixels within the video image frames and known positions of at least one of an image capture device and known objects relative to the course to estimate at least one of the position and orientation of the vehicle within the course.","['G06T7/0089', 'G01C21/203', 'G05D1/0206', 'G06T7/10', 'G06T7/194', 'G06T7/75', 'G06T2207/10016', 'G06T2207/10021', 'G06T2207/10028', 'G06T2207/30244', 'G06T2207/30252', 'G06T2207/30261']"
US11450075B2,Virtually trying cloths on realistic body model of user,"A method for generating a body model of a person wearing a cloth includes receiving an user input related to a person, wherein the user input comprises at least one image or photograph of the person, wherein at least one image of the person has face of the person, using a human body information to identify requirement of the other body part/s, receiving at least one image or photograph of other human body part/s based on identified requirement, processing the image/s of the person with the image/s or photograph/s of other human body part/s using the human body information to generate a body model of the person, wherein the body model represent the person whose image/photograph is received as user input, and the body model comprises face of the person, receiving an image of a cloth according to shape and size of the body model of the person, and combining the body model of the person and the image of the cloth to show the body model of the human wearing the cloth. The human body information comprises at least one of orientation of face of the person in the image of the person, orientation of body of the person in the image of the person, skin tone of the person, type of body part/s shown in the image of person, location and geometry of one or more body parts in image of the person, body/body parts shape, size of the person, weight of the person, height of the person, facial feature information, or nearby portion of facial features, or combination thereof. The facial feature information comprises at least one of shape or location of at least face, eyes, chin, neck, lips, nose, or ear, or combination thereof.","['G06T19/20', 'G06Q30/0643', 'G06T11/001', 'G06T13/40', 'G06T17/20', 'G06T19/00', 'G06V40/103', 'G06V40/171', 'G06T2210/16']"
WO2021179680A1,Machine vision-based windshield scratch continuous-detection apparatus and detection method,"A machine vision-based windshield scratch continuous-detection apparatus and detection method. The apparatus comprises a conveying device (1), cleaning devices (2), illumination devices (3), and image acquisition and processing devices (4). The conveying device (1) is wholly installed on a support frame (5), the upper and lower parts on the right side of the support frame (5) are dark chambers, and the right side surface of the support frame (5) is hollowed out. A cleaning device (2), illumination devices (3), and an image acquisition and processing device (4) are provided inside each dark chamber. A windshield is fixed by the conveying device (1) and conveyed to the dark chambers, the cleaning devices (2) clean the windshield entering the dark chambers, and the number of the illumination devices (3) used is determined by the size of the windshield. Upper and lower industrial cameras (45) continuously detect the front and back of the windshield, and send captured images to industrial computers to process and analyze the images, so as to finally determine whether there are scratches on windshield surfaces. The apparatus analyzes images on the basis of machine vision, and can adapt to windshields of different sizes, and the number of the illumination devices (3) can be adjusted to save power.","['G01N21/958', 'B08B1/143', 'B08B1/20', 'B08B11/04', 'G01N2021/9586']"
US11249555B2,Systems and methods to detect a user behavior within a vehicle,"Systems, methods and non-transitory computer-readable media for triggering actions based on touch-free gesture detection are disclosed. The disclosed systems may include at least one processor. A processor may be configured to receive image information from an image sensor, detect in the image information a gesture performed by a user, detect a location of the gesture in the image information, access information associated with at least one control boundary, the control boundary relating to a physical dimension of a device in a field of view of the user, or a physical dimension of a body of the user as perceived by the image sensor, and cause an action associated with the detected gesture, the detected gesture location, and a relationship between the detected gesture location and the control boundary.","['G06F3/017', 'B60K35/10', 'G06F3/011', 'G06F3/013', 'G06F3/0304', 'G06F3/0482', 'G06F3/0484', 'G06F3/167', 'B60K2360/1464', 'B60K35/65']"
US5491760A,Method and apparatus for summarizing a document without document image decoding,"A method and apparatus for excerpting and summarizing an undecoded document image, without first converting the document image to optical character codes such as ASCII text, identifies significant words, phrases and graphics in the document image using automatic or interactive morphological image recognition techniques, document summaries or indices are produced based on the identified significant portions of the document image. The disclosed method is particularly adept for improvement of reading machines for the blind.","['G06V30/262', 'G06V30/10']"
CA2903158C,Method and apparatus for learning-enhanced atlas-based auto-segmentation,Disclosed herein are techniques for enhancing the accuracy of atlas-based auto-segmentation (ABAS) using an automated structure classifier that was trained using a machine learning algorithm. Also disclosed is a technique for training the automated structure classifier using atlas data applied to the machine learning algorithm.,"['G06T7/0014', 'G06F18/254', 'G06T7/11', 'G06T7/12', 'G06T7/143', 'G06T7/174', 'G06V10/26', 'G06V10/809', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20128']"
US10699111B2,Page segmentation of vector graphics documents,Disclosed systems and methods generate page segmented documents from unstructured vector graphics documents. The page segmentation application executing on a computing device receives as input an unstructured vector graphics document. The application generates an element proposal for each of many areas on a page of the input document tentatively identified as being page elements. The page segmentation application classifies each of the element proposals into one of a plurality of defined type of categories of page elements. The page segmentation application may further refine at least one of the element proposals and select a final element proposal for each element within the unstructured vector document. One or more of the page segmentation steps may be performed using a neural network.,"['G06K9/00456', 'G06N3/08', 'G06F18/214', 'G06F18/2413', 'G06K9/00463', 'G06K9/3233', 'G06K9/342', 'G06K9/4628', 'G06K9/6256', 'G06K9/627', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/09', 'G06V10/25', 'G06V10/267', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V30/413', 'G06V30/414']"
US11379985B2,System and computer-implemented method for segmenting an image,"A computer-implemented method for segmenting an input image, the method comprises: generating a first segmentation of the input image using a first machine learning system, the first segmentation comprising multiple segments; receiving, from a user, at least one indication, wherein each indication corresponds to a particular segment from the multiple segments, and indicates one or more locations of the input image as belonging to that particular segment; constructing, for each segment of the multiple segments having at least one corresponding indication, a respective geodesic distance map, based on the input image and the user indications received for that segment; and generating a second segmentation using a second machine learning system based on the input image and the constructed geodesic distance maps.","['G06T7/0012', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/0495', 'G06N3/08', 'G06N3/09', 'G06N3/096', 'G06T7/11', 'G16H30/40', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10104', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/30016', 'G06T2207/30096']"
WO2020134255A1,Method for monitoring growth situations of fishes based on machine vision,"A method for monitoring growth situations of fishes based on machine vision, comprising: monitoring the growth situations of fishes in real time by means of the cooperation of an underwater fishing device and the steps of fish recognition and classification, fish length measurement and weight prediction, and accurately mastering the optimal fishing period. According to the method, fishes of specific species and specific sizes can be caught, and the weight of fish can be estimated, the growth situations of fishes can be monitored in real time, and the optimal fishing period can be mastered accurately.","['A01K61/95', 'G06N3/04']"
CN108107051B,Lithium battery defect detection system and method based on machine vision,"The invention provides a lithium battery defect detection method based on machine vision, which comprises the following steps: step S1, collecting images of the side edge of the lithium battery; step S2, processing and analyzing the collected image through visual detection software; and step S3, performing good product and defective product distribution processing on the lithium battery according to the judgment result of the image processing and analysis. According to the invention, a camera is used for shooting side images of the lithium battery after trimming and before glue dripping, whether the side of the lithium battery is damaged or not, liquid leakage, poor film sealing and trimming and other conditions are detected through image analysis, the detection result is fed back to an industrial personal computer in real time, and finally unqualified lithium batteries are picked out.","['G01N21/8851', 'G01N2021/8861', 'G01N2021/8874', 'G01N2021/888', 'G01N2021/8887']"
US10825219B2,Segmentation guided image generation with adversarial networks,"Embodiments provide methods and systems for image generation through use of adversarial networks. An embodiment trains an image generator comprising (i) a generator implemented with a first neural network configured to generate a fake image based on a target segmentation, (ii) a discriminator implemented with a second neural network configured to distinguish a real image from a fake image and output a discrimination result as a function thereof and (iii) a segmentor implemented with a third neural network configured to generate a segmentation from the fake image. The training includes (i) operating the generator to output the fake image to the discriminator and the segmentor and (ii) iteratively operating the generator, discriminator, and segmentor during a training period, whereby the discriminator and generator train in an adversarial relationship with each other and the generator and segmentor train in a collaborative relationship with each other.","['G06V10/82', 'G06F18/217', 'G06K9/6262', 'G06K9/726', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/08', 'G06N3/084', 'G06N3/088', 'G06N3/09', 'G06N3/094', 'G06T11/00', 'G06T11/60', 'G06T7/0002', 'G06T7/11', 'G06V10/7715', 'G06V30/274', 'G06V40/16', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30201', 'G06T2207/30248']"
CN111047548B,"Attitude transformation data processing method and device, computer equipment and storage medium","The application relates to a method and a device for processing attitude transformation data, computer equipment and a storage medium, which relate to an artificial intelligence image processing technology and comprise the following steps: acquiring a source image and a target three-dimensional posture, obtaining a three-dimensional segmentation voxel comprising voxel type information based on semantic segmentation reconstruction, projecting the three-dimensional segmentation voxel to obtain a corresponding target posture two-dimensional segmentation map, and labeling an object in the target posture two-dimensional segmentation map based on the type information to obtain a component type; acquiring a target two-dimensional gesture corresponding to the target three-dimensional gesture, and extracting features of a source image, a target gesture two-dimensional segmentation graph and the target two-dimensional gesture to synthesize a transformation image with an intermediate scale; respectively cutting a source image, a three-dimensional segmentation voxel, a target two-dimensional posture and a transformation image to obtain part layer data of each object part, and respectively carrying out part synthesis on the part layer data of each object part to generate a part image; and fusing the transformation image and the component image to obtain a target attitude image, so that the quality of the attitude transformation image is improved.","['G06T5/50', 'G06N3/045', 'G06T7/10', 'G06T2207/20084', 'G06T2207/20221']"
CN111104962B,"Semantic segmentation method and device for image, electronic equipment and readable storage medium","The invention discloses a semantic segmentation method and device for an image and electronic equipment. The method is implemented by a semantic segmentation model comprising a feature extraction module, a feature aggregation module and a feature fusion module, and comprises the following steps: extracting shallow features and deep features of the target image through a feature extraction module, and constructing a feature pyramid of the target image according to the deep features; the feature pyramid comprises deep features of the corresponding image on different scales; performing aggregation processing on deep features of different scales in a feature pyramid of a target image through a feature aggregation module to obtain an aggregation feature map; and fusing the shallow feature of the target image and the aggregation feature map through the feature fusion module to obtain a fusion feature map so as to obtain a corresponding semantic segmentation result according to the fusion feature map.","['G06F18/253', 'G06V10/26', 'G06V10/40']"
US12412209B2,Image processing arrangements,"Aspects of the detailed technologies concern training and use of neural networks for fine-grained classification of large numbers of items, e.g., as may be encountered in a supermarket. Mitigating false positive errors is an exemplary area of emphasis. Novel network topologies are also detailed—some employing recognition technologies in addition to neural networks. A great number of other features and arrangements are also detailed.","['G06Q30/0641', 'G06F18/214', 'G06F18/22', 'G06F18/2431', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G06N3/094', 'G06N3/096', 'G06V10/17', 'G06V10/462', 'G06V10/764', 'G06V10/774', 'G06V10/776', 'G06V10/82', 'G06V20/00', 'G06F16/906', 'G06N3/04', 'G06N3/08', 'G06V2201/09']"
US20210035313A1,Real-time concealed object tracking,"A computing system responsive to obtaining original image data, detects a set of data point(s), in the original image data, that indicates an object. The system determines, based on the set of data point(s), a set of pixels associated with the object in the original image data. The system generates an alternative visual identifier for the object that provides a unique identifier for the set of pixels absent in the original image data. The system generates, autonomously from intervention by any user of the computing system, pixel information to conceal feature(s) of the object. The system obtains modified image data comprising the alternative visual identifier. The modified image data further comprises the feature(s) of the object in the original image data visually concealed in the modified image data according to the pixel information. The system outputs an image representation of a trajectory of the object through the modified image data.","['G06T7/246', 'G05B13/048', 'G06F11/0751', 'G06F11/2025', 'G06F11/203', 'G06F11/2035', 'G06F11/2048', 'G06F11/3013', 'G06F11/3409', 'G06F11/3466', 'G06T11/60', 'G06T7/0004', 'G06T7/277', 'G06T7/292', 'G06V10/25', 'G06V20/20', 'G06V20/52', 'G06F2201/805', 'G06F2201/86', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20101', 'G06T2207/30241', 'G06V2201/06', 'G06V2201/07', 'G06V40/25']"
US10861159B2,"Method, system and computer program product for automatically altering a video stream","There is provided a method for automatically altering a digital video stream including multiple video input frames, to automatically obtain output frames with a target composition, in which frame metadata relating to objects in one or more of the video input frames is analyzed on a frame-by-frame basis and used by a processor to automatically alter one or more output frames to be more similar to, or to match, the target composition, wherein cropping is performed in 3D. A related system and a related computer program product are also provided.","['G06T11/00', 'G06K9/00201', 'G06K9/00711', 'G06K9/2081', 'G06T7/11', 'G06T7/593', 'G06T7/73', 'G06V20/40', 'G06V20/64', 'G06K2209/21', 'G06T2200/04', 'G06T2200/24', 'G06T2207/10021', 'G06T2207/10028', 'G06T2207/20132', 'G06V2201/07']"
US10593040B2,Methods for screening and diagnosing a skin condition,"Provided herein are digital-implemented methods for performing simultaneous analyses on an object on the skin of an animal body, for example, a human, to classify the object as a skin cancer, an ulcer or neither. The analyses are performed simultaneously on a hand-held imaging device.","['G06T7/0012', 'A61B5/0077', 'A61B5/444', 'G06K2209/05', 'G06K2209/053', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/30088', 'G06V2201/03', 'G06V2201/032']"
US10830755B2,Board lumber grading using deep learning semantic segmentation techniques,"A method of board lumber grading is performed in an industrial environment on a machine learning framework configured as an interface to a machine learning-based deep convolutional network that is trained end-to-end, pixels-to-pixels on semantic segmentation. The method uses deep learning techniques that are applied to semantic segmentation to delineate board lumber characteristics, including their sizes and boundaries.","['G01N33/46', 'G06F15/76', 'G06K9/3233', 'G06K9/6202', 'G06N20/00', 'G06N7/005', 'G06N7/01', 'G06T7/0004', 'G06N3/0418', 'G06T2207/30161']"
CN114648638B,"Training method of semantic segmentation model, semantic segmentation method and device","The disclosure provides a training method, a semantic segmentation device, electronic equipment and a medium for a semantic segmentation model, relates to the field of image processing and artificial intelligence, and particularly relates to computer vision, semantic segmentation and knowledge distillation. The training method of the semantic segmentation model comprises the following steps: determining at least one sample data, each sample data comprising a sample image, a true semantic segmentation mask, and a true semantic type; for each sample data: inputting the sample image into a first network to obtain a first image feature and a first prediction semantic segmentation mask; inputting the real semantic segmentation mask and the real semantic type into a second network to obtain a second image feature and a second prediction semantic segmentation mask; inputting the first image feature to a third network to obtain a first predicted semantic type, and inputting the second image feature to the third network to obtain a second predicted semantic type; determining a loss function; and adjusting a parameter of the first network based on the loss function.","['G06F18/214', 'G06F18/217', 'G06N3/042', 'G06N3/045', 'G06N3/08']"
CN111462135B,Semantic mapping method based on visual SLAM and two-dimensional semantic segmentation,"The invention relates to the field of cross fusion of computer vision and deep learning, in particular to a semantic mapping method based on vision SLAM and two-dimensional semantic segmentation. The method of the invention comprises the following steps: s1, calibrating camera parameters and correcting camera distortion; s2, acquiring an image frame sequence; s3, preprocessing an image; s4, judging whether the current image frame is a key frame, if so, turning to a step S6, and if not, turning to a step S5; s5, dynamic fuzzy compensation; s6, semantic segmentation, namely extracting ORB feature points for the image frames, and performing semantic segmentation by using a mask region convolution neural network algorithm model; s7, pose calculation, namely calculating the pose of the camera by using a sparse SLAM algorithm model; s8, establishing a dense semantic map with the assistance of semantic information, and realizing three-dimensional semantic map establishment of the global point cloud map. The method and the system can improve the performance of the unmanned aerial vehicle semantic mapping system, and remarkably improve the robustness of feature point extraction and matching aiming at a dynamic scene.","['G06T7/11', 'G06N3/045', 'G06N3/08', 'G06T7/50', 'G06T7/80', 'G06T7/90', 'G06T2207/10024', 'G06T2207/20084', 'Y02T10/40']"
EP3818503A1,Providing recording guidance in generating a multi-view interactive digital media representation,"Various embodiments of the present invention relate generally to systems and methods for collecting, analyzing, and manipulating images and video. According to particular embodiments, live images captured by a camera on a mobile device may be analyzed as the mobile device moves along a path. The live images may be compared with a target view. A visual indicator may be provided to guide the alteration of the positioning of the mobile device to more closely align with the target view.","['G06V40/161', 'G06F18/23', 'G06F18/25', 'G06V10/17', 'G06V10/242', 'G06V10/25', 'G06V10/74', 'G06V20/20', 'G06V20/46']"
CN107176432B,A kind of anchor pole foreign matter and belt tearing detection system based on machine vision,"The invention discloses a kind of anchor pole foreign matter and belt tearing detection system based on machine vision, including belt conveyor, be mounted on above belt conveyor for industrial camera, transmission of video images module and the belt conveyor foreign matter and belt tearing monitoring platform to acquire video image of the belt conveyor respectively under unloaded and loading condition, which carries out image filtering and image enhancement pretreatment using bilateral filtering and histogram equalization respectively；Image binaryzation processing is carried out using iteration self-adapting thresholding method；The detection and identification of target are completed based on connected component.The present invention has merged machine vision technique, computer technology, image procossing and identification technology, anchor pole foreign matter and the belt tearing detection that can be automatically performed in belt conveyor video surveillance image, it provides the necessary technical support for the safety monitoring of Belt Conveyors Underground Coal Mine, for ensureing that belt conveyor safe operation has great significance.","['B65G43/02', 'B65G2203/0275', 'B65G2203/044', 'B65G2207/40']"
CN113111703B,Foreign object detection method for airport pavement defects based on fusion of multiple convolutional neural networks,"The invention provides a method for detecting airport pavement diseases and foreign matters based on fusion of a plurality of convolutional neural networks, which comprises the following steps: collecting images of airport pavement diseases and foreign matters; constructing an airport pavement disease foreign matter database for training a neural network; constructing a target detection algorithm YOLOv3 and Mask R-CNN convolutional neural network; adjusting the super parameters of the convolutional neural network until convergence and the error loss value meets the requirement, and storing the network weight parameters at the moment to finish training of the YOLOv3 and Mask R-CNN convolutional neural network; fusing the trained YOLOv3 and Mask R-CNN convolutional neural networks to construct an intelligent segmentation model of airport pavement diseases and foreign object pixel levels; inputting the test image into a stored model, and outputting the segmentation result of diseases and foreign matters on the airport pavement; and (5) counting pixels of the image corresponding to the mask of the segmentation result, and outputting semantic information of the airport pavement diseases and the foreign matters. The invention has better robustness and generalization capability, and can improve the segmentation precision and efficiency of airport pavement disease and foreign matters.","['G06V20/00', 'G06F18/23213', 'G06F18/253', 'G06N3/045', 'G06N3/084', 'G06V10/25', 'G06V10/267', 'Y02T10/40']"
CN110378348B,"Video instance segmentation method, apparatus and computer-readable storage medium","The invention provides a video instance segmentation method, video instance segmentation equipment and a computer-readable storage medium. The method comprises the following steps: obtaining a main feature map and a multi-level feature map of each frame in a video; inputting the main feature graphs of adjacent frames in the video into an optical flow estimation sub-network to obtain optical flow information between the adjacent frames; obtaining a shared feature map of an adjacent frame according to the optical flow information, and fusing the shared feature map with a multi-level feature map of a next frame in the adjacent frame to generate a semantic feature map; inputting the multi-level feature map of the next frame into a target detection sub-network to obtain target detection information; the target detection information comprises a target type and a target area position; and inputting the target detection information and the semantic feature map into the example segmentation sub-network to obtain an example segmentation result. When the video example is segmented, the embodiment of the invention can better ensure the accuracy of the segmentation result, and can achieve better example segmentation effect even if the video example is influenced by appearance deterioration.","['G06F18/253', 'G06V10/267', 'G06V2201/07']"
US10239521B1,Multi-network-based path generation for vehicle parking,"Systems and methods of deep neural network based parking assistance is provided. A system can receive data sensed by one or more sensors mounted on a vehicle located at a parking zone. The system generates, from a first neural network, a digital map based on the data sensed by the one or more sensors. The system generates, from a second neural network, a first path based on the three-dimensional dynamic map. The system receives vehicle dynamics information from a second one or more sensors located on the vehicle. The system generates, with a third neural network, a second path to park the vehicle based on the first path, vehicle dynamics information and at least one historical path stored in vehicle memory. The system provides commands to control the vehicle to follow the second path to park the vehicle in the parking zone.","['G06V10/82', 'B60W30/06', 'B60W30/00', 'B62D15/02', 'B62D15/0285', 'G01S13/865', 'G01S13/867', 'G01S13/89', 'G01S13/931', 'G01S17/89', 'G01S17/931', 'G01S7/417', 'G05D1/0088', 'G05D1/0221', 'G05D1/0274', 'G05D1/0278', 'G06F18/24', 'G06K9/00', 'G06K9/00805', 'G06N3/00', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06V10/764', 'G06V20/58', 'G06V20/586', 'G08G1/0112', 'G08G1/0133', 'G08G1/0141', 'G08G1/14', 'G08G1/143', 'G08G1/146', 'B60W2420/403', 'B60W2420/408', 'B60W2420/54', 'B60W2530/14', 'B60W2550/10', 'B60W2552/15', 'B60W2552/25', 'B60W2552/35', 'B60W2552/40', 'B60W2552/53', 'B60W2554/00', 'B60W2554/20', 'B60W2554/4026', 'B60W2554/4029', 'B60W2554/60', 'B60W2554/80', 'B60W2555/20', 'B60W2556/10', 'G05D1/0248', 'G05D2201/0213']"
CN114463205A,Vehicle target segmentation method based on double-branch Unet noise suppression,"The invention discloses a vehicle target segmentation method based on double-branch UNet noise suppression, which comprises the steps of firstly collecting vehicle data sets and sorting the data sets; then selecting a part of UNet networks as a backbone network, and embedding a prediction branch module and a noise suppression branch module on the basis, wherein the prediction branch module mainly carries out fine adjustment on the acquired characteristic information and carries out pixel classification according to the fine adjustment; the noise suppression branch module mainly suppresses noise interference in data through a loss function so as to achieve accuracy of feature acquisition; and finally, importing the obtained vehicle data set into a model, and then transmitting the image characteristic information extracted from the main network to a prediction branch module and a noise suppression branch module, wherein the two branch modules respectively use a binary cross entropy loss function and an asymmetric exponential loss function to alternately optimize model parameters, so that the discrimination capability of the model on difficult samples is improved, and the overall performance of the model is further improved.","['G06T5/70', 'G06F18/214', 'G06N3/045', 'G06T7/10', 'G06T2207/20081', 'G06T2207/20084']"
US11361528B2,Systems and methods for stamp detection and classification,"In some aspects, the disclosure is directed to methods and systems for detection and classification of stamps in documents. The system can receive image data and textual data of a document. The system can pre-process and filter that data, and covert the textual data to a term frequency inverse document frequency (TF-IDF) vector. The system can detect the presence of a stamp on the document. The system can extract a subset of the image data including the stamp. The system can extract text from the subset of the image data. The system can classify the stamp using the extracted text, the image data, and the TF-IDF vector. The system can store the classification in a database.","['G06V10/25', 'G06V10/82', 'G06F18/24', 'G06K9/6267', 'G06N20/20', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/088', 'G06N3/09', 'G06N3/096', 'G06N3/0985', 'G06V10/817', 'G06V30/153', 'G06V30/41', 'G06V30/10']"
US11481862B2,"System and method for real-time, simultaneous object detection and semantic segmentation","System and method for simultaneous object detection and semantic segmentation. The system includes a computing device. The computing device has a processor and a non-volatile memory storing computer executable code. The computer executable code, when executed at the processor, is configured to: receive an image of a scene; process the image using a neural network backbone to obtain a feature map; process the feature map using an object detection module to obtain object detection result of the image; and process the feature map using a semantic segmentation module to obtain semantic segmentation result of the image. The object detection module and the semantic segmentation module are trained using a same loss function comprising an object detection component and a semantic segmentation component.","['G06T7/73', 'G06F18/2413', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/09', 'G06T1/20', 'G06T7/11', 'G06V10/451', 'G06V10/764', 'G06V10/82', 'G06V20/00', 'G06V30/194', 'G06N3/08', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30252']"
CN116229079B,A 3D point cloud semantic segmentation method and system based on visual assistance and feature enhancement,"The invention belongs to the field of computer vision graphics, and particularly relates to a three-dimensional point cloud semantic segmentation method and system based on vision assistance and feature enhancement, wherein the method comprises the steps of constructing a three-dimensional point cloud semantic segmentation depth learning model and training, inputting three-dimensional point cloud data to be segmented into the trained point cloud semantic segmentation model, explicitly extracting vision color features by designing a reconstruction assistance network, introducing a channel attention mechanism into a main segmentation network to be fully utilized, and constructing a point feature enhancement module in a decoding layer to further improve the segmentation capability of points of the model at boundaries of different semantic types; the method can effectively aggregate the local neighborhood of the points, improves the effect of the deep learning model on the semantic segmentation of the three-dimensional point cloud, and promotes the development of the related technical field.","['G06V10/26', 'G06N3/084', 'G06V10/56', 'G06V10/806', 'G06V10/82', 'Y02T10/40']"
US11062453B2,Method and system for scene parsing and storage medium,"A method for scene parsing includes: performing a convolution operation on a to-be-parsed image by using a deep neural network to obtain a first feature map, the first feature map including features of at least one pixel in the image; performing a pooling operation on the first feature map to obtain at least one second feature map, a size of the second feature map being less than that of the first feature map; and performing scene parsing on the image according to the first feature map and the at least one second feature map to obtain a scene parsing result of the image, the scene parsing result including a category of the at least one pixel in the image. A system for scene parsing and a non-transitory computer-readable storage medium can facilitate realizing the method.","['G06T7/11', 'G06K9/00', 'G06K9/00664', 'G06K9/4604', 'G06K9/4628', 'G06K9/66', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/084', 'G06N3/09', 'G06T7/187', 'G06V10/454', 'G06V10/82', 'G06V20/10', 'G06T2207/20084']"
US20220375165A1,"Method, device, and storage medium for segmenting three-dimensional object","The present disclosure describes a three-dimensional object segmentation method and device, and medium, and relates to the field of computer vision (CV) technologies of an artificial intelligence (AI) technology. The method includes obtaining, by a device, a point cloud of a three-dimensional object. The device includes a memory storing instructions and a processor in communication with the memory. The method includes determining, by the device, three scaling directions of the point cloud; scaling, by the device, positions of points in the point cloud along the three scaling directions to obtain a scaled point cloud, so that the scaled point cloud has the same scale in the three scaling directions; and segmenting, by the device, the three-dimensional object based on the scaled point cloud to obtain a segmentation result.","['G06T7/10', 'G06T7/12', 'G06T17/20', 'G06T17/00', 'G06T3/40', 'G06T2200/04', 'G06T2207/10028']"
US11596482B2,System and method for surgical performance tracking and measurement,"Computer implemented methods and systems are provided for training a machine learning architecture for surgical performance tracking and measurement based on surgical procedure video data set. The methods and systems include, in a first aspect, a sequential relation architecture and a dimensionality reduction architecture. In a second aspect, the methods and systems include a surgical instrument instance segmentation architecture, a decomposition model, and a sequential relation architecture. The video data is processed on a frame level to generate compressed or reduced representations of the video data.","['G06N3/08', 'A61B34/20', 'A61B34/25', 'G06F18/214', 'G06K9/6256', 'G06N20/00', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/0495', 'G06N3/063', 'G06N3/0895', 'G06N3/09', 'G06N3/096', 'G06V10/764', 'G06V10/82', 'G06V20/46', 'H04N19/54', 'A61B2034/102', 'A61B90/361']"
CN113902761B,Knowledge distillation-based unsupervised segmentation method for lung disease focus,"The invention discloses a knowledge distillation-based unsupervised segmentation method for lung disease focus, and belongs to the field of medical image processing and computer vision. The invention firstly builds and trains a self-encoder to obtain a pre-training teacher network with abundant CT image semantic knowledge, then only distills the knowledge of normal CT images from the pre-training teacher network to train a student network with the same architecture as the teacher network, and finally utilizes the teacher and the student network to segment focus according to the difference of the characteristics extracted by focus images. Meanwhile, in addition to the conventional pixel-level distillation, the method also designs affinity-level distillation considering the relationship between pixels so as to sufficiently distill effective knowledge. Experiments prove that the invention can effectively improve the focus segmentation precision on different data sets. The method is easy to construct, the unmarked lung disease focus segmentation result can be obtained only by means of normal data, and generalization and operation efficiency are relatively considerable.","['G06T7/11', 'G06N5/02', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20221', 'G06T2207/30061', 'G06T2207/30096']"
CN109948714B,Chinese scene text line identification method based on residual convolution and recurrent neural network,"The invention discloses a Chinese scene text line identification method based on residual convolution and a recurrent neural network, which comprises the following steps: collecting a Chinese scene text training image, performing normalization processing on the size of the training image, performing data augmentation processing on the training image, designing a residual convolution neural network, a residual recursive neural network and a CTC (central processing unit) model, training horizontal text lines and vertical text lines, and selecting a result with higher confidence as an identification result; the invention solves the problem of Chinese scene text line identification by combining the convolutional neural network and the recursive neural network, avoids the error identification caused by character segmentation and error segmentation of the text line, can accelerate the training of the model by adding residual connection in the convolutional neural network and the recursive neural network, thereby obtaining a practical Chinese scene text identification model, has strong robustness, and can identify the Chinese text lines with complex background, complex illumination and various fonts.",[]
US12277615B2,"Detecting and validating improper residency status through data mining, natural language processing, and machine learning","Described are medias, systems, and computer-implemented methods to detect improper residency status by performing a data mining task to data source to detect one or more improper residency indicia, applying a machine learning algorithm to identify an initial candidate, calculating a probability that the initial candidate has an improper residency status, and validating the detection. Further described are medias, systems, and computer-implemented methods to detect improper occupancy tax status.","['G06Q50/16', 'G06F16/2465', 'G06F16/29', 'G06F40/30', 'G06N20/00', 'G06N20/20', 'G06N3/044', 'G06N3/0442', 'G06N3/0464', 'G06N3/08', 'G06N3/0895', 'G06N3/09', 'G06N5/048', 'G06Q50/163', 'G06Q50/26', 'H04L67/02', 'H04L67/04', 'H04L67/1097', 'H04L67/125', 'G06F2216/03', 'G06F40/268', 'G06F40/279', 'G06N20/10', 'G06N5/01', 'G06N5/022', 'G06N5/041', 'G06N5/046', 'G06N7/01', 'H04L67/10']"
CA2888153C,Methods and arrangements for identifying objects,"In some arrangements, product packaging is digitally watermarked over most of its extent to facilitate high-throughput item identification at retail checkouts. Imagery captured by conventional or plenoptic cameras can be processed (e.g., by GPUs) to derive several different perspective -transformed views - further minimizing the need to manually reposition items for identification. Crinkles and other deformations in product packaging can be optically sensed, allowing such surfaces to be virtually flattened to aid identification. Piles of items can be 3D-modelled and virtually segmented into geometric primitives to aid identification, and to discover locations of obscured items. Other data (e.g., including data from sensors in aisles, shelves and carts, and gaze tracking for clues about visual saliency) can be used in assessing identification hypotheses about an item. Logos may be identified and used - or ignored - in product identification. A great variety of other features and arrangements are also detailed.","['G06V10/443', 'G06Q20/208', 'G07G1/0063', 'G06V10/245', 'G06V2201/09']"
US20240334209A1,Methods for Real-Time Wideband RF Waveform and Emission Classification,"Provided herein are methods and systems for identifying one or more unused or underused portions of a wireless radio frequency (RF) spectrum including providing a multi-label multi-class machine learning classifier trained using a set of RF transmission data, receiving, by a receiver, wireless RF signals in an environment suspected of containing unused or underused portions of said RF spectrum, classifying the received wireless RF signals using the classifier; and identifying unused or underused portions of said RF spectrum.","['G06N3/08', 'G06N3/0464', 'H04B1/12', 'H04W24/02']"
CN109426805B,"Method, apparatus and computer program product for object detection","A method, comprising: receiving as input video comprising video frames; generating a set of object suggestions from the video; generating an object track segment comprising regions that occur in successive frames of the video, the regions corresponding to object proposals having a predetermined confidence level; constructing a map comprising the object extract and superpixels, the superpixels grouped from pixels of the frame; calculating a first cost function of the superpixel likelihood according to the object proposal in the object track segment; calculating a second cost function of object proposal likelihood from the superpixels in the frame; minimizing the first and second cost functions to each other; calculating a posterior probability of each superpixel; and assigning each superpixel an object class with a maximum a posteriori probability to form a semantic object segmentation.","['G06T7/246', 'H04N19/17', 'G06T7/143', 'G06F17/18', 'G06F3/08', 'G06T7/10', 'G06V20/46', 'G06V20/49', 'G06T2207/10016', 'G06T2207/20072', 'G06T2207/20081', 'G06T2207/20084', 'G06T7/162']"
CN110516670B,Target detection method based on scene level and area suggestion self-attention module,"The invention discloses a target detection method based on a scene level and area suggestion self-attention module, which combines various advanced network structures and concepts and considers the importance of scene information and semantic information on visual identification. Firstly, constructing a target detection model of a depth separable shared network, a scene level-region suggestion self-attention module and a lightweight head network; then, training the target detection model by using the training image to obtain a trained target detection model; and finally, sending the image to be detected into a trained target detection model to obtain the position information and the category information of the target in the image. The method is not limited to the appearance characteristics of the target object in the image, but carries out modeling characteristic extraction processing on the relation information between the scene information and the object, and predicts the object in the image according to the structure, thereby greatly improving the detection accuracy.","['G06F18/214', 'G06N3/045', 'G06V10/25', 'G06V10/267']"
US9916514B2,Text recognition driven functionality,"Various approaches for providing textual information to an application, system, or service are disclosed. In particular, various embodiments enable a user to capture an image with a camera of a portable computing device. The computing device is capable of taking the image and processing it to recognize, identify, and/or isolate the text in order to forward the text to an application or function. The application or function can then utilize the text to perform an action in substantially real-time. The text may include an email, phone number, URL, an address, and the like and the application or function may be dialing the phone number, navigating to the URL, opening an address book to save contact information, displaying a map to show the address, and so on.","['G06K9/2081', 'H04N1/00968', 'G06F3/167', 'G06V30/1456', 'H04N1/00331', 'H04N1/32048', 'G06K2209/01', 'G06V30/10', 'H04N2201/0084']"
CN112771581B,"Multi-modal, multi-resolution deep learning neural network for segmentation, outcome prediction and longitudinal response monitoring for immunotherapy and radiation therapy","Systems and methods for a multi-modal, multi-resolution deep learning neural network for segmentation, outcome prediction, and longitudinal response monitoring for immunotherapy and radiation therapy are described in detail herein. The structure-specific generation countermeasure network (SSGAN) is used to synthesize realistic and structure-preserving images that were not generated using prior art GAN, and at the same time incorporate constraints to generate synthesized images. A deeply supervised, multi-modal, multi-resolution residual network (DEEPMMRRN) of tumor and organ-at-risk (OAR) segmentations may be used for tumor and OAR segmentations. DEEPMMRRN can be combined with multiple modalities for tumor and OAR segmentation. By using features of multiple scales and resolutions simultaneously and by feature selection of depth supervision to maximize network capacity, accurate segmentation can be achieved. DEEPMMRRN radiology can be used to predict and longitudinally monitor responses to immunotherapy. Automatic segmentation can be used in combination with radiometric analysis to predict response before treatment begins. Quantification of overall tumor burden can be used for automated response assessment.","['A61B6/5211', 'G06N20/20', 'G06N3/045', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/0495', 'G06N3/084', 'G06N3/088', 'G06N3/0895', 'G06N3/09', 'G06N3/094', 'G06N5/01', 'G06N7/01', 'G06T3/4053', 'G06T5/50', 'G06T7/0012', 'G06T7/11', 'G06T7/187', 'G16H50/20', 'A61B5/055', 'A61B6/03', 'A61B6/5229', 'G06T2207/10072', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30096', 'G06V2201/03']"
US10664716B2,"Portable substance analysis based on computer vision, spectroscopy, and artificial intelligence","A portable complete analysis solution that integrates computer vision, spectrometry, and artificial intelligence for providing self-adaptive, real time information and recommendations for objects of interest. The solution has three major key components: (1) a camera enabled mobile device to capture an image of the object, followed by fast computer vision analysis for features and key elements extraction; (2) a portable wireless spectrometer to obtain spectral information of the object at areas of interest, followed by transmission of the data (data from all built in sensors) to the mobile device and the cloud; and (3) a sophisticated cloud based artificial intelligence model to encode the features from images and chemical information from spectral analysis to decode the object of interest. The complete solution provides fast, accurate, and real time analyses that allows users to obtain clear information about objects of interest as well as personalized recommendations based on the information.","['G06K9/3233', 'G01J3/0264', 'G01J3/0272', 'G01J3/06', 'G06F18/2413', 'G06K9/2018', 'G06K9/22', 'G06K9/46', 'G06K9/4628', 'G06K9/4661', 'G06K9/627', 'G06N3/044', 'G06N3/0442', 'G06N3/0445', 'G06N3/045', 'G06N3/0454', 'G06N3/0455', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06N3/091', 'G06N3/092', 'G06N5/046', 'G06N7/005', 'G06N7/01', 'G06T7/0004', 'G06T7/0012', 'G06T7/11', 'G06V10/143', 'G06V10/17', 'G06V10/25', 'G06V10/454', 'G06V10/60', 'G06V10/764', 'G06V10/82', 'G06V20/66', 'G01J2003/068', 'G06K2009/4657', 'G06K9/00362', 'G06K9/00664', 'G06T2207/20084', 'G06T2207/30088', 'G06T2207/30128', 'G06T2207/30188', 'G06T2207/30196', 'G06V10/58', 'G06V20/10', 'G06V40/10']"
US8494285B2,Joint semantic segmentation of images and scan data,"Systems, methods, and apparatus are described that that increase computer vision analysis in the field of semantic segmentation. With images accompanied by scan data, both two-dimensional and three-dimensional image information is employed for joint segmentation. Through the established correspondence between image data and scan data, two-dimensional and three-dimensional information respectively associated therewith is integrated. Using trained random forest classifiers, the probability of each pixel in images belonging to different object classes is predicted. With the predicted probability, optimization of the labeling of images and scan data is performed by integrating multiples cues in the markov random field.","['G06T7/162', 'G06T7/11', 'G06T2207/10024', 'G06T2207/20072', 'G06T2207/30252']"
US10609284B2,"Controlling generation of hyperlapse from wide-angled, panoramic videos","Hyperlapse results are generated from wide-angled, panoramic video. A set of wide-angled, panoramic video data is obtained. Video stabilization is performed on the obtained set of wide-angled, panoramic video data. Without user intervention, a smoothed camera path is automatically determined using at least one region of interest that is determined using saliency detection and semantically segmented frames of stabilized video data resulting from the video stabilization. A set of frames is determined to vary the velocity of wide-angled, panoramic rendered display of the hyperlapse results.","['G06T5/50', 'H04N5/23238', 'G06T5/00', 'G06T5/70', 'G06T7/20', 'H04N23/67', 'H04N23/698', 'G06T2207/10016', 'G06T2207/10024', 'H04N23/62', 'H04N23/63', 'H04N23/661', 'H04N5/23206', 'H04N5/23216', 'H04N5/23293']"
US7646902B2,Computerized detection of breast cancer on digital tomosynthesis mammograms,"A method for using computer-aided diagnosis (CAD) for digital tomosynthesis mammograms (DTM) including retrieving a DTM image file having a plurality of DTM image slices; applying a three-dimensional gradient field analysis to the DTM image file to detect lesion candidates; identifying a volume of interest and locating its center at a location of high gradient convergence; segmenting the volume of interest by a three dimensional region growing method; extracting one or more three dimensional object characteristics from the object corresponding to the volume of interest, the three dimensional object characteristics being one of a morphological feature, a gray level feature, or a texture feature; and invoking a classifier to determine if the object corresponding to the volume of interest is a breast cancer lesion or normal breast tissue.","['G06T7/0012', 'G06T7/44', 'G06T2207/10081', 'G06T2207/30068', 'G06V2201/03']"
WO2021120695A1,"Image segmentation method and apparatus, electronic device and readable storage medium","An image segmentation method and apparatus, a device and a storage medium, which aim to improve the accuracy of image segmentation. The method comprises: obtaining image features of an image to be segmented (S11); performing an up-sampling operation on the image features to obtain a foreground feature map and a background feature map (S12); normalizing pixel values of pixel points of the foreground feature map and pixel values of pixel points of the background feature map to obtain a target region mask map and a background region mask map, wherein the pixel value of each pixel point in the target region mask map represents the probability of a pixel point that corresponds to the pixel point in the image belonging to a target region, and the pixel value of each pixel point in the background region mask map represents the probability of a pixel point that corresponds to the pixel point in the image belonging to a background region (S13); and according to the target region mask map and the background region mask map, segmenting the image (S14).","['G06V40/1347', 'G06T7/11', 'G06T7/136', 'G06T7/194', 'G06V10/26', 'G06T2207/10004']"
US11521273B2,Identifying flood damage to an indoor environment using a virtual representation,"Disclosed is a method for using a virtual representation of an indoor environment to identify contents that have been damaged (e.g., by flooding). A virtual representation of a physical scene of an indoor environment is processed to identify a list of contents in the physical scene. The virtual representation may include 2-dimensional representations of the physical scene (e.g., images or video) or a 3-dimensional representation of the physical scene (e.g., 3D digital model). A reference line is determined in the virtual representation that is indicative of a maximum vertical extent of the damage in the physical scene. The position of the reference line is compared with the position of the identified contents in the virtual representation to determine contents that are likely to be damaged. For example, the contents that are at or below a plane represented by the reference line in the virtual representation may be identified as damaged.","['G06Q30/0205', 'G06F16/29', 'G06F18/2413', 'G06F3/04815', 'G06F3/0484', 'G06F3/04842', 'G06N20/00', 'G06N3/044', 'G06N3/0445', 'G06N3/0464', 'G06N3/09', 'G06Q10/06395', 'G06Q10/0875', 'G06Q30/0206', 'G06Q40/08', 'G06T17/00', 'G06T17/20', 'G06T19/00', 'G06T7/0002', 'G06T7/20', 'G06T7/50', 'G06T7/80', 'G06V10/764', 'G06V10/82', 'G06V20/00', 'G06V20/64', 'H04N23/631', 'H04N23/80', 'H04N23/90', 'H04N5/232933', 'G01S17/06', 'G01S17/89', 'G01S19/42', 'G06F18/24', 'G06K9/6267', 'G06N3/08', 'G06Q50/16', 'G06T2207/20081', 'G06T2219/004', 'Y02A10/40']"
CN109658523A,The method for realizing each function operation instruction of vehicle using the application of AR augmented reality,"The invention discloses a kind of methods for realizing each function operation instruction of vehicle using the application of AR augmented reality, utilize enhancing virtual reality technology, it is handled by image/sound data information of the computer system to acquisition, and by the dummy object of generation, scene or system prompt information superposition into real scene, its process realized includes: first, target identification is carried out according to scan image, realizes that the article scanned to user mobile phone responds；Then, the target item after scanning recognition is obtained into scanning result compared with the Item Information of storage；Finally, user is sent to computer system processor using scanning result as request by terminal APP, processor handles corresponding request, and corresponding article function is sent to APP in response and is shown on user mobile phone, enhance perception of the user to real world, realize the operation instruction of each function of vehicle, it is convenient, simple, intuitive, lively and economic and environment-friendly.","['G06T19/20', 'G06Q30/0627', 'G06T2219/2016', 'Y02T10/40']"
US12093465B2,Methods and systems for hand gesture-based control of a device,"Methods and systems for gesture-based control of a device are described. An input frame is processed to determine a location of a distinguishing anatomical feature in the input frame. A virtual gesture-space is defined based on the location of the distinguishing anatomical feature, the virtual gesture-space being a defined space for detecting a gesture input. The input frame is processed in only the virtual gesture-space, to detect and track a hand. Using information generated from detecting and tracking the at least one hand, a gesture class is determined for the at least one hand. The device may be a smart television, a smart phone, a tablet, etc.","['G06F3/017', 'G06F3/012', 'G06N3/045', 'G06N3/0464', 'G06N3/048', 'G06N3/09', 'G06V10/141', 'G06V10/25', 'G06V10/82', 'G06V40/107', 'G06V40/161', 'G06V40/172', 'G06V40/20', 'G06V40/28', 'H04N21/4223', 'H04N21/44218']"
CN110017773B,Package volume measuring method based on machine vision,"The invention discloses a parcel volume measuring method based on machine vision, which comprises the steps of S1, fixedly installing a vision camera, calibrating the vision camera, and converting the vision camera into a world coordinate system; s2, initializing the height of the visual camera; s3, placing the package on a measuring table, and collecting a depth image A2 by a vision camera; s3-1, judging to obtain a parcel edge according to the depth image A2, and judging the size type of the parcel; s4, carrying out image segmentation on the depth image A2 to obtain a wrapping area of the image; s4-1, calculating and acquiring point clouds of a parcel area of the image, processing point cloud splicing and smoothing to obtain parcel point clouds; s4-2, analyzing the size of the parcel point cloud of S4-1, S5, calculating the length, width and height of a parcel area of the image, and calculating to obtain the actual volume V of the parcel; the invention can replace the existing manual work to finish the measurement of the package size, and can effectively improve the measurement precision, reduce the labor cost and improve the operation efficiency.","['G01B11/00', 'G01B11/002', 'G01B11/03', 'G01B11/2433', 'G06T7/194', 'G06T7/62', 'G06T2207/10028']"
CN105844257B,The early warning system and method for road sign are missed based on machine vision travelling in fog day,"The present invention is based on early warning systems and method that machine vision travelling in fog day misses road sign, belong to intelligent vehicle safety auxiliary driving technology field.The present invention identifies neuroid classifier, driver fatigue pattern classifier by road and traffic sign plates recognition classifier, road and traffic sign plates character classification, has fogless Image Classifier, image defogging model, constructs Real-time Road landmark identification prediction policy.Track, frequency of wink, PERCLOS value are watched attentively according to driver's sight, if it is determined that driver's driving fatigue or in driving procedure execute subtask lead to miss road signs information, then vehicle-mounted loudspeaker issues the audible alert of real-time sign board information and in the flag information of car-mounted display screen display warning, simultaneously if it is determined that the greasy weather, it equally gives driver and vision, aural alert is provided, it realizes under the conditions of missing road sign in greasy weather and driver, driver is made to obtain road and traffic sign plates information and traffic safety early warning.","['G06V20/582', 'B60Q9/00', 'B60R1/00', 'G06F18/24', 'G06F18/2411', 'G06T5/73', 'G06V20/597', 'G08B21/24', 'B60R2300/8006', 'B60R2300/804', 'G06T2207/10048', 'G06T2207/30256', 'G06T2207/30268']"
CN111814719B,Skeleton behavior recognition method based on 3D space-time diagram convolution,"The invention provides a skeleton behavior recognition method based on 3D space-time diagram convolution, which not only can realize the simultaneous spatial modeling and time modeling of skeleton information, but also can represent the connectivity between space-time information; meanwhile, the method can obtain excellent recognition accuracy on a large skeleton data set, and has good generalization performance. According to the technical scheme, a 3D space-time diagram convolutional neural network model is constructed by combining a Laplacian of a 2D diagram convolution and a time Laplacian of a plurality of frames, and updating of a current node in the 3D space-time diagram convolutional neural network model depends on the state of a joint node connected with the current node in the current 2D diagram, and is related to the node state of a corresponding node in the adjacent 2D diagram; and the convolution of the 3D graph is constructed by combining the related state information in the current 2D graph and the state information of the same node in the adjacent 2D graphs which are adjacent front and back, so that the communication of the space information and the time information is realized.","['G06V40/20', 'G06F18/214', 'G06N3/045', 'G06V20/40']"
US6911995B2,Computer vision depth segmentation using virtual surface,"The location of an object in a physical scene is identified with a stereo camera. A virtual surface is identified in the physical scene, and an approximate disparity set is constructed for the virtual surface. A main and a reference image of the scene are acquired by the stereo camera. The reference image is warped according to the disparity set, and subtracted from the main image to determine a set of depth residual values. Pixels having a substantially non-zero residual are identified with a surface of the object not coincident with the virtual surface.","['G06T7/246', 'G06T7/11', 'G06T7/194', 'G06T7/593', 'G06T2207/10012', 'H04N13/111', 'H04N13/15', 'H04N13/239', 'H04N13/246', 'H04N13/254', 'H04N13/257', 'H04N13/286', 'H04N13/324', 'H04N13/363', 'H04N2013/0081', 'H04N2013/0092']"
WO2021191908A1,Deep learning-based anomaly detection in images,"A method comprising: receiving, as input, training images, wherein at least a majority of the training images represent normal data instances; receiving, as input, a target image; extracting (i) a set of feature representations from a plurality of image locations within each of the training images, and (ii) target feature representations from a plurality of target image locations within the target image; calculating, with respect to a target image location of the plurality of target image locations in the target image, a distance between (iii) the target feature representation of the target image location, and (iv) a subset from the set of feature representations comprising the k nearest the feature representations to the target feature representation; and determining that the target image location is anomalous, when the calculated distance exceeds a predetermined threshold.","['G06N3/088', 'G06F18/24147', 'G06F18/2433', 'G06N3/045', 'G06V10/44', 'G06V10/761', 'G06V10/762', 'G06V10/82']"
US11568542B2,Body-mounted or object-mounted camera system,"An object or body-mounted camera apparatus for recording surgery is provided that is adapted for tracking a relevant visual field of an on-going operation. To help maintain visibility and/or focus of the visual field, specific machine learning approaches are proposed in combination with control commands to shift a physical positioning or a perspective of the camera apparatus. Additional variations are directed to tracking obstructions based on the visual field of the camera, which can be utilized for determining a primary recording for use when there are multiple cameras being used in concert.","['G06T1/0014', 'G06T7/10', 'A61B34/10', 'A61B90/361', 'F16M11/041', 'F16M11/123', 'F16M11/18', 'F16M11/2021', 'F16M13/04', 'G06T1/20', 'G06T1/60', 'G06T7/11', 'G06T7/73', 'H04N23/60', 'H04N23/685', 'H04N23/695', 'H04N5/765', 'A61B2034/2048', 'A61B2034/2065', 'A61B2090/3612', 'A61B2090/502', 'A61B90/53', 'G06T2207/20081', 'G06T2207/20084']"
US20220161815A1,Autonomous vehicle system,"According to one embodiment, an apparatus includes an interface to receive sensor data from a plurality of sensors of an autonomous vehicle. The apparatus also includes processing circuitry to apply a sensor abstraction process to the sensor data to produce abstracted scene data, and to use the abstracted scene data in a perception phase of a control process for the autonomous vehicle. The sensor abstraction process may include one or more of: applying a Sensor data response normalization process to the sensor data, applying a warp process to the sensor data, and applying a filtering process to the sensor data.","['B60W60/001', 'G06N20/00', 'B60W30/182', 'B60W40/02', 'B60W40/04', 'B60W40/08', 'B60W40/09', 'B60W40/10', 'B60W50/00', 'B60W50/0097', 'B60W50/0098', 'B60W50/14', 'B60W50/16', 'B60W60/00', 'B60W60/0011', 'B60W60/0013', 'B60W60/00274', 'B60W60/0053', 'B60W60/0057', 'G05D1/00', 'G05D1/0038', 'G05D1/0061', 'G05D1/02', 'G05D1/0282', 'G06N7/01', 'G06T1/0007', 'G06T9/00', 'G08G1/0112', 'G08G1/0116', 'G08G1/0129', 'G08G1/0141', 'G08G1/09626', 'G08G1/096725', 'G08G1/096741', 'G08G1/09675', 'G08G1/096758', 'G08G1/096775', 'G08G1/096783', 'G08G1/162', 'G08G1/163', 'G08G1/166', 'G08G1/167', 'H04L9/3213', 'H04W4/40', 'H04W4/46', 'B60W2050/0052', 'B60W2050/0064', 'B60W2050/0075', 'B60W2050/0083', 'B60W2050/143', 'B60W2050/146', 'B60W2420/403', 'B60W2420/408', 'B60W2420/42', 'B60W2420/52', 'B60W2540/043', 'B60W2540/047', 'B60W2540/215', 'B60W2540/22', 'B60W2540/221', 'B60W2540/223', 'B60W2540/30', 'B60W2554/4046', 'B60W2556/35', 'B60W2556/45', 'B60W2556/50', 'B60W2556/65', 'G06N3/006', 'G06N3/045', 'G06N3/049', 'G06N3/08']"
US11810358B2,Video search segmentation,"Embodiments are directed to video segmentation based on a query. Initially, a first segmentation such as a default segmentation is displayed (e.g., as interactive tiles in a finder interface, as a video timeline in an editor interface), and the default segmentation is re-segmented in response to a user query. The query can take the form of a keyword and one or more selected facets in a category of detected features. Keywords are searched for detected transcript words, detected object or action tags, or detected audio event tags that match the keywords. Selected facets are searched for detected instances of the selected facets. Each video segment that matches the query is re-segmented by solving a shortest path problem through a graph that models different segmentation options.","['G11B27/34', 'G06V20/49', 'G06F16/7837', 'G06F16/7844', 'G06F18/22', 'G06V10/82', 'G11B27/031', 'G06V10/759', 'G06V40/172']"
US20230409749A1,Systems and methods for surgical video de-identification,"An improved approach is described herein wherein an automated de-identification system is provided to process the raw captured data. The automated de-identification system utilizes specific machine learning data architectures and transforms the raw captured data into processed captured data by modifying, replacing, or obscuring various identifiable features. The processed captured data can include transformed video or audio data.","['G06F21/6254', 'G06V10/25', 'A61B90/361', 'G06T7/11', 'G06T7/20', 'G06V10/82', 'G06V20/70', 'G06V40/10', 'H04N21/234345', 'H04N21/4318', 'G06N3/045', 'G06N3/08', 'G06T2200/28', 'G06T2207/10016', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196', 'G06V2201/07']"
US11455731B2,Video segmentation based on detected video features using a graphical model,"Embodiments are directed to video segmentation based on detected video features. More specifically, a segmentation of a video is computed by determining candidate boundaries from detected feature boundaries from one or more feature tracks; modeling different segmentation options by constructing a graph with nodes that represent candidate boundaries, edges that represent candidate segments, and edge weights that represent cut costs; and computing the video segmentation by solving a shortest path problem to find the path through the edges (segmentation) that minimizes the sum of edge weights along the path (cut costs). A representation of the video segmentation is presented, for example, using interactive tiles or a video timeline that represent(s) the video segments in the segmentation.","['G06T7/12', 'G06F18/23', 'G06T7/162', 'G06V10/762', 'G06V20/46', 'G06V20/49', 'G06V40/161', 'G11B27/031', 'G11B27/10', 'G11B27/28', 'G11B27/34', 'G06T2207/10016']"
US12033669B2,Snap point video segmentation identifying selection snap points for a video,"Embodiments are directed to a snap point segmentation that defines the locations of selection snap points for a selection of video segments. Candidate snap points are determined from boundaries of feature ranges of the video indicating when instances of detected features are present in the video. In some embodiments, candidate snap point separations are penalized for being separated by less than a minimum duration corresponding to a minimum pixel separation between consecutive snap points on a video timeline. The snap point segmentation is computed by solving a shortest path problem through a graph that models different snap point locations and separations. When a user clicks or taps on the video timeline and drags, a selection snaps to the snap points defined by the snap point segmentation. In some embodiments, the snap points are displayed during a drag operation and disappear when the drag operation is released.","['G11B27/031', 'G11B27/036', 'G06F3/0481', 'G06F3/0482', 'G06F3/04847', 'G06F3/0486', 'G06F3/0488', 'G06V10/82', 'G06V20/41', 'G11B27/34', 'G10L15/26', 'G10L17/00', 'G10L25/57', 'G10L25/78']"
US12125211B2,System and method for multi-scale coarse-to-fine segmentation of images to detect pancreatic ductal adenocarcinoma,"Methods, systems, apparatus, and computer programs, for processing images through multiple neural networks that are trained to detect a pancreatic ductal adenocarcinoma. In one aspect, a method includes actions of obtaining a first image that depicts a first volume of voxels, performing coarse segmentation of the first image using a first neural network trained (i) to process images having the first volume of voxels and (ii) to produce first output data, determining a region of interest of the first image based on the coarse segmentation, performing multi-stage fine segmentation on a plurality of other images that are each based on the region of interest of the first image to generate output data for each stage of the multi-stage fine segmentation, and determining based on the first output data and the output data of each stage of the multi-stage fine segmentation, whether the first image depicts a tumor.","['G06T7/0012', 'G06T7/11', 'G06T2207/10072', 'G06T2207/10081', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30096']"
US6574353B1,Video object tracking using a hierarchy of deformable templates,"A hierarchy of deformation operations is implemented to deform a template and match the deformed template to an object in a video frame. At each level, the constraints on the template deformations are relaxed, while the spatial range of the object boundary search is narrowed. At a highest level, an initial template is translated, rotated and scaled to coarsely locate the object within a given image frame. At a middle level, an affine transformation is implemented, globally or locally, to deform the template. For a local affine transformation process, a sup-portion, such as an articulation or appendage portion of the template is deformed. The middle level refines the template to get the template boundary close to the actual object boundary within a given frame. At the lowest level, a local segmentation algorithm is applied to deform the now close boundary to finely match the object boundary.","['G06T7/246', 'G06V10/754', 'G06V10/62']"
US11887371B2,Thumbnail video segmentation identifying thumbnail locations for a video,"Embodiments are directed to a thumbnail segmentation that defines the locations on a video timeline where thumbnails are displayed. Candidate thumbnail locations are determined from boundaries of feature ranges of the video indicating when instances of detected features are present in the video. In some embodiments, candidate thumbnail separations are penalized for being separated by less than a minimum duration corresponding to a minimum pixel separation (e.g., the width of a thumbnail) between consecutive thumbnail locations on a video timeline. The thumbnail segmentation is computed by solving a shortest path problem through a graph that models different thumbnail locations and separations. As such, a video timeline is displayed with thumbnails at locations on the timeline defined by the thumbnail segmentation, with each thumbnail depicting a portion of the video associated with the thumbnail location.","['G11B27/031', 'G06V20/49', 'G06V10/751', 'G06V10/82', 'G06V20/46', 'G06V40/161', 'G11B27/10', 'G11B27/34', 'G06V40/172']"
US11600072B2,Object left behind detection,"Techniques for automatically detecting objects left behind. An example method includes receiving video frames of a scene from a three-dimensional (3D) camera and establishing, based on 3D depths associated with the video frames, a background of the scene. The method also includes detecting, in the frames, a foreground blob in the scene based on the foreground blob having a 3D depth that is different from the background. The method further includes detecting that the foreground blob has separated into a blob corresponding to a person and a second blob corresponding to an object, based on the person having a 3D depth that is different from the object. The method additionally includes determining that the person has been separated from the object for a threshold, and responsive to determining that the person has been separated from the object for the threshold, generating an alert indicating that the object is left behind.","['G08B21/24', 'G06T7/194', 'G06V10/50', 'G06V20/52', 'G06T2207/10028', 'G06T2207/30196', 'G06T2207/30232', 'G06T2207/30242', 'G06T7/254', 'G08B13/196']"
CN109886179B,Image segmentation method and system of cervical cell smear based on Mask-RCNN,"The invention relates to a method and a system for segmenting images of cervical cell smears based on Mask-RCNN, comprising the following steps: a. a data set construction step, which comprises the preparation and marking of a training data set, a verification data set and a test data set, and the normalization and pretreatment of the data set; b. constructing a Mask-RCNN-based image segmentation model, training the model by using the training data set, and verifying the image segmentation result of the model by using the verification data set; c. and a step of verifying the model, which is to test the model by using the test data set and evaluate a segmentation result by using a similarity coefficient. The deep neural network model trained by using a large amount of data can be used for modeling and abstracting information contained in the large amount of data, so that cells and cell nucleuses in the cervical cytology smear image can be positioned, detected and subjected to example segmentation through a single model.",['Y02A90/10']
CN111429403B,Automobile gear finished product defect detection method based on machine vision,"The invention discloses a machine vision-based automobile gear finished product defect detection method. The method aims at the problem that the defect identification precision of the automobile gear part is not high by the current sampling digital image processing technology. The invention utilizes the digital image processing technology to extract the contour and edge parts of parts such as gears and the like, then the extracted gear boundary image is used as prior information to be merged into an improved UNet network structure, and bottom layer characteristic information is artificially supplemented to be used as the reference of network training, thereby achieving better information merging effect. According to the invention, the semantics of the gear image are segmented based on the UNet network structure, and the defects of tiny cracks, stains and the like can be completely identified.","['G06T7/0004', 'G01B11/24', 'G01N21/8851', 'G01N21/94', 'G06F18/2415', 'G06F18/253', 'G06N3/045', 'G06N3/047', 'G06T5/70', 'G06T7/10', 'G06T7/13', 'G06T7/136', 'G01N2021/8887', 'G06T2207/10004', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30164']"
CN114511778B,Image processing method and device,"The invention relates to the technical field of image processing and provides an image processing method and device, wherein the method comprises the steps of carrying out semantic segmentation on an initial RGB image to obtain a semantic feature map; the method comprises the steps of carrying out depth feature extraction on an initial depth image corresponding to an initial RGB image based on foreground probability and semantic features included in a semantic feature image to obtain a depth feature vector, carrying out feature fusion on the depth feature vector and the initial RGB image to obtain a fusion confidence image and a fusion depth image, and obtaining a target depth image based on the local confidence image, the local depth image, the fusion confidence image and the fusion depth image. According to the method, the semantic information of the initial RGB image is extracted, the local depth information of the accurately-depicted image is guided, meanwhile, the global depth map information is optimized, the thickness and the accuracy of the depth information of the target depth map are improved, and guarantee is provided for follow-up computer vision tasks.","['G06F18/214', 'G06F18/253']"
US10803312B2,AI-powered autonomous plant-growth optimization system that automatically adjusts input variables to yield desired harvest traits,"Inputs from sensors (e.g., image and environmental sensors) are used for real-time optimization of plant growth in indoor farms by adjusting the light provided to the plants and other environmental factors. The sensors use wireless connectivity to create an Internet of Things network. The optimization is determined using machine-learning analysis and image recognition of the plants being grown. Once a machine-learning model has been generated and/or trained in the cloud, the model is deployed to an edge device located at the indoor farm to overcome connectivity issues between the sensors and the cloud. Plants in an indoor farm are continuously monitored and the light energy intensity and spectral output are automatically adjusted to optimal levels at optimal times to create better crops. The methods and systems are self-regulating in that light controls the plant's growth, and the plant's growth in-turn controls the spectral output and intensity of the light.","['G06N3/08', 'G06K9/00657', 'G06N3/0418', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G06T7/0002', 'G06T7/0004', 'G06V10/82', 'G06V20/10', 'G06V20/188', 'G06K2009/00644', 'G06N20/10', 'G06N20/20', 'G06N5/01', 'G06T2207/10024', 'G06T2207/10048', 'G06T2207/30128', 'G06T2207/30188', 'G06V10/454', 'G06V20/194']"
US9349297B1,System and method for nutrition analysis using food image recognition,"The present disclosure provides a system and method for determining a nutritional value of a food item. The system and method utilizes a food container as a model to adjust various types of distortions that exists in an instant image of the food container that retains the food item. The instant image may be compared to the model image of the food container to correct any distortions. The food container includes a boundary which has a predetermined color. The predetermined color of the boundary can be used to adjust the color configuration of the instant image, thereby increasing the accuracy of the food identification.","['G09B5/00', 'A47G19/025', 'B65D25/04', 'G06F18/22', 'G06K9/4604', 'G06K9/4652', 'G06K9/52', 'G06K9/6201', 'G06T5/006', 'G06T7/0042', 'G06T7/0083', 'G06T7/0085', 'G06T7/11', 'G06T7/12', 'G06T7/13', 'G06T7/408', 'G06T7/60', 'G06T7/90', 'G06V10/761', 'G06V20/80', 'G09B19/0092', 'G09B5/02', 'G16H20/60', 'H04N7/18', 'H04N7/183', 'G06K2009/4666', 'G06T2207/20144', 'G06T7/194', 'G06V20/68']"
LU504274B1,Method for online detection of machine tool part based on machine vision,"The present application relates to the technical field of machine vision, and in particular to a method for online detection of a machine tool part based on machine vision. The method includes that: a confidence coefficient that a first pixel point in a to-be-detected image of a bevel gear belongs to a scratch category is acquired; the first pixel point with the maximum confidence coefficient in a semantic fuzzy region is determined as an initial seed point, and region growth is performed on the semantic fuzzy region; a similarity between a second pixel point in a set growth window in the semantic fuzzy region and the seed point is acquired; during region generation, a category label of the second pixel point is determined; In the present application, the accuracy for scratch detection of the bevel gear in the machine tool part is improved.","['G06T7/0004', 'G06T7/11', 'G06T7/13', 'G06V10/267', 'G06V10/44', 'G06V10/761', 'G06T2207/30164', 'Y02P90/30']"
US12142066B2,"Artificial intelligence-based image processing method, apparatus, device, and storage medium","This application discloses an artificial intelligence-based image processing method, apparatus, device, and storage medium, and relates to the field of computer technology. The method includes: obtaining a slice image; dividing the slice image to obtain a plurality of image blocks; feeding the plurality of image blocks into a labeling model, extracting, by the labeling model, a pixel feature of the slice image based on the plurality of image blocks, determining a plurality of vertex positions of a polygonal region in the slice image based on the pixel feature, concatenating the plurality of vertex positions, and outputting label information of the slice image, the polygonal region being a region in which a target pathological tissue of interest is located.","['G06V20/70', 'G06N3/044', 'G06N3/0442', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T3/40', 'G06T7/0012', 'G06T7/10', 'G06T7/11', 'G06T7/73', 'G06T9/00', 'G06V10/774', 'G06V10/776', 'G06V10/806', 'G06V10/82', 'G06V20/695', 'G06T2207/10004', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/30004', 'G06T2207/30024', 'G06V2201/03']"
US5572565A,"Automatic segmentation, skinline and nipple detection in digital mammograms","Digital mammograms are automatically segmented into background, corresponding to the region external of the breast, and foreground, corresponding to the region within the breast, by the detection of the skinline which forms a border between these regions. A binary array is produced representing an initial coarse segmentation between foreground and background in which a binary one value is assigned to each pixel in the binary array whose intensity exceeds an intensity threshold and/or whose gradient amplitude exceeds a gradient threshold.","['G06T7/0012', 'G06T7/12', 'G06T7/194', 'G06T2207/10116', 'G06T2207/30068']"
US12141981B2,System and method for performing semantic image segmentation,"Systems and techniques are provided for performing semantic image segmentation using a machine learning system (e.g., including one or more cross-attention transformer layers). For instance, a process can include generating one or more input image features for a frame of image data and generating one or more input depth features for a frame of depth data. One or more fused image features can be determined, at least in part, by fusing the one or more input depth features with the one or more input image features, using a first cross-attention transformer network. One or more segmentation masks can be generated for the frame of image data based on the one or more fused image features.","['G06T7/174', 'G06T7/10', 'G06N3/063', 'G06T7/11', 'G06T7/50', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084']"
WO2021093435A1,"Semantic segmentation network structure generation method and apparatus, device, and storage medium","A semantic segmentation network structure generation method and apparatus, an electronic device, and a computer-readable storage medium. The method comprises: generating corresponding architecture parameters for various units, which constitute a super unit, in a semantic segmentation network structure (101); optimizing the semantic segmentation network structure on the basis of an image sample, and removing a redundant unit in the super unit to which a target unit belongs, so as to obtain an improved semantic segmentation network structure (102); by means of a clustering unit in the improved semantic segmentation network structure, performing feature fusion on output of the super unit from which the redundant unit is removed, so as to obtain a fused feature map (103); performing identification processing on the fused feature map to determine a position corresponding to an object in the image sample (104); and on the basis of the position corresponding to the object in the image sample, and a label corresponding to the image sample, training the improved semantic segmentation network structure to obtain a trained semantic segmentation network structure (105).","['G06V40/161', 'G06F18/214', 'G06F18/217', 'G06F18/253', 'G06N3/04', 'G06N3/045', 'G06N3/082', 'G06N3/084', 'G06T7/11', 'G06T7/70', 'G06V10/267', 'G06V10/70', 'G06V10/774', 'G06V10/776', 'G06V10/806', 'G06V10/82', 'G06V20/41', 'G06V20/54', 'G06V20/56', 'G06V20/70', 'G06T2207/20081', 'G06T2207/20084', 'G06V2201/07']"
US10846873B2,Methods and apparatus for autonomous robotic control,"Sensory processing of visual, auditory, and other sensor information (e.g., visual imagery, LIDAR, RADAR) is conventionally based on “stovepiped,” or isolated processing, with little interactions between modules. Biological systems, on the other hand, fuse multi-sensory information to identify nearby objects of interest more quickly, more efficiently, and with higher signal-to-noise ratios. Similarly, examples of the OpenSense technology disclosed herein use neurally inspired processing to identify and locate objects in a robot's environment. This enables the robot to navigate its environment more quickly and with lower computational and power requirements.","['G06T7/70', 'G06V20/10', 'G01S13/86', 'G01S5/16', 'G01S7/41', 'G06F18/253', 'G06K9/00664', 'G06K9/3241', 'G06K9/4628', 'G06K9/629', 'G06N3/008', 'G06N3/0409', 'G06N3/049', 'G06V10/255', 'G06V10/454', 'G06V10/806', 'Y04S10/50', 'Y04S10/54', 'Y10S901/44']"
US9736455B2,Method and apparatus for downscaling depth data for view plus depth data compression,"A method, apparatus, and computer program product are disclosed for compression of a 2D-plus-depth representation based on spatial downsampling of an initial depth map. By utilizing the color image accompanying the initial depth map, it is possible to infer structural information that refines and reconstructs the initial depth map out of a heavily subsampled version of the depth map. In the process, no indexing of the exact positions of the subsampled depth values is needed, which leads to very efficient compression. An example method causes segmentation of the color image into a set of super-pixel segments, and causes downsampling of the initial depth map based on the set of super-pixel segments. The method subsequently causes generation and storage of a compressed representation based on the segmented color image and the downsampled depth map. A corresponding apparatus and computer program product are also provided.","['H04N13/0048', 'H04N13/161', 'H04N19/00', 'H04N19/147', 'H04N19/176', 'H04N19/46', 'H04N19/59', 'H04N19/597', 'H04N19/80', 'H04N19/96']"
US20220129556A1,Systems and Methods for Implementing Smart Assistant Systems,"In one embodiment, a system includes an automatic speech recognition (ASR) module, a natural-language understanding (NLU) module, a dialog manager, one or more agents, an arbitrator, a delivery system, one or more processors, and a non-transitory memory coupled to the processors comprising instructions executable by the processors, the processors operable when executing the instructions to receive a user input, process the user input using the ASR module, the NLU module, the dialog manager, one or more of the agents, the arbitrator, and the delivery system, and provide a response to the user input.","['G06F21/57', 'G06F21/6245', 'G06F18/217', 'G06F21/577', 'G06F21/602', 'G06F21/74', 'G06F3/167', 'G06K9/46', 'G06K9/6262', 'G06V10/25', 'G06V10/40', 'G06V10/82', 'G06F2221/033']"
US12229741B2,"Methods, systems, articles of manufacture, and apparatus for decoding purchase data using an image","Methods, apparatus, systems, and articles of manufacture are disclosed that decode purchase data using an image. An example apparatus includes a dictionary including associated product descriptions and barcodes, interface circuitry, and processing circuitry to execute machine readable instructions to obtain purchase details and barcodes corresponding to a receipt, the purchase details including receipt product descriptions, generate a search query that includes a first receipt product description of the receipt product descriptions, a list of barcodes corresponding to the barcodes, and a store identifier associated with the receipt, execute a search against the dictionary using the search query to identify a barcode from the list of barcodes that corresponds to the first receipt product description, and in response to identifying the barcode that corresponds to the first receipt product description, associating the barcode and the first receipt product description and adding the association to the dictionary.","['G06Q30/0245', 'G06Q20/201', 'G06K7/1413', 'G06Q30/0201', 'G06V30/147', 'G06V30/18057', 'G06V30/414', 'G06V30/42']"
US9996925B2,System and method for assessing wound,"The wound assessing method and system provide a convenient, quantitative mechanism for diabetic foot ulcer assessment.","['G06T7/0012', 'A61B5/445', 'G06T7/0081', 'G06T7/11', 'G06T7/194', 'G06T7/408', 'G06T7/90', 'A61B2560/0228', 'A61B2576/00', 'A61B5/0062', 'A61B5/1032', 'A61B5/1109', 'G06T2207/10004', 'G06T2207/10024', 'G06T2207/20036', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/30088', 'G06T2207/30096', 'G16H30/40']"
US12327425B2,"Methods, systems, articles of manufacture, and apparatus for decoding purchase data using an image","Methods, apparatus, systems, and articles of manufacture are disclosed that decode purchase data using an image. An example apparatus includes processor circuitry to execute machine readable instructions to at least crop an image of a receipt based on detected regions of interest, apply a first mask to a first cropped image to generate first bounding boxes corresponding to rows of the receipt, apply a second mask to a second cropped image to generate second bounding boxes corresponding to columns of the receipt, generate a structure of the receipt by mapping words detected by an optical character recognition engine to corresponding first bounding boxes and second bounding boxes based on a mapping criterion, classify the second bounding boxes by identifying an expression of interest in ones of the second bounding boxes, and generate purchase information by extracting text of interest from the structured receipt based on the classifications.","['G06Q30/0283', 'G06V30/147', 'G06V30/15', 'G06V30/413', 'G06V30/414', 'G06V30/416']"
WO2020173022A1,"Vehicle violation identifying method, server and storage medium","The present application relates to the field of image detection, and provides a vehicle violation identifying method, a server and a storage medium. Said method comprises: upon detecting that a detected object in a parking image is a motor vehicle, and upon detecting that a detected object in a parking image is a motor vehicle according to an edge detection mode, identifying a feature region of the detected object by means of an image identification algorithm, extracting gradient histogram features from the feature region and inputting same into a neural network classifier, so as to train a pre-trained model in the neural network classifier, calculating, according to position information of the detected object, a pixel mean value and a standard deviation with regard to the training results, determining a calculation result with the maximum pixel mean value and standard deviation as vehicle information and parking information of the detected object, and determining, according to the vehicle information and the parking information of the detected object, whether the detected object satisfies illegal parking conditions. The present solution can improve the accuracy and speed for identifying illegal parking, reducing the workload of traffic police, and alleviating increasingly severe illegal parking caused by the rapid increase of urban vehicles.","['G06V10/267', 'G06V20/52', 'G06V20/63', 'G06V20/625', 'G06V2201/08']"
US11341605B1,Document rectification via homography recovery using machine learning,"Techniques for document rectification via homography recovery using machine learning are described. An image rectification system can intelligently make use of multiple pipelines for rectifying document images based on the detected type of device that generated the images. The image rectification system can provide high-quality rectifications without requiring human cooperation, multiple views of the document in multiple images, and/or without being constrained to only be able to process images from one source context.","['G06T3/10', 'G06T3/0056', 'H04L67/1095', 'G06T1/20', 'G06T3/60', 'G06V30/1475', 'G06V30/412', 'H04L2101/668', 'H04L61/2514', 'H04L61/5007', 'H04L67/02']"
AU2021202716B2,Systems and methods for automated segmentation of individual organs in 3D anatomical images,"Presented herein, in certain embodiments, are approaches for robust bone splitting and segmentation in the context of small animal imaging, for example, microCT imaging. In certain embodiments, a method for calculating and applying single and hybrid second-derivative splitting filters to gray-scale images and binary bone masks is described. These filters can accurately identify the split lines/planes of the bones even for low-resolution data, and hence accurately morphologically disconnect the individual bones. The split bones can then be used as seeds in region growing techniques such as marker-controlled watershed segmentation. With this approach, the bones can be segmented with much higher robustness and accuracy compared to prior art methods. In other embodiments, similar approaches may be performed for automated segmentation of individual organs in 3D anatomical images. WO 2017/019059 PCT/US2015/042631 8/13 800, 'SAX 82 83 0\ ~ ~865 FIG. 8","['G06T7/0012', 'G06T7/11', 'G06T7/155', 'G06V10/443', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/20152', 'G06T2207/20156', 'G06T2207/30008']"
US10043313B2,"Information processing apparatus, information processing method, information processing system, and storage medium","To appropriately superimpose and display a virtual object on an image of a real space, an information processing apparatus according to exemplary embodiment of the present invention determines the display position of the virtual object based on information indicating an allowable degree of superimposition of a virtual object on each real object in the image of the real space, and a distance from a real object for which a virtual object is to be displayed in association with the real object.","['G06T19/006', 'G06K9/00671', 'G06K9/00704', 'G06V20/20', 'G06V20/39']"
US9965592B2,Methods of computing pericardial and abdominal fat and methods for motion compensation,"A new cardiac risk factors are disclosed along with method for deriving the components of the factors, for developing the factors and for using the factors. Methods for computing pericardial fat and abdominal fat are also disclosed as well as methods for motion compensation.","['G16H50/30', 'G06F19/3437', 'G06F19/3431', 'G06F19/345', 'G16H50/20', 'G16H50/50', 'G16Z99/00']"
US11010896B2,Methods and systems for generating 3D datasets to train deep learning networks for measurements estimation,Disclosed are systems and methods for generating data sets for training deep learning networks for key point annotations and measurements extraction from photos taken using a mobile device camera. The method includes the steps of receiving a 3D scan model of a 3D object or subject captured from a 3D scanner and a 2D photograph of the same 3D object or subject at a virtual workspace. The 3D scan model is rigged with one or more key points. A superimposed image of a pose-adjusted and aligned 3D scan model superimposed over the 2D photograph is captured by a virtual camera in the virtual workspace. Training data for a key point annotation DLN is generated by repeating the steps for a plurality of objects belonging to a plurality of object categories. The key point annotation DLN learns from the training data to produce key point annotations of objects from 2D photographs captured using any mobile device camera.,"['G06T7/0012', 'G06T7/75', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/344', 'G06T7/60', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196']"
US9527115B2,Computer vision and machine learning software for grading and sorting plants,"The present invention encompasses software that brings together computer vision and machine learning algorithms that can evaluate and sort plants into desired categories. While one embodiment of the present invention is directed toward strawberry plants, the software engine described is not specifically designed for strawberry plants but can be used for many different types of plants that require sophisticated quality sorting. The present invention is a sequence of software operations that can be applied to various crops (or other objects besides plants) in a re-usable fashion.",['B07C5/342']
CN113657387B,Semi-supervised three-dimensional point cloud semantic segmentation method based on neural network,"The invention belongs to the technical field of deep learning and computer vision, and particularly relates to a semi-supervised three-dimensional point cloud semantic segmentation method based on a neural network. The invention adopts a semi-supervised learning model and combines a three-dimensional point cloud semantic segmentation network model to form a whole semi-supervised three-dimensional point cloud semantic segmentation method frame; the segmentation network model is divided into a student network and a teacher network, and the two networks sample the same SSCNs network; the input of the student network is original point cloud which is not transformed, and the input of the teacher network is transformed point cloud; the output of the part with the label of the student network is supervised by the corresponding label, and meanwhile, the integral output of the student network and the teacher network is supervised in consistency, so that the weight of the student network is updated, and the weight of the teacher network is obtained by carrying out exponential sliding average on the weight of the student network. Experiments show that the performance of the network is obviously improved on each labeling rate by using semi-supervised learning with labeled data and unlabeled data.","['G06F18/2155', 'G06N3/08']"
US11749241B2,Systems and methods for transforming digitial audio content into visual topic-based segments,"A system for platform-independent visualization of audio content, in particular audio tracks utilizing a central computer system in communication with user devices via a computer network. The central system utilizes various algorithms to identify spoken content from audio tracks and selects visual assets associated with the identified content. Thereafter, a visualized audio track is available for users to listen and view. Audio tracks, for example Podcasts, may be segmented into topical audio segments based upon themes or topics, with segments from disparate podcasts combined into a single listening experience, based upon certain criteria, e.g., topics, themes, keywords, and the like.","['G06F16/61', 'G10H1/0008', 'G06F16/685', 'G06F16/686', 'G10H2220/106']"
CN112633330B,"Point cloud segmentation method, system, medium, computer equipment, terminal and application","The invention belongs to the technical field of computer vision, and discloses a point cloud segmentation method, a system, a medium, computer equipment, a terminal and application, wherein a feature extraction module based on a hierarchical graph Transformer is constructed, and the feature extraction module comprises a feature downsampling network and a feature upsampling network based on the hierarchical graph Transformer; building a point cloud segmentation network based on a layered graph Transformer, using a cross entropy loss function with weight, using a training set to perform supervised training on the layered graph Transformer point cloud segmentation network, and adjusting network parameters according to loss values by each round of training to obtain a network model; and predicting the point cloud in the test set by using the trained network model to obtain a segmentation result of each point in the point cloud. The invention enriches the initial characteristics of the point cloud; the graph Transformer can effectively extract the relation between the point cloud neighborhoods; the precision of the point cloud segmentation is improved.","['G06F18/214', 'G06F18/213', 'G06N3/044', 'G06T3/4023', 'G06T7/11', 'G06T2207/10028']"
WO2021179679A1,Automobile dashboard testing system and testing method based on machine vision,"An automatic automobile dashboard testing system and testing method based on machine vision. The system comprises: an upper computer, an electronic control unit, first to third visible light cameras, a dashboard testing apparatus, an IO module, a CAN bus, and dashboard testing software, wherein the upper computer sends a control instruction to the electronic control unit by means of the IO module and controls the visible light cameras to be turned on, and after processing data, the electronic control unit controls, by means of an MCU module, each portion of a dashboard to display a corresponding state; the visible light cameras realize counting synchronization with the dashboard by means of the IO module, thereby ensuring that a visible light camera at a corresponding position carries out candid photography as the display state of the dashboard changes, and a photographed picture is converted into a digital signal and is transmitted to the dashboard testing software; and the software processes the picture and then compares same with a related instruction in a database, and finally transmits, to an alarm device, a signal indicating whether the picture is up to standard, and gives an alarm if the picture is not up to standard.","['G01N21/95607', 'G01N21/01', 'G01N2021/0112', 'G01N2021/95615']"
US9600982B2,Methods and arrangements for identifying objects,"In some arrangements, product packaging is digitally watermarked over most of its extent to facilitate high-throughput item identification at retail checkouts. Imagery captured by conventional or plenoptic cameras can be processed (e.g., by GPUs) to derive several different perspective-transformed views—further minimizing the need to manually reposition items for identification. Crinkles and other deformations in product packaging can be optically sensed, allowing such surfaces to be virtually flattened to aid identification. Piles of items can be 3D-modeled and virtually segmented into geometric primitives to aid identification, and to discover locations of obscured items. Other data (e.g., including data from sensors in aisles, shelves and carts, and gaze tracking for clues about visual saliency) can be used in assessing identification hypotheses about an item. Logos may be identified and used—or ignored—in product identification. A great variety of other features and arrangements are also detailed.","['G07G1/0036', 'G06Q20/208', 'G06Q30/00', 'G06T5/006', 'G06T5/80', 'G06T7/0004', 'G06T7/0042', 'G06T7/73', 'G06V20/10', 'G07G1/0009', 'G07G1/0063', 'G07G1/06', 'G07G1/12', 'G06T2207/10016', 'G06T2207/10021', 'G06T2207/10024', 'G06V10/462']"
US8031060B2,Mobile system and method of operating mobile system,"A mobile system and method of operation thereof, comprising a radio frequency system, adapted to derive information relating to a position within an environment, based on communications with at least one terrestrial or extraterrestrial transmitter, and remotely transmit to and receive radio frequency information-bearing communications; a memory adapted to store at least a vehicle itinerary or position-related information; a controller, receiving the derived information and controlling a communication of the information-bearing communications relating to at least the stored itinerary or position related information; and a user interface, having a functionality defined by the controller, adapted to interface a user for receipt or presentation of information relating at least one of the itinerary or position-related information and the communicated information.","['G05B15/02', 'G05B19/042', 'G06V40/103', 'G05B2219/25323', 'G05B2219/2613', 'G05B2219/2615', 'G05B2219/34038']"
US10509947B1,Converting multi-dimensional data for image analysis,"Multi-dimensional data can be mapped to a projection shape and converted for image analysis. In some examples, the multi-dimensional data may include data captured by a LIDAR system for use in conjunction with a perception system for an autonomous vehicle. Converting operations can include converting three-dimensional LIDAR data to multi-channel two-dimensional data. Data points of the multi-dimensional data can be mapped to a projection shape, such as a sphere. Characteristics of the projection shape may include a shape, a field of view, a resolution, and a projection type. After data is mapped to the projection shape, the projection shape can be converted to a multi-channel, two-dimensional image. Image segmentation and classification may be performed on the two-dimensional data. Further, segmentation information may be used to segment the three-dimensional LIDAR data, while a rendering plane may be positioned relative to the segmented data to perform classification on a per-object basis.","['G06T3/00', 'G06K9/00208', 'G06F18/2411', 'G06K9/6269', 'G06T3/0037', 'G06T3/067', 'G06T7/10', 'G06T7/11', 'G06V10/454', 'G06V10/764', 'G06V10/82', 'G06V20/56', 'G06V20/647', 'G06T2207/20084']"
US10424045B2,Machine learning model for automatic image registration quality assessment and correction,"A medical registration training component executing within a medical registration system performs a training medical registration operation on a pair of medical studies. Responsive to the medical registration training system determining that the training medical registration operation succeeds, the medical registration training system records a medical registration instance for the pair of medical studies in a medical registration history and marks the medical registration instance as a positive instance in the medical registration history. Responsive to the medical registration training system determining that the training medical registration operation requires correction, the medical registration training system records a medical registration instance for the pair of medical studies in the medical registration history and marks the medical registration instance as a negative instance in the medical registration history. The medical registration training system trains a failure prediction machine learning model based on the medical registration history using machine learning such that the failure prediction machine learning model predicts whether a new medical registration operation will require correction. Responsive to the failure prediction machine learning model predicting that the new medical registration operation will require correction, the mechanism takes steps to automatically correct the new medical registration operation.","['G06T3/153', 'G06T7/30', 'G06T7/0002', 'G06T3/0081', 'G06T7/0012', 'G06T7/11', 'G06T7/344', 'G06T7/35', 'G06T2200/32', 'G06T2207/10004', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10104', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20104', 'G06T2207/20221', 'G06T2207/30004', 'G06T2207/30168']"
US11003923B2,Spatial and temporal information for semantic segmentation,"Systems and methods for segmenting an image using a convolutional neural network are described herein. A convolutional neural network (CNN) comprises an encoder-decoder architecture, and may comprise one or more Long Short Term Memory (LSTM) layers between the encoder and decoder layers. The LSTM layers provide temporal information in addition to the spatial information of the encoder-decoder layers. A subset of a sequence of images is input into the encoder layer of the CNN and a corresponding sequence of segmented images is output from the decoder layer. In some embodiments, the one or more LSTM layers may be combined in such a way that the CNN is predictive, providing predicted output of segmented images. Though the CNN provides multiple outputs, the CNN may be trained from single images or by generation of noisy ground truth datasets. Segmenting may be performed for object segmentation or free space segmentation.","['G06N3/08', 'G06K9/00805', 'G06K9/00671', 'G06K9/00973', 'G06N3/044', 'G06N3/045', 'G06N3/0454', 'G06T7/11', 'G06T7/174', 'G06V10/764', 'G06V10/82', 'G06V10/94', 'G06V20/10', 'G06V20/20', 'G06V20/58', 'G06V20/70', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30241', 'G06T2207/30261']"
CN118305809B,"Method, device, equipment and medium for grabbing workpiece by using mechanical arm","The present disclosure provides a method, apparatus, device and medium for workpiece gripping using a robotic arm, comprising: acquiring workpiece image data and workpiece point cloud data acquired by a 3D camera; performing workpiece grabbing recognition on the workpiece image data to obtain image posture information of a target workpiece under an image plane coordinate system corresponding to the workpiece image data; determining point cloud attitude information of a target workpiece under a point cloud space coordinate system corresponding to the point cloud data of the workpiece based on the point cloud data of the workpiece and the image attitude information; converting a coordinate system of the point cloud attitude information to obtain grabbing attitude information corresponding to the mechanical arm; and sending the grabbing gesture information to the mechanical arm so that the mechanical arm can execute grabbing operation on the target workpiece from the workpiece material frame based on the grabbing gesture information. Therefore, the problem that workpieces are difficult to grasp due to stacking and tilting in the mechanical arm grabbing process is solved, and workpiece grabbing efficiency is effectively improved.",['B25J9/1697']
CN111464834B,"Video frame processing method and device, computing equipment and storage medium","The application relates to the technical field of artificial intelligence, and provides a video frame processing method, a video frame processing device, a computing device and a storage medium, which are used for reducing jitter among video frames. The method comprises the following steps: carrying out segmentation processing on a target object in a current frame to obtain a first image mask of the target object; according to a second image mask of the target object in at least the previous frame of the current frame, smoothing the first image mask to obtain a target image mask of the target object in the current frame; and obtaining a target object area corresponding to the target image mask in the current frame.","['H04N21/234', 'H04N21/44', 'H04N5/21']"
US20190355169A1,Semantic mapping for low-power augmented reality using dynamic vision sensor,"An apparatus includes a dynamic vision sensor (DVS) configured to output an asynchronous stream of sensor event data, a CMOS image sensor configured to output frames of image data, an inertial measurement unit (IMU), a processor and a memory. The memory contains instructions, which when executed by the processor, cause the apparatus to generate a semantic segmentation of a time-stamped frame, which is based on one or more of an output of the CMOS image sensor, or a synthesized event frame based on an output from the DVS and an output from the IMU over a time interval. The semantic segmentation includes a semantic label associated with a region of the time-stamped frame. When executed, the instructions further cause the apparatus to determine, based on the semantic segmentation, a simplified object representation in a coordinate space, and update a stable semantic map based on the simplified object representation.","['G01S5/16', 'G01S3/781', 'G02B27/017', 'G06K9/628', 'G06T15/005', 'G06T15/10', 'G06T19/006', 'G06T7/11', 'G06V10/147', 'G06V20/10', 'G06V20/20', 'G06V20/70', 'H04N23/65', 'H04N23/684', 'H04N25/00', 'H04N25/42', 'H04N25/443', 'H04N25/47', 'H04N25/60', 'H04N25/76', 'H04N25/766', 'H04N5/374', 'G06F18/2431', 'G06T2207/10028', 'G06T2207/20084', 'G06T2210/12']"
US11893468B2,Imitation learning system,"Apparatuses, systems, and techniques to identify a goal of a demonstration. In at least one embodiment, video data of a demonstration is analyzed to identify a goal. Object trajectories identified in the video data are analyzed with respect to a task predicate satisfied by a respective object trajectory, and with respect to motion predicate. Analysis of the trajectory with respect to the motion predicate is used to assess intentionality of a trajectory with respect to the goal.","['G06N3/008', 'G06N20/00', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/0985', 'G06N5/04', 'G06N3/045']"
CN112991447B,Visual positioning and static map construction method and system in dynamic environment,"The invention relates to the field of mobile robot positioning and map construction, in particular to a visual positioning and static map construction method and system in a dynamic environment. The method comprises the following steps: preprocessing an input color picture and a depth picture, and dividing the color image by adopting a lightweight real-time semantic segmentation network to obtain a mask of the object type of the potential dynamic object; ORB feature points of the color image are extracted, feature points of the dynamic object are removed by combining semantic priori knowledge and an improved motion consistency judging algorithm, and static feature points are reserved to participate in pose tracking calculation; and constructing a depth image containing a dynamic object mask by combining semantic priori knowledge and depth information, selecting a proper key frame to carry out space point cloud splicing and filtering, and obtaining a dense point cloud map of the static scene. The invention improves the accuracy and the real-time performance based on visual positioning in a dynamic environment, and provides a usable static scene dense point cloud map for planning and navigation of the robot.","['G06T7/73', 'G01C21/206', 'G01C21/32', 'G06T5/77', 'G06T5/80', 'G06T7/246', 'G06T7/55', 'G06T7/80', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20028', 'G06T2207/20081', 'G06T2207/20084', 'Y02T10/40']"
US11229488B2,Systems and methods for guidance of intraluminal devices within the vasculature,"System for guiding an instrument within a vascular network of a patient are disclosed. In some embodiments, the system receives a medical image from a medical imaging device and identifies a distal tip and a direction the instrument in the image. The system may then determine a waypoint for the distal tip of the instrument based at least in part on the position and direction of the distal tip of the instrument. The system may then generate a trajectory command for moving the instrument through the vascular network from the current position to the waypoint. The system may operate in a closed loop. The system may provide the trajectory command to a robotic medical system configured to move the instrument according to the command.","['A61B34/20', 'A61B34/25', 'A61B34/30', 'A61B34/35', 'G16H20/40', 'G16H30/40', 'G16H40/63', 'A61B2034/107', 'A61B2034/2065', 'A61B2034/301', 'A61B2034/303', 'A61B2090/064', 'A61B2090/376', 'A61B2090/3762', 'A61B2090/3966']"
US20220188999A1,Image enhancement method and apparatus,"This application relates to an image enhancement technology in the field of computer vision in the field of artificial intelligence, and provides an image enhancement method and apparatus. This application relates to the field of artificial intelligence, and specifically, to the field of computer vision. The method includes: adjusting a pixel value of a to-be-processed image, to obtain K images, where pixel values of the K images are different, and K is a positive integer greater than 1; extracting local features of the K images; extracting a global feature of the to-be-processed image; and performing image enhancement processing on the to-be-processed image based on the global feature and the local features, to obtain an image-enhanced output image. This method helps to improve the effect of image quality enhancement processing.","['G06T5/00', 'G06T5/92', 'G06F18/253', 'G06T5/001', 'G06T5/50', 'G06T5/60', 'G06T7/162', 'G06V10/26', 'G06V10/267', 'G06V10/30', 'G06V10/42', 'G06V10/44', 'G06V10/454', 'G06V10/806', 'G06V10/82', 'G06T2207/20072', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221']"
US11170569B2,System and method for virtual modeling of indoor scenes from imagery,A method for determining a visual scene virtual representation and a highly accurate visual scene-aligned geometric representation for virtual interaction.,"['G06T17/05', 'G06T17/00', 'G06T5/50', 'G06T7/12', 'G06T7/174', 'G06T7/50', 'G06T7/579', 'G06T2207/10028', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30244', 'G06T2210/04', 'G06T2210/61']"
US11724401B2,Grasp determination for an object in clutter,"Apparatuses, systems, and techniques determine a set of grasp poses that would allow a robot to successfully grasp an object that is proximate to at least one additional object. In at least one embodiment, the set of grasp poses is modified based on a determination that at least one of the grasp poses in the set of grasp poses would interfere with at least one additional object that is proximate to the object.","['B25J9/1697', 'B25J13/08', 'B25J9/161', 'B25J9/1612', 'B25J9/1666', 'G05B19/402', 'G05B19/4155', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/08', 'G06N3/088', 'G06N3/09', 'G06T7/10', 'G06T7/50', 'G06T7/70', 'G05B2219/39484', 'G05B2219/40269', 'G06N3/044', 'G06N3/063', 'G06T2207/10028', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30244']"
US11911903B2,Systems and methods for robotic picking and perturbation,"Various embodiments of the present technology generally relate to robotic devices and artificial intelligence. More specifically, some embodiments relate to a robotic device for picking items from a bin and perturbing items in a bin. The robotic device may include one or more picking elements and one or more perturbation elements for disturbing a present arrangement of items in the bin. In an exemplary embodiment, a perturbation element comprises a compressed air valve. In some implementations, the robotic device may also include one or more computer-vision systems. Based on image data from the one or more computer-vision systems, a strategy for picking up items from the bin is determined. When no strategies with high probability of success exist, the robotic device may perturb the contents of the bin to create new available pick-up points.","['B25J9/163', 'B25J9/161', 'B25J9/1697', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/092', 'B25J15/0658', 'G05B2219/39508', 'G05B2219/40014', 'G05B2219/40053', 'G05B2219/45063', 'G06N3/045', 'G06N3/047']"
TWI830107B,Encoding by indicating feature map data,"The present disclosure relates to methods and apparatuses for encoding data for (still or video processing into a bitstream). In particular, the data are processed by a network which includes a plurality of cascaded layers. In the processing, feature maps are generated by the layers. The feature maps processed (output) by at least two different layers have different resolutions. In the processing, a layer is selected, out of the cascaded layers, which is different from the layer generating the feature map of the lowest resolution (e.g. latent space). The bitstream includes information related to the selected layer. With this approach, scalable processing which may operate on different resolutions is provided so that the bitstream may convey information relating to such different resolutions. Accordingly, the data may be efficiently coded within the bitstream, depending on the resolution which may vary depending on the content of the picture data coded.","['H04N19/46', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/0495', 'G06N3/088', 'H04N19/132', 'H04N19/59', 'H04N19/70']"
US11302011B2,Perspective conversion for multi-dimensional data analysis,"Multi-dimensional data can be mapped to a projection shape and converted for image analysis. In some examples, the multi-dimensional data may include data captured by a LIDAR system for use in conjunction with a perception system for an autonomous vehicle. Converting operations can include converting three-dimensional LIDAR data to multi-channel two-dimensional data. Data points of the multi-dimensional data can be mapped to a projection shape, such as a sphere. Characteristics of the projection shape may include a shape, a field of view, a resolution, and a projection type. After data is mapped to the projection shape, the projection shape can be converted to a multi-channel, two-dimensional image. Image segmentation and classification may be performed on the two-dimensional data. Further, segmentation information may be used to segment the three-dimensional LIDAR data, while a rendering plane may be positioned relative to the segmented data to perform classification on a per-object basis.","['G01S7/4808', 'G01S17/89', 'G01S17/931', 'G01S7/4802', 'G06F18/24', 'G06K9/6267', 'G06T19/00', 'G06T19/20', 'G06T7/11', 'G06V10/26', 'G06V10/44', 'G06V10/7715', 'G06V10/82', 'G06V20/58', 'G06V20/64', 'G06T2207/10028', 'G06T2207/20084', 'G06T2210/56', 'G06V2201/12']"
US10962404B2,Systems and methods for weight measurement from user photos using deep learning networks,Disclosed are systems and methods for body weight prediction from one or more images. The method includes the steps of receiving one or more subject parameters; receiving one or more images containing a subject; identifying one or more annotation key points for one or more body features underneath a clothing of the subject from the one or more images utilizing one or more annotation deep-learning networks; calculating one or more geometric features of the subject based on the one or more annotation key points; and generating a prediction of the body weight of the subject utilizing a weight machine-learning module based on the one or more geometric features of the subject and the one or more subject parameters.,"['A61B5/0064', 'A61B5/1075', 'A61B5/1077', 'A61B5/1079', 'A61B5/1128', 'A61B5/7264', 'A61B5/7278', 'G01G19/44', 'G01G19/50', 'G06K9/00369', 'G06N20/00', 'G06N20/20', 'G06N3/045', 'G06N3/0464', 'G06N3/09', 'G06N5/01', 'G06T7/62', 'G06V10/454', 'G06V10/82', 'G06V40/103', 'G06T2200/04', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196']"
US10635927B2,Systems for performing semantic segmentation and methods thereof,"Performing semantic segmentation of an image can include processing the image using a plurality of convolutional layers to generate one or more feature maps, providing at least one of the one or more feature maps to multiple segmentation branches, and generating segmentations of the image based on the multiple segmentation branches, including providing feedback to, or generating feedback from, at least one of the multiple segmentation branches in performing segmentation in another of the segmentation branches.","['G06K9/4671', 'G06V10/82', 'G06F18/2413', 'G06K9/3241', 'G06K9/34', 'G06K9/4609', 'G06K9/4628', 'G06K9/627', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N7/005', 'G06N7/01', 'G06T7/11', 'G06V10/26', 'G06V10/454', 'G06V10/764', 'G06K9/00791', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30196', 'G06T2207/30261', 'G06V20/56']"
US11890063B2,System and methods for a trackerless navigation system,"Systems and methods for creating images of an environment includes controlling at least one camera to acquire imaging data from the environment and selecting, from the imaging data, a three-dimensional-two-dimensional correspondence as a control point for use in a perspective-n-point problem to determine a position and orientation of the at least one camera from n known correspondences between three-dimensional object points and their two-dimensional image projections in the environment. The method also includes reprojecting at least a selected number of the projections, determining a reprojection error for each of the projections, and performing a weight assignment of reprojection errors to distinguish the inliers form outliers. These steps of the method are repeated to apply the weight assignment to outliers in a decreased fashion during iterations to reduce an impact of outliers in the real-time display of the environment.","['A61B34/20', 'A61B5/065', 'G06T17/00', 'G06T7/73', 'G06V10/98', 'H04N13/204', 'A61B2034/2065', 'A61B2090/365', 'A61B2090/367', 'G06F3/147', 'G06T2207/10021', 'G06T2207/10056', 'G06T2207/10068', 'G06T2207/30004', 'G06T2207/30244', 'G09G2380/08']"
US11157742B2,Methods and systems for multiplayer tagging for ball game analytics generation with a mobile computing device,"Methods and systems for image clustering are described, and include determining a plurality of images from a video of a game, the video captured by a camera on a mobile device, where at least one image of the plurality of images is segmented from a video frame of the video; determining a feature vector from the at least one image; dividing the images into a first subset and a second subset based on the feature vector; tagging a first player in a first image of the first subset with an identifier, where the identifier differentiates the images in the first subset to a plurality of players; and identifying a second player in a second image in the second subset by propagating the identifier of the first subset, based on a distance measure associated with the feature vector. Running on a mobile computing device, this invention allows multiplayer tagging to be easily performed in almost any environment.","['G06K9/00724', 'G06T7/73', 'G06F18/23213', 'G06F18/24147', 'G06K9/00342', 'G06K9/6223', 'G06K9/6276', 'G06N20/00', 'G06N3/045', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/11', 'G06T7/20', 'G06T7/90', 'G06V10/763', 'G06V10/764', 'G06V20/42', 'G06V40/23', 'G06T2207/10016', 'G06T2207/20021', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30221']"
CN112750140B,Image segmentation method of camouflage target based on information mining,"The invention belongs to the technical field of scene segmentation in computer vision, and discloses a disguised target image segmentation method based on information mining. The PFNet comprises a multilayer feature extractor, a positioning module and a focusing module in sequence, wherein the multilayer feature extractor acquires context features of different grades by using a traditional feature extraction network; firstly, primarily determining the position of a disguised target in an image by using RGB characteristic information through a positioning module; and the focusing module is used for mining information and removing interference information on the basis of the RGB characteristic information and the preliminary position information of the image, and finally determining the boundary of the camouflage target step by step. The method of the invention introduces the concept of interference information into the problem of disguised target segmentation, develops a new information exploration and interference information removal strategy and helps to segment the disguised target image. From the results, the PFNet segmentation result is excellent, and the degree of fineness at the camouflaged object boundary is also satisfactory. Meanwhile, the method has wider applicability.","['G06T7/11', 'G06T7/12', 'G06N3/04', 'G06N3/08', 'G06T7/0002', 'G06T7/136', 'G06T7/194', 'G06T7/73', 'G06T2207/10024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30181']"
US10685455B2,Method and system for automated vehicle charging,"A method for facilitating automated vehicle charging, which can include one or more of: detecting a charge port cover, determining a cover actuation point, opening the cover, locating a charging connector of the vehicle, locating electrical pins of the charging connector, connecting to the vehicle, and/or disconnecting from the vehicle. A system for facilitating automated vehicle charging, which can include one or more connectors, sensors, and/or actuators. The system is preferably configured to perform the method (e.g., in the presence of an electric vehicle).","['B60L53/37', 'G06K9/00624', 'G06T7/13', 'G06T7/64', 'G06T7/73', 'G06V10/751', 'G06V20/00', 'H04N23/695', 'H04N5/23299', 'G06T2207/20084', 'G06T2207/30244', 'G06T2207/30252', 'Y02T10/70', 'Y02T10/7072', 'Y02T90/12', 'Y02T90/14']"
US20240009851A1,Grasp determination for an object in clutter,"Apparatuses, systems, and techniques determine a set of grasp poses that would allow a robot to successfully grasp an object that is proximate to at least one additional object. In at least one embodiment, the set of grasp poses is modified based on a determination that at least one of the grasp poses in the set of grasp poses would interfere with at least one additional object that is proximate to the object.","['B25J9/1697', 'B25J13/08', 'B25J9/161', 'B25J9/1612', 'B25J9/1666', 'G05B19/402', 'G05B19/4155', 'G06N3/08', 'G06T7/10', 'G06T7/50', 'G06T7/70', 'G05B2219/40269', 'G06T2207/10028', 'G06T2207/20084', 'G06T2207/20132', 'G06T2207/30244']"
US12079737B1,Data-mining and AI workflow platform for structured and unstructured data,"A method of data mining and AI workflow platform for structured and unstructured data is described. The method comprises receiving data from a data source, wherein the data comprises at least one data format of at least one of structured data, semi-structured data and unstructured data; indexing and analyzing the received data; scheduling and uploading automatically the data to a database as per the indexing; visualizing the data and determining at least one of the structured data, the semi-structured data, and the unstructured data from the data uploaded; cleansing and filtering the data based on at least one of an input from a user, and a predefined rule; labeling and annotating seamlessly the data available in the database; and building an artificial intelligence (AI) model based on at least one of the data available in the database, the input from the user, and the predefined rule.","['G06N5/04', 'G06F16/22', 'G06F16/2379', 'G06F16/26', 'G06F16/285', 'G06N20/00']"
CA2947817C,"Methods, systems, and devices relating to real-time object identification","The various embodiments disclosed herein relate to identification of one or more specific objects among several other objects. Certain exemplary implementations relate to systems, methods, and devices for identification of one or more specific plant species among several different plants, especially real-time identification of weed plant species in a real-world environment such as a crop field. Other embodiments relate to identification systems, methods, or devices in combination with action components, such as a spraying system for spraying the weed plants identified in real time.","['A01M21/043', 'A01M7/0089', 'B05B12/02', 'B05B12/122', 'G06F18/2413', 'G06T1/20', 'G06V10/955', 'G06T2207/10024', 'G06T2207/10028']"
US9367770B2,Methods and arrangements for identifying objects,"In some arrangements, product packaging is digitally watermarked over most of its extent to facilitate high-throughput item identification at retail checkouts. Imagery captured by conventional or plenoptic cameras can be processed (e.g., by GPUs) to derive several different perspective-transformed views—further minimizing the need to manually reposition items for identification. Crinkles and other deformations in product packaging can be optically sensed, allowing such surfaces to be virtually flattened to aid identification. Piles of items can be 3D-modelled and virtually segmented into geometric primitives to aid identification, and to discover locations of obscured items. Other data (e.g., including data from sensors in aisles, shelves and carts, and gaze tracking for clues about visual saliency) can be used in assessing identification hypotheses about an item. A great variety of other features and arrangements are also detailed.","['G06K9/78', 'G06F3/147', 'G06K7/10861', 'G06K7/1456', 'G06Q30/00', 'G06V10/147', 'G06V10/242', 'G06V10/243', 'G06V10/245', 'G06V10/30', 'G06V10/44', 'G06V10/464', 'G06V20/20', 'G06V20/52', 'G06V20/63', 'G06V20/64', 'G06V30/19173', 'G06V30/224', 'G06V30/2247', 'G07G1/0036', 'G07G1/0045', 'G07G1/0054', 'G07G1/0063', 'G06K2017/0051', 'G06K2017/0093', 'G06Q10/08', 'G06Q10/087', 'G06Q20/201', 'G06Q20/208', 'G06Q30/0601', 'G06T2201/0065', 'G06V10/40', 'G06V2201/09', 'G09G2340/12', 'G09G2380/04', 'G09G2380/06']"
CN107590813A,A kind of image partition method based on deep layer interactive mode geodesic distance,"A kind of image partition method based on deep layer interactive mode geodesic distance proposed in the present invention, its main contents include：Geodesic distance figure based on user mutual, convolutional neural networks structure using extension convolution retains resolution ratio, can backpropagation condition random field network, its process is, first check that initial segmentation proposes the initial segmentation result that network automatically obtains by user, and provide scribble or click on instruction erroneous segmentation, the variable in feature space is encoded with geodesic distance, and it is combined carry out semantic segmentation with random forests algorithm, then network structure is transformed, extension convolution is used in each piece, finally using can backpropagation condition random field network generation have freely in pairs potential and user constraint.The resolution ratio that the present invention provides more preferable dense prediction retains network, and user intervention is few, and the time used is less, improves accuracy and reliability；Meanwhile recovered edge details and reduced the noise in pixel classifications, effectively improve picture quality.","['G06T7/11', 'G06T7/0012', 'G06T7/194', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20104', 'G06T2207/30016']"
CN111931664B,"Mixed-pasting bill image processing method and device, computer equipment and storage medium","The invention discloses a processing method, a device, computer equipment and a storage medium for a mixed-pasting bill image, wherein the method comprises the following steps: acquiring a mixed-pasting bill image, wherein the mixed-pasting bill image comprises a plurality of bill objects; dividing the plurality of bill objects from the mixed-pasting bill image to obtain a bill sub-image corresponding to each bill object; determining an image feature vector and a text feature vector corresponding to a bill sub-image, and determining a target bill type according to the image feature vector and the text feature vector corresponding to the bill sub-image; determining a content field area in the bill sub-image and a field type label corresponding to the content field area based on a content field area detection model matched with the target bill type; and identifying the text information in the content field area, and determining the output of the text information according to the field category label corresponding to the content field area. The invention improves the output accuracy and precision of the text information of each bill in the mixed-pasting bill.","['G06V30/40', 'G06F18/214', 'G06F18/22', 'G06F18/24', 'G06N3/045', 'G06V10/44', 'G06V30/153', 'G06V30/10', 'Y02D10/00']"
US11615694B2,Clean surface sensor indicator and system,"Discloses is a workstation monitoring system. The system can include a camera configured to capture images and/or video of a workstation. The system can include a monitoring module configured to: receive images and/or video from the camera; identify a surface and a state for the surface, the state including any one or combination of occupied, vacant, clean, dirty, contaminated, attended to, or not attended to; track behavior of an individual, movement of an object, and/or an occurrence for the surface that causes a change in the surface's state; and generate a trigger event signal based on the change in the surface's state.","['G08B21/245', 'G08B21/22']"
US12293557B2,"Image processing method and device, equipment, and computer-readable storage medium","An image processing method and device, an equipment, and a computer-readable storage medium are provided. The method includes the following. An image is obtained. Object detection is performed on the image with an object detection network to obtain an object detection result. The object detection network is a network obtained by performing object detection on sample images in a sample image set based on at least one cluster center value with an initial object detection network and training iteratively the initial object detection network based on results of the object detection. The at least one cluster center value is obtained by clustering labeled regions in a labeled region set corresponding to the sample image set based on preset attribute information. The image is processed based on the object detection result.","['G06V10/25', 'G06N20/00', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06V10/762', 'G06V10/774', 'G06V10/82', 'G06V40/161', 'G06V40/165', 'G06V40/45']"
US11169614B2,"Gesture detection method, gesture processing device, and computer readable storage medium","A gesture detection method, a gesture processing device, and a computer readable storage medium. The gesture detection method includes: performing a shrinking process and a background modeling on a first frame of original image, to generate a first frame of background image; determining a motion region of the first frame of background image based on the first frame of background image; determining a first gesture detection area in the first frame of original image according to the motion region of the first frame of background image; and performing a gesture detection within the first gesture detection area.","['G06F3/017', 'G06V40/28', 'G06F3/011', 'G06F3/0304', 'G06K9/00335', 'G06K9/00355', 'G06T7/11', 'G06T7/13', 'G06T7/136', 'G06T7/174', 'G06T7/194', 'G06T7/248', 'G06T7/66', 'G06V10/25', 'G06V10/267', 'G06V10/34', 'G06V10/7715', 'G06V40/113', 'G06V40/20', 'G06T2207/10016']"
CN113792566B,A laser point cloud processing method and related equipment,"The embodiment of the application discloses a processing method of laser point cloud and related equipment, which can be applied to the field of laser perception in the field of automatic driving, and particularly can be applied to intelligent traveling agents (such as intelligent automobiles and intelligent network automobiles), and the method comprises the following steps: clustering the laser point clouds (e.g. clustering the laser point clouds in the OGM through DFS) to obtain N initial clustering clusters of coarse classification, performing semantic segmentation on the laser point clouds through a neural network (e.g. PointSeg, deepSeg) to obtain category labels of all the laser points, inquiring the category labels corresponding to all the laser points for each initial clustering cluster, and reprocessing (e.g. re-segmenting) all the initial clustering clusters according to the inquired category labels to obtain target clustering clusters corresponding to the target object. In the embodiment of the application, the problems of over-segmentation, under-segmentation and the like of the laser point cloud in laser perception are improved by carrying out semantic segmentation on the laser point cloud and combining a traditional clustering algorithm, so that the detection performance of the key obstacle is improved.","['G06F18/232', 'G05D1/0231', 'G06F18/23211', 'G06N3/045', 'G06N3/0464', 'G06N3/09']"
US20210004962A1,Generating effects on images using disparity guided salient object detection,"Systems, methods, and computer-readable media are provided for generating an image processing effect via disparity-guided salient object detection. In some examples, a system can detect a set of superpixels in an image; identify, based on a disparity map generated for the image, an image region containing at least a portion of a foreground of the image; calculate foreground queries identifying superpixels in the image region having higher saliency values than other superpixels in the image region; rank a relevance between each superpixel and one or more foreground queries; generate a saliency map for the image based on the ranking of the relevance between each superpixel and the one or more foreground queries; and generate, based on the saliency map, an output image having an effect applied to a portion of the output image.","['G06T7/11', 'G06F18/2113', 'G06K9/726', 'G06T5/003', 'G06T5/73', 'G06T7/194', 'G06T7/90', 'G06V10/25', 'G06V10/454', 'G06V10/771', 'G06V30/19173', 'G06V30/274', 'G06T2207/20084', 'G06T2207/20164']"
US10474858B2,"Methods of identifying barcoded items by evaluating multiple identification hypotheses, based on data from sensors including inventory sensors and ceiling-mounted cameras","A variety of technologies having practical application in retail stores are detailed. One is an improved method of identifying items selected by customers. This method includes receiving sensor data from plural sensors, including (a) ceiling-mounted cameras that monitor tracks of customers through aisles of the store, and (b) inventory sensors that are positioned to monitor removal of stock from store shelves. This received sensor data is employed in evaluating plural alternate item identification hypotheses. These hypotheses include a first hypothesis that a customer selected an item having a first identity, and a second hypothesis that the customer selected an item having a second identity. A confidence score is associated with each of the first and second item selection hypotheses. These confidence scores are refined as sensor data is received, e.g., increasing a confidence score of one hypothesis, and reducing a confidence score of another. Such refining continues until one of the hypotheses becomes a winner, due to an associated confidence score fulfilling a predetermined criterion (e.g., reaching a threshold value), at which time the item can be added to a tally for that individual. The winning item identification hypothesis may identify a barcoded item, without that item's barcode ever having been read by a barcode reader. A great number of other features and arrangements are also detailed.","['G06Q30/00', 'G06K7/10861', 'G06K7/1456', 'G06K9/00671', 'G06K9/18', 'G06K9/209', 'G06K9/78', 'G06Q10/087', 'G06Q20/201', 'G06Q20/203', 'G06Q20/208', 'G06V10/10', 'G06V10/147', 'G06V20/20', 'G06V20/52', 'G06V20/63', 'G06V30/224', 'G07G1/0036', 'G07G1/0045', 'G07G1/0063', 'G06F3/147', 'G06K9/46', 'G06Q10/08', 'G06Q30/0601', 'G06T2201/0065', 'G06V10/247', 'G06V10/40', 'G09G2380/04', 'G09G2380/06']"
US11055824B2,Hybrid machine learning systems,"A machine learning system for processing image data obtained from an image sensor is provided. The system includes a front end comprising one or more hard-coded filters, each of the one or more hard-coded filters being arranged to perform a set task. The system includes a neural network arranged to receive and process output from the front end. The one or more hard-coded filters include one or more hard-coded noise compensation filters that are hard-coded to compensate for a noise profile of the image sensor from which the image data is obtained. A method of processing image data in a machine learning system is also provided. A system for processing image data is provided.","['H04N5/21', 'G06V10/82', 'G06T5/002', 'G06F18/2413', 'G06F18/2451', 'G06F18/25', 'G06K9/00288', 'G06K9/4609', 'G06K9/4628', 'G06K9/4652', 'G06K9/627', 'G06K9/6286', 'G06K9/6288', 'G06N3/0464', 'G06N3/09', 'G06T5/70', 'G06T7/10', 'G06T7/44', 'G06V10/443', 'G06V10/454', 'G06V10/56', 'G06V10/764', 'G06V10/80', 'G06V40/172', 'H04N1/409', 'H04N1/58', 'G06N3/045', 'G06N3/0454', 'G06T2207/20081', 'G06T2207/20084']"
US8050454B2,Processing digital video using trajectory extraction and spatiotemporal decomposition,"Methods to process digital video using trajectory extraction and spatiotemporal decomposition for search and retrieval of video are described. An example method extracts interest point data from data representing a plurality of video frames. The interest point data is extracted from each of the video frames independent of the other video frames. Subsequent to extracting the interest point data, the example method links at least some of the interest point data to generate corresponding trajectory information. The example method also clusters the trajectory information to form clustered trajectory information and extracts a representative feature index from the clustered trajectory information.","['G06T7/246', 'G06F16/7837', 'G06F16/786', 'G06V10/462', 'G06V20/40', 'G06T2207/10016', 'G06T2207/30241']"
CN110796031B,Table identification method and device based on artificial intelligence and electronic equipment,"The present disclosure provides an artificial intelligence based form recognition method, an artificial intelligence based form recognition device and an electronic device, and mainly relates to techniques such as computer vision, natural language processing, machine learning, etc. The method comprises the following steps: image segmentation is carried out on the image to be identified based on image semantics to obtain a foreground line image; determining a plurality of table cells according to the line relation in the foreground line image, and establishing a spreadsheet according to the plurality of table cells; performing text recognition on the image to be recognized to obtain text information in the image to be recognized; and filling the text information into the electronic table according to the position of the text information in the image to be identified and the position of the table cell in the electronic table. The method can avoid mutual interference of the table structure and the table content in the identification process by dividing, identifying and recombining the table structure and the table content, thereby improving the identification accuracy of the table.","['G06V30/412', 'G06V30/414']"
US20220375193A1,Saliency-based object counting and localization,"Methods, systems, and computer programs are presented for adding new features to a network service. An example method includes accessing an image from a user device to determine a salient object count of a plurality of objects in the image. A salient object count of the plurality of objects in the image is determined. An indicator of the salient object count of the plurality of objects in the image is caused to be displayed on the user device.","['G06T7/11', 'G06F16/00', 'G06F18/251', 'G06K9/6289', 'G06N3/045', 'G06N3/0454', 'G06N3/084', 'G06T7/194', 'G06V10/462', 'G06V10/803', 'G06V20/10', 'G06N5/022', 'G06T2207/20084', 'G06T2207/30242']"
US10176196B2,"System, method and apparatus for scene recognition","An image processing system for recognizing the scene type of an input image generates an image distance metric from a set of images. The image processing system further extracts image features from the input image and each image in the set of images. Based on the distance metric and the extracted image features, the image processing system computes image feature distances for selecting a subset of images. The image processing system derives a scene type from the scene type of the subset of images. In one embodiment, the image processing system is a cloud computing system.","['G06F17/30244', 'G06V20/35', 'G06F16/122', 'G06F16/128', 'G06F16/164', 'G06F16/1734', 'G06F16/245', 'G06F16/50', 'G06F16/51', 'G06F16/951', 'G06F17/3028', 'G06F17/30424', 'G06F17/30864', 'G06F18/22', 'G06F18/24', 'G06K9/00248', 'G06K9/00268', 'G06K9/00288', 'G06K9/00302', 'G06K9/00624', 'G06K9/00684', 'G06K9/00979', 'G06K9/52', 'G06K9/6201', 'G06K9/6267', 'G06V10/95', 'G06V40/165', 'G06V40/168', 'G06V40/172', 'G06V40/174']"
US10831360B2,Telepresence framework for region of interest marking using headmount devices,"This disclosure relates generally to ROI marking, and more particularly to system and method for marking ROI in a media stream using touchless hand gesture interface such as headmount devices. In one embodiment, the method includes recognizing a pointing object representative of a gesture in frames of the media stream while capturing the media stream. The media stream comprises a scene captured from a first person view (FPV) of a user. Locus of the pointing object is detected in subsequent frames subsequent of the media stream to select a ROI in the media stream. The locus of the pointing object configures a bounding box around the ROI. The ROI is tracked in frames of the media stream occurring subsequent to the subsequent frames in the media stream. The bounding box is updated around the ROI based on the tracking, wherein the updated bounding box encloses the ROI.","['G06F3/017', 'G06F1/1686', 'G06F3/0304', 'G06F3/0487', 'G06K9/00355', 'G06K9/2081', 'G06K9/3233', 'G06T7/70', 'G06V10/235', 'G06V10/25', 'G06V40/28', 'H04N19/167', 'G06K9/00671', 'G06T2207/10016', 'G06V20/20']"
US12105773B2,Semantic relation preserving knowledge distillation for image-to-image translation,"GANs based generators are useful to perform image to image translations. GANs models have large storage sizes and resource use requirements such that they are too large to be deployed directly on mobile devices. Systems and methods define through conditioning a student GANs model having a student generator that is scaled downwardly from a teacher GANs model (and generator) using knowledge distillation. A semantic relation knowledge distillation loss is used to transfer semantic knowledge from an intermediate layer of the teacher to an intermediate layer of the student. Student generators thus defined are stored and executed by mobile devices such as smartphones and laptops to provide augmented reality experiences. Effects are simulated on images, including makeup, hair, nail and age simulation effects.","['G06F18/22', 'G06N3/088', 'G06N3/04', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/047', 'G06N3/0475', 'G06N3/0495', 'G06N3/09', 'G06N3/094', 'G06N3/096', 'G06N5/02', 'G06Q30/0631', 'G06Q30/0643', 'G06T19/006']"
US12106477B2,"Segmenting permeability, neovascularization, necrosis, collagen breakdown, or inflammation to characterize atherosclerotic plaque, coronary artery disease, or vasculopathy","Systems and methods for analyzing pathologies utilizing quantitative imaging are presented herein. Advantageously, the systems and methods of the present disclosure utilize a hierarchical analytics framework that identifies and quantify biological properties/analytes from imaging data and then identifies and characterizes one or more pathologies based on the quantified biological properties/analytes. This hierarchical approach of using imaging to examine underlying biology as an intermediary to assessing pathology provides many analytic and processing advantages over systems and methods that are configured to directly determine and characterize pathology from underlying imaging data.","['G16H50/20', 'A61B6/032', 'G06F18/211', 'G06F18/2148', 'G06F18/24', 'G06N20/00', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/096', 'G06T3/00', 'G06T5/73', 'G06T7/0012', 'G06T7/11', 'G06V10/25', 'G06V10/764', 'G06V10/82', 'G06V20/69', 'G16H30/40', 'G06T2207/10048', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/10101', 'G06T2207/10104', 'G06T2207/10108', 'G06T2207/10132', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30096', 'G06T2207/30104', 'G06V2201/03']"
US10977802B2,Motion assisted image segmentation,"Techniques and systems are provided for segmenting one or more frames. For example, incremental optical flow maps can be determined between adjacent frames of a plurality of frames. Using the incremental optical flow maps, a cumulative optical flow map can be determined between a first frame of the plurality of frames and a last frame of the plurality of frames. A segmentation mask can be determined using the first frame of the plurality of frames. Foreground pixels of the segmentation mask for the last frame of the plurality of frames can then be adjusted relative to corresponding foreground pixels for the first frame. The foreground pixels can be adjusted using the cumulative optical flow map between the first frame and the last frame of the plurality of frames.","['G06T7/215', 'G06T7/11', 'G06T7/174', 'G06T7/194', 'G06T7/269', 'G06T2207/10016', 'G06T2207/30201']"
US9799098B2,Method and apparatus for image processing,"Identifying objects in images is a difficult problem, particularly in cases an original image is noisy or has areas narrow in color or grayscale gradient. A technique employing a convolutional network has been identified to identify objects in such images in an automated and rapid manner. One example embodiment trains a convolutional network including multiple layers of filters. The filters are trained by learning and are arranged in successive layers and produce images having at least a same resolution as an original image. The filters are trained as a function of the original image or a desired image labeling; the image labels of objects identified in the original image are reported and may be used for segmentation. The technique can be applied to images of neural circuitry or electron microscopy, for example. The same technique can also be applied to correction of photographs or videos.","['G06T5/001', 'G06V10/267', 'G06K9/342', 'G06K9/4628', 'G06T5/60', 'G06T7/11', 'G06V10/454', 'G06T2207/20084']"
US20220083807A1,Generating labels for synthetic images using one or more neural networks,"Apparatuses, systems, and techniques to determine pixel-level labels of a synthetic image. In at least one embodiment, the synthetic image is generated by one or more generative networks and the pixel-level labels are generated using a combination of data output by a plurality of layers of the generative networks.","['G06N3/084', 'G06F18/2431', 'G06F18/214', 'G06K9/628', 'G06K9/6232', 'G06K9/6256', 'G06N3/045', 'G06N3/0454', 'G06N3/047', 'G06N3/088', 'G06T11/60', 'G06V10/774', 'G06N3/063', 'G06T2200/24']"
CN110490884B,Lightweight network semantic segmentation method based on countermeasure,"The invention relates to a countermeasure-based lightweight network semantic segmentation method which is used for solving the problems of low prediction accuracy, low network processing speed and difficulty in meeting the requirement of real-time prediction. The invention provides a countermeasure-based lightweight semantic segmentation method from the perspective of improving semantic segmentation speed and precision. Firstly, reducing parameter quantity in jump connection by utilizing asymmetric convolution, increasing feature map receptive field by using cavity convolution, improving network information acquisition capability by channel scrambling operation, and constructing a lightweight asymmetric coding and decoding semantic segmentation network; then, the recognition network is used for recognizing the segmented image and the calibrated semantic label by utilizing the countermeasure idea, a recognition loss function and a segmentation loss function are designed, and the segmentation network and the recognition network are alternately updated by a back propagation method until the recognition network cannot distinguish the labels generated by the segmentation network and the real labels, so that the semantic segmentation of the image is realized. The invention ensures real-time property of the segmentation network by utilizing the lightweight model and the countermeasure idea, and has higher segmentation precision.","['G06N3/045', 'G06N3/082', 'G06T7/11', 'G06T2207/20081', 'G06T2207/20084', 'Y02T10/40']"
CN108243623B,Automobile anti-collision early warning method and system based on binocular stereo vision,"The automobile anti-collision early warning method and system based on stereoscopic vision comprise the following steps: obtaining a disparity map by a binocular camera carried on a vehicle body; obtaining a V disparity map from the disparity map; binarizing the V disparity map; fitting to obtain a segmented straight line from points of the V disparity map by using a RANSAC method; smoothing a filtering straight line according to the multi-frame image; obtaining a travelable region in the original gray level image through the extracted straight line; calculating three-dimensional coordinates of points belonging to the ground in a real world coordinate system, and fitting a ground plane model by using RANSAC; converting the coordinates of the whole scene into world coordinates from a camera, generating a plan view, and solving an occupation map from the plan view; dividing the occupied map to obtain the position of each obstacle, and calculating the distance from the obstacle to the vehicle through the disparity map; and when the distance between the current vehicles is smaller than a certain threshold value, giving an alarm or making a further decision. The method is suitable for various road surfaces and road conditions, has low requirement on the precision of the parallax map, and does not depend on the influence caused by data and artificial design characteristics.","['G06V20/56', 'G06T7/00', 'G06V20/58']"
US10679355B2,System and method for detecting moving obstacles based on sensory prediction from ego-motion,"Described is a system for detecting moving objects. During operation, the system obtains ego-motion velocity data of a moving platform and generates a predicted image of a scene proximate the moving platform by projecting three-dimensional (3D) data into an image plane based on pixel values of the scene. A contrast image is generated based on a difference between the predicted image and an actual image taken at a next step in time. Next, an actionable prediction map is then generated based on the contrast mage. Finally, one or more moving objects may be detected based on the actionable prediction map.","['G06T7/20', 'G06V10/82', 'B60W30/09', 'G01P3/42', 'G01S17/50', 'G01S17/86', 'G01S17/89', 'G01S17/93', 'G01S17/931', 'G01S17/933', 'G05D1/024', 'G05D1/0251', 'G05D1/0253', 'G06F18/24133', 'G06K9/00805', 'G06K9/4628', 'G06K9/6271', 'G06T5/002', 'G06T5/70', 'G06T7/254', 'G06T7/292', 'G06T7/70', 'G06T7/74', 'G06V10/454', 'G06V10/764', 'G06V20/58', 'G08G1/04', 'G08G1/166', 'G08G5/04', 'G08G5/80', 'G05D1/0094', 'G05D2201/0213', 'G06T2207/10012', 'G06T2207/10016', 'G06T2207/10028', 'G06T2207/20076', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20221', 'G06T2207/30261', 'G08G5/0039', 'G08G5/34']"
US11995895B2,Multi-object tracking using correlation filters in video analytics applications,"In various examples, image areas may be extracted from a batch of one or more images and may be scaled, in batch, to one or more template sizes. Where the image areas include search regions used for localization of objects, the scaled search regions may be loaded into Graphics Processing Unit (GPU) memory and processed in parallel for localization. Similarly, where image areas are used for filter updates, the scaled image areas may be loaded into GPU memory and processed in parallel for filter updates. The image areas may be batched from any number of images and/or from any number of single- and/or multi-object trackers. Further aspects of the disclosure provide approaches for associating locations using correlation response values, for learning correlation filters in object tracking based at least on focused windowing, and for learning correlation filters in object tracking based at least on occlusion maps.","['G06T7/248', 'G06T7/292', 'G06F17/15', 'G06F18/2414', 'G06T1/20', 'G06T11/20', 'G06V10/764', 'G06V10/82', 'G06V20/10', 'G06V20/58', 'G06T2207/10021', 'G06T2207/10048', 'G06T2207/20084', 'G06T2207/30201', 'G06T2207/30256', 'G06T2207/30261', 'G06T2210/12']"
US10740694B2,System and method for capture and adaptive data generation for training for machine vision,"A computer-implemented method of performing machine vision prediction of digital images using synthetically generated training assets comprises digitally capturing a plurality of assets; configuring each of the assets in the plurality of assets with a plurality of asset attributes; under computer program control, selecting a plurality of different combinations of parameters from among the plurality of asset attributes, and creating a plurality of sets of different synthetic dataset parameters; using computer graphics software, and example parameter values from among the synthetic dataset parameters, creating a synthetic dataset by compiling from a plurality of example images and metadata; configuring a plurality of machine learning trials and executing the trials to train a machine vision model, resulting in creating and storing a trained machine vision model; executing a validation of the trained machine vision model; and inferring a prediction using the trained machine vision model. Trained models are scored against success criteria and re-trained using pseudo-random sampling of different parameters clustered around failure points. As a result, machine vision models may be trained with high accuracy using large datasets of synthesized digital images that are richly parameterized, rather than human captured digital images.","['G06N20/00', 'G06N5/04']"
US8638986B2,Online reference patch generation and pose estimation for augmented reality,"A reference patch of an unknown environment is generated on the fly for positioning and tracking. The reference patch is generated using a captured image of a planar object with two perpendicular sets of parallel lines. The planar object is detected in the image and axes of the world coordinate system are defined using the vanishing points for the two sets of parallel lines. The camera rotation is recovered based on the defined axes, and the reference patch of at least a portion of the image of the planar object is generated using the recovered camera rotation. The reference patch can then be used for vision based detection and tracking. The planar object may be detected in the image as sets of parallel lines or as a rectangle.","['G06T7/00', 'G01C11/00', 'G01C15/00', 'G01S3/786', 'G01S3/7864', 'G06T7/75', 'G06T2207/10016', 'G06T2207/30204', 'G06T2207/30244']"
US11430131B2,Foreground segmentation and nucleus ranking for scoring dual ISH images,"The present disclosure describes a method of foreground segmentation and nucleus ranking for scoring dual ISH images. The method has been developed to better identify those nuclei, within a selected field of view, that meet the criteria for dual ISH scoring.","['G06T7/194', 'G06T5/20', 'G06T7/0012', 'G06T7/0014', 'G06T7/11', 'G06T7/136', 'G06T7/90', 'G06V20/69', 'G06V20/695', 'G06T2200/24', 'G06T2207/10024', 'G06T2207/10056', 'G06T2207/20012', 'G06T2207/20021', 'G06T2207/30024', 'G06T2207/30242']"
US11127139B2,Enhanced semantic segmentation of images,"Enhanced methods and systems for the semantic segmentation of images are described. A refined segmentation mask for a specified object visually depicted in a source image is generated based on a coarse and/or raw segmentation mask. The refined segmentation mask is generated via a refinement process applied to the coarse segmentation mask. The refinement process correct at least a portion of both type I and type II errors, as well as refine boundaries of the specified object, associated with the coarse segmentation mask. Thus, the refined segmentation mask provides a more accurate segmentation of the object than the coarse segmentation mask. A segmentation refinement model is employed to generate the refined segmentation mask based on the coarse segmentation mask. That is, the segmentation model is employed to refine the coarse segmentation mask to generate more accurate segmentations of the object. The refinement process is an iterative refinement process carried out via a trained neural network.","['G06T7/11', 'G06N3/045', 'G06N3/0454', 'G06N3/0455', 'G06N3/0464', 'G06N3/084', 'G06N3/09', 'G06T2207/10024', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084']"
US10115199B2,Image based object locator,"Techniques, systems, and devices are disclosed for analyzing a reconstructed charged particle image of a volume of interest from charged particle detector measurements to determine a location and boundaries of one or more objects or an orientation of the one or more objects. The technique can include performing a segmentation operation on the reconstructed charged particle image of the volume. The segmentation operation identifies a subset of a set of voxels of the image of the volume as object candidate voxels. The technique can include locating corners of the one or more objects to determine the location, boundaries, or the orientation of the one or more objects. The technique can also include the computation of the center of mass of the one or more objects. The technique can include performing a morphological operation on the image and can include performing a connected-component analysis on the identified object-candidate voxels.","['G06T7/0042', 'G01V5/0016', 'G01V5/22', 'G06F18/24', 'G06K9/2054', 'G06K9/44', 'G06K9/6267', 'G06T11/008', 'G06T5/30', 'G06T7/12', 'G06T7/155', 'G06T7/73', 'G06T2207/10072', 'G06T2207/20036', 'G06T2207/30112']"
US20230079018A1,Deep learning-based detection of item sizes for autonomous checkout in a cashier-less shopping store,"Systems and methods for detecting a size of an inventory item in an area of real space are disclosed. The method includes receiving respective sequences of images of corresponding fields of view in the area of real space from a plurality of sensors. The method includes determining a two-dimensional plane in the area of real space by matching at least three points on a first image obtained by a first sensor and a second image obtained by a second sensor. The method includes selecting an inventory item, from the first image, for which a size is to be determined. The method includes warping the first image onto the two-dimensional plane. The method includes cropping out the selected inventory item from the warped image. The method includes, inputting, the cropped out image of the inventory item and another image of an inventory item to a trained size determination model.","['G06T7/60', 'G06Q10/087', 'G06F16/55', 'G06F16/75', 'G06F18/24', 'G06Q10/08', 'G06Q10/0875', 'G06Q30/0223', 'G06T3/0093', 'G06T3/18', 'G06T3/60', 'G06T7/0002', 'G06T7/20', 'G06T7/246', 'G06T7/251', 'G06T7/73', 'G06V10/12', 'G06V10/242', 'G06V10/243', 'G06V10/25', 'G06V10/26', 'G06V10/74', 'G06V10/7515', 'G06V10/764', 'G06V10/774', 'G06V20/52', 'G06V30/224', 'G06V40/10', 'G06V40/28', 'G06N20/00', 'G06N3/02', 'G06Q10/0833', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20132', 'G06T5/60', 'G06T5/80', 'G06T7/292', 'G06V10/82', 'G06V20/68', 'G06V20/70', 'G06V30/10', 'G06V30/1463', 'G06V30/1473', 'G06V30/1475']"
US20210157998A1,Methods and arrangements for identifying objects,"In some arrangements, product packaging is digitally watermarked over most of its extent to facilitate high-throughput item identification at retail checkouts. Imagery captured by conventional or plenoptic cameras can be processed (e.g., by GPUs) to derive several different perspective-transformed views—further minimizing the need to manually reposition items for identification. Crinkles and other deformations in product packaging can be optically sensed, allowing such surfaces to be virtually flattened to aid identification. Piles of items can be 3D-modelled and virtually segmented into geometric primitives to aid identification, and to discover locations of obscured items. Other data (e.g., including data from sensors in aisles, shelves and carts, and gaze tracking for clues about visual saliency) can be used in assessing identification hypotheses about an item. Logos may be identified and used—or ignored—in product identification. A great variety of other features and arrangements are also detailed.","['G06Q10/087', 'G06K7/10861', 'G06K7/1456', 'G06K9/00671', 'G06K9/18', 'G06K9/209', 'G06K9/78', 'G06Q20/201', 'G06Q20/208', 'G06Q30/00', 'G06V10/10', 'G06V10/147', 'G06V10/242', 'G06V10/243', 'G06V10/245', 'G06V10/30', 'G06V10/44', 'G06V10/464', 'G06V20/20', 'G06V20/52', 'G06V20/63', 'G06V20/64', 'G06V30/19173', 'G06V30/224', 'G06V30/2247', 'G07G1/0036', 'G07G1/0045', 'G07G1/0054', 'G07G1/0063', 'G06F3/147', 'G06Q10/08', 'G06Q30/0601', 'G06T2201/0065', 'G06V10/40', 'G06V2201/09', 'G09G2340/12', 'G09G2380/04', 'G09G2380/06']"
CN110231036B,Robot positioning device and method based on cross laser and machine vision,"The invention discloses a robot positioning device and method based on cross laser and machine vision, comprising the following steps: the system comprises a first robot, a second robot, a cross laser, a camera, a cross laser line, a vision processing system and a second robot control system; the cross laser is arranged on the first robot, the cross laser irradiates cross laser lines on the ground, and the camera is arranged on the second robot; according to the invention, the camera is used for collecting and processing the image of the cross laser line to obtain the offset angle and the offset, the vision processing system outputs the calculation result to the second robot control system to enable the second robot to carry out pose adjustment relative to the first robot, so that the purpose of accurately grabbing materials or conveying materials is achieved, the defect of navigation accuracy of the traditional laser radar navigation technology is overcome, the cost is reduced, and the operation convenience is improved.","['G01C21/005', 'G01C21/20', 'G01S17/86']"
US10176382B1,Method and apparatus for sparse associative recognition and recall for visual media reasoning,"Described is system and method for visual media reasoning. An input image is filtered using a first series of kernels tuned to represent objects of general categories, followed by a second series of sparse coding filter kernels tuned to represent objects of specialized categories, resulting in a set of sparse codes. Object recognition is performed on the set of sparse codes to generate object and semantic labels for the set of sparse codes. Pattern completion is performed on the object and semantic labels to recall relevant meta-data in the input image. Bi-directional feedback is used to fuse the input data with the relevant meta-data. An annotated image with information related to who is in the input image, what is in the input image, when the input image was captured, and where the input image was captured is generated.","['G06V20/41', 'G06K9/00751', 'G06F18/2136', 'G06F18/2413', 'G06F18/251', 'G06K9/00718', 'G06T7/0083', 'G06T7/0095', 'G06T7/12', 'G06T7/168', 'G06V10/764', 'G06V10/7715', 'G06V10/803', 'G06V10/82', 'G06T2207/20016', 'G06T2207/20024', 'G06T2207/20084', 'G06T2210/12', 'G06V10/513']"
US20210073953A1,Method for applying bokeh effect to image and recording medium,A method for applying a bokeh effect on an image at a user terminal is provided. The method for applying a bokeh effect may include: receiving an image and inputting the received image to an input layer of a first artificial neural network model to generate a depth map indicating depth information of pixels in the image; and applying the bokeh effect on the pixels in the image based on the depth map indicating the depth information of the pixels in the image. The first artificial neural network model may be generated by receiving a plurality of reference images to the input layer and performing machine learning to infer the depth information included in the plurality of reference image.,"['G06T5/002', 'G06T5/70', 'G06T5/20', 'G06F3/04845', 'G06N3/045', 'G06N3/0454', 'G06N3/084', 'G06T11/001', 'G06T5/60', 'G06T5/80', 'G06T7/11', 'G06T7/194', 'G06T7/50', 'G06T7/571', 'G06N3/08', 'G06T2200/21', 'G06T2207/10004', 'G06T2207/20024', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/20224', 'G06T2207/30196']"
US10810436B2,System and method for machine-assisted segmentation of video collections,"According to various embodiments, a system for accessing video content is disclosed. The system includes one or processors on a video hosting platform for hosting the video content, where the processors are configured to generate an automated transcription of the video content and apply text clustering modules based on a trained neural network to segment the video content.","['G06K9/00765', 'G06V10/764', 'G06F16/7328', 'G06F16/75', 'G06F16/7844', 'G06K9/00751', 'G06V10/82', 'G06V20/47', 'G06V20/49']"
US6630937B2,Workstation interface for use in digital mammography and associated methods,"A workstation-user interface for evaluating computer assisted diagnosis (CAD) methods for digital mammography is disclosed. Implementation of such an interface enables multiple, large-size images to be handled at high speeds. Furthermore, controls such as contrast, pan, and zoom, and tools such as reporting forms, case information, and analysis of results are included. The software and hardware used to develop such a workstation and interface were based on Sun platforms and the Unix operating system. The software is user friendly, and comparable to standard mammography film reading in terms of display layout and speed. The software, as designed, will work on entry-level workstations as well as high-end workstations with specialized hardware, thus being usable in an educational, training, or clinical environment for annotation purposes using CAD techniques as well as primary diagnosis.","['G06T7/0012', 'G01N23/046', 'G16H30/20', 'G16H40/63', 'G01N2223/419', 'G01N2223/612']"
CN109146948B,Vision-based analysis method of crop growth phenotype parameter quantification and yield correlation,"The invention belongs to the technical field of agriculture, and discloses a crop growth phenotype parameter quantification and yield correlation analysis method based on vision, which provides more ideas for agricultural analysis through continuous breakthroughs in technologies such as image processing, wireless sensors and artificial intelligence, so that the traditional manual agriculture and mechanized agriculture are continuously developed to modern intelligent and refined agriculture. The method is one of important analysis subjects of the agricultural field, namely intelligent and refined agriculture, and has important significance in the fields of plant virtualization, yield prediction, garden design and the like; the analysis of the characterization parameters and morphological characteristics in the growth process of crops has great significance: on one hand, the method can predict the relevant parameters of crops and has a guiding effect on the configuration of the cultivation environment; the virtual plant growth technology can simulate the plant growth state at any time, thereby reducing the analysis time and the analysis cost.","['G06T7/60', 'G06T7/10', 'G06T2207/10004', 'G06T2207/20152']"
US20210374502A1,Technique to perform neural network architecture search with federated learning,"Apparatuses, systems, and techniques to select a nueral network architecture from a plurality of neural networs in a federated learning (FL) settng. In at least one embodiment, a neural network is trained by combining training resutls from different FL computing systesms, where each of the different FL computing systems, for example, trains different portions of the nerual network.","['G06N3/0454', 'G06N3/045', 'G06N3/063', 'G06N3/08', 'G06T7/10', 'G06V10/95', 'G06V10/955', 'G06N3/084', 'G06T2207/20081', 'G06T2207/20084', 'G06V20/56', 'G06V20/695']"
US12062249B2,System and method for generating image landmarks,"A system, neural network, and corresponding method generate 3D landmarks associated with an object in a 2D image. An embodiment is a system comprising a neural network detector configured to produce planar coordinates of landmarks at points of the object in the 2D image and a depth coordinate estimator. The planar coordinates include planar coordinate pairs. The depth coordinate estimator is configured to receive the 2D image and the planar coordinates and to estimate a depth coordinate for each planar coordinate pair of each landmark to generate the 3D landmarks. The system reduces network parameters from MB to KB and has better performance relative to state-of-the-art methods. The system may be configured to apply the 3D landmarks for face alignment, virtual face makeup, face recognition, eye gaze tracking, face synthesis, or other face related application.","['G06V40/165', 'G06F18/21', 'G06N3/04', 'G06N3/0464', 'G06N3/0495', 'G06N3/09', 'G06T7/50', 'G06T7/70', 'G06V10/454', 'G06V40/171', 'G06T2207/20084', 'G06T2207/30201']"
US9852344B2,Systems and methods for semantically classifying and normalizing shots in video,"The present disclosure relates to systems and methods for classifying videos based on video content. For a given video file including a plurality of frames, a subset of frames is extracted for processing. Frames that are too dark, blurry, or otherwise poor classification candidates are discarded from the subset. Generally, material classification scores that describe type of material content likely included in each frame are calculated for the remaining frames in the subset. The material classification scores are used to generate material arrangement vectors that represent the spatial arrangement of material content in each frame. The material arrangement vectors are subsequently classified to generate a scene classification score vector for each frame. The scene classification results are averaged (or otherwise processed) across all frames in the subset to associate the video file with one or more predefined scene categories related to overall types of scene content of the video file.","['G06V20/41', 'G06K9/00718', 'G06F18/22', 'G06K9/00751', 'G06K9/52', 'G06K9/6215', 'G06T7/174', 'G06V20/47', 'G06T2207/10016', 'G06T2207/20021']"
CN106530305B,Semantic segmentation model training and image partition method and device calculate equipment,"The invention discloses a kind of semantic segmentation model training and image partition method and device, calculate equipment, belong to technical field of computer vision, wherein method includes: that training image is input to semantic segmentation model, obtains the PRELIMINARY RESULTS of the semantic segmentation of the training image of semantic segmentation model output；The multiple local candidate regions selected according to Weakly supervised information and from training image, carry out local candidate region fusion, obtain the correction result of the semantic segmentation of training image；According to PRELIMINARY RESULTS and correction as a result, being modified to the model parameter of semantic segmentation model；Iteration executes training step until the training result of the semantic segmentation model meets predetermined convergence condition.The direct supervision of the existing pixel scale of the present invention program, and semantic segmentation model can be optimized end to end, and the result of segmentation branch can be improved according to the judgement to local candidate region.","['G06T2207/10004', 'G06T2207/20021', 'G06T2207/20081']"
US10467500B1,Method and system for semantic segmentation involving multi-task convolutional neural network,"Methods and systems involving convolutional neural networks as applicable for semantic segmentation, including multi-task convolutional networks employing curriculum based transfer learning, are disclosed herein. In one example embodiment, a method of semantic segmentation involving a convolutional neural network includes training and applying the convolutional neural network. The training of the convolutional neural network includes each of training a semantic segmentation decoder network of the convolutional neural network, generating first feature maps by way of an encoder network of the convolutional neural network, based at least in part upon a dataset received at the encoder network, and training an instance segmentation decoder network of the convolutional neural network based at least in part upon the first feature maps. The applying includes receiving an image, and generating each of a semantic segmentation map and an instance segmentation map in response to the receiving of the image, in a single feedforward pass.","['G06K9/6232', 'G06N3/084', 'G06V10/82', 'G06F18/2148', 'G06K9/6257', 'G06K9/726', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06N3/096', 'G06T7/10', 'G06T7/11', 'G06T9/002', 'G06V10/7747', 'G06V30/274', 'G06K2209/21', 'G06K2209/27', 'G06T2207/20081', 'G06T2207/20084', 'G06T7/168', 'G06V2201/07', 'G06V2201/10']"
CN112906649B,"Video segmentation method, device, computer device and medium","The application discloses a video segmentation method, video segmentation equipment, computer equipment and a medium. The method comprises the following steps: dividing the video into segments based on correlation coefficients between adjacent video frames in the video; for the video frames in the fragments, identifying the scenes of the video frames to obtain scene feature vectors; for the video frames in the fragments, identifying local behavior characteristics of the video frames to obtain local behavior characteristic vectors; identifying a behavior category of the video frame and a confidence level corresponding to the behavior category based on the scene feature vector and the local behavior feature vector; determining the behavior category of the segment based on the behavior category and the confidence level of the video frame of the segment; and merging the segments with the same adjacent behavior categories to obtain the segmentation result of the video. The method can fuse the two-way models at the same time, comprehensively utilizes two dimensions of the scene and the local behavior, and extracts the whole behavior information, thereby rapidly segmenting the video.","['G06V20/49', 'G06F18/2411', 'G06F18/25', 'G06V20/46']"
US12223655B2,Applications of automatic anatomy recognition in medical tomographic imagery based on fuzzy anatomy models,"A computerized method of providing automatic anatomy recognition (AAR) includes gathering image data from patient image sets, formulating precise definitions of each body region and organ and delineating them following the definitions, building hierarchical fuzzy anatomy models of organs for each body region, recognizing and locating organs in given images by employing the hierarchical models, and delineating the organs following the hierarchy. The method may be applied, for example, to body regions including the thorax, abdomen and neck regions to identify organs.","['G06T7/11', 'A61N5/1039', 'G06T7/0012', 'G06T7/136', 'G06T7/143', 'G06T7/70', 'G06V10/457', 'A61N5/103', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/20016', 'G06T2207/20128', 'G06T2207/30004', 'G06V2201/031']"
CN110442869B,"Medical text processing method and device, equipment and storage medium thereof","The application provides a medical text processing method, a device, equipment and a storage medium thereof, wherein the method comprises the following steps: acquiring a medical entity in a medical text to be processed, and matching the medical entity with a first language medical ontology base based on an artificial intelligence matching model; when the target medical terms which accord with the matching conditions with the medical entities are not matched in the first language medical ontology base, performing word segmentation processing on the medical entities to obtain medical entity word segments; mapping medical entity participles with synonyms in the medical entities into corresponding synonyms to obtain mapped medical entities; and determining a concept identifier corresponding to the medical entity based on the target medical term which is in the first language medical ontology base and accords with the matching condition with the mapped medical entity, and establishing a mapping relation between the medical entity and the concept identifier. Through the application, the medical entity can be mapped to the medical entity by taking the mature medical entity as a carrier, so that the normalization work of the medical entity is realized.","['G06F16/355', 'G16H10/60']"
US12062114B2,"Method, system, and device of virtual dressing utilizing image processing, machine learning, and computer vision","A computerized method, system and computer program product may comprise: generating or receiving a search result in response to a user query by a user that searches for an item, the search result depicting the item; and modifying the search result by generating a combination image that depicts user-defined subject matter engaging with the item. The present invention may provide other and/or additional benefits or advantages.","['G06T11/00', 'G06F16/538', 'G06F16/583', 'G06N20/00', 'G06T19/00', 'G06T7/11', 'G06T7/194', 'G06V40/10', 'G06N20/10', 'G06N7/01']"
US9274551B2,Method and apparatus for data entry input,"An apparatus and method of inputting data for an electronic data entry device are provided. In one embodiment, main keys on a keyboard of a device are used in which the main keys have multiple symbols or functions associated therewith, and one or more assisting keys of the device are used to select between the symbols or functions on the main keys. In a further embodiment, identification of an input object such as the particular fingers of a user that are used to actuate a key region is performed. The symbol associated with the actuated key region and the finger (or other input object) used is determined. Various types of sensors and recognition technology are implemented in order to identify fingers (or other input objects) in various embodiments.","['G06F3/023', 'G06F1/1616', 'G06F1/1624', 'G06F1/1662', 'G06F1/1673', 'G06F3/0233', 'G06F3/0235', 'G06F3/0237', 'G06F3/0416', 'G06F3/04166', 'G06F3/0418', 'G06F3/0426', 'G06F3/0428', 'G06F3/04883', 'G06F3/04886']"
US8577705B1,Method and system for rating the role of a product category in the performance of a store area,"The present invention is a method and system for measuring a set of shopper behavior metrics that represent the strength of a product category or a group of categories in the performance of a store area. A set of rating parameters are defined in order to provide a unified and standardized rating system. The rating system represents the effectiveness of the product category in a store area. The metrics are defined in a manner that is normalized so that they can be used across different types of product categories. The datasets are measured per category or group of categories over time to identify how the strength has varied over time, and to monitor trends in the category performance. The measured datasets are further analyzed based on various demographic groups and behavior segments. The analysis facilitates a better understanding of the strength of the category for different shopper segments, which in turn can be applied for developing better store area optimization strategies.",['G06Q30/02']
CN113888744B,Image semantic segmentation method based on transform visual up-sampling module,"The invention discloses an image semantic segmentation method based on a transform visual up-sampling module. According to the invention, an up-sampling module for carrying out feature map based on a visual transducer module is introduced into an image semantic segmentation task, and part of feature map information stored originally is fused into the up-sampling process as up-sampling auxiliary information. Compared with the original traditional up-sampling module, the method avoids the operation of 0 supplementing in unknown information and simultaneously avoids chessboard effects caused by deconvolution and the like. The innovation process relying on windowed downsampling solves the embarrassment that the deep feature map pays attention to global semantic information and loses local detail information, brings more excellent detail information to the transform extraction of the local window, simultaneously solves the influence caused by insufficient computing power, provides possibility for the transform to solve the large-scale problem, and can improve the performance of image semantic segmentation.","['G06F18/24', 'G06F18/253', 'G06N3/045', 'G06N3/048', 'G06N3/08']"
US11379987B2,Image object segmentation based on temporal information,"A temporal object segmentation system determines a location of an object depicted in a video. In some cases, the temporal object segmentation system determines the object's location in a particular frame of the video based on information indicating a previous location of the object in a previous video frame. For example, an encoder neural network in the temporal object segmentation system extracts features describing image attributes of a video frame. A convolutional long-short term memory neural network determines the location of the object in the frame, based on the extracted image attributes and information indicating a previous location in a previous frame. A decoder neural network generates an image mask indicating the object's location in the frame. In some cases, a video editing system receives multiple generated masks for a video, and modifies one or more video frames based on the locations indicated by the masks.","['G06T7/11', 'G06T7/246', 'G06T7/194', 'G06T7/73', 'G06T2207/10016', 'G06T2207/20081', 'G06T2207/20084', 'G11B27/031']"
US10354392B2,Image guided video semantic object segmentation method and apparatus,"The disclosure involves an image guided video semantic object segmentation method and apparatus, locate a target object in a sample image to obtain an object sample; extract a candidate region from each frame; match multiple candidate regions extracted from the each frame with the object sample to obtain a similarity rating of each candidate region; rank the similarity rating of each candidate region to select a predefined candidate region number of high rating candidate region ranked by the similarity; preliminarily segment a foreground and a background from the selected high rating candidate region; construct an optimization function for the preliminarily segmented foreground and background; solve the optimization function to obtain a optimal candidate region set; and propagate a preliminary foreground segmentation corresponding to the optimal candidate region to an entire video to obtain a semantic object segmentation of the input video.","['G06T7/194', 'G06K9/00744', 'G06K9/34', 'G06K9/38', 'G06K9/4604', 'G06K9/6202', 'G06T7/11', 'G06T7/143', 'G06T7/174', 'G06V10/26', 'G06V10/28', 'G06V10/751', 'G06V20/46', 'G06T2207/10016']"
US8010402B1,Method for augmenting transaction data with visually extracted demographics of people using computer vision,"The present invention is a system and framework for augmenting any retail transaction system with information about the involved customers. This invention provides a method to combine the transaction data records and a customer or a group of customers with the automatically extracted demographic features (e.g., gender, age, and ethnicity), shopping group information, and behavioral information using computer vision algorithms. First, the system detects faces from face view, tracks them individually, and estimates poses of each of the tracked faces to normalize. These facial images are processed by the demographics classification module to determine and record the demographics feature vector. The system detects and tracks customers to analyze the dynamic behavior of the tracked customers so that their shopping group membership and checkout behavior can be recognized. Then the instances of faces and the instances of bodies can be matched and combined. Finally, the transaction data from the transaction data and the demographics, group, and checkout behavior data that belong to the same person or the same group of people are combined.","['G06Q10/00', 'G06Q30/00', 'G06Q30/0201', 'G06Q30/0251', 'G06Q99/00']"
US11704888B2,Product onboarding machine,A method for generating training examples for a product recognition model is disclosed. The method includes capturing images of a product using an array of cameras. A product identifier for the product is associated with each of the images. A bounding box for the product is identified in each of the images. The bounding boxes are smoothed temporally. A segmentation mask for the product is identified in each bounding box. The segmentation masks are optimized to generate an optimized set of segmentation masks. A machine learning model is trained using the optimized set of segmentation masks to recognize an outline of the product. The machine learning model is run to generate a set of further-optimized segmentation masks. The bounding box and further-optimized segmentation masks from each image are stored in a master training set with its product identifier as a training example to be used to train a product recognition model.,"['G06V10/25', 'G06F18/214', 'G06F18/217', 'G06V10/34', 'G06V10/774', 'G06V20/653', 'G06V2201/10']"
CN110889851B,Robust use of semantic segmentation for depth and disparity estimation,"The present disclosure relates to robust use of semantic segmentation for depth and disparity estimation. The present disclosure relates to techniques for generating robust depth estimates of captured images using semantic segmentation. Semantic segmentation may be defined as the process of creating a mask over an image, where pixels are segmented into a predefined set of semantic categories. Such segmentation may be binary (e.g., ""human pixels"" or ""non-human pixels"") or multi-class (e.g., pixels may be labeled as ""human"", ""dog"", ""cat"", etc.). With the increase in accuracy and adoption of semantic segmentation techniques, it is becoming increasingly important to develop methods that utilize such segmentation and develop flexible techniques for integrating segmentation information into existing computer vision applications (such as depth and/or disparity estimation) to produce improved results in a wide range of image capture scenarios. In some implementations, an optimization framework may be used to optimize an initial scene depth/disparity estimate for a camera device that employs both semantic segmentation and color regularization in a robust manner.","['G06T7/194', 'H04N13/271', 'G06T7/10', 'G06N3/045', 'G06N3/08', 'G06T5/20', 'G06T5/70', 'G06T7/11', 'G06T7/50', 'H04N13/128', 'H04N13/15', 'H04N13/156', 'G06T2207/20081', 'G06T2207/20084', 'H04N2013/0077', 'H04N2013/0081', 'H04N2013/0092']"
CN112837257B,Curved surface label splicing detection method based on machine vision,"The invention provides a machine vision-based curved label splicing detection method, belongs to the technical field of vision detection, and is used for solving the problem of detection of cylindrical beverage bottle labels. The method comprises the following steps of multi-camera combined calibration, curved surface label splicing and detection technology, four-camera combined calibration, expansion of a cylindrical object into a plane, splicing and fusion of plane label images of all cameras in a characteristic extraction mode, and obtaining spliced label plane images. According to the machine vision-based curved surface label splicing detection method, detection and control strategy research on appearance labels, surface dirt and label damage of cylindrical packaging products is realized through the machine vision-based detection method, and the quality efficiency of a production line is greatly improved.","['G06T7/0004', 'G06T3/4038', 'G06T7/136', 'G06T7/187', 'G06T7/70', 'G06T7/80', 'G06T7/90', 'G06V10/141', 'G06V10/30', 'G06V20/10', 'G06V20/62', 'G06V30/146', 'G06T2207/20016', 'G06T2207/20221', 'Y02B20/40', 'Y02P90/30']"
CN112328730B,"Map data updating method, related device, equipment and storage medium","The application discloses a map data updating method based on an artificial intelligence technology, which comprises the steps of obtaining vehicle data; generating an anchor point according to the vehicle data, wherein the electronic map comprises target original map data, and the target original map data comprises nodes and node connecting edges; generating a fused driving track according to the anchor points and the vehicle data; acquiring target map data according to the fused driving track, wherein the target map data comprises updated nodes and updated node connecting edges; and updating the target original map data into target map data. The application also provides a related device, equipment and a storage medium. According to the method and the device, anchor points can be constructed on the electronic map based on a large amount of vehicle data, the anchor points and a large amount of vehicle data are further combined to generate a fusion driving track, namely, the driving track can be optimized by using the fixed anchor points, and therefore more accurate map data can be obtained.","['G06F16/29', 'G01C21/32']"
US20180247450A1,A computer-implemented method and a system for creating a three-dimensional mineral model of a sample of a heterogenous medium,"An initial 3D microstructural image of at least a part of a sample consisting of at least one mineral is obtained. Then, a mineral distribution image of at least one part of the sample is obtained so that each obtained mineral distribution image at least partially overlaps with the obtained initial 3D microstructural image and spatial registration with the obtained initial 3D microstructural image is provided in overlapping regions. Then at least one local feature in each point of the obtained initial 3D microstructural image is extracted by a computing system. A correspondence is found between the extracted local features in each point of the overlapping regions in the obtained initial 3D microstructural image and the minerals in the corresponding points in the overlapping regions in the obtained mineral distribution images. The extracted local features in each point of the obtained initial 3D microstructural image and the found correspondence are used for segmenting the obtained initial 3D microstructural image. A 3D mineral model of the sample is created from the segmented initial 3D microstructural image.","['G06T17/10', 'G06T15/08', 'G06T17/00', 'G06T7/33', 'G06T7/564', 'G06T7/60', 'G01N23/046', 'G06T2207/10061', 'G06T2207/10081', 'G06T2207/10121']"
CN109801267B,Inspection target defect detection method based on feature point detection and SVM classifier,"The invention relates to a method for detecting a patrol target defect based on feature point detection and an SVM classifier, which comprises the following steps: acquiring a real-time photographed inspection target image, and carrying out gray value transformation on the inspection target image; processing the converted image by using a maximum inter-class variance method and a SUSAN edge detection method, and determining a candidate region of a target to be detected; screening and fusing the candidate areas according to the shape characteristics of the object to be detected; and extracting LBP and LPQ characteristics of the processed candidate region, fusing the LBP and LPQ characteristics, and inputting the LBP and LPQ characteristics into a pre-trained SVM classifier for classification and identification. Compared with the traditional template matching method, the detection method does not need to carry out complex registration work on the two images in the early stage, and meanwhile, the method adopting the combination of LBP and LPQ features can describe the target features more accurately, so that the accuracy of defect identification results is improved; the method is simple to operate, has higher environmental adaptability, and can meet the identification requirement on the integrity of the parts of the oil extraction equipment.",['Y02P90/30']
CN111091166B,"Image processing model training method, image processing device, and storage medium","The application discloses an image processing model training method, an image processing device and a storage medium. The method comprises the following steps: acquiring a first sample image, a second sample image, a first segmentation label, a second segmentation label and an image processing model; coding the first sample image and the concatenation feature map of the first segmentation label through a first coding processing model to obtain a first target feature map; coding the second sample image through a second coding processing model to obtain a second target characteristic diagram; performing correlation processing on the first target characteristic diagram and the second target characteristic diagram through a correlation processing model to obtain a third target characteristic diagram; decoding the second target characteristic diagram and the third target characteristic diagram through a decoding processing model to obtain a segmentation result; determining a loss function; and training by using a loss function to obtain a target image processing model. The image processing model obtained by training in the mode has a wide application range and a good image processing effect.","['G06F18/214', 'G06F18/25', 'G06T9/00']"
CN112131350B,"Text label determining method, device, terminal and readable storage medium","The application relates to a text label determining method, a text label determining device, a text label determining terminal and a readable storage medium, and belongs to the field of label mining. The method comprises the following steps: performing word segmentation processing on a target text to obtain a word segmentation set, wherein the word segmentation set comprises word segmentation words obtained by word segmentation of the target text, and the target text is a text of a label to be determined; determining a first candidate tag of the target text according to the context relation of the word segmentation vocabulary; determining a second candidate tag of the target text according to a first frequency parameter of the word segmentation vocabulary in the target text and a second frequency parameter of the word segmentation vocabulary in a text set; and determining the label of the target text according to the first candidate label and the second candidate label. The method solves the problem of low accuracy of label determination caused by the fact that the context semantic environment is not considered in the label determination process, and improves the accuracy of label acquisition.","['G06F16/3331', 'G06F16/7834', 'G06F16/7844', 'G06F16/9535', 'G06F40/289', 'G06F40/30']"
CN103347163B,Ultra high definition video image processing and transmitting system and method thereof,"The invention discloses an ultra high definition video image processing and transmitting system which comprises an input module, a video processing unit and a display unit, wherein the input module receives ultra high definition digital video images, an output end of the input module is connected with an input end of the video processing unit, and an output end of the video processing unit is connected with an input end of the display unit; a controller and a time sequence control module which are connected to each other are arranged in the display unit. According to the ultra high definition video image processing and transmitting system, segmentation and compressing processes are carried out on image frames of the ultra high definition video images after decoding, merge application is carried out on the image frames in the display unit through asynchronous transmission, then the image frames are further amplified to be restored to original signals, and transmission and playing displaying for the ultra high definition signals are achieved. According to the processing method, the transmission data bandwidth can be compressed, the transmission bandwidth is further reduced, 4K2K ultra high definition video image transmission is achieved due to the fact that data are transmitted through an FHD interface, and the effect of transmission of a large number of ultra high definition video image data is achieved.","['H04N21/440245', 'H04N21/43635']"
CN113569088B,Music recommendation method and device and readable storage medium,"The application discloses a music recommendation method, a device and a readable storage medium, wherein the method comprises the following steps: acquiring a video segment in a target video associated with a target object, performing audio and video analysis on the video segment to obtain audio and video characteristics corresponding to the video segment, and determining a video attribute tag of the video segment based on the audio and video characteristics; acquiring object data, acquiring video information and music information associated with a target object from the object data, and determining an object attribute label of the target object based on historical comprehensive characteristic labels associated with the video information and the music information; and generating a target comprehensive characteristic label based on the video attribute label and the object attribute label, acquiring target music matched with the target comprehensive characteristic label from a music recommendation database, and pushing the target music to a target object as first matching music of the video clip. By the aid of the method and the device, the precision of video music matching can be improved.","['G06F16/735', 'G06F16/7834', 'G06F16/7844', 'G10L25/57', 'H04N21/8113']"
EP4067821A1,Path planning method for vehicle and path planning apparatus for vehicle,"This application provides a vehicle path planning method and a path planning apparatus. The path planning method includes: detecting a first operation performed by a user to indicate to enable automatic parking (610); displaying location information of a candidate parking spot on a display in response to the first operation (620); detecting a second operation performed by the user to indicate a target parking spot in the candidate parking spot (630); and displaying a target traveling path on the display in response to the second operation, where the target traveling path is used by a vehicle to travel from an initial parking pose indicating automatic parking enabling to a target parking pose in the target parking spot, the target traveling path is obtained by performing segmented planning on a path between the initial parking pose and the target parking pose based on an intermediate pose of the vehicle, and when the vehicle is in the intermediate pose, at least a part of the vehicle is located outside the target parking spot (640). Segmented planning is performed on a parking path of the vehicle through the intermediate pose, so that a success rate of path planning of the vehicle is improved.","['B62D15/0285', 'B60W30/06', 'B60W60/001', 'G01C21/165', 'G01C21/20', 'G01C21/343', 'G01C21/3626', 'B60W2520/10', 'B60W2540/18', 'B60W2552/50']"
US20220122001A1,Imitation training using synthetic data,"Approaches presented herein provide for the generation of synthetic data to fortify a dataset for use in training a network via imitation learning. In at least one embodiment, a system is evaluated to identify failure cases, such as may correspond to false positives and false negative detections. Additional synthetic data imitating these failure cases can then be generated and utilized to provide a more abundant dataset. A network or model can then be trained, or retrained, with the original training data and the additional synthetic data. In one or more embodiments, these steps may be repeated until the evaluation metric converges, with additional synthetic training data being generated corresponding to the failure cases at each training pass.","['G06N20/20', 'G06N3/084', 'A63F13/50', 'G06F18/214', 'G06N20/00', 'G06N3/045', 'G06N3/0454']"
CN112101410B,A method and system for image pixel semantic segmentation based on multimodal feature fusion,"The invention provides an image pixel semantic segmentation method based on multi-modal feature fusion, which comprises the steps of respectively carrying out image enhancement processing on multi-modal image data; extracting pixel-level RGB images and depth images based on the multi-modal image data after enhancement processing and a pre-constructed depth neural network, and performing pixel-level feature fusion on the pixel-level RGB images and the depth images in the homomorphic images to obtain fused hierarchical features; performing multi-modal pixel-level feature fusion on the fused hierarchical features by adopting a multi-modal adaptive mechanism to obtain pixel-level multi-modal fusion features; based on pixel-level multi-modal fusion characteristics, a pre-trained classifier is adopted to obtain the probability that pixels are correctly classified, and the highest value of the probability that pixels are correctly classified is selected to perform image pixel semantic segmentation; the invention better performs the fusion of complementary features among multiple modes, improves the segmentation performance of the model on the boundary object and accelerates the convergence of the network.","['G06F18/253', 'G06F18/2415', 'G06T5/00', 'G06T7/10', 'G06T7/55', 'G06T2207/10024', 'G06T2207/10028', 'G06T2207/20016', 'G06T2207/20081', 'G06T2207/20084']"
US20210209421A1,"Method and apparatus for constructing quality evaluation model, device and storage medium","Embodiments of the present disclosure disclose a method and apparatus for constructing a quality evaluation model, an electronic device and a computer-readable storage medium. A specific implementation mode of the method comprises: acquiring samples of knowledge contents; extracting statistical features, semantic features, and image features respectively from the samples of knowledge contents; and constructing a quality evaluation model for knowledge according to the statistical features, the semantic features, and the image features. On the basis of the prior art, this implementation mode additionally uses semantic features and image features of knowledge contents to construct a more accurate quality evaluation model based on multi-dimensional features that characterize the actual quality of a knowledge, which may well discover some brief but very useful summary knowledge in an enterprise and may recommend high-quality knowledge more accurately for employees in the enterprise.","['G06F16/901', 'G06F16/335', 'G06K9/6262', 'G06F16/2458', 'G06F16/36', 'G06F16/583', 'G06F16/906', 'G06F18/213', 'G06F18/217', 'G06F18/253', 'G06F40/205', 'G06F40/268', 'G06F40/30', 'G06K9/00456', 'G06K9/629', 'G06N3/0442', 'G06N3/045', 'G06N3/049', 'G06N3/08', 'G06T7/0002', 'G06V10/82', 'G06V30/18057', 'G06V30/19173', 'G06V30/40', 'G06V30/413', 'G06F18/2132', 'G06K9/6234', 'G06T2207/30168', 'G06T2207/30176', 'G06V30/10', 'Y02P90/30']"
US8463044B2,Method and device of detecting object in image and system including the device,"The present invention provides a method for detecting a specific object in an image to be detected, including: a feature extraction step for extracting an image feature of the image to be detected; and a detection step for detecting detection windows with various sizes of the image to be detected according to the extracted image feature by using classifiers with various sizes corresponding to at least a part of the detection windows with various sizes, so as to determine the presence and location of a specific object in the image to be detected. The invention further provides an object detection device and a system including the device. The method, device and system for detecting a specific object in an image to be detected can improve the precision and increase the speed of the object detection.","['G06V10/26', 'G06V10/50', 'G06V30/2504', 'G06V40/103']"
US11386666B2,Inter-trajectory anomaly detection using adaptive voting experts in a video surveillance system,"A sequence layer in a machine-learning engine configured to learn from the observations of a computer vision engine. In one embodiment, the machine-learning engine uses the voting experts to segment adaptive resonance theory (ART) network label sequences for different objects observed in a scene. The sequence layer may be configured to observe the ART label sequences and incrementally build, update, and trim, and reorganize an ngram trie for those label sequences. The sequence layer computes the entropies for the nodes in the ngram trie and determines a sliding window length and vote count parameters. Once determined, the sequence layer may segment newly observed sequences to estimate the primitive events observed in the scene as well as issue alerts for inter-sequence and intra-sequence anomalies.","['G06T7/248', 'G06F18/23211', 'G06F18/29', 'G06K9/6222', 'G06K9/6296', 'G06V10/763', 'G06V10/84', 'G06V20/52', 'G06T2207/10016', 'G06T2207/30196', 'G06T2207/30232', 'G06T2207/30236', 'G06T2207/30241']"
US11613016B2,"Systems, apparatuses, and methods for rapid machine learning for floor segmentation for robotic devices","Systems, apparatuses, and methods for rapid machine learning for floor segmentation for robotic devices are disclosed herein. According to at least one non-limiting exemplary embodiment, a robotic system is disclosed. The robotic system may comprise a neural network embodied therein capable of learning associations between color values of pixels and corresponding classifications of those pixels, wherein neural network is trained initially to identify floor and non-floor pixels within images. A user input may be provided to the neural network to further configure the neural network to be able to identify navigable floors and unnavigable floors unique to an environment without a need for additional annotated training images specific to the environment.","['G05D1/6485', 'B25J9/1666', 'G05D1/0221', 'G06F18/2178', 'G06F18/24', 'G06K9/6263', 'G06K9/6267', 'G06N3/0464', 'G06N3/08', 'G06N3/09', 'G06T7/10', 'G06T7/90', 'G06V10/56', 'G06V10/764', 'G06V10/7784', 'G06V10/82', 'G06V20/10', 'G05D1/0246', 'G05D2105/10', 'G05D2107/63', 'G05D2109/10', 'G05D2201/0203', 'G05D2201/0215', 'G06F18/24133', 'G06K9/6271']"
CN109454006B,Detection and classification method based on device for online detection and classification of chemical fiber spindle tripping defects,"The invention discloses a device for detecting and grading the stumbling defect of a chemical fiber spindle on line, which comprises a spindle conveyer, an image acquisition device and a defect eliminating module which are arranged on the spindle conveyer, and a good product waiting area and a defective product waiting area which are connected with the spindle conveyer; the defect eliminating module comprises an image processing and decision-making module, a quality statistics evaluation module and a defect control module; the image acquisition device comprises closed black boxes arranged above and below the filament ingot conveying device, and an LED light source group, a CCD camera group, an image acquisition card, a digital signal processor and a memory which are arranged in each closed black box. The invention has the following beneficial effects: according to the invention, the collected filament image is processed by combining machine vision and image processing technologies, the number and morphological characteristics of tripwire and interference filament can be detected simultaneously, the detected tripwire information can be used for grading the filament with tripwire defect, and the counted interference information is used for tracing the source.","['B07C5/02', 'B07C5/34', 'B07C5/362', 'G06T7/0004', 'G06T7/11', 'G06T7/136', 'G06T7/62', 'G06T2207/30168']"
US10140544B1,Enhanced convolutional neural network for image segmentation,"This disclosure relates to digital image segmentation and region of interest identification. A computer implemented image segmentation method and system are particularly disclosed, including a predictive model trained based on a deep fully convolutional neural network. The model is trained using a loss function in at least one intermediate layer in addition to a loss function at the final stage of the full convolutional neural network. The predictive segmentation model trained in such a manner requires less training parameters and facilitates quicker and more accurate identification of relevant local and global features in the input image. In one implementation, the fully convolutional neural network is further supplemented with a conditional adversarial neural networks iteratively trained with the fully convolutional neural network as a discriminator measuring the quality of the predictive model generated by the fully convolutional neural network.","['G06K9/4628', 'G06N3/084', 'G06F15/18', 'G06K9/66', 'G06N20/00', 'G06N3/045', 'G06N3/0454', 'G06N3/0464', 'G06N3/09', 'G06N3/094', 'G06N5/046', 'G06T7/11', 'G06T7/143', 'G06V10/25', 'G06V10/454', 'G06V10/82', 'G06T2207/10081', 'G06T2207/10088', 'G06T2207/30061']"
US10304193B1,Image segmentation and object detection using fully convolutional neural network,"This disclosure relates to digital image segmentation, region of interest identification, and object recognition. This disclosure describes a method, a system, for image segmentation based on fully convolutional neural network including an expansion neural network and contraction neural network. The various convolutional and deconvolution layers of the neural networks are architected to include a coarse-to-fine residual learning module and learning paths, as well as a dense convolution module to extract auto context features and to facilitate fast, efficient, and accurate training of the neural networks capable of producing prediction masks of regions of interest. While the disclosed method and system are applicable for general image segmentation and object detection/identification, they are particularly suitable for organ, tissue, and lesion segmentation and detection in medical images.","['G06T7/11', 'G06F18/2148', 'G06F18/217', 'G06K9/6232', 'G06K9/6257', 'G06K9/6262', 'G06N3/045', 'G06N3/0455', 'G06N3/0464', 'G06N3/048', 'G06N3/084', 'G06N3/09', 'G06T7/0012', 'G06V10/764', 'G06V10/82', 'G16H30/20', 'G16H30/40', 'G16H50/20', 'G06K2209/05', 'G06T2207/10088', 'G06T2207/20081', 'G06T2207/20084', 'G06T2207/30081', 'G06T2207/30096', 'G06V2201/03', 'G06V2201/031']"
US10849693B2,Systems for augmented reality guidance for bone resections including robotics,Devices and methods for performing a surgical step or surgical procedure with visual guidance using an optical head mounted display are disclosed.,"['A61B34/10', 'A61B17/155', 'A61B17/157', 'A61B17/1703', 'A61B17/1742', 'A61B17/1764', 'A61B17/1775', 'A61B17/1778', 'A61B34/74', 'A61B90/36', 'A61B90/37', 'A61F2/32', 'A61F2/3859', 'A61F2/389', 'H05K999/99', 'A61B17/15', 'A61B17/1666', 'A61B17/17', 'A61B2017/00216', 'A61B2017/568', 'A61B2034/104', 'A61B2034/107', 'A61B2034/2048', 'A61B2034/2055', 'A61B2034/2059', 'A61B2090/365', 'A61B2090/368', 'A61B2090/372', 'A61B2090/373', 'A61B2090/374', 'A61B2090/376', 'A61B2090/3762', 'A61B2090/502', 'A61F2002/4018', 'A61F2002/4205', 'A61F2002/4207', 'Y02A90/10']"
